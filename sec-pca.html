<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Principal Component Analysis</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" Randall Pruim ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math",
    "renderActions": {
      "findScript": [
        10,
        function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        },
        ""
      ]
    }
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-sage",
  "linked": true,
  "linkKey": "linked-sage",
  "autoeval": false,
  "languages": [
    "sage"
  ],
  "evalButtonText": "Evaluate (Sage)"
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.3/pretext_search.js"></script><link href="https://pretextbook.org/css/0.7/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.3</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.3/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.3/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.3/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link href="https://pretextbook.org/css/0.7/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra:</span> <span class="subtitle">Data Science Edition</span></a></h1>
<p class="byline">Randall Pruim</p>
</div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" class="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms" class="searchterms"></span>
</h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" aria-label="Show or hide table of contents"><span class="icon">‚ò∞</span><span class="name">Contents</span></button><a class="index-button button" href="index-1.html" title="Index"><span class="name">Index</span></a><button id="user-preferences-button" class="user-preferences-button button" title="Modify user preferences"><span id="avatarbutton" class="avatarbutton name">You!</span><div id="preferences_menu_holder" class="preferences_menu_holder hidden"><ol id="preferences_menu" class="preferences_menu" style="font-family: 'Roboto Serif', serif;">
<li data-env="avatar" tabindex="-1">Choose avatar<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden avatar">
<li data-val="You!" tabindex="-1">
<span id="theYou!" class="avatarcheck">‚úîÔ∏è</span>You!</li>
<li data-val="üò∫" tabindex="-1">
<span id="theüò∫" class="avatarcheck"></span>üò∫</li>
<li data-val="üë§" tabindex="-1">
<span id="theüë§" class="avatarcheck"></span>üë§</li>
<li data-val="üëΩ" tabindex="-1">
<span id="theüëΩ" class="avatarcheck"></span>üëΩ</li>
<li data-val="üê∂" tabindex="-1">
<span id="theüê∂" class="avatarcheck"></span>üê∂</li>
<li data-val="üêº" tabindex="-1">
<span id="theüêº" class="avatarcheck"></span>üêº</li>
<li data-val="üåà" tabindex="-1">
<span id="theüåà" class="avatarcheck"></span>üåà</li>
</ol>
</li>
<li data-env="fontfamily" tabindex="-1">Font family<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fontfamily">
<li data-val="face" data-change="OS" tabindex="-1" style="font-family: 'Open Sans'">
<span id="theOS" class="ffcheck">‚úîÔ∏è</span><span class="name">Open Sans</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
<li data-val="face" data-change="RS" tabindex="-1" style="font-family: 'Roboto Serif'">
<span id="theRS" class="ffcheck"></span><span class="name">Roboto Serif</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
</ol>
</li>
<li data-env="font" tabindex="-1">Adjust font<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fonts">
<li>Size</li>
<li><span id="thesize">12</span></li>
<li data-val="size" data-change="-1" tabindex="-1" style="font-size: 80%">Smaller</li>
<li data-val="size" data-change="1" tabindex="-1" style="font-size: 110%">Larger</li>
<li>Width</li>
<li><span id="thewdth">100</span></li>
<li data-val="wdth" data-change="-5" tabindex="-1" style="font-variation-settings: 'wdth' 60">narrower</li>
<li data-val="wdth" data-change="5" tabindex="-1" style="font-variation-settings: 'wdth' 150">wider</li>
<li>Weight</li>
<li><span id="thewght">400</span></li>
<li data-val="wght" data-change="-50" tabindex="-1" style="font-weight: 200">thinner</li>
<li data-val="wght" data-change="50" tabindex="-1" style="font-weight: 700">heavier</li>
<li>Letter spacing</li>
<li>
<span id="thelspace">0</span><span class="byunits">/200</span>
</li>
<li data-val="lspace" data-change="-1" tabindex="-1">closer</li>
<li data-val="lspace" data-change="1" tabindex="-1">f a r t h e r</li>
<li>Word spacing</li>
<li>
<span id="thewspace">0</span><span class="byunits">/50</span>
</li>
<li data-val="wspace" data-change="-1" tabindex="-1">smaller‚ÄÖgap‚ÄÉ</li>
<li data-val="wspace" data-change="1" tabindex="-1">larger‚ÄÉgap</li>
<li>Line Spacing</li>
<li>
<span id="theheight">135</span><span class="byunits">/100</span>
</li>
<li data-val="height" data-change="-5" tabindex="-1" style="line-height: 1">closer<br>together</li>
<li data-val="height" data-change="5" tabindex="-1" style="line-height: 1.75">further<br>apart</li>
</ol>
</li>
<li data-env="atmosphere" tabindex="-1">Light/dark mode<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden atmosphere">
<li data-val="default" tabindex="-1">
<span id="thedefault" class="atmospherecheck">‚úîÔ∏è</span>default</li>
<li data-val="pastel" tabindex="-1">
<span id="thepastel" class="atmospherecheck"></span>pastel</li>
<li data-val="darktwilight" tabindex="-1">
<span id="thedarktwilight" class="atmospherecheck"></span>twilight</li>
<li data-val="dark" tabindex="-1">
<span id="thedark" class="atmospherecheck"></span>dark</li>
<li data-val="darkmidnight" tabindex="-1">
<span id="thedarkmidnight" class="atmospherecheck"></span>midnight</li>
</ol>
</li>
<li data-env="ruler" tabindex="-1">Reading ruler<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden ruler">
<li data-val="none" tabindex="-1">
<span id="thenone" class="rulercheck">‚úîÔ∏è</span>none</li>
<li data-val="underline" tabindex="-1">
<span id="theunderline" class="rulercheck"></span>underline</li>
<li data-val="lunderline" tabindex="-1">
<span id="thelunderline" class="rulercheck"></span>L-underline</li>
<li data-val="greybar" tabindex="-1">
<span id="thegreybar" class="rulercheck"></span>grey bar</li>
<li data-val="lightbox" tabindex="-1">
<span id="thelightbox" class="rulercheck"></span>light box</li>
<li data-val="sunrise" tabindex="-1">
<span id="thesunrise" class="rulercheck"></span>sunrise</li>
<li data-val="sunriseunderline" tabindex="-1">
<span id="thesunriseunderline" class="rulercheck"></span>sunrise underline</li>
<li class="moveQ">Motion by:</li>
<li data-val="mouse" tabindex="-1">
<span id="themouse" class="motioncheck">‚úîÔ∏è</span>follow the mouse</li>
<li data-val="arrow" tabindex="-1">
<span id="thearrow" class="motioncheck"></span>up/down arrows - not yet</li>
<li data-val="eye" tabindex="-1">
<span id="theeye" class="motioncheck"></span>eye tracking - not yet</li>
</ol>
</li>
</ol></div></button><span class="treebuttons"><a class="previous-button button" href="sec-quadratic-forms.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="up-button button" href="chap7.html" title="Up"><span class="icon">^</span><span class="name">Up</span></a><a class="next-button button" href="sec-svd-intro.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a></span><div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" class="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\boldsymbol a}}
\newcommand{\bvec}{{\boldsymbol b}}
\newcommand{\cvec}{{\boldsymbol c}}
\newcommand{\dvec}{{\boldsymbol d}}
\newcommand{\dtil}{\widetilde{\boldsymbol d}}
\newcommand{\evec}{{\boldsymbol e}}
\newcommand{\fvec}{{\boldsymbol f}}
\newcommand{\mvec}{{\boldsymbol m}}
\newcommand{\nvec}{{\boldsymbol n}}
\newcommand{\pvec}{{\boldsymbol p}}
\newcommand{\qvec}{{\boldsymbol q}}
\newcommand{\rvec}{{\boldsymbol r}}
\newcommand{\svec}{{\boldsymbol s}}
\newcommand{\tvec}{{\boldsymbol t}}
\newcommand{\uvec}{{\boldsymbol u}}
\newcommand{\vvec}{{\boldsymbol v}}
\newcommand{\wvec}{{\boldsymbol w}}
\newcommand{\xvec}{{\boldsymbol x}}
\newcommand{\yvec}{{\boldsymbol y}}
\newcommand{\zvec}{{\boldsymbol z}}
\newcommand{\betavec}{{\boldsymbol \beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\zerovec}{{\boldsymbol 0}}
\newcommand{\onevec}{{\boldsymbol 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\xmean}{\overline{\xvec}}
\newcommand{\yhat}{\widehat{\yvec}}
\newcommand{\ymean}{\overline{\yvec}}
\newcommand{\betahat}{\widehat{\betavec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\by}{\times}
\newcommand{\transpose}{\top}
\newcommand{\proj}[2]{\operatorname{proj}\left(#1 \to #2\right)}
\newcommand{\projsub}[2]{\operatorname{proj}_{#2}(#1)}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural">
<li>
<div class="toc-item"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="colophon-1.html" class="internal"><span class="title">Colophon</span></a></div></li>
<li><div class="toc-item"><a href="preface-1.html" class="internal"><span class="title">Our goals -- Preface to David Austin‚Äôs original edition</span></a></div></li>
<li><div class="toc-item"><a href="preface-2.html" class="internal"><span class="title">What‚Äôs different in the data science edition?</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap1.html" class="internal"><span class="codenumber">1</span> <span class="title">Scalars, Vectors and Matrices</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-vectors.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsection-1" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Three ways to think about vectors</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-vectors.html#subsection-2" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Vector operations: scalar multiplication and vector addition.</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-scalar-multiplication" class="internal"><span class="codenumber">1.1.2.1</span> <span class="title">Scalar Multiplication</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-addition" class="internal"><span class="codenumber">1.1.2.2</span> <span class="title">Vector addition</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-properties" class="internal"><span class="codenumber">1.1.2.3</span> <span class="title">Mathematical properties of vector operations</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-3" class="internal"><span class="codenumber">1.1.3</span> <span class="title">The (Euclidean) length of a vector</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-4" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Summary</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-vectors-in-python.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Vectors in Python</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-intro-to-python" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Introduction to Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-numpy-vectors" class="internal"><span class="codenumber">1.2.2</span> <span class="title"><code class="code-inline tex2jax_ignore">numpy</code> vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-vector-length-numpy" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Vector length</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsection-8" class="internal"><span class="codenumber">1.2.4</span> <span class="title">Plotting vectors</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-combos-of-vectors.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Linear combinations of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#subsection-9" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#exercises-1" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-matrices.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrices.html#subsec-matrices-and-their-uses" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Matrices and their uses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-11" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Scalar multiplication and addition of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-12" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Matrix-vector multiplication and linear combinations</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-13" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Matrix-vector multiplication and linear systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#sec-matrices-in-python" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Matrices in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-15" class="internal"><span class="codenumber">1.4.6</span> <span class="title">Matrix-matrix products</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsec-special-matrices" class="internal"><span class="codenumber">1.4.7</span> <span class="title">Some special types of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-17" class="internal"><span class="codenumber">1.4.8</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#exercises-2" class="internal"><span class="codenumber">1.4.9</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-tensors.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Tensors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-tensors.html#subsec-numpy-tensors" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Tensors in NumPy</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-axes" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Aggregation and Axes</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-ndarray-append" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expanding an array</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-broadcasting" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Broadcasting</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#exercises-1-5" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap2.html" class="internal"><span class="codenumber">2</span> <span class="title">Systems of equations: Solving <span class="process-math">\(A \xvec = \bvec\)</span></span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-expect.html" class="internal"><span class="codenumber">2.1</span> <span class="title">What can we expect</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-expect.html#subsection-22" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Some simple examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-23" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Systems of linear equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-24" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#exercises-4" class="internal"><span class="codenumber">2.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-finding-solutions.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Finding solutions to linear systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-25" class="internal"><span class="codenumber">2.2.1</span> <span class="title">Gaussian elimination</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-26" class="internal"><span class="codenumber">2.2.2</span> <span class="title">Augmented matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-27" class="internal"><span class="codenumber">2.2.3</span> <span class="title">Reduced row echelon form</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsec-solving-matrix-equations" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Solving matrix equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-29" class="internal"><span class="codenumber">2.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#exercises-5" class="internal"><span class="codenumber">2.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-python-introduction.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Computational Linear Algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-30" class="internal"><span class="codenumber">2.3.1</span> <span class="title">Reduced row echelon form in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-linalg-solve" class="internal"><span class="codenumber">2.3.2</span> <span class="title">np.linalg.solve()</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-compute-effort" class="internal"><span class="codenumber">2.3.3</span> <span class="title">Computational effort</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-33" class="internal"><span class="codenumber">2.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-6" class="internal"><span class="codenumber">2.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pivots.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Pivots and their relationship to solution spaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pivots.html#subsection-34" class="internal"><span class="codenumber">2.4.1</span> <span class="title">The existence of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-35" class="internal"><span class="codenumber">2.4.2</span> <span class="title">The uniqueness of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-36" class="internal"><span class="codenumber">2.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#exercises-7" class="internal"><span class="codenumber">2.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap3.html" class="internal"><span class="codenumber">3</span> <span class="title">Linear combinations and transformations</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-span.html" class="internal"><span class="codenumber">3.1</span> <span class="title">The span of a set of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-span.html#subsection-37" class="internal"><span class="codenumber">3.1.1</span> <span class="title">The span of a set of vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-38" class="internal"><span class="codenumber">3.1.2</span> <span class="title">Pivot positions and span</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsec-span-and-linear-models" class="internal"><span class="codenumber">3.1.3</span> <span class="title">Span and linear models</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-40" class="internal"><span class="codenumber">3.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#exercises-8" class="internal"><span class="codenumber">3.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-dep.html" class="internal"><span class="codenumber">3.2</span> <span class="title">Linear independence</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-41" class="internal"><span class="codenumber">3.2.1</span> <span class="title">Linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-42" class="internal"><span class="codenumber">3.2.2</span> <span class="title">How to recognize linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-43" class="internal"><span class="codenumber">3.2.3</span> <span class="title">Homogeneous equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-44" class="internal"><span class="codenumber">3.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#exercises-9" class="internal"><span class="codenumber">3.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-trans.html" class="internal"><span class="codenumber">3.3</span> <span class="title">Matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-matrix-trans" class="internal"><span class="codenumber">3.3.1</span> <span class="title">Matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-linear-trans" class="internal"><span class="codenumber">3.3.2</span> <span class="title">Linear transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-47" class="internal"><span class="codenumber">3.3.3</span> <span class="title">Composing matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-dynamical-systems" class="internal"><span class="codenumber">3.3.4</span> <span class="title">Discrete Dynamical Systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-49" class="internal"><span class="codenumber">3.3.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#exercises-10" class="internal"><span class="codenumber">3.3.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transforms-geom.html" class="internal"><span class="codenumber">3.4</span> <span class="title">The geometry of matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-50" class="internal"><span class="codenumber">3.4.1</span> <span class="title">The geometry of <span class="process-math">\(2\by2\)</span> matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-51" class="internal"><span class="codenumber">3.4.2</span> <span class="title">Matrix transformations and computer animation</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-52" class="internal"><span class="codenumber">3.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#exercises-11" class="internal"><span class="codenumber">3.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap4.html" class="internal"><span class="codenumber">4</span> <span class="title">Invertibility, bases, and coordinate systems</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-matrix-inverse.html" class="internal"><span class="codenumber">4.1</span> <span class="title">Invertibility</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-53" class="internal"><span class="codenumber">4.1.1</span> <span class="title">Invertible matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-54" class="internal"><span class="codenumber">4.1.2</span> <span class="title">Solving equations with an inverse</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-55" class="internal"><span class="codenumber">4.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#exercises-12" class="internal"><span class="codenumber">4.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="subsec-triangular-invertible.html" class="internal"><span class="codenumber">4.2</span> <span class="title">Triangular matrices and Gaussian elimination</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-triangular-matrices" class="internal"><span class="codenumber">4.2.1</span> <span class="title">Triangular matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-elementary-matrices" class="internal"><span class="codenumber">4.2.2</span> <span class="title">Elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsection-58" class="internal"><span class="codenumber">4.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#exercises-13" class="internal"><span class="codenumber">4.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-bases.html" class="internal"><span class="codenumber">4.3</span> <span class="title">Bases and coordinate systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-bases.html#subsection-59" class="internal"><span class="codenumber">4.3.1</span> <span class="title">Bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-60" class="internal"><span class="codenumber">4.3.2</span> <span class="title">Coordinate systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-61" class="internal"><span class="codenumber">4.3.3</span> <span class="title">Examples of bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-62" class="internal"><span class="codenumber">4.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#exercises-14" class="internal"><span class="codenumber">4.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-jpeg.html" class="internal"><span class="codenumber">4.4</span> <span class="title">Image compression</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-63" class="internal"><span class="codenumber">4.4.1</span> <span class="title">Color models</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-64" class="internal"><span class="codenumber">4.4.2</span> <span class="title">The JPEG compression algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-65" class="internal"><span class="codenumber">4.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#exercises-15" class="internal"><span class="codenumber">4.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-determinants.html" class="internal"><span class="codenumber">4.5</span> <span class="title">Determinants</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsection-66" class="internal"><span class="codenumber">4.5.1</span> <span class="title">Determinants of <span class="process-math">\(2\by2\)</span> matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-67" class="internal"><span class="codenumber">4.5.2</span> <span class="title">Determinants and invertibility</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-68" class="internal"><span class="codenumber">4.5.3</span> <span class="title">Cofactor expansions</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-69" class="internal"><span class="codenumber">4.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#exercises-16" class="internal"><span class="codenumber">4.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-subspaces.html" class="internal"><span class="codenumber">4.6</span> <span class="title">Subspaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-70" class="internal"><span class="codenumber">4.6.1</span> <span class="title">Subspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-71" class="internal"><span class="codenumber">4.6.2</span> <span class="title">The column space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-72" class="internal"><span class="codenumber">4.6.3</span> <span class="title">The null space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-73" class="internal"><span class="codenumber">4.6.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#exercises-17" class="internal"><span class="codenumber">4.6.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gaussian-revisited.html" class="internal"><span class="codenumber">4.7</span> <span class="title">Partial pivoting and LU factorizations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal"><span class="codenumber">4.7.1</span> <span class="title">Partial pivoting</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-75" class="internal"><span class="codenumber">4.7.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-76" class="internal"><span class="codenumber">4.7.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#exercises-18" class="internal"><span class="codenumber">4.7.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap5.html" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-eigen-intro.html" class="internal"><span class="codenumber">5.1</span> <span class="title">An introduction to eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-77" class="internal"><span class="codenumber">5.1.1</span> <span class="title">A few examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsec-eigen-use" class="internal"><span class="codenumber">5.1.2</span> <span class="title">The usefulness of eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-79" class="internal"><span class="codenumber">5.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#exercises-19" class="internal"><span class="codenumber">5.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-find.html" class="internal"><span class="codenumber">5.2</span> <span class="title">Finding eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-80" class="internal"><span class="codenumber">5.2.1</span> <span class="title">The characteristic polynomial</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-81" class="internal"><span class="codenumber">5.2.2</span> <span class="title">Finding eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-82" class="internal"><span class="codenumber">5.2.3</span> <span class="title">The characteristic polynomial and the dimension of eigenspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-83" class="internal"><span class="codenumber">5.2.4</span> <span class="title">Using Python to find eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-84" class="internal"><span class="codenumber">5.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#exercises-20" class="internal"><span class="codenumber">5.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-diag.html" class="internal"><span class="codenumber">5.3</span> <span class="title">Diagonalization, similarity, and powers of a matrix</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-85" class="internal"><span class="codenumber">5.3.1</span> <span class="title">Diagonalization of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-86" class="internal"><span class="codenumber">5.3.2</span> <span class="title">Powers of a diagonalizable matrix</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-87" class="internal"><span class="codenumber">5.3.3</span> <span class="title">Similarity and complex eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-88" class="internal"><span class="codenumber">5.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#exercises-21" class="internal"><span class="codenumber">5.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-dynamical.html" class="internal"><span class="codenumber">5.4</span> <span class="title">Dynamical systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-89" class="internal"><span class="codenumber">5.4.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-90" class="internal"><span class="codenumber">5.4.2</span> <span class="title">Classifying dynamical systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-91" class="internal"><span class="codenumber">5.4.3</span> <span class="title">A <span class="process-math">\(3\by3\)</span> system</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-92" class="internal"><span class="codenumber">5.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#exercises-22" class="internal"><span class="codenumber">5.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-stochastic.html" class="internal"><span class="codenumber">5.5</span> <span class="title">Markov chains and Google‚Äôs PageRank algorithm</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-93" class="internal"><span class="codenumber">5.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-94" class="internal"><span class="codenumber">5.5.2</span> <span class="title">Markov chains</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsec-google" class="internal"><span class="codenumber">5.5.3</span> <span class="title">Google‚Äôs PageRank algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-96" class="internal"><span class="codenumber">5.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#exercises-23" class="internal"><span class="codenumber">5.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-power-method.html" class="internal"><span class="codenumber">5.6</span> <span class="title">Finding eigenvectors numerically</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-power-method.html#subsection-97" class="internal"><span class="codenumber">5.6.1</span> <span class="title">The power method</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-98" class="internal"><span class="codenumber">5.6.2</span> <span class="title">Finding other eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-99" class="internal"><span class="codenumber">5.6.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#exercises-24" class="internal"><span class="codenumber">5.6.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap6.html" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-dot-product.html" class="internal"><span class="codenumber">6.1</span> <span class="title">The dot product</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dot-product.html#sec-projections-and-dot-products" class="internal"><span class="codenumber">6.1.1</span> <span class="title">Projections and dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsec-computing-dot-products" class="internal"><span class="codenumber">6.1.2</span> <span class="title">Computing dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-102" class="internal"><span class="codenumber">6.1.3</span> <span class="title"><span class="process-math">\(k\)</span>-means clustering</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-103" class="internal"><span class="codenumber">6.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#exercises-25" class="internal"><span class="codenumber">6.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transpose.html" class="internal"><span class="codenumber">6.2</span> <span class="title">Orthogonal complements and the matrix transpose</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transpose.html#subsection-104" class="internal"><span class="codenumber">6.2.1</span> <span class="title">Orthogonal complements</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-105" class="internal"><span class="codenumber">6.2.2</span> <span class="title">The matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-106" class="internal"><span class="codenumber">6.2.3</span> <span class="title">Properties of the matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-107" class="internal"><span class="codenumber">6.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#exercises-26" class="internal"><span class="codenumber">6.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-orthogonal-bases.html" class="internal"><span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-108" class="internal"><span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-109" class="internal"><span class="codenumber">6.3.2</span> <span class="title">Orthogonal projections</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-110" class="internal"><span class="codenumber">6.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#exercises-27" class="internal"><span class="codenumber">6.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gram-schmidt.html" class="internal"><span class="codenumber">6.4</span> <span class="title">Finding orthogonal bases</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-111" class="internal"><span class="codenumber">6.4.1</span> <span class="title">Gram-Schmidt orthogonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-112" class="internal"><span class="codenumber">6.4.2</span> <span class="title"><span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-113" class="internal"><span class="codenumber">6.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#exercises-28" class="internal"><span class="codenumber">6.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-least-squares.html" class="internal"><span class="codenumber">6.5</span> <span class="title">Least squares methods</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-114" class="internal"><span class="codenumber">6.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-linear-model-framework" class="internal"><span class="codenumber">6.5.2</span> <span class="title">The linear model framework</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-116" class="internal"><span class="codenumber">6.5.3</span> <span class="title">Solving least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-117" class="internal"><span class="codenumber">6.5.4</span> <span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-118" class="internal"><span class="codenumber">6.5.5</span> <span class="title">Polynomial Regression</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-skl-lm" class="internal"><span class="codenumber">6.5.6</span> <span class="title">Fitting linear models with standard tools</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-120" class="internal"><span class="codenumber">6.5.7</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#exercises-29" class="internal"><span class="codenumber">6.5.8</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap7.html" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-symmetric-matrices.html" class="internal"><span class="codenumber">7.1</span> <span class="title">Symmetric matrices and variance</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-121" class="internal"><span class="codenumber">7.1.1</span> <span class="title">Symmetric matrices and orthogonal diagonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-122" class="internal"><span class="codenumber">7.1.2</span> <span class="title">Variance</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-123" class="internal"><span class="codenumber">7.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#exercises-30" class="internal"><span class="codenumber">7.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-quadratic-forms.html" class="internal"><span class="codenumber">7.2</span> <span class="title">Quadratic forms</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-124" class="internal"><span class="codenumber">7.2.1</span> <span class="title">Quadratic forms</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-125" class="internal"><span class="codenumber">7.2.2</span> <span class="title">Definite symmetric matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-126" class="internal"><span class="codenumber">7.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#exercises-31" class="internal"><span class="codenumber">7.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li class="active">
<div class="toc-item"><a href="sec-pca.html" class="internal"><span class="codenumber">7.3</span> <span class="title">Principal Component Analysis</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pca.html#subsection-127" class="internal"><span class="codenumber">7.3.1</span> <span class="title">Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-128" class="internal"><span class="codenumber">7.3.2</span> <span class="title">Using Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-129" class="internal"><span class="codenumber">7.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#exercises-32" class="internal"><span class="codenumber">7.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-intro.html" class="internal"><span class="codenumber">7.4</span> <span class="title">Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-130" class="internal"><span class="codenumber">7.4.1</span> <span class="title">Finding singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-131" class="internal"><span class="codenumber">7.4.2</span> <span class="title">The structure of singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-132" class="internal"><span class="codenumber">7.4.3</span> <span class="title">Reduced singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-133" class="internal"><span class="codenumber">7.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#exercises-33" class="internal"><span class="codenumber">7.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-uses.html" class="internal"><span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-134" class="internal"><span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-135" class="internal"><span class="codenumber">7.5.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-136" class="internal"><span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-137" class="internal"><span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-138" class="internal"><span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-139" class="internal"><span class="codenumber">7.5.6</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#exercises-34" class="internal"><span class="codenumber">7.5.7</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="backmatter.html" class="internal"><span class="title">Back Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="app-notation.html" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></div></li>
<li>
<div class="toc-item"><a href="app-python-reference.html" class="internal"><span class="codenumber">B</span> <span class="title">Python Reference</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsection-140.html" class="internal"><span class="codenumber">B.1</span> <span class="title">Accessing Python</span></a></div></li>
<li><div class="toc-item"><a href="subsection-141.html" class="internal"><span class="codenumber">B.2</span> <span class="title">Packages and libraries for data science</span></a></div></li>
<li><div class="toc-item"><a href="subsec-frequently-used-python.html" class="internal"><span class="codenumber">B.3</span> <span class="title">Frequently used Python commands</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="index-1.html" class="internal"><span class="title">Index</span></a></div></li>
<li><div class="toc-item"><a href="colophon-2.html" class="internal"><span class="title">Colophon</span></a></div></li>
</ul>
</li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content">
<section class="section" id="sec-pca"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">7.3</span><span class="space"> </span><span class="title">Principal Component Analysis</span>
</h2>
<section class="introduction" id="introduction-39"><div class="para" id="p-7893">We are sometimes presented with a dataset having many data points that live in a high dimensional space.  For instance, we looked at a dataset describing body fat index (BFI) in <a href="" class="xref" data-knowl="./knowl/activity-BFI.html" title="Activity 6.5.4">Activity¬†6.5.4</a> where each data point is six-dimensional.  Developing an intuitive understanding of the data is hampered by the fact that it cannot be visualized.</div> <div class="para" id="p-7894">This section explores a technique called <em class="emphasis">principal component analysis</em>, which enables us to reduce the dimension of a dataset so that it may be visualized or studied in a way so that interesting features more readily stand out. Our previous work with variance and the orthogonal diagonalization of symmetric matrices provides the key ideas.</div> <article class="exploration project-like" id="exploration-27"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">7.3.1</span><span class="period">.</span>
</h3>
<div class="para logical" id="p-7895">
<div class="para">We will begin by recalling our earlier discussion of variance.  Suppose we have a dataset that leads to the covariance matrix</div>
<div class="displaymath process-math">
\begin{equation*}
C = \begin{bmatrix}
7 \amp -4 \\
-4 \amp 13
\end{bmatrix}.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5355"><div class="para" id="p-7896">Suppose that <span class="process-math">\(\uvec\)</span> is a unit eigenvector of <span class="process-math">\(C\)</span> with eigenvalue <span class="process-math">\(\lambda\text{.}\)</span>  What is the variance <span class="process-math">\(V_{\uvec}\)</span> in the <span class="process-math">\(\uvec\)</span> direction?</div></li>
<li id="li-5356">
<div class="para" id="p-7897">Find an orthogonal diagonalization of <span class="process-math">\(C\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-257"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5357"><div class="para" id="p-7898">What is the total variance?</div></li>
<li id="li-5358"><div class="para" id="p-7899">In which direction is the variance greatest and what is the variance in this direction?  If we project the data onto this line, how much variance is lost?</div></li>
<li id="li-5359"><div class="para" id="p-7900">In which direction is the variance smallest and how is this direction related to the direction of maximum variance?</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-392" id="solution-392"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-392"><div class="solution solution-like"><div class="para logical" id="p-7901"><ol class="lower-alpha">
<li id="li-5360"><div class="para" id="p-7902"><span class="process-math">\(V_{\uvec} = \uvec\cdot(C\uvec) =
\lambda\uvec\cdot\uvec = \lambda\text{.}\)</span></div></li>
<li id="li-5361"><div class="para logical" id="p-7903">
<div class="para">We can write <span class="process-math">\(C=QDQ^{\transpose}\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
D=\begin{bmatrix}
15 \amp 0 \\
0 \amp 5 \\
\end{bmatrix},~~~
Q = \begin{bmatrix}
\frac1{\sqrt{5}} \amp \frac2{\sqrt{5}} \\
-\frac2{\sqrt{5}} \amp \frac1{\sqrt{5}} \\
\end{bmatrix}.
\end{equation*}
</div>
</div></li>
<li id="li-5362"><div class="para" id="p-7904">The total variance is the sum of the eigenvalues, <span class="process-math">\(V=\lambda_1 + \lambda_2 = 15 + 5 = 20\text{.}\)</span>
</div></li>
<li id="li-5363"><div class="para" id="p-7905">The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by <span class="process-math">\(\twovec{\frac{1}{\sqrt{5}}}{-\frac2{\sqrt{5}}}\text{,}\)</span> and the variance is 15 in this direction.</div></li>
<li id="li-5364"><div class="para" id="p-7906">The variance is smallest in the direction defined by <span class="process-math">\(\twovec{\frac2{\sqrt{5}}}{\frac1{\sqrt{5}}}\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article> <div class="para logical" id="p-7907">
<div class="para">Here are some ideas we‚Äôve seen previously that will be particularly useful for us in this section.  Remember that the covariance matrix of a dataset is <span class="process-math">\(C=\frac 1N AA^{\transpose}\)</span> where <span class="process-math">\(A\)</span> is the matrix of <span class="process-math">\(N\)</span> demeaned data points.</div>
<ul class="disc">
<li id="li-5365"><div class="para" id="p-7908">When <span class="process-math">\(\uvec\)</span> is a unit vector, the variance of the demeaned data after projecting onto the line defined by <span class="process-math">\(\uvec\)</span> is given by the quadratic form <span class="process-math">\(V_{\uvec} =
\uvec\cdot(C\uvec)\text{.}\)</span>
</div></li>
<li id="li-5366"><div class="para" id="p-7909">In particular, if <span class="process-math">\(\uvec\)</span> is a unit eigenvector of <span class="process-math">\(C\)</span> with associated eigenvalue <span class="process-math">\(\lambda\text{,}\)</span> then <span class="process-math">\(V_{\uvec} = \lambda\text{.}\)</span>
</div></li>
<li id="li-5367"><div class="para logical" id="p-7910">
<div class="para">Moreover, variance is additive, as we recorded in <a href="" class="xref" data-knowl="./knowl/prop-variance-additivity.html" title="Proposition 7.1.17: Additivity of Variance">Proposition¬†7.1.17</a>: if <span class="process-math">\(W\)</span> is a subspace having an orthonormal basis <span class="process-math">\(\uvec_1,\uvec_2,\ldots,\uvec_n\text{,}\)</span> then the variance</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/prop-variance-additivity.html">
\begin{equation*}
V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots +
V_{\uvec_n}\text{.} 
\end{equation*}
</div>
</div></li>
</ul>
</div></section><section class="subsection" id="subsection-127"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.3.1</span><span class="space"> </span><span class="title">Principal Component Analysis</span>
</h3>
<div class="para" id="p-7911">Let‚Äôs begin by looking at an example that illustrates the central theme of this technique.</div>
<article class="activity project-like" id="activity-97"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.3.2</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7912">
<div class="para">Suppose that we work with a dataset having 100 five-dimensional data points.  The demeaned data matrix <span class="process-math">\(A\)</span> is therefore <span class="process-math">\(5\by100\)</span> and leads to the covariance matrix <span class="process-math">\(C=\frac1{100}~AA^{\transpose}\text{,}\)</span> which is a <span class="process-math">\(5\by5\)</span> matrix.  Because <span class="process-math">\(C\)</span> is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that <span class="process-math">\(C = QDQ^{\transpose}\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
Q = \begin{bmatrix}
\uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4 \amp \uvec_5
\end{bmatrix},\hspace{24pt}
D = \begin{bmatrix}
13 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 10 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 2 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0
\end{bmatrix}.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5368"><div class="para" id="p-7913">What is <span class="process-math">\(V_{\uvec_2}\text{,}\)</span> the variance in the <span class="process-math">\(\uvec_2\)</span> direction?</div></li>
<li id="li-5369"><div class="para" id="p-7914">Find the variance of the data projected onto the line defined by <span class="process-math">\(\uvec_4\text{.}\)</span>  What does this say about the data?</div></li>
<li id="li-5370"><div class="para" id="p-7915">What is the total variance of the data?</div></li>
<li id="li-5371"><div class="para" id="p-7916">Consider the 2-dimensional subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?</div></li>
<li id="li-5372"><div class="para" id="p-7917">How does this question change if we project onto the 3-dimensional subspace spanned by <span class="process-math">\(\uvec_1\text{,}\)</span> <span class="process-math">\(\uvec_2\text{,}\)</span> and <span class="process-math">\(\uvec_3\text{?}\)</span>
</div></li>
<li id="li-5373"><div class="para" id="p-7918">What does this tell us about the data?</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-315" id="answer-315"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-315"><div class="answer solution-like"><div class="para logical" id="p-7919"><ol class="lower-alpha">
<li id="li-5374"><div class="para" id="p-7920"><span class="process-math">\(\displaystyle 10\)</span></div></li>
<li id="li-5375"><div class="para" id="p-7921">
<span class="process-math">\(0\text{,}\)</span> which tells us every data point is in the orthogonal complement of <span class="process-math">\(\uvec_4\text{.}\)</span>
</div></li>
<li id="li-5376"><div class="para" id="p-7922"><span class="process-math">\(\displaystyle 25\)</span></div></li>
<li id="li-5377"><div class="para" id="p-7923">
<span class="process-math">\(92\%\)</span> of the variance</div></li>
<li id="li-5378"><div class="para" id="p-7924">
<span class="process-math">\(100\%\)</span> of the variance.</div></li>
<li id="li-5379"><div class="para" id="p-7925">All of the data lies in the <span class="process-math">\(3\)</span>-dimensional subspace spanned by <span class="process-math">\(\uvec_1\text{,}\)</span> <span class="process-math">\(\uvec_1\text{,}\)</span> and <span class="process-math">\(\uvec_1\text{.}\)</span>
</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-393" id="solution-393"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-393"><div class="solution solution-like"><div class="para logical" id="p-7926"><ol class="lower-alpha">
<li id="li-5380"><div class="para" id="p-7927"><span class="process-math">\(\displaystyle V_{\uvec_2} = \lambda_2 = 10\)</span></div></li>
<li id="li-5381"><div class="para" id="p-7928">
<span class="process-math">\(V_{\uvec_4} = \lambda_4 = 0\text{,}\)</span> which tells us there is no variance in the <span class="process-math">\(\uvec_4\)</span> direction. Therefore, when we project onto the line defined by <span class="process-math">\(\uvec_4\text{,}\)</span> every data point projects to <span class="process-math">\(\zerovec\)</span> so every data point is in the orthogonal complement of <span class="process-math">\(\uvec_4\text{.}\)</span>
</div></li>
<li id="li-5382"><div class="para" id="p-7929"><span class="process-math">\(V = V_{\uvec_1} + V_{\uvec_2} + V_{\uvec_3} +
V_{\uvec_4} + V_{\uvec_5}  = 13+10+2+0+0 = 25\text{.}\)</span></div></li>
<li id="li-5383"><div class="para" id="p-7930">The variance of the data projected onto this subspace is <span class="process-math">\(13+10=23\text{,}\)</span> which represents <span class="process-math">\(23/25=92\%\)</span> of the variance.</div></li>
<li id="li-5384"><div class="para" id="p-7931">Projecting onto this 3-dimensional subspace retains all of the variance.</div></li>
<li id="li-5385"><div class="para" id="p-7932">All of the data lies in the <span class="process-math">\(3\)</span>-dimensional subspace spanned by <span class="process-math">\(\uvec_1\text{,}\)</span> <span class="process-math">\(\uvec_1\text{,}\)</span> and <span class="process-math">\(\uvec_1\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7933">This activity demonstrates how the eigenvalues of the covariance matrix can tell us when data are clustered around, or even wholly contained within, a smaller dimensional subspace.  In particular, the original data is 5-dimensional, but we see that it actually lies in a 3-dimensional subspace of <span class="process-math">\(\real^5\text{.}\)</span>  Later in this section, we‚Äôll see how to use this observation to work with the data as if it were three-dimensional, an idea known as <em class="emphasis">dimensional reduction</em>.</div>
<div class="para logical" id="p-7934">
<div class="para"> The eigenvectors <span class="process-math">\(\uvec_j\)</span> of the covariance matrix are called <em class="emphasis">principal components</em>, and we will order them so that their associated eigenvalues decrease.  Generally speaking, we hope that the first few principal components retain most of the variance, as the example in the activity demonstrates.  In that example, we have the sequence of subspaces</div>
<ul class="disc">
<li id="li-5386"><div class="para" id="p-7935">
<span class="process-math">\(W_1\text{,}\)</span> the 1-dimensional subspace spanned by <span class="process-math">\(\uvec_1\text{,}\)</span> which retains <span class="process-math">\(13/25 = 52\%\)</span> of the total variance,</div></li>
<li id="li-5387"><div class="para" id="p-7936">
<span class="process-math">\(W_2\text{,}\)</span> the 2-dimensional subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{,}\)</span> which retains <span class="process-math">\(23/25
= 92\%\)</span> of the variance, and</div></li>
<li id="li-5388"><div class="para" id="p-7937">
<span class="process-math">\(W_3\text{,}\)</span> the 3-dimensional subspace spanned by <span class="process-math">\(\uvec_1\text{,}\)</span> <span class="process-math">\(\uvec_2\text{,}\)</span> and <span class="process-math">\(\uvec_3\text{,}\)</span> which retains all of the variance.</div></li>
</ul>
</div>
<div class="para" id="p-7938">Notice how we retain more of the total variance as we increase the dimension of the subspace onto which the data are projected. Eventually, projecting the data onto <span class="process-math">\(W_3\)</span> retains all the variance, which tells us the data must lie in <span class="process-math">\(W_3\text{,}\)</span> a smaller dimensional subspace of <span class="process-math">\(\real^5\text{.}\)</span>
</div>
<div class="para" id="p-7939">In fact, these subspaces are the best possible.  We know that the first principal component <span class="process-math">\(\uvec_1\)</span> is the eigenvector of <span class="process-math">\(C\)</span> associated to the largest eigenvalue.  This means that the variance is as large as possible in the <span class="process-math">\(\uvec_1\)</span> direction.  In other words, projecting onto any other line will retain a smaller amount of variance.  Similarly, projecting onto any other 2-dimensional subspace besides <span class="process-math">\(W_2\)</span> will retain less variance than projecting onto <span class="process-math">\(W_2\text{.}\)</span>  The principal components have the wonderful ability to pick out the best possible subspaces to retain as much variance as possible.</div>
<div class="para" id="p-7940">Of course, this is a contrived example.  Typically, the presence of noise in a dataset means that we do not expect all the points to be wholly contained in a smaller dimensional subspace.  In fact, the 2-dimensional subspace <span class="process-math">\(W_2\)</span> retains <span class="process-math">\(92\%\)</span> of the variance.  Depending on the situation, we may want to write off the remaining <span class="process-math">\(8\%\)</span> of the variance as noise in exchange for the convenience of working with a smaller dimensional subspace.  As we‚Äôll see later, we will seek a balance using a number of principal components large enough to retain most of the variance but small enough to be easy to work with.</div>
<article class="activity project-like" id="activity-98"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.3.3</span><span class="period">.</span>
</h4>
<div class="para" id="p-7941">We will work here with a dataset having 100 3-dimensional demeaned data points.  Evaluating the following cell will plot those data points and define the demeaned data matrix <code class="code-inline tex2jax_ignore">A</code> whose shape is <span class="process-math">\(3\by100\text{.}\)</span>
</div> <pre class="ptx-sagecell sagecell-sage" id="sage-258"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_demo.py', globals())
</script></pre> <div class="para logical" id="p-7942">
<div class="para">Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.</div>
<ol class="lower-alpha">
<li id="li-5389">
<div class="para" id="p-7943">Use the matrix <code class="code-inline tex2jax_ignore">A</code> to construct the covariance matrix <span class="process-math">\(C\text{.}\)</span>  Then determine the variance in the direction of <span class="process-math">\(\uvec=\threevec{1/3}{2/3}{2/3}\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-259"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5390">
<div class="para" id="p-7944">Find the eigenvalues of <span class="process-math">\(C\)</span> and determine the total variance.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-260"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-7945">Notice that Sage does not necessarily sort the eigenvalues in decreasing order.</div>
</li>
<li id="li-5391"><div class="para" id="p-7946">Use the <code class="code-inline tex2jax_ignore">right_eigenmatrix()</code> command to find the eigenvectors of <span class="process-math">\(C\text{.}\)</span>  Remembering that the Sage command <code class="code-inline tex2jax_ignore">B.column(1)</code> retrieves the vector represented by the second column of <code class="code-inline tex2jax_ignore">B</code>, define vectors <code class="code-inline tex2jax_ignore">u1</code>, <code class="code-inline tex2jax_ignore">u2</code>, and <code class="code-inline tex2jax_ignore">u3</code> representing the three principal components in order of decreasing eigenvalues.  How can you check if these vectors are an orthonormal basis for <span class="process-math">\(\real^3\text{?}\)</span>
</div></li>
<li id="li-5392"><div class="para" id="p-7947">What fraction of the total variance is retained by projecting the data onto <span class="process-math">\(W_1\text{,}\)</span> the subspace spanned by <span class="process-math">\(\uvec_1\text{?}\)</span>  What fraction of the total variance is retained by projecting onto <span class="process-math">\(W_2\text{,}\)</span> the subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{?}\)</span>  What fraction of the total variance do we lose by projecting onto <span class="process-math">\(W_2\text{?}\)</span>
</div></li>
<li id="li-5393">
<div class="para logical" id="p-7948">
<div class="para">If we project a data point <span class="process-math">\(\xvec\)</span> onto <span class="process-math">\(W_2\text{,}\)</span> the Projection Formula tells us we obtain</div>
<div class="displaymath process-math">
\begin{equation*}
\xhat = (\uvec_1\cdot\xvec) \uvec_1 +
(\uvec_2\cdot\xvec) \uvec_2.
\end{equation*}
</div>
<div class="para">Rather than viewing the projected data in <span class="process-math">\(\real^3\text{,}\)</span> we will record the coordinates of <span class="process-math">\(\xhat\)</span> in the basis defined by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{;}\)</span>  that is, we will record the coordinates</div>
<div class="displaymath process-math">
\begin{equation*}
\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.  
\end{equation*}
</div>
<div class="para">Construct the matrix <span class="process-math">\(Q\)</span> so that <span class="process-math">\(Q^{\transpose}\xvec =
\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}\text{.}\)</span>
</div>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-261"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5394">
<div class="para" id="p-7949">Since each column of <span class="process-math">\(A\)</span> represents a data point, the matrix <span class="process-math">\(Q^{\transpose}A\)</span> represents the coordinates of the projected data points.  Evaluating the following cell will plot those projected data points.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-262"><script type="text/x-sage">pca_plot(Q.T*A)
</script></pre>
<div class="para" id="p-7950">Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?</div>
</li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-316" id="answer-316"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-316"><div class="answer solution-like"><div class="para logical" id="p-7951"><ol class="lower-alpha">
<li id="li-5395"><div class="para" id="p-7952"><span class="process-math">\(\displaystyle V_{\uvec} = 7885\)</span></div></li>
<li id="li-5396"><div class="para" id="p-7953"><span class="process-math">\(\displaystyle V=12195\)</span></div></li>
<li id="li-5397"><div class="para" id="p-7954">If <span class="process-math">\(P\)</span> is the matrix of eigenvectors, evaluate <span class="process-math">\(P^{\transpose}P\text{.}\)</span>
</div></li>
<li id="li-5398"><div class="para" id="p-7955">
<span class="process-math">\(W_1\)</span> retains <span class="process-math">\(83\%\)</span> of the total variance, and <span class="process-math">\(W_2\)</span> retains <span class="process-math">\(98\%\text{.}\)</span>
</div></li>
<li id="li-5399"><div class="para" id="p-7956"><span class="process-math">\(\displaystyle Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}\)</span></div></li>
<li id="li-5400"><div class="para" id="p-7957">Because the variance in the <span class="process-math">\(\uvec_1\)</span> direction is greater than the variance in the <span class="process-math">\(\uvec_2\)</span> direction.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-394" id="solution-394"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-394"><div class="solution solution-like"><div class="para logical" id="p-7958"><ol class="lower-alpha">
<li id="li-5401"><div class="para" id="p-7959">After constructing the covariance matrix <span class="process-math">\(C =
\frac{1}{100}AA^{\transpose}\text{,}\)</span> we find that <span class="process-math">\(V_{\uvec} =
\uvec\cdot(C\uvec) = 7885\text{.}\)</span>
</div></li>
<li id="li-5402"><div class="para" id="p-7960">The total variance <span class="process-math">\(V\)</span> is the sum of the eigenvalues of <span class="process-math">\(C\)</span> so we obtain <span class="process-math">\(V=12195\text{.}\)</span>
</div></li>
<li id="li-5403"><div class="para" id="p-7961">If we obtain <span class="process-math">\(P\text{,}\)</span> the matrix of eigenvectors, from Sage, computing <span class="process-math">\(P^{\transpose}P\)</span> evaluates the dot products between the columns.  Since <span class="process-math">\(P^{\transpose}P=I\text{,}\)</span> the basis provided by Sage is orthonormal.</div></li>
<li id="li-5404"><div class="para" id="p-7962">Projecting onto <span class="process-math">\(W_1\text{,}\)</span> we see that <span class="process-math">\(\lambda_1/V = 0.83\)</span> so <span class="process-math">\(W_1\)</span> retains about <span class="process-math">\(83\%\)</span> of the total variance.  The subspace <span class="process-math">\(W_2\)</span> retains <span class="process-math">\((\lambda_1+\lambda_2)/V=0.98\)</span> or <span class="process-math">\(98\%\)</span> of the total variance.  If we project onto <span class="process-math">\(W_2\)</span> we lose less than <span class="process-math">\(2\%\)</span> of the variance.</div></li>
<li id="li-5405"><div class="para" id="p-7963"><span class="process-math">\(\displaystyle Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}\)</span></div></li>
<li id="li-5406"><div class="para" id="p-7964">The plot is wider because the variance in the <span class="process-math">\(\uvec_1\)</span> direction, which corresponds to the horizontal coordinate, is greater than the variance in the <span class="process-math">\(\uvec_2\)</span> direction.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7965">This example is a more realistic illustration of principal component analysis.  The plot of the 3-dimensional data appears to show that the data lies close to a plane, and the principal components will identify this plane.  Starting with the <span class="process-math">\(3\by100\)</span> matrix of demeaned data <span class="process-math">\(A\text{,}\)</span> we construct the covariance matrix <span class="process-math">\(C=\frac{1}{100} ~AA^{\transpose}\)</span> and study its eigenvalues.  Notice that the first two principal components account for more than 98% of the variance, which means we can expect the points to lie close to <span class="process-math">\(W_2\text{,}\)</span> the two-dimensional subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>
</div>
<div class="para logical" id="p-7966">
<div class="para">Since <span class="process-math">\(W_2\)</span> is a subspace of <span class="process-math">\(\real^3\text{,}\)</span> projecting the data points onto <span class="process-math">\(W_2\)</span> gives a list of 100 points in <span class="process-math">\(\real^3\text{.}\)</span>  In order to visualize them more easily, we instead consider the coordinates of the projections in the basis defined by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  For instance, we know that the projection of a data point <span class="process-math">\(\xvec\)</span> is</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/fig-pca-coords.html">
\begin{equation*}
\xhat = (\uvec_1\cdot\xvec)\uvec_1 +
(\uvec_2\cdot\xvec)\uvec_2,
\end{equation*}
</div>
<div class="para">which is a three-dimensional vector.  Instead, we can record the coordinates <span class="process-math">\(\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}\)</span> and plot them in the two-dimensional coordinate plane, as illustrated in <a href="" class="xref" data-knowl="./knowl/fig-pca-coords.html" title="Figure 7.3.1">Figure¬†7.3.1</a>.</div>
</div>
<figure class="figure figure-like" id="fig-pca-coords"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:52.6315789473684%;"><img src="external/images/pca-proj.svg" role="img" class="contained"></div>
<div class="sbspanel top" style="width:42.1052631578947%;"><img src="external/images/pca-coords.svg" role="img" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.3.1<span class="period">.</span></span><span class="space"> </span>The projection <span class="process-math">\(\xhat\)</span> of a data point <span class="process-math">\(\xvec\)</span> onto <span class="process-math">\(W_2\)</span> is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span></figcaption></figure><div class="para logical" id="p-7967">
<div class="para">If we form the matrix <span class="process-math">\(Q=\begin{bmatrix}\uvec_1 \amp \uvec_2
\end{bmatrix}\text{,}\)</span> then we have</div>
<div class="displaymath process-math">
\begin{equation*}
Q^{\transpose}\xvec = \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.
\end{equation*}
</div>
<div class="para">This means that the columns of <span class="process-math">\(Q^{\transpose}A\)</span> represent the coordinates of the projected points, which may now be plotted in the plane.</div>
</div>
<div class="para" id="p-7968">In this plot, the first coordinate, represented by the horizontal coordinate, represents the projection of a data point onto the line defined by <span class="process-math">\(\uvec_1\)</span> while the second coordinate represents the projection onto the line defined by <span class="process-math">\(\uvec_2\text{.}\)</span>  Since <span class="process-math">\(\uvec_1\)</span> is the first principal component, the variance in the <span class="process-math">\(\uvec_1\)</span> direction is greater than the variance in the <span class="process-math">\(\uvec_2\)</span> direction.  For this reason, the plot will be more spread out in the horizontal direction than in the vertical.</div></section><section class="subsection" id="subsection-128"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.3.2</span><span class="space"> </span><span class="title">Using Principal Component Analysis</span>
</h3>
<div class="para" id="p-7969">Now that we‚Äôve explored the ideas behind principal component analysis, we will look at a few examples that illustrate its use.</div>
<article class="activity project-like" id="activity-99"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.3.4</span><span class="period">.</span>
</h4>
<div class="para" id="p-7970">The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom.  The units for each entry are grams per person per week.</div> <pre class="ptx-sagecell sagecell-sage" id="sage-263"><script type="text/x-sage">import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/uk-diet.csv', index_col=0)
data_mean = vector(df.T.mean())
A = matrix([vector(row) for row in (df.T-df.T.mean()).values]).T
df
</script></pre> <div class="para" id="p-7971">We will view this as a dataset consisting of four points in <span class="process-math">\(\real^{17}\text{.}\)</span>  As such, it is impossible to visualize and studying the numbers themselves doesn‚Äôt lead to much insight.</div> <div class="para logical" id="p-7972">
<div class="para">In addition to loading the data, evaluating the cell above created a vector <code class="code-inline tex2jax_ignore">data_mean</code>, which is the mean of the four data points, and <code class="code-inline tex2jax_ignore">A</code>, the <span class="process-math">\(17\by4\)</span> matrix of demeaned data.</div>
<ol class="lower-alpha">
<li id="li-5407">
<div class="para" id="p-7973">What is the average consumption of Beverages across the four nations?</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-264"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5408"><div class="para" id="p-7974">Find the covariance matrix <span class="process-math">\(C\)</span> and its eigenvalues.  Because there are four points in <span class="process-math">\(\real^{17}\)</span> whose mean is zero, there are only three nonzero eigenvalues.</div></li>
<li id="li-5409"><div class="para" id="p-7975">For what percentage of the total variance does the first principal component account?</div></li>
<li id="li-5410">
<div class="para" id="p-7976">Find the first principal component <span class="process-math">\(\uvec_1\)</span> and project the four demeaned data points onto the line defined by <span class="process-math">\(\uvec_1\text{.}\)</span>  Plot those points on <a href="" class="xref" data-knowl="./knowl/fig-pca-1d.html" title="Figure 7.3.2">Figure¬†7.3.2</a>
</div>
<figure class="figure figure-like" id="fig-pca-1d"><div class="sidebyside"><div class="sbsrow" style="margin-left:5%;margin-right:5%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/pca-plot-1.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.3.2<span class="period">.</span></span><span class="space"> </span>A plot of the demeaned data projected onto the first principal component.</figcaption></figure>
</li>
<li id="li-5411"><div class="para" id="p-7977">For what percentage of the total variance do the first two principal components account?</div></li>
<li id="li-5412">
<div class="para" id="p-7978">Find the coordinates of the demeaned data points projected onto <span class="process-math">\(W_2\text{,}\)</span> the two-dimensional subspace of <span class="process-math">\(\real^{17}\)</span> spanned by the first two principal components.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-265"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-7979">Plot these coordinates in <a href="" class="xref" data-knowl="./knowl/fig-pca-2d.html" title="Figure 7.3.3">Figure¬†7.3.3</a>.</div>
<figure class="figure figure-like" id="fig-pca-2d"><div class="sidebyside"><div class="sbsrow" style="margin-left:5%;margin-right:5%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/pca-plot-2.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.3.3<span class="period">.</span></span><span class="space"> </span>The coordinates of the demeaned data points projected onto the first two principal components.</figcaption></figure>
</li>
<li id="li-5413"><div class="para" id="p-7980">What information do these plots reveal that is not clear from consideration of the original data points?</div></li>
<li id="li-5414"><div class="para" id="p-7981">Study the first principal component <span class="process-math">\(\uvec_1\)</span> and find the first component of <span class="process-math">\(\uvec_1\text{,}\)</span> which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use <code class="code-inline tex2jax_ignore">N(u1, digits=2)</code> for a result that‚Äôs easier to read.) If a data point lies on the far right side of the plot in <a href="" class="xref" data-knowl="./knowl/fig-pca-2d.html" title="Figure 7.3.3">Figure¬†7.3.3</a>, what does it mean about that nation‚Äôs consumption of Alcoholic Drinks?</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original has-tabular" data-refid="hk-answer-317" id="answer-317"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-317"><div class="answer solution-like"><div class="para logical" id="p-7982"><ol class="lower-alpha">
<li id="li-5415"><div class="para" id="p-7983"><span class="process-math">\(\displaystyle 57.5\)</span></div></li>
<li id="li-5416"><div class="para" id="p-7984">
<span class="process-math">\(78805\text{,}\)</span> <span class="process-math">\(33946\text{,}\)</span> and <span class="process-math">\(4093\)</span>
</div></li>
<li id="li-5417"><div class="para" id="p-7985"><span class="process-math">\(\displaystyle 67\%\)</span></div></li>
<li id="li-5418">
<div class="para" id="p-7986">The coordinates are</div>
<div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="r m b1 r0 l0 t0 lines">Nation</td>
<td class="r m b1 r0 l0 t0 lines">Coordinate</td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-145\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Northern Ireland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(477\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Scotland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-92\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Wales</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-241\)</span></td>
</tr>
</table></div>
</li>
<li id="li-5419"><div class="para" id="p-7987"><span class="process-math">\(\displaystyle 96\%\)</span></div></li>
<li id="li-5420">
<div class="para" id="p-7988">The coordinates are</div>
<div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="r m b1 r0 l0 t0 lines">Nation</td>
<td class="r m b1 r0 l0 t0 lines">Coordinates</td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-145, 3)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Northern Ireland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((477, 59)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Scotland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-92, -286)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-241, 225)\)</span></td>
</tr>
</table></div>
</li>
<li id="li-5421"><div class="para" id="p-7989">Northern Ireland appears to be significantly different from the other three nations.</div></li>
<li id="li-5422"><div class="para" id="p-7990">The average consumption of Alcoholic Drinks will be less than the mean.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original has-tabular" data-refid="hk-solution-395" id="solution-395"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-395"><div class="solution solution-like"><div class="para logical" id="p-7991"><ol class="lower-alpha">
<li id="li-5423"><div class="para" id="p-7992">Beverages is the second category so this would be the second component of the <code class="code-inline tex2jax_ignore">data_mean</code> vector, which is <span class="process-math">\(57.5\text{.}\)</span>
</div></li>
<li id="li-5424"><div class="para" id="p-7993">The three nonzero eigenvalues are <span class="process-math">\(78805\text{,}\)</span> <span class="process-math">\(33946\text{,}\)</span> and <span class="process-math">\(4093\text{.}\)</span>
</div></li>
<li id="li-5425"><div class="para" id="p-7994">The total variance <span class="process-math">\(V=116844\)</span> is the sum of the eigenvalues so the first principal component accounts for <span class="process-math">\(\lambda_1/V = 67\%\)</span> of the total variance.</div></li>
<li id="li-5426">
<div class="para" id="p-7995">The coordinates are</div>
<div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="r m b1 r0 l0 t0 lines">Nation</td>
<td class="r m b1 r0 l0 t0 lines">Coordinate</td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-145\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Northern Ireland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(477\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Scotland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-92\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Wales</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\(-241\)</span></td>
</tr>
</table></div>
</li>
<li id="li-5427"><div class="para" id="p-7996">The first two principal components account for <span class="process-math">\(96\%\)</span> of the total variance.</div></li>
<li id="li-5428">
<div class="para" id="p-7997">The coordinates are</div>
<div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="r m b1 r0 l0 t0 lines">Nation</td>
<td class="r m b1 r0 l0 t0 lines">Coordinates</td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-145, 3)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Northern Ireland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((477, 59)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">Scotland</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-92, -286)\)</span></td>
</tr>
<tr>
<td class="r m b0 r0 l0 t0 lines">England</td>
<td class="r m b0 r0 l0 t0 lines"><span class="process-math">\((-241, 225)\)</span></td>
</tr>
</table></div>
</li>
<li id="li-5429"><div class="para" id="p-7998">Northern Ireland appears to be significantly different from the other three nations.  There are several possible reasons for this, both historical and geographical, that we might explore.</div></li>
<li id="li-5430"><div class="para" id="p-7999">The first component of <span class="process-math">\(\uvec_1\)</span> is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean.  This can be confirmed by looking at the original data.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-8000">This activity demonstrates how principal component analysis enables us to extract information from a dataset that may not be easily obtained otherwise.  As in our previous example, we see that the data points lie quite close to a two-dimensional subspace of <span class="process-math">\(\real^{17}\text{.}\)</span>  In fact, <span class="process-math">\(W_2\text{,}\)</span> the subspace spanned by the first two principal components, accounts for more than 96% of the variance. More importantly, when we project the data onto <span class="process-math">\(W_2\text{,}\)</span> it becomes apparent that Northern Ireland is fundamentally different from the other three nations.</div>
<div class="para" id="p-8001">With some additional thought, we can determine more specific ways in which Northern Ireland is different.  On the <span class="process-math">\(2\)</span>-dimensional plot, Northern Ireland lies far to the right compared to the other three nations.  Since the data has been demeaned, the origin <span class="process-math">\((0,0)\)</span> in this plot corresponds to the average of the four nations.  The coordinates of the point representing Northern Ireland are about <span class="process-math">\((477, 59)\text{,}\)</span> meaning that the projected data point differs from the mean by about <span class="process-math">\(477\uvec_1+59\uvec_2\text{.}\)</span>
</div>
<div class="para" id="p-8002">Let‚Äôs just focus on the contribution from <span class="process-math">\(\uvec_1\text{.}\)</span>  We see that the ninth component of <span class="process-math">\(\uvec_1\text{,}\)</span> the one that describes Fresh Fruit, is about <span class="process-math">\(-0.63\text{.}\)</span>  This means that the ninth component of <span class="process-math">\(477\uvec_1\)</span> differs from the mean by about <span class="process-math">\(477(-0.63) = -300\)</span> grams per person per week.  So roughly speaking, people in Northern Ireland are eating about 300 fewer grams of Fresh Fruit than the average across the four nations.  This is borne out by looking at the original data, which show that the consumption of Fresh Fruit in Northern Ireland is significantly less than the other nations.  Examing the other components of <span class="process-math">\(\uvec_1\)</span> shows other ways in which Northern Ireland differs from the other three nations.</div>
<article class="activity project-like" id="activity-pca-iris"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.3.5</span><span class="period">.</span>
</h4>
<div class="para" id="p-8003">In this activity, we‚Äôll look at a <a class="external" href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">well-known dataset</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-6" id="fn-6"><sup>‚Äâ1‚Äâ</sup></a> that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.</div> <figure class="figure figure-like" id="fig-iris"><div class="sidebyside"><div class="sbsrow" style="margin-left:15%;margin-right:15%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/Iris_versicolor.jpg" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.3.4<span class="period">.</span></span><span class="space"> </span>One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: <a class="external" href="https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg" target="_blank">Wikipedia</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-7" id="fn-7"><sup>‚Äâ2‚Äâ</sup></a>, License: <a class="external" href="https://commons.wikimedia.org/wiki/Commons:GNU_Free_Documentation_License,_version_1.2" target="_blank">GNU Free DOcumetation License</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-8" id="fn-8"><sup>‚Äâ3‚Äâ</sup></a>)</figcaption></figure> <div class="para" id="p-8004">Evaluating the following cell will load the dataset, which consists of 150 points in <span class="process-math">\(\real^4\text{.}\)</span>  In addition, we have a vector <code class="code-inline tex2jax_ignore">data_mean</code>, a four-dimensional vector holding the mean of the data points, and <code class="code-inline tex2jax_ignore">A</code>, the <span class="process-math">\(4\by150\)</span> demeaned data matrix.</div> <pre class="ptx-sagecell sagecell-sage" id="sage-266"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_iris.py', globals())
df.T
</script></pre> <div class="para" id="p-8005">Since the data is four-dimensional, we are not able to visualize it.  Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.</div> <pre class="ptx-sagecell sagecell-sage" id="sage-267"><script type="text/x-sage">sepal_plot()
</script></pre> <div class="para logical" id="p-8006"><ol class="lower-alpha">
<li id="li-5431"><div class="para" id="p-8007">What is the mean sepal width?</div></li>
<li id="li-5432">
<div class="para" id="p-8008">Find the covariance matrix <span class="process-math">\(C\)</span> and its eigenvalues.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-268"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5433"><div class="para" id="p-8009">Find the fraction of variance for which the first two principal components account.</div></li>
<li id="li-5434">
<div class="para" id="p-8010">Construct the first two principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> along with the matrix <span class="process-math">\(Q\)</span> whose columns are <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-269"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5435">
<div class="para" id="p-8011">As we have seen, the columns of the matrix <span class="process-math">\(Q^{\transpose}A\)</span> hold the coordinates of the demeaned data points after projecting onto <span class="process-math">\(W_2\text{,}\)</span> the subspace spanned by the first two principal components.  Evaluating the following cell shows a plot of these coordinates.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-270"><script type="text/x-sage">pca_plot(Q.T*A)
</script></pre>
<div class="para" id="p-8012">Suppose we have a flower whose coordinates in this plane are <span class="process-math">\((-2.5, -0.75)\text{.}\)</span>  To what species does this iris most likely belong?  Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.</div>
</li>
<li id="li-5436">
<div class="para" id="p-8013">Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm.  Knowing only these two measurements, determine the coordinates <span class="process-math">\((c_1, c_2)\)</span> in the plane where this iris lies.  To what species does this iris most likely belong?  Now estimate the petal length and petal width of this iris.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-271"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5437">
<div class="para" id="p-8014">Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm.  Find the coordinates <span class="process-math">\((c_1, c_2)\)</span> of this iris and determine the species to which it most likely belongs.  Also, estimate the sepal length and the petal length.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-272"><script type="text/x-sage">
</script></pre>
</li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-318" id="answer-318"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-318"><div class="answer solution-like"><div class="para logical" id="p-8015"><ol class="lower-alpha">
<li id="li-5438"><div class="para" id="p-8016"><span class="process-math">\(3.05\text{.}\)</span></div></li>
<li id="li-5439"><div class="para" id="p-8017">
<span class="process-math">\(4.20\text{,}\)</span> <span class="process-math">\(0.24\text{,}\)</span> <span class="process-math">\(0.08\text{,}\)</span> <span class="process-math">\(0.02\text{.}\)</span>
</div></li>
<li id="li-5440"><div class="para" id="p-8018"><span class="process-math">\(\displaystyle 97.8\%\)</span></div></li>
<li id="li-5441"><div class="para" id="p-8019">The columns of <span class="process-math">\(Q\)</span> are for the first two principal components.</div></li>
<li id="li-5442"><div class="para" id="p-8020">Iris setosa and the vector of measurements is <span class="process-math">\(\fourvec{5.43}{3.81}{1.49}{0.25}\text{.}\)</span>
</div></li>
<li id="li-5443"><div class="para" id="p-8021">The petal length is <span class="process-math">\(3.99\)</span> and the petal width is <span class="process-math">\(1.29\text{.}\)</span>
</div></li>
<li id="li-5444"><div class="para" id="p-8022">The sepal length is <span class="process-math">\(7.23\)</span> and the petal length is <span class="process-math">\(6.15\text{.}\)</span>
</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-396" id="solution-396"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-396"><div class="solution solution-like"><div class="para logical" id="p-8023"><ol class="lower-alpha">
<li id="li-5445"><div class="para" id="p-8024">The second component of <code class="code-inline tex2jax_ignore">data_mean</code>, which is the one corresponding to sepal width, is <span class="process-math">\(3.05\text{.}\)</span>
</div></li>
<li id="li-5446"><div class="para" id="p-8025">The eigenvalues are <span class="process-math">\(4.20\text{,}\)</span> <span class="process-math">\(0.24\text{,}\)</span> <span class="process-math">\(0.08\text{,}\)</span> and <span class="process-math">\(0.02\text{.}\)</span>
</div></li>
<li id="li-5447"><div class="para" id="p-8026">The first two principal components account for <span class="process-math">\(97.8\%\)</span> of the variance.</div></li>
<li id="li-5448"><div class="para" id="p-8027">If <span class="process-math">\(P\)</span> is the matrix whose columns are an orthonormal basis of eigenvectors, then <span class="process-math">\(Q\)</span> is formed from the first two columns of <span class="process-math">\(P\text{.}\)</span>
</div></li>
<li id="li-5449"><div class="para" id="p-8028">This would most likely belong to Iris setosa.  To find its measurements, we evaluate <span class="process-math">\(-2.5\uvec_1 -
0.75\uvec_2 + \mvec\)</span> where <span class="process-math">\(\mvec\)</span> is the vector of means.  This is the same as <span class="process-math">\(Q\twovec{-2.5}{-0.75} + \mvec\text{,}\)</span> which gives the vector of measurements <span class="process-math">\(\fourvec{5.43}{3.81}{1.49}{0.25}\text{.}\)</span>
</div></li>
<li id="li-5450"><div class="para" id="p-8029">Subtracting the mean sepal length and sepal width, we have <span class="process-math">\((-0.19, -0.30)\text{.}\)</span>  Then the first two components of <span class="process-math">\(c_1\uvec_1+c_2\uvec_2 =
Q\twovec{c_1}{c_2} = \twovec{-0.19}{-0.30}\text{.}\)</span>  This gives <span class="process-math">\((c_1, c_2) = (0.18, 0.40)\text{.}\)</span>  This looks like an Iris versicolor.  As in the previous part, we can now find the petal length to be <span class="process-math">\(3.99\)</span> and the petal width to be <span class="process-math">\(1.29\text{.}\)</span>
</div></li>
<li id="li-5451"><div class="para" id="p-8030">Using the same approach as the last part, we find <span class="process-math">\((c_1,c_2)=(2.90, -0.53)\text{,}\)</span> which gives a sepal length of <span class="process-math">\(7.23\)</span> and a petal length of <span class="process-math">\(6.15\text{.}\)</span>  Most likely, this flower belongs to Iris virginica.</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsection-129"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.3.3</span><span class="space"> </span><span class="title">Summary</span>
</h3>
<div class="para logical" id="p-8031">
<div class="para">This section has explored principal component analysis as a technique to reduce the dimension of a dataset.  From the demeaned data matrix <span class="process-math">\(A\text{,}\)</span> we form the covariance matrix <span class="process-math">\(C= \frac1N ~AA^{\transpose}\text{,}\)</span> where <span class="process-math">\(N\)</span> is the number of data points.</div>
<ul class="disc">
<li id="li-5452"><div class="para" id="p-8032">The eigenvectors <span class="process-math">\(\uvec_1, \uvec_2, \ldots \uvec_m\text{,}\)</span> of <span class="process-math">\(C\)</span> are called the principal components.  We arrange them so that their corresponding eigenvalues are in decreasing order.</div></li>
<li id="li-5453"><div class="para" id="p-8033">If <span class="process-math">\(W_n\)</span> is the subspace spanned by the first <span class="process-math">\(n\)</span> principal components, then the variance of the demeaned data projected onto <span class="process-math">\(W_n\)</span> is the sum of the first <span class="process-math">\(n\)</span> eigenvalues of <span class="process-math">\(C\text{.}\)</span>  No other <span class="process-math">\(n\)</span>-dimensional subspace retains more variance when the data is projected onto it.</div></li>
<li id="li-5454"><div class="para" id="p-8034">If <span class="process-math">\(Q\)</span> is the matrix whose columns are the first <span class="process-math">\(n\)</span> principal components, then the columns of <span class="process-math">\(Q^{\transpose}A\)</span> hold the coordinates, expressed in the basis <span class="process-math">\(\uvec_1,\ldots,\uvec_n\text{,}\)</span> of the data once projected onto <span class="process-math">\(W_n\text{.}\)</span>
</div></li>
<li id="li-5455"><div class="para" id="p-8035">Our goal is to use a number of principal components that is large enough to retain most of the variance in the dataset but small enough to be manageable.</div></li>
</ul>
</div></section><section class="exercises" id="exercises-32"><h3 class="heading hide-type">
<span class="type">Exercises</span><span class="space"> </span><span class="codenumber">7.3.4</span><span class="space"> </span><span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-276"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="para logical" id="p-8036">
<div class="para">Suppose that</div>
<div class="displaymath process-math">
\begin{equation*}
Q = \begin{bmatrix}
-1/\sqrt{2} \amp 1/\sqrt{2} \\
1/\sqrt{2} \amp 1/\sqrt{2} \\
\end{bmatrix}, \hspace{24pt}
D_1 = \begin{bmatrix}
75 \amp 0 \\
0 \amp 74
\end{bmatrix}, \hspace{24pt}
D_2 = \begin{bmatrix}
100 \amp 0 \\
0 \amp 1 
\end{bmatrix}
\end{equation*}
</div>
<div class="para">and that we have two datasets, one whose covariance matrix is <span class="process-math">\(C_1 = QD_1Q^{\transpose}\)</span> and one whose covariance matrix is <span class="process-math">\(C_2 = QD_2Q^{\transpose}\text{.}\)</span> For each dataset, find</div>
<ol class="lower-alpha">
<li id="li-5456"><div class="para" id="p-8037">the total variance.</div></li>
<li id="li-5457"><div class="para" id="p-8038">the fraction of variance represented by the first principal component.</div></li>
<li id="li-5458"><div class="para" id="p-8039">a verbal description of how the demeaned data points appear when plotted in the plane.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-277"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="para logical" id="p-8048">
<div class="para">Suppose that a dataset has mean <span class="process-math">\(\threevec{13}{5}{7}\)</span> and that its associated covariance matrix is <span class="process-math">\(C=\begin{bmatrix}
275 \amp -206 \amp 251 \\
-206 \amp 320 \amp -206 \\
251 \amp -206 \amp 275
\end{bmatrix}
\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-273"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5465"><div class="para" id="p-8049">What fraction of the variance is represented by the first two principal components?</div></li>
<li id="li-5466"><div class="para" id="p-8050">If <span class="process-math">\(\threevec{30}{-3}{26}\)</span> is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.</div></li>
<li id="li-5467"><div class="para" id="p-8051">If a projected data point has coordinates <span class="process-math">\(\twovec{12}{-25}\text{,}\)</span> find an estimate for the original data point.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-278"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="para logical" id="p-8060">
<div class="para">Evaluating the following cell loads a <span class="process-math">\(2\by100\)</span> demeaned data matrix <code class="code-inline tex2jax_ignore">A</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-274"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_ex.py', globals())
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5474"><div class="para" id="p-8061">Find the principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> and the variance in the direction of each principal component.</div></li>
<li id="li-5475"><div class="para" id="p-8062">What is the total variance?</div></li>
<li id="li-5476"><div class="para" id="p-8063">What can you conclude about this dataset?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-279"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="para logical" id="p-8072">
<div class="para">Determine whether the following statements are true or false and explain your thinking.</div>
<ol class="lower-alpha">
<li id="li-5483"><div class="para" id="p-8073">If the eigenvalues of the covariance matrix are <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> and <span class="process-math">\(\lambda_3\text{,}\)</span> then <span class="process-math">\(\lambda_3\)</span> is the variance of the demeaned data points when projected on the third principal component <span class="process-math">\(\uvec_3\text{.}\)</span>
</div></li>
<li id="li-5484"><div class="para" id="p-8074">Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.</div></li>
<li id="li-5485"><div class="para" id="p-8075">If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in <span class="process-math">\(\real^3\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-280"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="para logical" id="p-8084">
<div class="para">In <a href="" class="xref" data-knowl="./knowl/activity-pca-iris.html" title="Activity 7.3.5">Activity¬†7.3.5</a>, we looked at a dataset consisting of four measurements of 150 irises.  These measurements are sepal length, sepal width, petal length, and petal width.</div>
<ol class="lower-alpha">
<li id="li-5492"><div class="para" id="p-8085">Find the first principal component <span class="process-math">\(\uvec_1\)</span> and describe the meaning of its four components.  Which component is most significant? What can you say about the relative importance of the four measurements?</div></li>
<li id="li-5493"><div class="para" id="p-8086">When the dataset is plotted in the plane defined by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{,}\)</span> the specimens from the species iris-setosa lie on the left side of the plot.  What does this tell us about how iris-setosa differs from the other two species in the four measurements?</div></li>
<li id="li-5494"><div class="para" id="p-8087">In general, which species is closest to the ‚Äúaverage iris‚Äù?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-281"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="para" id="p-8096">This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of <a href="" class="xref" data-knowl="./knowl/fig-penguins.html" title="Figure 7.3.5">Figure¬†7.3.5</a>, as well as both male and female penguins in the dataset.</div> <figure class="figure figure-like" id="fig-penguins"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:57.8947368421053%;"><img src="external/images/lter_penguins.png" class="contained"></div>
<div class="sbspanel top" style="width:36.8421052631579%;"><img src="external/images/culmen_depth.png" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.3.5<span class="period">.</span></span><span class="space"> </span>Artwork by <a class="external" href="https://github.com/allisonhorst/palmerpenguins/blob/master/README.md" target="_blank">@allison_horst</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-9" id="fn-9"><sup>‚Äâ4‚Äâ</sup></a></figcaption></figure> <div class="para logical" id="p-8097">
<div class="para">Evaluating the next cell will load and display the data.  The meaning of the culmen length and width is contained in the illustration on the right of <a href="" class="xref" data-knowl="./knowl/fig-penguins.html" title="Figure 7.3.5">Figure¬†7.3.5</a>. <pre class="ptx-sagecell sagecell-sage" id="sage-275"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_penguins.py', globals())
df.T
</script></pre> This dataset is a bit different from others that we‚Äôve looked at because the scale of the measurements is significantly different.  For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation.  The result is stored in the <span class="process-math">\(4\by333\)</span> matrix <code class="code-inline tex2jax_ignore">A</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-276"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5501"><div class="para" id="p-8098">Find the covariance matrix and its eigenvalues.</div></li>
<li id="li-5502"><div class="para" id="p-8099">What fraction of the total variance is explained by the first two principal components?</div></li>
<li id="li-5503"><div class="para" id="p-8100">Construct the <span class="process-math">\(2\by333\)</span> matrix <span class="process-math">\(B\)</span> whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot. <pre class="ptx-sagecell sagecell-sage" id="sage-277"><script type="text/x-sage">pca_plot(B)
</script></pre>
</div></li>
<li id="li-5504"><div class="para" id="p-8101">Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?</div></li>
<li id="li-5505"><div class="para" id="p-8102">What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?</div></li>
<li id="li-5506"><div class="para" id="p-8103">You can plot just the males or females using the following cell. <pre class="ptx-sagecell sagecell-sage" id="sage-278"><script type="text/x-sage">pca_plot(B, sex='female')
</script></pre> What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?</div></li>
</ol>
</div></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-6"><div class="fn"><code class="code-inline tex2jax_ignore">archive.ics.uci.edu</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-7"><div class="fn"><code class="code-inline tex2jax_ignore">gvsu.edu/s/21D</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-8"><div class="fn"><code class="code-inline tex2jax_ignore">gvsu.edu/s/21E</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-9"><div class="fn"><code class="code-inline tex2jax_ignore">gvsu.edu/s/21G</code></div></div>
</div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-quadratic-forms.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon">^</span><span class="name">Top</span></a><a class="next-button button" href="sec-svd-intro.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
