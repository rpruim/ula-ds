var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "colophon-1",
  "level": "1",
  "url": "colophon-1.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " 2023 Data Science Edition     copyright   Understanding Linear Algebra was written and is maintained by David Austin. It can be found at    Understading Linear Algebra for Data Science is a derived version. Differences to the original include the use of Python (with the numpy and sympy packages) rather than Sage, and some additional emphasis on data science topics.  "
},
{
  "id": "preface-1",
  "level": "1",
  "url": "preface-1.html",
  "type": "Preface",
  "number": "",
  "title": "Our goals -- Preface to David Austin’s original edition",
  "body": " Our goals -- Preface to David Austin's original edition  This is a textbook for a first-year course in linear algebra. Of course, there are already many fine linear algebra textbooks available. Even if you are reading this one online for free, you should know that there are other free linear algebra textbooks available online. You have choices! So why would you choose this one?  This book arises from my belief that linear algebra, as presented in a traditional undergraduate curriculum, has for too long lived in the shadow of calculus. Many mathematics programs currently require their students to complete at least three semesters of calculus, but only one semester of linear algebra, which often has two semesters of calculus as a prerequisite.  In addition, what linear algebra students encounter is frequently presented in an overly formal way that does not fully represent the range of linear algebraic thinking. Indeed, many programs use a first course in linear algebra as an introduction to proofs course. While linear algebra provides an excellent introduction to mathematical reasoning, to only emphasize this aspect of the subject neglects some important student needs.  Of course, linear algebra is based on a set of abstract principles. However, these principles underlie an astonishingly wide range of technology that shapes our society in profound ways. The interplay between these principles and their applications provides a unique opportunity for working with students. First, the consideration of significant real-world problems grounds abstract mathematical thinking in a way that deepens students' understanding. At the same time, the variety of ways in which these abstract principles may be applied clearly demonstrates for students the power of mathematical abstraction. Linear algebra empowers students to experience what the physicist Eugene Wigner called the unreasonable effectiveness of mathematics in the natural sciences, an aspect of mathematics that is both fundamental and mysterious.  Neglecting this experience does not serve our students well. For instance, only about 15% of current mathematics majors will go on to attend graduate school. The remainder are headed for careers that will ask them to use their mathematical training in business, industry, and government. What do these careers look like? Right now, data analytics and data mining, computer graphics, software development, finance, and operations research. These careers depend much more on linear algebra than calculus. In addition to helping students appreciate the profound changes that mathematics has brought to our society, more training in linear algebra will help our students participate in the inevitable developments yet to come.  These thoughts are not uniquely mine nor are they particularly new. The Linear Algebra Curriculum Study Group, a broadly-based group of mathematicians and mathematics educators funded by the National Science Foundation, formed to improve the teaching of linear algebra. In their final report, they wrote   There is a growing concern that the linear algebra curriculum at many schools does not adequately address the needs of the students it attempts to serve. In recent years, demand for linear algebra training has risen in client disciplines such as engineering, computer science, operations research, economics, and statistics. At the same time, hardware and software improvements in computer science have raised the power of linear algebra to solve problems that are orders of magnitude greater than dreamed possible a few decades ago. Yet in many courses, the importance of linear algebra in applied fields is not communicated to students, and the influence of the computer is not felt in the classroom, in the selection of topics covered or in the mode of presentation. Furthermore, an overemphasis on abstraction may overwhelm beginning students to the point where they leave the course with little understanding or mastery of the basic concepts they may need in later courses and their careers.   Furthermore, among their recommendations is this:   We believe that a first course in linear algebra should be taught in a way that reflects its new role as a scientific tool. This implies less emphasis on abstraction and more emphasis on problem solving and motivating applications.   What may be surprising is that this was written in 1993; that is, before the introduction of Google's PageRank algorithm, before Pixar's Toy Story , and before the ascendence of what we call Big Data made these statements only more relevant.  With these thoughts in mind, the aim of this book is to facilitate a fuller, richer experience of linear algebra for all students, which informs the following decisions.   This book is written without the assumption that students have taken a calculus course. In making this decision, I hope that students will gain a more authentic experience of mathematics through linear algebra at an earlier stage of their academic careers.  Indeed, a common barrier to student success in calculus is its relatively high prerequisite tower culminating in a course often called Precalculus . By contrast, linear algebra begins with much simpler assumptions about our students' preparation: the expressions studied are linear so that may be manipulated using only the four basic arithmetic operations.  The most common explanation I hear for requiring calculus as a prerequisite for linear algebra is that calculus develops in students a beneficial mathematical maturity. Given persistent student struggles with calculus, however, it seems just as reasonable to develop students' abilities to reason mathematically through linear algebra.   The text includes a number of significant applications of important linear algebraic concepts, such as computer animation, the JPEG compression algorithm, and Google's PageRank algorithm. In my experience, students find these applications more authentic and compelling than typical applications presented in a calculus class. These applications also provide a strong justification for mathematical abstraction, which often frustrates beginning students.    Each section begins with a preview activity and includes a number of activities that can be used to facilitate active learning in a classroom. By now, active learning's effectiveness in helping students develop a deep understanding of important mathematical concepts is beyond dispute. The activities here are designed to reinforce ideas already encountered, motivate the need for upcoming ideas, and help students recognize various manifestations of simple underlying themes. As much as possible, students are asked to develop new ideas and take ownership of them.   The activities emphasize a broad range of mathematical thinking. Rather than providing the traditional cycle of Definition-Theorem-Proof, Understanding Linear Algebra aims to develop an appreciation of ideas as arising in response to a need that students perceive. Working much as research mathematicians do, students are asked to consider examples that illustrate the importance of key concepts so that definitions arise as natural labels used to identify these concepts. Again using examples as motivation, students are asked to reason mathematically and explain general phenomena they observe, which are then recorded as theorems and propositions. It is not, however, the intention of this book to develop students' formal proof-writing abilities.   There are frequent embedded Sage cells that help develop students' computational proficiency. The impact that linear algebra is having on our society is inextricably tied to the phenomenal increase in computing power witnessed in the last half-century. Indeed, Carl Cowen, former president of the Mathematical Association of America, has said, No serious application of linear algebra happens without a computer. This means that an understanding of linear algebra is not complete without an understanding of how important quantities are practically computed.   The text aims to leverage geometric intuition to enhance algebraic thinking. In spite of the fact that it may be difficult to visualize , many linear algebraic concepts may be effectively illustrated in or and the resulting intuition applied more generally. Indeed, this useful interplay between geometry and algebra illustrates another mysterious mathematical connection between seemingly disparate areas.    I hope that Understanding Linear Algebra is useful for you, whether you are a student taking a linear algebra class, someone just interested in self-study, or an instructor seeking out some ideas to use with your students. I would be more than happy to hear your feedback.  "
},
{
  "id": "preface-2",
  "level": "1",
  "url": "preface-2.html",
  "type": "Preface",
  "number": "",
  "title": "What’s different in the data science edition?",
  "body": " What's different in the data science edition?  David Austin generously made the source of his Understanding Linear Algebra publicly available and licensed so that others could make modifications. The Data Science Edition is one example. You may be wondering how the two compare and which is the right version for you. Here is a summary of differences between the two editions.   Python instead of Sage  This is probably the biggest and most noticeable change. This edition uses Python and important Python libraries for data science, like numpy , scipy , and pandas . Data scientists are much more likely to encounter these tools than they are to use Sage.    A different starting point  The original version motivates linear algebra through an attempt to solve linear systems of equations. That is an important application, but why do we want to solve linear systems in the first place? And how is that related to data science?  This edition begins by exploring how vectors and matrices can be used to store data and emphasizing three complementary ways to think about vectors, which we might call the data science perspective, the geometry perspective, and the mathematical perspective. Right from the start, we want to develop skill in moving among these three ways of thinking.    Some additional data science applications  Linear models make their first appearance much earlier -- as an example of linear combinations and matrix-vector multiplication, even though least squares methods don't come until later. Tensors (multi-dimensional arrays) are introduced immediately after matrices, in part because they help demystify how numpy approaches things like aggregation with matrices, and in part because many data science applications make use of higher-dimensional arrays.  Over time, I hope to add additional data science examples.    A different approach to the dot product  The traditional approach begins with a computational form and links this to geometry using the Law of Cosines, a result that is unfamiliar to many students. The result is that the computational and geometric forms can seem unrelated, and projections can feel a bit mysterious.  Here we motivate dot products from a desire to compute projections and establish the connection to the computational formala without citing the Law of Cosines. Our hope is that this will make these topics feel more natural and intuitive. In the end, we will end in the same place, relying on the important interplay between three ways of thinking: data-centric, geometric, and mathematical.     "
},
{
  "id": "sec-vectors",
  "level": "1",
  "url": "sec-vectors.html",
  "type": "Section",
  "number": "1.1",
  "title": "Vectors",
  "body": " Vectors   Scalars, vectors, and matrices will play an inportant part in using linear algebra in data science, so it will be good for us to become acquainted with them and how to work with them in Python. This section will focus on scalars and vectors. Section will focus on matrices.    Three ways to think about vectors  There are three ways we can think about vectors, and each one will play a role in our study of linear algebra.   The data\/computer science perspective.  Probably the simplest way to think about a vector is as an ordered list of numbers. This is how we will usually represent vectors in the computer. This is also what allows us to connect linear algebra to data. Suppose we have selling prices of 100 homes, for example. We could put those 100 numbers into an ordered list and call it a vector of home prices. That allows us to think about all 100 home prices together as one unit -- the selling price of (these particular 100) homes. This kind of vector can be called a variable vector since in contains all the information about one variable (in this case price). A richer data set could have many variables, each one stored as a vector. variable vector   Alternatively, we may consider a single home and record several different pieces of information about it (selling price, number of bedrooms, size of the lot, etc.). All the information about one home can be stored as a vector. This kind of variable can be called a sample vector or a case vector , since the vector represents one observational unit. A large data set will consist of many case vectors. case vector   When working with data, our list of numbers might be very long. We could have hundreds, or thousands, or millions, or more cases or variables that we are keeping track of. The good news is that vectors behave the same way no matter how long the list of numbers. This means we can learn a lot from smaller examples, which are easier to work with (and visualize).  So let's consider some smaller examples. and are both vectors. The entries in a vector are called its components or entries .  component  of a vector    entry  of a vector    dimension  of a vector  Since the vector has two components, we say that it is a two-dimensional vector; in the same way, the vector is a four-dimensional vector.  Because the numbers we are using are \"real\" numbers (numbers of any size that can be represented as fractions or decimals), we will denote the set of all -dimensional vectors by . Consequently, if is a 3-dimensional vector, we say that is in (written ). Note that this will require us to recode non-numeric data using numbers.     -dimensional vector space      The physics\/geometry perspective.  This perpective helps us \"see\" linear algebra. From this perspective, a vector is something that has magnitude (length) and direction . In 2 or 3 dimensions (actually, in 1 dimension as well), vectors can eaily be visualized as arrows. Figure shows an example in 2 dimensions.   A graphical representation of the vector in 2 dimensions.      If you are familar with this perspecitve -- perhaps from physics where it is used frequently to describe position, velocity, force, etc. -- note that in linear algebra it is almost always the case that all vectors (arrows) will begin at the same position, called the origin . origin   The geometric and data prespectives are linked using Cartesian coordinates . If you look back at Figure , you can see that we have placed a grid in the background. This makes it easy to see that our vector stretches 2 units in the horizontal (or x) direction and 1 unit in the vertical (y) direction. So Figure is a representation of the vector .  Any vector in , , or can be represented in a similar way. Vectors in live on a line, vectors in live on a plane, and vectors in live in 3-d \"space\", which we can represent reasonably well with perspective drawings in the plane. Beyond three diminsions, its more difficult to visualize vectors, but we will use the intuitions we develop in low dimension to help us understand what is happening in higher dimensions.    The mathematical perspective.  The mathematical perspetive is a little bit different again. From this perspective, what is important are the properties that vectors have, and anything sharing these properties (arrows, lists of numbers, or something else entirely) can be considered a vector. Actually, we don't think about individual vectors so much as a collection of vectors, called a vector space . For each positive integer , is an example of a vector space. Indeed, those will be the vector spaces of primary concern in data science. But there are many other vector spaces. vector space   Vector spaces allow for two important operations: scalar multiplication and vector addition , and these operations must satisfy a few basic properties for the vectors to be considered a vector space. \"Scalar\" is just a fancy word for a number. In scalar multiplication, we multiply a scalar (number) and a vector . The result is a vector (in the same vector space). For vector addition, we add two vectors and obtain another vector (in the same vector space). scalar   Before enumerating the properties these operations must have, let's define these operations using the data and geometric perspectives.       Vector operations: scalar multiplication and vector addition.   Scalar Multiplication  Let's begin with the data perspective. In this perspective a vector is just a list of numbers like . To multiply by the scalar , we simply multiply each component by 3. So . The same thing works for any vector and any scalar -- we simply multiply each component of the vector by the scalar to get a new list of numbers.  Now consider what happens to the arrow representing when we multiply by the scalar . Each component of the vector will be three times as large, so the vector will stretch three times as far horizontally and also three times as far vertically. In other words, the vector will be three times as long, but still point in the same direction as . See Figure . This explains the name \"scalar\" multiplication: This type of multiplication rescales the vector. Note that if the scalar is negative, then the arrow will point in the oposite direction.   Scalar multiplication \"scales\" a vector by stretching or shrinking it. If the scalar is positive, then the scalar product will point in the same direction as . If the scalar is negative, then the scalar product will point in the opposite direction.     A note on notation  It is important to be able to distinguish vectors from scalars. You may have noticed that we have been using bold (lower case) letters to represent vectors. This will be our standard notation to help us distinguish vectors from scalars, which we will not put inbold.  You may have seen other notation for vectors, including notation that places a small arrow above variables representing vectors. That notation works poorly in statistical contexts where that space is often used for other things (e.g., ).  When writing by hand, bold is tricky. We suggest placing a \"wiggle\" below letters that would be typeset in bold.     Vector addition  From the data perspective, vector addition works componentwise . That is, we obtain the sum by adding corresponding values in the two vectors. For example For this operation to make sense, it is important that both vectors have the same number of components (i.e., that they live in the same vector space). In our example, the vectors are in .  To represent the sum geometrically, we imagine walking from the origin for distance and direction specified by . From there, we continue our walk using the distance and direction prescribed by , after which we arrive at the sum . This is illustrated on the left of where the tail of is placed on the tip of .   Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.        commutative Alternatively, we may construct the parallelogram with and as two sides. The sum is then the diagonal of the parallelogram, as illustrated on the right of . Just as was the case for the list of numbers approach, it doesn't matter which vector comes first and which second, we end up in the same place either way. That is for any vectors and that live in the same vector space. If you like fancy words, you can say that vector addition is commutative .   Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .   Sketch the vectors on this grid.        What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.    Sketch the vectors using .   Sketch the vectors on this grid.        Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.   Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and . linear combination of vectors     Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .       Solutions to this preview activity are given in the text below.    If you worked through the activty above, you should have discovered that the set of vectors having the form is a line through the origin. To form the set of vectors , we can begin with the vector and add multiples of . Geometrically, this means that we begin from the tip of and move in a direction parallel to . The effect is to translate the line by the vector , as shown in .   The set of vectors form a line.      At times, it will be useful for us to think of vectors and points interchangeably. That is, we may wish to think of the vector as describing the point and vice-versa. When we say that the vectors having the form form a line, we really mean that the tips of the vectors all lie on the line passing through and parallel to . When thinking about many (possibly infinitely many) vectors, it is often easier to visualize them as points rather than as arrows.    Mathematical properties of vector operations   Even though these vector operations are new, it is straightforward to check that the following 8 properties hold for all vectors and all scalars .   Associativity   .    Commutativity   .    Distributivity 1   .    Distributivity 2   .    Zero vector  There is a vector (denoted ) such that .    Vector Inverse  For every vector , there is an inverse vector such that .    Compatability of scalar operations   .    Scalar Identity   .      These are exactly the properties that define a vector space mathematically. For scalars we can use any field (think: a number system where addition, subtraction, mulitiplication, and division are defined and have the properites you are familiar with from the real numbers). The vectors can be anything as long as the operations satisfy the eight properties listed above.     The (Euclidean) length of a vector  length of a vector norm  magnitude of a vector norm  norm of a vector  The length , also called the magnitude or norm , of a vector is denoted . We can compute the length of a vector using using the Pythagorean Theorem. For example, for a vector , we have as is illustrated in .   Calculating the length of a vector in using the Pythagorean Theorem.    More generally, we have   Vector length  norm of a vector    the (Euclidean) norm of a vector    The Euclidean length of a vector is given by     We have emphasized that this is the Euclidean length. That is likely the length you are most familiar with since it corresponds to our usual notions of geometry. But there are other norms or lengths that have important applications as well.    Summary     We can think of vectors in three different ways: (1) as a list of numbers (the data perspective); (2) as magnitude and direction, i.e., arrows (the geomtric perspective), or (3) as a set of objects with certain properties (the mathematical perspective). The ability to move betwen these three ways of thinking is one of the keys to understanding linear algebra.   We are primarily interested in , the space of vectors consisting of real numbers in the data perspective. But there are many other vector spaces.    There are two operations we can perform with vectors: scalar multiplication and vector addition. Both of these operations are easily computed from a list of numbers but also have geometric meaning.    The Euclidean length (or norm) of a vector can be computed using the Pythagorean. The square of length is the sum of the squares of the vector's components.       "
},
{
  "id": "p-32",
  "level": "2",
  "url": "sec-vectors.html#p-32",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "variable vector sample vector case vector components entries magnitude direction origin Cartesian coordinates vector space scalar multiplication vector addition "
},
{
  "id": "fig-scalar-mult-0",
  "level": "2",
  "url": "sec-vectors.html#fig-scalar-mult-0",
  "type": "Figure",
  "number": "1.1.2",
  "title": "",
  "body": " Scalar multiplication \"scales\" a vector by stretching or shrinking it. If the scalar is positive, then the scalar product will point in the same direction as . If the scalar is negative, then the scalar product will point in the opposite direction.   "
},
{
  "id": "remark-1",
  "level": "2",
  "url": "sec-vectors.html#remark-1",
  "type": "Remark",
  "number": "1.1.3",
  "title": "A note on notation.",
  "body": " A note on notation  It is important to be able to distinguish vectors from scalars. You may have noticed that we have been using bold (lower case) letters to represent vectors. This will be our standard notation to help us distinguish vectors from scalars, which we will not put inbold.  You may have seen other notation for vectors, including notation that places a small arrow above variables representing vectors. That notation works poorly in statistical contexts where that space is often used for other things (e.g., ).  When writing by hand, bold is tricky. We suggest placing a \"wiggle\" below letters that would be typeset in bold.  "
},
{
  "id": "p-53",
  "level": "2",
  "url": "sec-vectors.html#p-53",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "componentwise "
},
{
  "id": "fig-vector-sum",
  "level": "2",
  "url": "sec-vectors.html#fig-vector-sum",
  "type": "Figure",
  "number": "1.1.4",
  "title": "",
  "body": " Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.      "
},
{
  "id": "p-55",
  "level": "2",
  "url": "sec-vectors.html#p-55",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "commutative "
},
{
  "id": "exploration-1",
  "level": "2",
  "url": "sec-vectors.html#exploration-1",
  "type": "Preview Activity",
  "number": "1.1.1",
  "title": "Scalar Multiplication and Vector Addition.",
  "body": " Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .   Sketch the vectors on this grid.        What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.    Sketch the vectors using .   Sketch the vectors on this grid.        Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.   Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and . linear combination of vectors     Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .       Solutions to this preview activity are given in the text below.   "
},
{
  "id": "fig-parametric-line",
  "level": "2",
  "url": "sec-vectors.html#fig-parametric-line",
  "type": "Figure",
  "number": "1.1.8",
  "title": "",
  "body": " The set of vectors form a line.     "
},
{
  "id": "observation-1",
  "level": "2",
  "url": "sec-vectors.html#observation-1",
  "type": "Observation",
  "number": "1.1.9",
  "title": "",
  "body": " Even though these vector operations are new, it is straightforward to check that the following 8 properties hold for all vectors and all scalars .   Associativity   .    Commutativity   .    Distributivity 1   .    Distributivity 2   .    Zero vector  There is a vector (denoted ) such that .    Vector Inverse  For every vector , there is an inverse vector such that .    Compatability of scalar operations   .    Scalar Identity   .     "
},
{
  "id": "p-76",
  "level": "2",
  "url": "sec-vectors.html#p-76",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "field "
},
{
  "id": "p-77",
  "level": "2",
  "url": "sec-vectors.html#p-77",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "length magnitude norm "
},
{
  "id": "fig-pythagorean-length",
  "level": "2",
  "url": "sec-vectors.html#fig-pythagorean-length",
  "type": "Figure",
  "number": "1.1.10",
  "title": "",
  "body": " Calculating the length of a vector in using the Pythagorean Theorem.   "
},
{
  "id": "def-vector-length",
  "level": "2",
  "url": "sec-vectors.html#def-vector-length",
  "type": "Definition",
  "number": "1.1.11",
  "title": "Vector length.",
  "body": " Vector length  norm of a vector    the (Euclidean) norm of a vector    The Euclidean length of a vector is given by    "
},
{
  "id": "p-81",
  "level": "2",
  "url": "sec-vectors.html#p-81",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "length "
},
{
  "id": "sec-vectors-in-python",
  "level": "1",
  "url": "sec-vectors-in-python.html",
  "type": "Section",
  "number": "1.2",
  "title": "Vectors in Python",
  "body": " Vectors in Python   Now that we have been introduced to vectors, it's time to learn how to work with vectors in Python.    Introduction to Python  SageMath is a free open-source mathematics software system licensed under the GPL. It builds on top of many existing open-source packages: NumPy, SciPy, matplotlib, Sympy, Maxima, GAP, FLINT, R and many more. Access their combined power through a common, Python-based language or directly via interfaces or wrappers. In this book we will use SageMath as a way to embed Python cells that you can edit and execute as you learn linear algebra.  Here is a Python cell containing a command that asks Python to multiply 5 and 3. You may execute the command by pressing the Evaluate button.   Of course, you can run this sort of Python code in other Python environments as well, including in a Jupyter notebook, in an RMarkdown or Quarto document, in posit RStudio, in a Sage cell server, etc. The Python cells in this text provide a handy way to try things out as you are reading, but you should adopt one of these other platforms for saving work.  Throughout the text, we will introduce new Python commands that allow us to explore linear algebra concepts. These commands are collected and summarized in the reference found in Appendix A .   Basic Python commands      Python uses the standard operators + , - , * , and \/ for the usual arithmetic operations. ** is used for exponentiation. By entering text in the cell below, ask Python to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this cell?   Notice that we don't see any output from the cell we just evaluated. In order to see the results, we use print() :     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     We can run other sorts of Python code in these Python cells as well. For example, you may be familiar with Python for loops:            3 + 4*(2**4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cell prints the integers from 0 to 9.           3 + 4*(2**4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cell prints the integers from 0 to 9.         numpy vectors  Since a vector is just a list of numbers, if you know Python, you might expect that we will create vectors using Python lists. This is partially correct. But Python lists don't know about our operations (scalar multiplication and vector addition). n-dimensional array In order to get vectors that know about these operations, we will convert our Python lists into ndarrays (n-dimensional arrays) using the Python package numpy .   Importing NumPy  NumPy is a python module that is heavily used in data science. In order to use NumPy, we must first import it like this:   This loads numpy and declares that we will refer to it as np (the standard convention) in subsequent code. In an interactive environment, this cell will be auto-evaluated and NumPy will be available in all the cells in this section. In your own work, don't forget to import NumPy (or any other packages you need).   In numpy parlance, a vector is a 1-dimensional array. Vectors are created using np.array() , which is provided with a list of numbers. Python can perform scalar multiplication and vector addition. We define a vector using np.array() ; then * and + denote scalar multiplication and vector addition.   Especially when we work with vectors with many components, we will often want to have Python tell us how many components there are. We can use .shape to find out how many components are in one of our vectors. Give it a try. Type print(v.shape) in the code block above and re-evaluate. shape, of a vector    Caution: Dimension vs Dimension  dimension   numpy uses the term dimension differently from how we used that term in . In NumPy parlance, all of the vectors we have seen are 1-dimensional arrays because there is just one column of numbers. This will make more sense when we see 2-dimensional arrays (matrices) and higher dimensional arrays (tensors). The number returned by .shape is the other kind of dimension -- the number of components in the vector.   For low dimensional vectors, manually typing in each component is reasonable. But ofen we will work with vectors with many components. So we will need some better ways to create these vectors. Here are some examples showing how to create structured or random vectors.  numpy.random.normal()    numpy.arange()     Indexing in Python is 0-based. So the first component of the vector v is v[0] . If v has 100 components, then the last component is v[99] . We can also use v[-1] to access the last component. More generally, v[-k] is the same as v[v.shape - k] when k is positive (so -k is negative). indexing, Python vectors   Once we have an ndarray, we can compute various summaries of it. The sum, mean, and standard deviation are commonly used summaries. For output purposes, it is often useful to round output values.  numpy.mean()    numpy.std()    numpy.sum()    numpy.round()      Linked Python cells  In interactive versions of this text, all the cells in a section are linked, so you can refer to objects created in one cell while working in another cell, provided you have already executed the code in the first cell.     In the cell below, we can refer to the vector v and w from above -- if we have executed the cell where they were defined (and have not subsequently changed their values).       Vector length  norm of a vector  We can compute the (Euclidean) length of a vector two ways. The first takes advantage of python comprehension. The second uses the numpy.linalg.norm() function.  List comprehension is a powerful feature of Python that lets us compute with lists, including numpy arrays. Here is a simple exmaple that creates a new list that contains the squares of the elements in the original list.   Notice that the code above computes all the terms we need to add when computing the length of a vector. So we can combine with numpy.sum() to get the square of the length. Also notice that squares is a Python list, not an numpy array. We can see that from the way that numbers and squares are printed. (The are no commas in the dispaly of numbers .) You can confirm the types using type() .   Computing the length of a vector is common enough that numpy includes a function for this: numpy.linalg.norm() . numpy.linalg.norm() norm of a vector It can compute several different norms, but the default is the familiar Euclidean norm.     Plotting vectors  We can use matplotlib.pyplot.quiver() to visualize 2-d and 3-d vectors. Below are two examples showing the sum of two vectors. vector plotting  matplotlib.plt.quiver()  quiver() matplotlib.plt.quiver() For simplicity, we plot each vector as a separate command, but it is possible to plot multiple arrows all at once. This explain the name quiver .  The arguments to quiver include the coordiantes of the starting point for the arrow and coordinates specifying the amount of change in each direction. This makes it easy to plot vectors originating at the origin or originating at some other place, as is useful to illustrating the sum. Notice the use of * in these examples. *origin , for example, is shorthand for origin[0], origin[1] (in the 2-d case). By default, for the 2-d plots, Matplotlib will rescale the lengths of the vectors. Additional arugments are included here so that the vectors are plotted on their natural scale.     NumPy documentation  Official numpy documentation can be found at . You may also find it handy to use which provides documentation on many languages (including Python) and packages (including numpy ) and provides a powerful mechanism for searching within documentation.  A brief visual introduction to numpy can be found at .    Matplotlib documentation  Official matplotlib documentation can be found at . Matplotlib documentation is also avilable via .    Figures in this book  The figures in this book were also created in Python using PiScript , a system for creating mathematical diagrams created by Bill Casselman.     "
},
{
  "id": "activity-1",
  "level": "2",
  "url": "sec-vectors-in-python.html#activity-1",
  "type": "Activity",
  "number": "1.2.1",
  "title": "Basic Python commands.",
  "body": " Basic Python commands      Python uses the standard operators + , - , * , and \/ for the usual arithmetic operations. ** is used for exponentiation. By entering text in the cell below, ask Python to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this cell?   Notice that we don't see any output from the cell we just evaluated. In order to see the results, we use print() :     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     We can run other sorts of Python code in these Python cells as well. For example, you may be familiar with Python for loops:            3 + 4*(2**4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cell prints the integers from 0 to 9.           3 + 4*(2**4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cell prints the integers from 0 to 9.      "
},
{
  "id": "p-108",
  "level": "2",
  "url": "sec-vectors-in-python.html#p-108",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "ndarrays "
},
{
  "id": "note-1",
  "level": "2",
  "url": "sec-vectors-in-python.html#note-1",
  "type": "Note",
  "number": "1.2.1",
  "title": "Importing NumPy.",
  "body": " Importing NumPy  NumPy is a python module that is heavily used in data science. In order to use NumPy, we must first import it like this:   This loads numpy and declares that we will refer to it as np (the standard convention) in subsequent code. In an interactive environment, this cell will be auto-evaluated and NumPy will be available in all the cells in this section. In your own work, don't forget to import NumPy (or any other packages you need).  "
},
{
  "id": "warning-dim-v-dim",
  "level": "2",
  "url": "sec-vectors-in-python.html#warning-dim-v-dim",
  "type": "Warning",
  "number": "1.2.2",
  "title": "Caution: Dimension vs Dimension.",
  "body": " Caution: Dimension vs Dimension  dimension   numpy uses the term dimension differently from how we used that term in . In NumPy parlance, all of the vectors we have seen are 1-dimensional arrays because there is just one column of numbers. This will make more sense when we see 2-dimensional arrays (matrices) and higher dimensional arrays (tensors). The number returned by .shape is the other kind of dimension -- the number of components in the vector.  "
},
{
  "id": "note-2",
  "level": "2",
  "url": "sec-vectors-in-python.html#note-2",
  "type": "Note",
  "number": "1.2.3",
  "title": "Linked Python cells.",
  "body": " Linked Python cells  In interactive versions of this text, all the cells in a section are linked, so you can refer to objects created in one cell while working in another cell, provided you have already executed the code in the first cell.  "
},
{
  "id": "example-linked-cells",
  "level": "2",
  "url": "sec-vectors-in-python.html#example-linked-cells",
  "type": "Example",
  "number": "1.2.4",
  "title": "",
  "body": "  In the cell below, we can refer to the vector v and w from above -- if we have executed the cell where they were defined (and have not subsequently changed their values).    "
},
{
  "id": "note-3",
  "level": "2",
  "url": "sec-vectors-in-python.html#note-3",
  "type": "Note",
  "number": "1.2.5",
  "title": "Figures in this book.",
  "body": " Figures in this book  The figures in this book were also created in Python using PiScript , a system for creating mathematical diagrams created by Bill Casselman.  "
},
{
  "id": "sec-linear-combos-of-vectors",
  "level": "1",
  "url": "sec-linear-combos-of-vectors.html",
  "type": "Section",
  "number": "1.3",
  "title": "Linear combinations of vectors",
  "body": " Linear combinations of vectors    linear combination  weights   The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.    More generally, a linear combination of \"things\" has the form . That is, each term is the product of a scalar and a thing, and finitely many such terms are added together.   If you have seen linear models, then this should look familiar to you. A linear model can be expressed as a linear combination of data variables. We typically use to denote the scalars, which are the parameters of the linear model. And we write the data variables as . So our linear model looks like And the fitted model as      In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .       The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?    What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.    Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?    Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?    Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?    Can the vector be expressed as a linear combination of and ? What about the vector ? For each write down a system of equations that can be used to answer the question. If you know how to solve such a system, go ahead. But we will be learning a general method for this soon.    Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ? (Hint: Think geometrically.)          The linear combinations lie on the line defined by .     .    They lie on the line through parallel to .    Yes, with weights .    Yes, with weights and .    This can be done by writing the appropriate linear system for the weights.    No, any two-dimensional vector can be expressed as a linear combination of and .          When we vary with , the linear combination moves along the line defined by .    When and , we find     When and is allowed to vary, the linear combinations lie on the line through parallel to .    If the weights and , then the linear combination is the vector .    If the weights and , then the linear combination is the vector . You could determine this by a little trial and error, or using a little algebra. is equivalent to We will learn general methods for solving systems of equations like this soon. But in this case, the second equation tells us that . Plugging that into the first equation gives , so and .    Our two systems of equations are and We don't have a 0 on the right hand side this time, so the algebra is a little messier. But both systems have a solution. Stay tuned to learn how to systematically solve such systems.    Every two-dimensional vector can be written as a linear combination of and . The number chooses a line parallel to , and chooses a point along that line. Since we can pick any line parallel to and any point along that line, this allows us to get to any point in the plane. Use the interactive app above to illustrate this. Note that this only works because and are not parllel.  We will soon learn an algebraic way to arrive at the same conclusion.       This activity illustrates how linear combinations are constructed geometrically: the linear combination is found by walking along a total of times followed by walking along a total of times. When one of the weights is held constant while the other varies, the vector moves along a line.    Suppose we have vectors and . Suppose we want to know whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system  This means that is a linear combination of and if and only if this linear system of equations has a solution. linear system of equations      demonstrates a connection between linear combinations of vectors and linear systems of equations: Asking whether a vector is a linear combination of vectors is equivalent to asking whether an associated linear system has a solution. In we will study linear systems more carefully, giving a careful definition and demonstrating how we can determine the solutions to any such system of equations.   Linear combinations and linear systems      Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and .     Now let's go in the other direction. Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"              We find vectors            We find the linear system      We find vectors     This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: augmented matrix  matrix augmented             Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on that line. In addition, a vector that is not on the line, say , should not be expressible as a linear combination of and .       Summary  This section has introduced linear combinations of vectors and their connection to linear systems.   Given a set of vectors and a set of scalars we call weights, we can create a linear combination using scalar multiplication and vector addition.    To know whether one vector can be formed as a linear combination of some other vectors is equivalent to solving a (linear) system of equations.     We will learn more about both linear combinations and linear systems in what follows.     Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .     Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .     Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as the case vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.     "
},
{
  "id": "definition-2",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#definition-2",
  "type": "Definition",
  "number": "1.3.1",
  "title": "",
  "body": " linear combination  weights   The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.   "
},
{
  "id": "remark-2",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#remark-2",
  "type": "Remark",
  "number": "1.3.2",
  "title": "",
  "body": " If you have seen linear models, then this should look familiar to you. A linear model can be expressed as a linear combination of data variables. We typically use to denote the scalars, which are the parameters of the linear model. And we write the data variables as . So our linear model looks like And the fitted model as   "
},
{
  "id": "activity-2",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#activity-2",
  "type": "Activity",
  "number": "1.3.1",
  "title": "",
  "body": "  In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .       The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?    What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.    Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?    Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?    Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?    Can the vector be expressed as a linear combination of and ? What about the vector ? For each write down a system of equations that can be used to answer the question. If you know how to solve such a system, go ahead. But we will be learning a general method for this soon.    Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ? (Hint: Think geometrically.)          The linear combinations lie on the line defined by .     .    They lie on the line through parallel to .    Yes, with weights .    Yes, with weights and .    This can be done by writing the appropriate linear system for the weights.    No, any two-dimensional vector can be expressed as a linear combination of and .          When we vary with , the linear combination moves along the line defined by .    When and , we find     When and is allowed to vary, the linear combinations lie on the line through parallel to .    If the weights and , then the linear combination is the vector .    If the weights and , then the linear combination is the vector . You could determine this by a little trial and error, or using a little algebra. is equivalent to We will learn general methods for solving systems of equations like this soon. But in this case, the second equation tells us that . Plugging that into the first equation gives , so and .    Our two systems of equations are and We don't have a 0 on the right hand side this time, so the algebra is a little messier. But both systems have a solution. Stay tuned to learn how to systematically solve such systems.    Every two-dimensional vector can be written as a linear combination of and . The number chooses a line parallel to , and chooses a point along that line. Since we can pick any line parallel to and any point along that line, this allows us to get to any point in the plane. Use the interactive app above to illustrate this. Note that this only works because and are not parllel.  We will soon learn an algebraic way to arrive at the same conclusion.      "
},
{
  "id": "example-linear-systems-preview",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#example-linear-systems-preview",
  "type": "Example",
  "number": "1.3.4",
  "title": "",
  "body": "  Suppose we have vectors and . Suppose we want to know whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system  This means that is a linear combination of and if and only if this linear system of equations has a solution. linear system of equations    "
},
{
  "id": "p-164",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#p-164",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "linear system "
},
{
  "id": "activity-3",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#activity-3",
  "type": "Activity",
  "number": "1.3.2",
  "title": "Linear combinations and linear systems.",
  "body": " Linear combinations and linear systems      Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and .     Now let's go in the other direction. Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"              We find vectors            We find the linear system      We find vectors     This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: augmented matrix  matrix augmented          "
},
{
  "id": "example-3",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#example-3",
  "type": "Example",
  "number": "1.3.5",
  "title": "",
  "body": "  Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on that line. In addition, a vector that is not on the line, say , should not be expressible as a linear combination of and .    "
},
{
  "id": "exercise-1",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#exercise-1",
  "type": "Exercise",
  "number": "1.3.2.1",
  "title": "",
  "body": " Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .   "
},
{
  "id": "exercise-2",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#exercise-2",
  "type": "Exercise",
  "number": "1.3.2.2",
  "title": "",
  "body": " Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .   "
},
{
  "id": "exercise-3",
  "level": "2",
  "url": "sec-linear-combos-of-vectors.html#exercise-3",
  "type": "Exercise",
  "number": "1.3.2.3",
  "title": "",
  "body": " Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as the case vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.   "
},
{
  "id": "sec-matrices",
  "level": "1",
  "url": "sec-matrices.html",
  "type": "Section",
  "number": "1.4",
  "title": "Matrices",
  "body": " Matrices    Our goal in this section is to introduce matrices, operations on matrices, and their connection to linear combinations and linear systems.    Matrices and their uses  A matrix is a rectangular array of numbers. shape  of a matrix  matrix shape If we say that the shape of a matrix is , we mean that it has rows and columns. For instance, the shape of the matrix below is : .  There are many applications that use matrices, here are just a few.   Rectangular data  The typical organization places observational units (usually referred to a as a case or a subject when the observational units are people) in rows and the variables recorded for each of them in columns. observational unit  case  subject So if we have numerical variables recorded for each of observational units, we can store this information in an matrix. For categorical variables, we will need to convert the values into numbers.    Images  Digital images can also be stored in matrices. For a black and white image, each value in the matrix represents the color on a gray scale with 0 representing black, 255 representing white, and values between these extremes representing different shades of gray.  For color images, we can use several matrices. For an RGB image, for example, there will be a matrix for red, another for green, and a third for blue. The resulting images combine values from each of these layers . See for a discussion of how to combine multiple layers into a single object, called a tensor.    Coefficient matrices for linear systems  All the important information in a linear system of equations like can be stored in a matrix of coefficients for the left hand side and a vector for the right hand side.   We could even combine both the matrix and vector into a single matrix by adding the vector as an additional column. We will call such a a matrix an augmented matrix because we are augmenting the original matrix by addiging an additional column (or columns): augmented matrix  matrix augmented    As we will soon see, this matrix representation will allow us to solve these systems computationally.     Recalling our connection between linear combinations of vectors and linear systems of equations, the linear system above will have a solution if and only if is a linear combination of the vectors   As shorthand, we can write this augmented matrix replacing each column with its vector representation: . Using this shorthand, we can restate the connection between linear combinations of vectors and linear systems of equations as the following proposition.    The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix has a solution. A solution to this linear system gives weights such that .      Scalar multiplication and addition of matrices  It is often useful to think of the columns of a matrix as vectors. For instance, the matrix . may be represented as where . In this way, we see that the matrix is equivalent to an ordered list of 4 vectors in .   matrix, addition  matrix, scalar multiplication This means that we may define scalar multiplication and matrix addition operations using the corresponding column-wise vector operations. For instance,   In other words,   scalar multiplication with a matrix multiplies every element of the matrix by the scalar, and    two matrices of the same shape can be added componentwise.      Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ? In this situation, is it always the case that ?   The matrix , which we call the identity matrix , is the matrix whose entries are zero except for the main diagonal entries, all of which are 1. (The main diagonal is the diagonal going from top left to bottom right. These are the elements for which the row index and column index are the same.) For instance,  matrix  identity    identity matrix     -dimensional idenity matrix   . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same and both sums will also be the same.  The shape of must be .               The shapes must be the same.  The shape of must be . In this case and will be the same because for any numbers and , , so we get the same result for each element of the sum matrices.        As this preview activity shows, the operations of scalar multiplication and addition of matrices are natural extensions of their vector counterparts. Some care, however, is required when adding matrices. Since we need the same number of vectors to add and since those vectors must be of the same dimension, two matrices must have the same shape if we wish to form their sum.    Matrix-vector multiplication and linear combinations  A more important operation will be matrix multiplication as it allows us to compactly express linear systems. We now introduce the product of a matrix and a vector with an example.   Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.    More generally, we have the following definition.   Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.    The next activity explores some properties of matrix-vector multiplication.   Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Express as a linear system of equations.          The dimension of must three, and the dimension of must be four.   .   .   .  Carry out the multiplication and simplify to obtain a linear system.      We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have  is the unique solution.      Multiplication of a matrix and a vector is defined as a linear combination of the columns of . However, there is another way to compute such a product. Let's look at our previous example and focus on the first row of the product. .  To find the first component of the product, we consider the first row of the matrix. We then multiply the first entry in that row by the first component of the vector, the second entry by the second component of the vector, and so on, and add the results. In this way, we see that the third component of the product would be obtained from the third row of the matrix by computing .  You are encouraged to evaluate the product of using this new method and compare the result to what you found while completing that activity.    Matrix-vector multiplication and linear systems  The connections among matrix-vector multiplication, linear combinations of vectors, and linear systems of equations are so important that they are worth repeating. So far, we have begun with a matrix and a vector and formed their product . We would now like to turn this around: Suppose we know and but don't know , can find a vector such that ? This question will naturally lead back to linear systems.  To see the connection between the matrix equation and linear systems, let's write the matrix in terms of its columns and in terms of its components. .  We know that the matrix product forms a linear combination of the columns of . Therefore, the equation is merely a compact way of writing the equation for the weights : . We have seen this equation before: Remember that says that the solutions of this equation are the same as the solutions to the linear system whose augmented matrix is .  This gives us three different ways of looking at the situation.    If and , then the following statements are equivalent.    .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix       When the matrix , we will frequently write and say that the matrix is augmented by the vector .  The equation gives a notationally compact way to write a linear system. Moreover, this notation will allow us to focus on important features of the system that determine its solutions.     Matrices in Python   We may ask Python to create matrices by using nested lists (a list of lists). But just as we did with vectors, we will convert these nested lists to a numpy n-dimensional array using np.array() .   The shape of the resulting array tells us the number of rows and columns in the matrix.     We may ask Python to create the matrix by entering       Python can find the product of a matrix and vector using the @ operator. For example,      Use Python to evaluate the product from of the .     In Python, define the matrix and vectors .    What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = np.array([[1, 2, 0, -1], [2, 4, -3, -2], [-1, -2, 6, 1]]) v = np.array([3, 1, -1, 1]) A@v   We define A = np.array([[-2, 0, 3],[1, 4, 2]]) zero = np.array([0, 0]) v = np.array([-2, 3]) w = np.array([1, 2])    .   .        This activity demonstrates several general properties satisfied by matrix multiplication that we record here.   Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .       Python practices  Here are some practices that you may find helpful when working with matrices in Python.   Break the matrix entries across lines, one for each row, for better readability by pressing Enter between rows. A = np.arry([[ 1, 2, -1, 0], [-3, 0, 4, 3 ])    For small matrices, you can print your original matrix to check that you have entered it correctly. For larger matrices, you should at least confirm that that shape matches your expectation.    You may want to also print labels or to include a dividing line to separate different parts of your output. A = np.array([ 1, 2, 2, 2]) print (A) print (\"---------\") print(\"shape of A =\", A.shape)         Matrix-matrix products  In this section, we have developed some algebraic operations on matrices and seen how they can be used to simplifying our description of linear systems. We now introduce a final operation, the product of two matrices, that will become important when we study linear transformations in .   Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining       Given the matrices , we have . That is, we multiply by each of the column vectors in to obtain the column vectors of the product.     It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .  This can be visualized by writing out the shapes of the two matrices, one after the other. The \"middle\" values must match and \"cancel\", leaving the number of rows in the first matrix and the number of columns of the second matrix as the shape of the procuct. In , we have      Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product by hand.  Python can multiply matrices using the @ operator. Define the matrices and in the Python cell below and check your work by computing .    Are we able to form the matrix product ? If so, use the Python cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.    Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and .   If , is it necessarily true that ?   Again, with real numbers, we know that if , then either or . Define and compute .   If , is it necessarily true that either or ?        We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .     It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .     Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.     This activity demonstrated some general properties about products of matrices, which mirror some properties about operations with real numbers.   Properties of Matrix-matrix Multiplication  If , , and are matrices such that the following operations are defined, it follows that  Associativity:   .  Distributivity:   .   .      At the same time, there are a few properties that hold for real numbers that do not hold for matrices.   Caution  The following properties hold for real numbers but not for matrices.  Commutativity:  It is not generally true that .   Cancellation:  It is not generally true that implies that .   Zero divisors:  It is not generally true that implies that either or .        Some special types of matrices  identity matrix  We have already encountered the -dimensional identity matrix  . This square matrix has ones along the main diagonal (where the row index and column index are the same) and zeros everywhere else. For example,   More generally, a matrix is called diagonal if the only non-zero values are along the main diagonal. Here are some example of diagonal matrices. diagonal matrix  matrix  diagonal  Note that   A diagonal matrix is allowed to have 0's along the digonal.    A diagonal matrix might not be square, but in that case all elements in the \"extra\" rows or columns must be 0's. Square diagonal matrices are especially important, however.     A square matrix for which for every and is called a symmetric matrix . symmetric matrix  matrix symmetric Here are some examples.   The symmetry of a symmetric matrix is symmetry across the main diagonal: If we exchange the rows and the columns, we get the same matrix. The matrix obtained by exchanging rows and columns of a matrix is called the transpose of , written . For formally, the transpose satisfies for every and . transpose of a matrix  matrix  transpose    transpose of matrix  For example, .  Square diagonal matrices are always symmetric and symmetric matrices are always square. So we will use the terms square diagonal matrix and symmetric diagonal matrix interchangeably.    Summary  In this section, we have found an especially simple way to express linear systems using matrix multiplication.   If is an matrix and an -dimensional vector, then is the linear combination of the columns of using the components of as weights. The vector is -dimensional.   The solutions to the equation are the same as the solutions to the linear system using coefficients from and . We can represent all these coefficients in a single autmented matrix : matrix augmented  augmented matrix  .  If is an matrix and is an matrix, we can form the product . will be an matrix. The columns of are the products of and the columns of .         Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .    Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .        Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .     "
},
{
  "id": "p-216",
  "level": "2",
  "url": "sec-matrices.html#p-216",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "shape "
},
{
  "id": "p-217",
  "level": "2",
  "url": "sec-matrices.html#p-217",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "observational units case subject RGB layers linear system of equations augmented matrix "
},
{
  "id": "prop-system-comb",
  "level": "2",
  "url": "sec-matrices.html#prop-system-comb",
  "type": "Proposition",
  "number": "1.4.1",
  "title": "",
  "body": "  The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix has a solution. A solution to this linear system gives weights such that .   "
},
{
  "id": "expl-simple-matrix-operations",
  "level": "2",
  "url": "sec-matrices.html#expl-simple-matrix-operations",
  "type": "Preview Activity",
  "number": "1.4.1",
  "title": "Matrix operations.",
  "body": " Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ? In this situation, is it always the case that ?   The matrix , which we call the identity matrix , is the matrix whose entries are zero except for the main diagonal entries, all of which are 1. (The main diagonal is the diagonal going from top left to bottom right. These are the elements for which the row index and column index are the same.) For instance,  matrix  identity    identity matrix     -dimensional idenity matrix   . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same and both sums will also be the same.  The shape of must be .               The shapes must be the same.  The shape of must be . In this case and will be the same because for any numbers and , , so we get the same result for each element of the sum matrices.       "
},
{
  "id": "example-4",
  "level": "2",
  "url": "sec-matrices.html#example-4",
  "type": "Example",
  "number": "1.4.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.   "
},
{
  "id": "def-matrix-times-vector",
  "level": "2",
  "url": "sec-matrices.html#def-matrix-times-vector",
  "type": "Definition",
  "number": "1.4.3",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.   "
},
{
  "id": "example-matrix-vector-product",
  "level": "2",
  "url": "sec-matrices.html#example-matrix-vector-product",
  "type": "Activity",
  "number": "1.4.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Express as a linear system of equations.          The dimension of must three, and the dimension of must be four.   .   .   .  Carry out the multiplication and simplify to obtain a linear system.      We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have  is the unique solution.     "
},
{
  "id": "prop-matrix-eq-solution",
  "level": "2",
  "url": "sec-matrices.html#prop-matrix-eq-solution",
  "type": "Proposition",
  "number": "1.4.4",
  "title": "",
  "body": "  If and , then the following statements are equivalent.    .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix      "
},
{
  "id": "example-first-matrix",
  "level": "2",
  "url": "sec-matrices.html#example-first-matrix",
  "type": "Example",
  "number": "1.4.5",
  "title": "",
  "body": " We may ask Python to create the matrix by entering   "
},
{
  "id": "activity-5",
  "level": "2",
  "url": "sec-matrices.html#activity-5",
  "type": "Activity",
  "number": "1.4.3",
  "title": "",
  "body": "  Python can find the product of a matrix and vector using the @ operator. For example,      Use Python to evaluate the product from of the .     In Python, define the matrix and vectors .    What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = np.array([[1, 2, 0, -1], [2, 4, -3, -2], [-1, -2, 6, 1]]) v = np.array([3, 1, -1, 1]) A@v   We define A = np.array([[-2, 0, 3],[1, 4, 2]]) zero = np.array([0, 0]) v = np.array([-2, 3]) w = np.array([1, 2])    .   .       "
},
{
  "id": "prop-matrix-mult-prop",
  "level": "2",
  "url": "sec-matrices.html#prop-matrix-mult-prop",
  "type": "Proposition",
  "number": "1.4.6",
  "title": "Linearity of matrix multiplication.",
  "body": " Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .     "
},
{
  "id": "definition-4",
  "level": "2",
  "url": "sec-matrices.html#definition-4",
  "type": "Definition",
  "number": "1.4.7",
  "title": "Matrix-matrix multiplication.",
  "body": " Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining    "
},
{
  "id": "example-matrix-product",
  "level": "2",
  "url": "sec-matrices.html#example-matrix-product",
  "type": "Example",
  "number": "1.4.8",
  "title": "",
  "body": "  Given the matrices , we have . That is, we multiply by each of the column vectors in to obtain the column vectors of the product.   "
},
{
  "id": "observation-2",
  "level": "2",
  "url": "sec-matrices.html#observation-2",
  "type": "Observation",
  "number": "1.4.9",
  "title": "",
  "body": " It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .  This can be visualized by writing out the shapes of the two matrices, one after the other. The \"middle\" values must match and \"cancel\", leaving the number of rows in the first matrix and the number of columns of the second matrix as the shape of the procuct. In , we have   "
},
{
  "id": "activity-6",
  "level": "2",
  "url": "sec-matrices.html#activity-6",
  "type": "Activity",
  "number": "1.4.4",
  "title": "",
  "body": "  Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product by hand.  Python can multiply matrices using the @ operator. Define the matrices and in the Python cell below and check your work by computing .    Are we able to form the matrix product ? If so, use the Python cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.    Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and .   If , is it necessarily true that ?   Again, with real numbers, we know that if , then either or . Define and compute .   If , is it necessarily true that either or ?        We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .     It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .     Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.    "
},
{
  "id": "p-367",
  "level": "2",
  "url": "sec-matrices.html#p-367",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "identity matrix main diagonal "
},
{
  "id": "p-368",
  "level": "2",
  "url": "sec-matrices.html#p-368",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diagonal "
},
{
  "id": "p-371",
  "level": "2",
  "url": "sec-matrices.html#p-371",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "symmetric matrix "
},
{
  "id": "p-372",
  "level": "2",
  "url": "sec-matrices.html#p-372",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "transpose "
},
{
  "id": "p-373",
  "level": "2",
  "url": "sec-matrices.html#p-373",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "square diagonal matrix symmetric diagonal matrix "
},
{
  "id": "p-374",
  "level": "2",
  "url": "sec-matrices.html#p-374",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "autmented matrix "
},
{
  "id": "exercise-4",
  "level": "2",
  "url": "sec-matrices.html#exercise-4",
  "type": "Exercise",
  "number": "1.4.9.1",
  "title": "",
  "body": " Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .  "
},
{
  "id": "exercise-5",
  "level": "2",
  "url": "sec-matrices.html#exercise-5",
  "type": "Exercise",
  "number": "1.4.9.2",
  "title": "",
  "body": " Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .      "
},
{
  "id": "exercise-6",
  "level": "2",
  "url": "sec-matrices.html#exercise-6",
  "type": "Exercise",
  "number": "1.4.9.3",
  "title": "",
  "body": " Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .   "
},
{
  "id": "sec-tensors",
  "level": "1",
  "url": "sec-tensors.html",
  "type": "Section",
  "number": "1.5",
  "title": "Tensors",
  "body": " Tensors   As we have seen, numpy treats vectors and matrices as 1-dimensional and 2-dimensional arrays (both created using np.array() ). While mathematically 2-dimensional arrays (i.e, matrices) are the most important (and vectors can be treated like matrices); computationally, there is no reason we need to stop at rows and columns. We could add additional dimensions. Higher dimensional arrays are often called tensors . tensor   There are two primary reasons for taking some time to think about tensors here. First, there are some interesting applications that use tensors. One example of this is color images. Recall that color images store a matrix of values for each color channel . So we have 3 indices into this data: vertical position (row), horizontal position (column), and color channel. If we have many images, we could store them in a 4-dimensional array, using the additional index to indicate which photo is which.  The second reason to discuss tensors is that most of the numpy code that works with vectors and matrices is designed to work with higher dimensional tensors as well. Understanding this can help demistify how some of the numpy functions work.    Tensors in NumPy  In order to use NumPy, we first load the numpy package.   Let's create a small 3-dimensional tensor. Notice that we can assign the shape (as long as the product of the items in our tuple is equal to the number values in the array).   Alternatively, we can use the reshape method of an array. reshape()    The output above expresses A as two matrices, and we can separate out those two matrices as A[0] and A[1] .   We can refer to individual numbers in an ndarray by providing the required number of indices.   The indexing is more flexible than demonstrated above. In each slot we can provide a slice of values. A slice has the format start:stop:step . If omitted, start is taken to be 0, stop is taken to be the length of the array in that dimension, and step is taken to be 1. To get all the values in some dimension, we can use : . slice, of a matrix or tensor   An interesting question is what the shape of the resulting array should be, and we have some control over whether dimensions are dropped or not. Consider the following examples. axis of an array or tensor    The first two methods drop a dimension. The latter two retain all three axes, even though one of them has only one legal index (0). It is always important to know the shape of the arrays you are working with. When in doubt, check.    An RGB image that is 400 pixels wide and 300 pixels tall might correspond to a tensor with shape (400, 300, 3) . Or perhaps (3, 300, 400) , or ... The computer doesn't really care, but we need to know which slot is being used for what in order to work with this image correctly. The usual arrangement is row, column, channel. This is called the channels-last convention . But somtimes a channels-first convention is used instead. channels-last convention   We can plot images (or other matrices that we would like to view as an image) using a number of different tools. Here we demonstrate how to do so using matplotlib . matplotlib supports a variety of image formats. For RGB images, the RGB values can be integers from 0 to 255 or floating point values between 0 and 1.   Notice that dog contains a 3-dimensional array of unsiged 8-bit integers (0 -- 255) and that the format is channels-last. We can plot individual channels in the same way.   We've added a color bar to this image to show how matplotlib is mapping the values 0 to 255 to colors. Remember, this is the red channel. So a colormap that runs from dark blue to bright yellow probably isn't the best choice. We can use a scale of reds by setting cmap=\"Reds\" like this   If you use cmap = \"Reds_r\" (give it a try), the order of the colors will be reversed and provide a better visual representation (for human eyes -- it's the same data no matter what color scheme we use for display).  You can find out more about the many colormaps that are available in matplotlib at .     The Sage Cell Server restricts internet access  Feel free to experiment with this code using other images, but note that the Sage Cell Server only allows access to images on selected websites. This list includes sites like GitHub (used here) and Wikimedia Commons.     Aggregation and Axes  We have already discussed axes in the context of slices that do or do not drop axes. Aggregation (computing a value based on a subset of values in an n-dimensional array) is another place where axes are important.   Row and column sums and means  We often want to know the sum or mean of all rows or columns of a matrix. NumPy provides two ways to compute aggregating functions like this.      Expanding an array  Sometimes it is useful to build up a matrix or other -dimensional array bit by bit or to add an additional \"layer\" (think row or column) to an existing array. For example, to create an augmented matrix, we will often append an additional column matrix to an existing matrix, increasing the number of columns by one. np.append() allows us to do this, provided our arrays (a) have the same number of dimensions and (b) compatible shapes along the desired axis.  Here's an example where we add a column onto a matrix.   Notice that np.append() does not modify its arguments. It returns a new (larger) array.  NumPy provides several other functions that can be used to create larger arrays by combining smaller arrays.    np.concetentate() takes a sequence of arrays and repeatedly appends. This is especially useful for combining three or more arrays.     np.stack() (and specialized versions np.hstack() , np.vstack() , and np.column_stack() ) concatenate along a new axis. This can be used to combine vectors into an array or arrays into a tensor. The resulting object will have a higher dimension (more axes) than the inputs.     np.block() creates a new array from a nested sequences of blocks.     np.insert() allows insertion into a slice.       In scenes were some regions are very bright and others very dark, it can be challenging to create a good exposure over the entire image. Exposure bracketing is a technique that has been used since the 1850's to help with this situation. This technique involves capturing multiple images, each with a different exposure. Later one can choose the best of the captured images or combine them in some way. In the 1850's, Gustave Le Gray printed seascape photos from two images, using one exposure for the sky and the other for the water. Modern HDR methods can combine multiple digital images in much more complex ways.  In this example we stack multiple images with different exposures and use a very simple method (averaging) to convert the stack of images into a single HDR (high dynamic range) image. high dynamic range (HDR) images This works because each images loses some information in the brightest or darkest regions due to the limits on the values (typically 0 to 255) that each pixel may have. The averages will remain in the appropriate range, but do a better job of distinguishing among the very dark and very bright regions of the image.  The images were obtained from . We will use low resolutoin images below, but you can repeat this example with higher resolution images if you like. Notice the use of np.stack() to combine the images into one 4-dimensional tensor, and the use of np.rint() and astype(int) to make sure the resulting data is again integer (the nearest integer to the average).   More sophisticated algorithms can do an even better job of creating the final image from multiple exposures of the same scene.          Broadcasting  According to the NumPy documentation,   The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.   For many operations on arrays, the shapes of the two arrays must be identical. Broadcasting allows us to relax that condition. Two arrays are said to be broadcastable to the same shape if when working from the tail (rightmost) end of the shape each dimension is compatible . Dimensions are compatible if  They are equal, or  One of them is 1, or  One of them is missing.    For example, suppose two arrays and have shapes and . Working backwards, we first compare the two 10's . They are equal, so they are compatible. Then we compare the 1 and the 5. They are not equal, but one of them is 1, so they are compatible. Finally, we compare 3 to a missing dimension. These are also compatbile.  Broadcasting will treat each array is if it had shape . To do that, each array must be \"stretched\". Broadcasting treats as if it were with . Similarly, is treated as if it were with . That is, each array is treated as if data were copied to fill in the missing dimensions. Importantly for computational efficiency, this copying does not actually happen. But conceptually, we can imagine that copying if it helps us understand how the arrays are made compatible.  The simplest form of broadcasting is when a scalar is broadcast to make it compatible with an array. In this case it is as if an array of the same size were filled entirely with that single scalar value.   Broadcasting   For each of the ten pairs formed from seven , A , B , C , and D , what result to you get when you add? See if you can figure out what the restults will be before executing the code to check.       "
},
{
  "id": "p-409",
  "level": "2",
  "url": "sec-tensors.html#p-409",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "tensors "
},
{
  "id": "p-417",
  "level": "2",
  "url": "sec-tensors.html#p-417",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "slice "
},
{
  "id": "example-7",
  "level": "2",
  "url": "sec-tensors.html#example-7",
  "type": "Example",
  "number": "1.5.1",
  "title": "",
  "body": "  An RGB image that is 400 pixels wide and 300 pixels tall might correspond to a tensor with shape (400, 300, 3) . Or perhaps (3, 300, 400) , or ... The computer doesn't really care, but we need to know which slot is being used for what in order to work with this image correctly. The usual arrangement is row, column, channel. This is called the channels-last convention . But somtimes a channels-first convention is used instead. channels-last convention   We can plot images (or other matrices that we would like to view as an image) using a number of different tools. Here we demonstrate how to do so using matplotlib . matplotlib supports a variety of image formats. For RGB images, the RGB values can be integers from 0 to 255 or floating point values between 0 and 1.   Notice that dog contains a 3-dimensional array of unsiged 8-bit integers (0 -- 255) and that the format is channels-last. We can plot individual channels in the same way.   We've added a color bar to this image to show how matplotlib is mapping the values 0 to 255 to colors. Remember, this is the red channel. So a colormap that runs from dark blue to bright yellow probably isn't the best choice. We can use a scale of reds by setting cmap=\"Reds\" like this   If you use cmap = \"Reds_r\" (give it a try), the order of the colors will be reversed and provide a better visual representation (for human eyes -- it's the same data no matter what color scheme we use for display).  You can find out more about the many colormaps that are available in matplotlib at .   "
},
{
  "id": "note-4",
  "level": "2",
  "url": "sec-tensors.html#note-4",
  "type": "Note",
  "number": "1.5.2",
  "title": "The Sage Cell Server restricts internet access.",
  "body": " The Sage Cell Server restricts internet access  Feel free to experiment with this code using other images, but note that the Sage Cell Server only allows access to images on selected websites. This list includes sites like GitHub (used here) and Wikimedia Commons.  "
},
{
  "id": "example-8",
  "level": "2",
  "url": "sec-tensors.html#example-8",
  "type": "Example",
  "number": "1.5.3",
  "title": "Row and column sums and means.",
  "body": " Row and column sums and means  We often want to know the sum or mean of all rows or columns of a matrix. NumPy provides two ways to compute aggregating functions like this.   "
},
{
  "id": "example-9",
  "level": "2",
  "url": "sec-tensors.html#example-9",
  "type": "Example",
  "number": "1.5.4",
  "title": "",
  "body": "  In scenes were some regions are very bright and others very dark, it can be challenging to create a good exposure over the entire image. Exposure bracketing is a technique that has been used since the 1850's to help with this situation. This technique involves capturing multiple images, each with a different exposure. Later one can choose the best of the captured images or combine them in some way. In the 1850's, Gustave Le Gray printed seascape photos from two images, using one exposure for the sky and the other for the water. Modern HDR methods can combine multiple digital images in much more complex ways.  In this example we stack multiple images with different exposures and use a very simple method (averaging) to convert the stack of images into a single HDR (high dynamic range) image. high dynamic range (HDR) images This works because each images loses some information in the brightest or darkest regions due to the limits on the values (typically 0 to 255) that each pixel may have. The averages will remain in the appropriate range, but do a better job of distinguishing among the very dark and very bright regions of the image.  The images were obtained from . We will use low resolutoin images below, but you can repeat this example with higher resolution images if you like. Notice the use of np.stack() to combine the images into one 4-dimensional tensor, and the use of np.rint() and astype(int) to make sure the resulting data is again integer (the nearest integer to the average).   More sophisticated algorithms can do an even better job of creating the final image from multiple exposures of the same scene.       "
},
{
  "id": "p-444",
  "level": "2",
  "url": "sec-tensors.html#p-444",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "broadcastable compatible "
},
{
  "id": "example-10",
  "level": "2",
  "url": "sec-tensors.html#example-10",
  "type": "Example",
  "number": "1.5.5",
  "title": "Broadcasting.",
  "body": " Broadcasting   For each of the ten pairs formed from seven , A , B , C , and D , what result to you get when you add? See if you can figure out what the restults will be before executing the code to check.     "
},
{
  "id": "sec-expect",
  "level": "1",
  "url": "sec-expect.html",
  "type": "Section",
  "number": "2.1",
  "title": "What can we expect",
  "body": " What can we expect   At its heart, the subject of linear algebra is about linear equations and, more specifically, sets of two or more linear equations. shows that there multiple ways to express this important concept notationally. And we will want to become comfortable with all of these ways, and with moving back and forth among them.  Google routinely deals with a set of trillions of equations each of which has trillions of unknowns. We will eventually understand how to deal with that kind of complexity. To begin, however, we will look at situations in which there are a small number of equations and a small number of unknowns. In spite of their relative simplicity, these smaller examples are rich enough to demonstrate some fundamental concepts that will motivate much of our exploration.    Some simple examples    In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.          The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.         The examples in this introductory activity demonstrate several possible outcomes for the solutions to a set of linear equations. Notice that we are interested in points that satisfy each equation in the set and that these are seen as intersection points of the lines. Similar to the examples considered in the activity, three types of outcomes are seen in .   Three possible graphs for sets of linear equations in two unknowns.    In this figure, we see that  With a single equation, there are infinitely many points satisfying that equation.  Adding a second equation adds another condition we place on the points resulting in a single point that satisfies both equations.  Adding a third equation adds a third condition on the points , and there is no point that satisfies all three equations.    Generally speaking, a single equation will have many solutions, in fact, infinitely many. As we add equations, we add conditions which lead, in a sense we will make precise later, to a smaller number of solutions. Eventually, we have too many equations and find there are no points that satisfy all of them.  This example illustrates a general principle to which we will frequently return.   Solutions to sets of linear equations  Given a set of linear equations, there are either:   infinitely many points,    exactly one point, or    no points  that satisfy every equation in the set.   Notice that we can see a bit more. In , we are looking at equations in two unknowns. Here we see that   One equation has infinitely solutions.    Two equations have exactly one solution.   Three equations have no solutions.    It seems reasonable to wonder if the number of solutions depends on whether the number of equations is less than, equal to, or greater than the number of unknowns. Of course, one of the examples in the activity shows that there are exceptions to this simple rule, as seen in . For instance, two equations in two unknowns may correspond to parallel lines so that the set of equations has no solutions. It may also happen that a set of three equations in two unknowns has a single solution. However, it seems safe to think that the more equations we have, the smaller the set of solutions will usually be.   A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.       Let's also consider some examples of equations having three unknowns, which we call , , and . Just as solutions to linear equations in two unknowns formed straight lines, solutions to linear equations in three unknowns form planes.  When we consider an equation in three unknowns graphically, we need to add a third coordinate axis, as shown in .   Coordinate systems in two and three dimensions.       As shown in , a linear equation in two unknowns, such as , is a line while a linear equation in three unknowns, such as , is a plane.   The solutions to the equation in two dimensions and in three.       In three unknowns, the set of solutions to one linear equation forms a plane. The set of solutions to a pair of linear equations is seen graphically as the intersection of the two planes. As in , we typically expect this intersection to be a line.   A single plane and the intersection of two planes.       When we add a third equation, we are looking for the intersection of three planes, which we expect to form a point, as in the left of . However, in certain special cases, it may happen that there are no solutions, as seen on the right.   Two examples showing the intersections of three planes.         This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.        Systems of linear equations  Now that we have seen some simple examples, let's agree on some terminology to help us think more precisely about sets of equations.  First, we considered a linear equation having the form . It will be convenient for us to rewrite this so that all the unknowns are on one side of the equation: . More generally, the equation of a line can always be expressed in the form which gives us the flexibility to describe all lines. For instance, vertical lines, such as , may be represented in this form.  Notice that each term on the left is the product of a constant and the first power of an unknown. In the future, we will want to consider equations having many more unknowns, which we will sometimes denote as . This leads to the following definition:   linear equation   A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a linear system of equations or a linear system  linear system , we mean a set of linear equations written in a common set of unknowns.    For instance, is an example of a linear system.   solution to a linear system   A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.    For instance, we earlier considered the linear system To check that is a solution, we verify that the following equations are true.    solution space   We call the set of all solutions the solution space of the linear system.     Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.         There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found two solutions to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.          Summary  The point of this section is to build some intuition about the behavior of solutions to linear systems through consideration of some simple examples. We will develop a deeper and more precise understanding of these phenomena in our future explorations.   A linear equation is one that may be written in the form .  A linear system is a set of linear equations and a solution is a set of values assigned to the unknowns that make each equation true.   We came to expect that a linear system has either infinitely many solutions, exactly one solution, or no solutions.   When we add more equations to a system, the solution space usually seems to become smaller.      Give a brief explanation of your response to the following questions.  Suppose we have a system of 500 linear equations in 10 unknowns. What would be a reasonable guess about which of the three possibilities for the set of solutions holds?  Suppose we have a system of 10 linear equations in 500 unknowns. What would be a reasonable guess about which of the three possibilities for the set of solutions holds?     Which of the following equations are linear? Please provide a justification for your response.   .   .   .      Consider the system of linear equations:    Is a solution?  Is a solution?  Is a solution?  Can you find a solution in which ?  Do you think there are other solutions? Please explain your response.     This exercise concerns systems of equations having three unknowns. In this case, we know that the solution space of a single equation is a plane. If it helps with visualization, consider using index cards to represent planes.    Is it possible that the solution space of a system of four equations in three unknowns is a line? Either sketch an example or give a reason why it can't happen.    What would you usually expect for the solution space to a system of four equations in three unknowns?    Is it possible that there are no solutions to a system of two equations in three unknowns? Either sketch an example or give a reason why it can't happen.    Is it possible that there is exactly one solution to a system of two equations in three unknowns? Either sketch an example or give a reason why it can't happen.      "
},
{
  "id": "activity-7",
  "level": "2",
  "url": "sec-expect.html#activity-7",
  "type": "Activity",
  "number": "2.1.1",
  "title": "",
  "body": "  In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.          The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.        "
},
{
  "id": "fig-three-possibilities",
  "level": "2",
  "url": "sec-expect.html#fig-three-possibilities",
  "type": "Figure",
  "number": "2.1.1",
  "title": "",
  "body": " Three possible graphs for sets of linear equations in two unknowns.   "
},
{
  "id": "solution-exceptions",
  "level": "2",
  "url": "sec-expect.html#solution-exceptions",
  "type": "Figure",
  "number": "2.1.2",
  "title": "",
  "body": " A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.      "
},
{
  "id": "fig-coordinates",
  "level": "2",
  "url": "sec-expect.html#fig-coordinates",
  "type": "Figure",
  "number": "2.1.3",
  "title": "",
  "body": " Coordinate systems in two and three dimensions.      "
},
{
  "id": "fig-plane-z0",
  "level": "2",
  "url": "sec-expect.html#fig-plane-z0",
  "type": "Figure",
  "number": "2.1.4",
  "title": "",
  "body": " The solutions to the equation in two dimensions and in three.      "
},
{
  "id": "fig-two-planes",
  "level": "2",
  "url": "sec-expect.html#fig-two-planes",
  "type": "Figure",
  "number": "2.1.5",
  "title": "",
  "body": " A single plane and the intersection of two planes.      "
},
{
  "id": "fig-three-planes",
  "level": "2",
  "url": "sec-expect.html#fig-three-planes",
  "type": "Figure",
  "number": "2.1.6",
  "title": "",
  "body": " Two examples showing the intersections of three planes.      "
},
{
  "id": "activity-8",
  "level": "2",
  "url": "sec-expect.html#activity-8",
  "type": "Activity",
  "number": "2.1.2",
  "title": "",
  "body": "  This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.     "
},
{
  "id": "definition-5",
  "level": "2",
  "url": "sec-expect.html#definition-5",
  "type": "Definition",
  "number": "2.1.7",
  "title": "",
  "body": " linear equation   A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a linear system of equations or a linear system  linear system , we mean a set of linear equations written in a common set of unknowns.   "
},
{
  "id": "definition-6",
  "level": "2",
  "url": "sec-expect.html#definition-6",
  "type": "Definition",
  "number": "2.1.8",
  "title": "",
  "body": " solution to a linear system   A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.   "
},
{
  "id": "definition-7",
  "level": "2",
  "url": "sec-expect.html#definition-7",
  "type": "Definition",
  "number": "2.1.9",
  "title": "",
  "body": " solution space   We call the set of all solutions the solution space of the linear system.   "
},
{
  "id": "activity-9",
  "level": "2",
  "url": "sec-expect.html#activity-9",
  "type": "Activity",
  "number": "2.1.3",
  "title": "Linear equations and their solutions.",
  "body": " Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.         There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found two solutions to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.       "
},
{
  "id": "exercise-7",
  "level": "2",
  "url": "sec-expect.html#exercise-7",
  "type": "Exercise",
  "number": "2.1.4.1",
  "title": "",
  "body": " Give a brief explanation of your response to the following questions.  Suppose we have a system of 500 linear equations in 10 unknowns. What would be a reasonable guess about which of the three possibilities for the set of solutions holds?  Suppose we have a system of 10 linear equations in 500 unknowns. What would be a reasonable guess about which of the three possibilities for the set of solutions holds?   "
},
{
  "id": "exercise-8",
  "level": "2",
  "url": "sec-expect.html#exercise-8",
  "type": "Exercise",
  "number": "2.1.4.2",
  "title": "",
  "body": " Which of the following equations are linear? Please provide a justification for your response.   .   .   .    "
},
{
  "id": "exercise-9",
  "level": "2",
  "url": "sec-expect.html#exercise-9",
  "type": "Exercise",
  "number": "2.1.4.3",
  "title": "",
  "body": " Consider the system of linear equations:    Is a solution?  Is a solution?  Is a solution?  Can you find a solution in which ?  Do you think there are other solutions? Please explain your response.   "
},
{
  "id": "exercise-10",
  "level": "2",
  "url": "sec-expect.html#exercise-10",
  "type": "Exercise",
  "number": "2.1.4.4",
  "title": "",
  "body": " This exercise concerns systems of equations having three unknowns. In this case, we know that the solution space of a single equation is a plane. If it helps with visualization, consider using index cards to represent planes.    Is it possible that the solution space of a system of four equations in three unknowns is a line? Either sketch an example or give a reason why it can't happen.    What would you usually expect for the solution space to a system of four equations in three unknowns?    Is it possible that there are no solutions to a system of two equations in three unknowns? Either sketch an example or give a reason why it can't happen.    Is it possible that there is exactly one solution to a system of two equations in three unknowns? Either sketch an example or give a reason why it can't happen.    "
},
{
  "id": "sec-finding-solutions",
  "level": "1",
  "url": "sec-finding-solutions.html",
  "type": "Section",
  "number": "2.2",
  "title": "Finding solutions to linear systems",
  "body": " Finding solutions to linear systems   In the previous section, we looked at systems of linear equations from a graphical perspective. Since the equations had only two or three variables, we could study the solution spaces as the intersections of lines and planes.  Because we will eventually consider systems with many equations and many variables, this graphical approach will not generally be a useful strategy. Instead, we will approach this problem algebraically and develop a technique to describe the solution spaces of general linear systems.    Gaussian elimination   Gaussian elimination We will develop an algorithm, which is usually called Gaussian elimination , that allows us to describe the solution space of a linear system. This algorithm plays a central role in much of what is to come.    In this activity, we will consider some simple examples that will guide us in finding a more general approach.   Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .        .   .   .  Any real number is a solution.  There are no solutions.        The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .       These examples lead to a few observations that motivate a general approach to finding solutions of linear systems.   First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system     Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .    We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain      which gives us      In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.   Based on these observations, we take note of three operations that transform a system of linear equations into a new system of equations having the same solution space. These are called the three elementary row operations . elementary row operation Our goal is to create a new system whose solution space is the same as the original system's and may be easily described.  Elemetary Row Operations    Scaling  ( )   We can multiply one equation by a nonzero number. For instance, has the same set of solutions as or .     Interchange  ( )  Interchanging equations will not change the set of solutions. For instance, has the same set of solutions as      Replacement  ( )  As we saw above, we may multiply one equation by a real number and add it to another equation. We call this process replacement .       Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.   We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation. ( )        Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third. ( )        Scale the second equation by multiplying it by . ( )       Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution. ( )        After scaling the third equation by , we have found the value for . ( )        We eliminate from the second equation by multiplying the third equation by -1 and adding to the second. ( )        Finally, multiply the second equation by -2 and add to the first to obtain ( )       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .    One could find the same result by applying a different sequence of replacement and scaling operations. However, we chose this particular sequence guided by our desire to first transform the system into a triangular one. To do this, we eliminated the first variable from all but one equation and then proceeded to the next variables working left to right. Once we had a triangular system, we used back substitution moving through the variables right to left.  We call this process Gaussian elimination and note that it is our primary tool for solving systems of linear equations.   Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions space. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                  There is a single solution .  There are infinitely many solutions.  There are no solutions.       Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.       Augmented matrices  After performing Gaussian elimination a few times, you probably noticed that you spent most of the time concentrating on the coefficients and simply recorded the variables as place holders. Based on this observation, we will introduce a shorthand description of linear systems.   augmented matrix When writing a linear system, we always write the variables in the same order in each equation. We then construct an augmented matrix by simply omitting the variables and recording the numerical data in a rectangular array. For instance, the system of equations below has the following augmented matrix          The vertical line reminds us where the equals signs appear in the equations. Entries in the matrix to the left of the vertical line correspond to coefficients of the equations. We sometimes choose to focus only on the coefficients of the system in which case we write the coefficient matrix as coefficient matrix    The three operations we perform on systems of equations translate naturally into operations on matrices. For instance, the replacement operation that multiplies the first equation by 2 and adds it to the second may be performed by multiplying the first row of the augmented matrix by 2 and adding it to the second row:    .   The symbol between the matrices indicates that the two matrices are related by a sequence of scaling, interchange, and replacement operations. Since these operations act on the rows of the matrices, we say that the matrices are row equivalent . Notice that the linear systems corresponding to two row equivalent augmented matrices have the same solution space. row equivalent    Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to . Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to . Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number. .   How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to . Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.      The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.       Reduced row echelon form  There is a special class of matrices whose form makes it especially easy to describe the solution space of the corresponding linear system. As we describe the properties of this class of matrices, it may be helpful to consider an example, such as the following matrix. Once again, an asterisk ( ) represents an element that can be any real number. .   reduced row echelon form   We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .    We have been intentionally vague about whether the matrix we are considering is an augmented matrix corresponding to a linear system or a coefficient matrix since we will consider both possibilities in the future.   Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.    .    .    .    .    .       The row equivalent reduced row echelon form is , and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is . There are infinitely many solutions.  The row equivalent reduced row echelon form is . This system has the single solution .       Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. . We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. . Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .     The examples in the indicate that there is a sequence of row operations that transforms any matrix into one in reduced row echelon form. Moreover, the conditions that define reduced row echelon matrices guarantee that this matrix is unique.   For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.   Once we have this reduced row echelon matrix, we may describe the set of solutions to the corresponding linear system with relative ease.   Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as . From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix . The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix . Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.        Solving matrix equations  Recall from that any linear system of equations is equivalent to a matrix equation of the form . The augmented matrix for this system of equations is So Gaussian elimination can be used to find solutions to this type of matrix equation.    Find all solutions to each of the following equations.                              .     .    No solutions.    Infinitely many solutions: . Notice that when , we get just as we saw above.         Summary  We saw several important concepts in this chapter.  We can describe the solution space to a linear system by transforming it into a new linear system having the same solution space through a sequence of scaling , interchange , and replacement operations.  We can represent a linear system by an augmented matrix . Using scaling, interchange, and replacement operations, the augmented matrix is row equivalent to exactly one reduced row echelon matrix . The process of constructing this reduced row echelon matrix is called Gaussian elimination .  The reduced row echelon matrix allows us to easily describe the solution space of a linear system.       For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.       Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.      For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.        For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.     Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.      Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.     Identify each statement as true or false and explain your reasoning.   A linear system with 4 equations in 6 unknowns can have exactly 7 solutions.    A linear system that has a free variable has an infinite number of solutions.    Every consistent linear system has a free variable.    If the RREF of for a consistent linear system has 2 zero rows, there will be an infinite number of solutions.    If the RREF of has no zero rows, the linear system is consistent.       "
},
{
  "id": "p-571",
  "level": "2",
  "url": "sec-finding-solutions.html#p-571",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gaussian elimination "
},
{
  "id": "exploration-3",
  "level": "2",
  "url": "sec-finding-solutions.html#exploration-3",
  "type": "Preview Activity",
  "number": "2.2.1",
  "title": "",
  "body": "  In this activity, we will consider some simple examples that will guide us in finding a more general approach.   Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .        .   .   .  Any real number is a solution.  There are no solutions.        The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .      "
},
{
  "id": "observation-3",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-3",
  "type": "Observation",
  "number": "2.2.1",
  "title": "",
  "body": " First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system   "
},
{
  "id": "observation-4",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-4",
  "type": "Observation",
  "number": "2.2.2",
  "title": "",
  "body": " Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .  "
},
{
  "id": "observation-5",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-5",
  "type": "Observation",
  "number": "2.2.3",
  "title": "",
  "body": " We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain      which gives us      In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.  "
},
{
  "id": "p-601",
  "level": "2",
  "url": "sec-finding-solutions.html#p-601",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "elementary row operations "
},
{
  "id": "example-11",
  "level": "2",
  "url": "sec-finding-solutions.html#example-11",
  "type": "Example",
  "number": "2.2.4",
  "title": "",
  "body": "  Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.   We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation. ( )        Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third. ( )        Scale the second equation by multiplying it by . ( )       Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution. ( )        After scaling the third equation by , we have found the value for . ( )        We eliminate from the second equation by multiplying the third equation by -1 and adding to the second. ( )        Finally, multiply the second equation by -2 and add to the first to obtain ( )       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .   "
},
{
  "id": "p-623",
  "level": "2",
  "url": "sec-finding-solutions.html#p-623",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gaussian elimination "
},
{
  "id": "activity-10",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-10",
  "type": "Activity",
  "number": "2.2.2",
  "title": "Gaussian Elimination.",
  "body": " Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions space. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                  There is a single solution .  There are infinitely many solutions.  There are no solutions.       Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.    "
},
{
  "id": "p-638",
  "level": "2",
  "url": "sec-finding-solutions.html#p-638",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "augmented matrix "
},
{
  "id": "p-641",
  "level": "2",
  "url": "sec-finding-solutions.html#p-641",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "coefficient matrix "
},
{
  "id": "p-644",
  "level": "2",
  "url": "sec-finding-solutions.html#p-644",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row equivalent "
},
{
  "id": "activity-11",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-11",
  "type": "Activity",
  "number": "2.2.3",
  "title": "Augmented matrices and solution spaces.",
  "body": " Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to . Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to . Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number. .   How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to . Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.      The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.    "
},
{
  "id": "reduced-row-echelon",
  "level": "2",
  "url": "sec-finding-solutions.html#reduced-row-echelon",
  "type": "Definition",
  "number": "2.2.5",
  "title": "",
  "body": " reduced row echelon form   We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .   "
},
{
  "id": "activity-identify-rref",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-identify-rref",
  "type": "Activity",
  "number": "2.2.4",
  "title": "Identifying reduced row echelon matrices.",
  "body": " Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.    .    .    .    .    .       The row equivalent reduced row echelon form is , and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is . There are infinitely many solutions.  The row equivalent reduced row echelon form is . This system has the single solution .       Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. . We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. . Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .    "
},
{
  "id": "thm-rref-is-unique",
  "level": "2",
  "url": "sec-finding-solutions.html#thm-rref-is-unique",
  "type": "Theorem",
  "number": "2.2.6",
  "title": "",
  "body": " For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.  "
},
{
  "id": "example-12",
  "level": "2",
  "url": "sec-finding-solutions.html#example-12",
  "type": "Example",
  "number": "2.2.7",
  "title": "Describing the solution space from a reduced row echelon matrix.",
  "body": " Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as . From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix . The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix . Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.     "
},
{
  "id": "example-13",
  "level": "2",
  "url": "sec-finding-solutions.html#example-13",
  "type": "Example",
  "number": "2.2.8",
  "title": "",
  "body": "  Find all solutions to each of the following equations.                              .     .    No solutions.    Infinitely many solutions: . Notice that when , we get just as we saw above.      "
},
{
  "id": "p-708",
  "level": "2",
  "url": "sec-finding-solutions.html#p-708",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "solution space scaling interchange replacement augmented matrix reduced row echelon matrix Gaussian elimination "
},
{
  "id": "exercise-11",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-11",
  "type": "Exercise",
  "number": "2.2.6.1",
  "title": "",
  "body": " For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.   "
},
{
  "id": "exercise-12",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-12",
  "type": "Exercise",
  "number": "2.2.6.2",
  "title": "",
  "body": " Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.     "
},
{
  "id": "exercise-13",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-13",
  "type": "Exercise",
  "number": "2.2.6.3",
  "title": "",
  "body": " Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.    "
},
{
  "id": "exercise-14",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-14",
  "type": "Exercise",
  "number": "2.2.6.4",
  "title": "",
  "body": " For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.      "
},
{
  "id": "exercise-15",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-15",
  "type": "Exercise",
  "number": "2.2.6.5",
  "title": "",
  "body": " For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.   "
},
{
  "id": "exercise-16",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-16",
  "type": "Exercise",
  "number": "2.2.6.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.    "
},
{
  "id": "exercise-17",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-17",
  "type": "Exercise",
  "number": "2.2.6.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.   "
},
{
  "id": "exercise-18",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-18",
  "type": "Exercise",
  "number": "2.2.6.8",
  "title": "",
  "body": " Identify each statement as true or false and explain your reasoning.   A linear system with 4 equations in 6 unknowns can have exactly 7 solutions.    A linear system that has a free variable has an infinite number of solutions.    Every consistent linear system has a free variable.    If the RREF of for a consistent linear system has 2 zero rows, there will be an infinite number of solutions.    If the RREF of has no zero rows, the linear system is consistent.     "
},
{
  "id": "sec-python-introduction",
  "level": "1",
  "url": "sec-python-introduction.html",
  "type": "Section",
  "number": "2.3",
  "title": "Computational Linear Algebra",
  "body": " Computational Linear Algebra   Linear algebra owes its prominence as a powerful scientific tool to the ever-growing power of computers. Carl Cowen, a former president of the Mathematical Association of America, has said, No serious application of linear algebra happens without a computer. Indeed, Cowen notes that, in the 1950s, working with a system of 100 equations in 100 variables was difficult. Today, scientists (including data scientists) routinely work on problems that are vastly larger. This is only possible because of today's computing power.  For learning the principles of linear algebra, small examples in 2 or 3 dimensions are useful because (a) we can visualize things geometrically, and (b) we can see all the numbers and computations involved. But as we build our understanding and intuition with these small examples, we also want to be learning how to do linear algebra computationally so that we can work with much larger examples.  There are many computational tools that could be used. One of them, Matlab (and its open source analog, Octave), was specifically designed (and named) with matrix algebra as its primary use case. Linear algebra is important for statistics and machine learning, but so are other things, like data wrangling and visualing data and models. For this reason, langauges like R and Python are much more important in the data science community. Although linear algebra is used extensively in both languages, end users of Python are more likely to manipulate data in vector and matrix format than are users of R, where this is often hidden behind an additional layer of abstraction. For this reason, we'll focus primarily on Python in this text, with some references to how things differ in R thrown in along the way.    Reduced row echelon form in Python  When we encounter a matrix, tells us that there is exactly one reduced row echelon matrix that is row equivalent to it.  In fact, the uniqueness of this reduced row echelon matrix is what motivates us to define this particular form. When solving a system of linear equations using Gaussian elimination, there are other row equivalent matrices that reveal the structure of the solution space. The reduced row echelon matrix is simply a convenience as it is an agreement we make with one another to seek the same matrix.  An added benefit is that we can ask a computer to find reduced row echelon matrices for us. We will learn how to do this now that we have a little familiarity with Python.  In addition to numpy , we will use an additional package called sympy .    Augmented matrices in numpy  Notice that in numpy there is no way of specifying whether a matrix is an augmented matrix or a coefficient matrix, so it will be up to us to interpret our results appropriately.    Python syntax  Some common mistakes when entering a matrix in Python include   forgetting the square brackets around the list of entries,   forgetting an entry from the list or to add an extra one,   forgetting to separate the rows, and entries within a row with commas, and   forgetting the closing parenthesis (because you are so worried about getting the correct number of square brackets, perhaps).  If you see an error message, carefully proofread your input and try again.    Using Python to find row reduced echelon matrices      Enter the following matrix into Python.      Give the matrix the name by entering A = np.array([ ... ])    numpy does not have a built-in method for computing the reduced row echelon form of a matrix, so we will use another package, sympy . We may then find its reduced row echelon form by entering import sympy A = np.array([ ... ]) sympy.Matrix(A).rref() Note that we must first convert our numpy array into a sympy matrix. A common mistake is to forget the parentheses after rref .  Use Python to find the reduced row echelon form of the matrix from of this activity.     Notice that rref() returns a tuple with two elements. The first element is the reduced row echelon form of our matrix. See if you can figure out what the second element is.    Use Python to describe the solution space of the system of linear equations      Consider the two matrices:  augmentation of a matrix We say that is an augmentation of because it is obtained from by adding some more columns.  We can use np.concatenate() to augment matrices as shown below. np.concatenate()  concatenate() np.concatenate() Notice that 1-dimensional vectors must first be reshaped into 2-dimensional column matrices. Now compute the reduced row echelon forms for and . What do you notice about the relationship between the two reduced row echelon forms?     Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           np.array([[-1,-2, 2,-1], [ 2, 4,-1, 5], [ 1, 2, 0, 3]])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            np.array([[-1,-2, 2,-1], [ 2, 4,-1, 5], [ 1, 2, 0, 3]])     The reduced row echelon form of the matrix is     Python tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is This can be computed with      (0,2) tells us that the leading 1 in the row 0 is in column 0 and that the leading 1 in row 1 is in column 2. This uses 0-based indexing. We might more naturally say that the leading 1 in the first row is in the first column and the leading 1 in the second row is in the third column.       The last part of the previous activity, , demonstrates something that will be helpful for us in the future. In that activity, we started with a matrix , which we augmented by adding some columns to obtain a matrix . We then noticed that the reduced row echelon form of is itself an augmentation of the reduced row echelon form of .  To illustrate, we can consider the reduced row echelon form of the augmented matrix:   We can then determine the reduced row echelon form of the coefficient matrix by looking inside the augmented matrix.   If we trace through the steps in the Gaussian elimination algorithm carefully, we see that this is a general principle, which we now state.   Augmentation Principle  augementation principle  reduced row echelon form (RREF) augementation principle  augementation of a matrix   If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .      np.linalg.solve()   np.linalg.solve()  solve() np.linalg.solve() We mentioned that numpy (and scipy ) do not include methods to compute the reduced row echelon form of a matrix. There are other utilities in those packages that can help us acheive many of the same goals. One example is np.linalg.solve() which will find the solution to an equation provided there is a unique solution. (Otherwise it throws an error.)     Computational effort  At the beginning of this section, we indicated that linear algebra has become more prominent as computers have grown more powerful. Computers, however, still have limits. Let's consider how much effort is expended when we ask to find the reduced row echelon form of a matrix. We will measure, very roughly, the effort by the number of times the algorithm requires us to multiply or add two numbers.  We will assume that our matrix has the same number of rows as columns, which we call . We are mainly interested in the case when is very large, which is when we need to worry about how much effort is required.  Let's first consider the effort required for each of our row operations.   Scaling a row multiplies each of the entries in a row by some number, which requires operations.   Interchanging two rows requires no multiplications or additions so we won't worry about the effort required by an interchange.   A replacement requires us to multiply each entry in a row by some number, which takes operations, and then add the resulting entries to another row, which requires another operations. The total number of operations is .    Our goal is to transform a matrix to its reduced row echelon form, which looks something like this: . We roughly perform one replacement operation for every 0 entry in the reduced row echelon matrix. When is very large, most of the entries in the reduced row echelon form are 0 so we need roughly replacements. Since each replacement operation requires operations, the number of operations resulting from the needed replacements is roughly .  Each row is scaled roughly one time so there are roughly scaling operations, each of which requires operations. The number of operations due to scaling is roughly .  Therefore, the total number of operations is roughly . When is very large, the term is much smaller than the term. We therefore state that   The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .   This is a very rough measure of the effort required to find the reduced row echelon form; a more careful accounting shows that the number of arithmetic operations is roughly . As we have seen, some matrices require more effort than others, but the upshot of this observation is that the effort is proportional to . We can think of this in the following way: If the size of the matrix grows by a factor of 10, then the effort required grows by a factor of .  While today's computers are powerful, they cannot handle every problem we might ask of them. Eventually, we would like to be able to consider matrices that have (a trillion) rows and columns. In very broad terms, the effort required to find the reduced row echelon matrix will require roughly operations.  To put this into context, imagine we need to solve a linear system with a trillion equations and a trillion variables and that we have a computer that can perform a trillion, , operations every second. Finding the reduced row echelon form would take about years. At this time, the universe is estimated to be approximately years old. If we had started the calculation years ago, then we'd be about one-millionth of the way through.  This may seem like an absurd situation, but we'll see in how we use the results of such a computation every day. Clearly, we will need some better tools to deal with really big problems like this one.    Summary  We learned some basic more features of Python with an emphasis on finding the reduced row echelon form of a matrix.    Our preference is to use numpy.array() to create matrices.  We can find the reduced row echelon form using the rref() method after converting to a sympy.Matrix . The output is a tuple of length 2. The first element is the reduced row echelon form of the matrix and the second is a tuple identifying the location of the pivots -- the topic of the next section.    We saw an example of the Augmentation Principle , which we then stated as a general principle.   We saw that the computational effort required to find the reduced row echelon form of an matrix is proportional to .   Appendix A contains a reference outlining the Python commands that we have encountered.     Consider the linear system Write this system as an augmented matrix and use Python to find a description of the solution space.    There is exactly one solution .   We can use Python to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .    Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .     Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .     Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .    A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.      Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Python to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Python cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Python to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .         A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Python to solve the linear system.    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Python to find its reduced row echelon form, we obtain     The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.       This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:        We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. Consider the linear system Let's solve this three ways, taking advantage of some features of sympy . As the name suggests, sympy is capable of doing symbolic mathematics as well as numerical mathematics. In particular, sympy can work with rational numbers (fractions) and do exact arithmetic with them. sympy.Rational()  Rational() sympy.Rational() Consider these three ways to represent : The first is our usual floating point approximation. The second attempts to approximate that approximation with a rational number. (Sometimes this works better; try s.Rational(1\/2) , for example.) The third represents  exactly by storing the numerator and denominator.  Create the augmented matrix for this linear system in the three corresponding ways. Call them A , B , and C . With each, have sympy compute the reduce row echelon form and use it to find the solutoin the original linear system.   Most computers do arithmetic using either 32 or 64 bits. sympy.evalf()  evalf() sympy.evalf() We can exagerate the effect by rounding the matrix more heavily. Solve the linear system again using What does Python give for the solution now? Compare this to the exact solution that you found previously.    Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior. (See .)         The exact solution is while the 3-digit approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we first round to 3 digits, we find the approximate solution      This solution to the first system is and . However, with just a small change in the system, we find the solution and .          Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .      Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,      Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.     The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .    inverse of a matrix matrix inverse  In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the . Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .     Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .     Suppose that is an matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.     Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .     Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so      "
},
{
  "id": "note-5",
  "level": "2",
  "url": "sec-python-introduction.html#note-5",
  "type": "Note",
  "number": "2.3.1",
  "title": "Augmented matrices in numpy.",
  "body": " Augmented matrices in numpy  Notice that in numpy there is no way of specifying whether a matrix is an augmented matrix or a coefficient matrix, so it will be up to us to interpret our results appropriately.  "
},
{
  "id": "activity-13",
  "level": "2",
  "url": "sec-python-introduction.html#activity-13",
  "type": "Activity",
  "number": "2.3.1",
  "title": "Using Python to find row reduced echelon matrices.",
  "body": " Using Python to find row reduced echelon matrices      Enter the following matrix into Python.      Give the matrix the name by entering A = np.array([ ... ])    numpy does not have a built-in method for computing the reduced row echelon form of a matrix, so we will use another package, sympy . We may then find its reduced row echelon form by entering import sympy A = np.array([ ... ]) sympy.Matrix(A).rref() Note that we must first convert our numpy array into a sympy matrix. A common mistake is to forget the parentheses after rref .  Use Python to find the reduced row echelon form of the matrix from of this activity.     Notice that rref() returns a tuple with two elements. The first element is the reduced row echelon form of our matrix. See if you can figure out what the second element is.    Use Python to describe the solution space of the system of linear equations      Consider the two matrices:  augmentation of a matrix We say that is an augmentation of because it is obtained from by adding some more columns.  We can use np.concatenate() to augment matrices as shown below. np.concatenate()  concatenate() np.concatenate() Notice that 1-dimensional vectors must first be reshaped into 2-dimensional column matrices. Now compute the reduced row echelon forms for and . What do you notice about the relationship between the two reduced row echelon forms?     Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           np.array([[-1,-2, 2,-1], [ 2, 4,-1, 5], [ 1, 2, 0, 3]])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            np.array([[-1,-2, 2,-1], [ 2, 4,-1, 5], [ 1, 2, 0, 3]])     The reduced row echelon form of the matrix is     Python tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is This can be computed with      (0,2) tells us that the leading 1 in the row 0 is in column 0 and that the leading 1 in row 1 is in column 2. This uses 0-based indexing. We might more naturally say that the leading 1 in the first row is in the first column and the leading 1 in the second row is in the third column.      "
},
{
  "id": "principle-augmentation-principle",
  "level": "2",
  "url": "sec-python-introduction.html#principle-augmentation-principle",
  "type": "Proposition",
  "number": "2.3.2",
  "title": "Augmentation Principle.",
  "body": " Augmentation Principle  augementation principle  reduced row echelon form (RREF) augementation principle  augementation of a matrix   If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .   "
},
{
  "id": "observation-6",
  "level": "2",
  "url": "sec-python-introduction.html#observation-6",
  "type": "Observation",
  "number": "2.3.3",
  "title": "",
  "body": " The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .  "
},
{
  "id": "exercise-19",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-19",
  "type": "Exercise",
  "number": "2.3.5.1",
  "title": "",
  "body": " Consider the linear system Write this system as an augmented matrix and use Python to find a description of the solution space.    There is exactly one solution .   We can use Python to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .  "
},
{
  "id": "exercise-20",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-20",
  "type": "Exercise",
  "number": "2.3.5.2",
  "title": "",
  "body": " Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .   "
},
{
  "id": "exercise-21",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-21",
  "type": "Exercise",
  "number": "2.3.5.3",
  "title": "",
  "body": " Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .   "
},
{
  "id": "exercise-22",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-22",
  "type": "Exercise",
  "number": "2.3.5.4",
  "title": "",
  "body": " Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .  "
},
{
  "id": "exercise-23",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-23",
  "type": "Exercise",
  "number": "2.3.5.5",
  "title": "",
  "body": " A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.    "
},
{
  "id": "exercise-24",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-24",
  "type": "Exercise",
  "number": "2.3.5.6",
  "title": "",
  "body": " Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Python to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Python cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Python to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .       "
},
{
  "id": "exercise-25",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-25",
  "type": "Exercise",
  "number": "2.3.5.7",
  "title": "",
  "body": " A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Python to solve the linear system.    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Python to find its reduced row echelon form, we obtain   "
},
{
  "id": "exercise-26",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-26",
  "type": "Exercise",
  "number": "2.3.5.8",
  "title": "",
  "body": " The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.     "
},
{
  "id": "exercise-27",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-27",
  "type": "Exercise",
  "number": "2.3.5.9",
  "title": "",
  "body": " This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:      "
},
{
  "id": "exercise-28",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-28",
  "type": "Exercise",
  "number": "2.3.5.10",
  "title": "",
  "body": " We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. Consider the linear system Let's solve this three ways, taking advantage of some features of sympy . As the name suggests, sympy is capable of doing symbolic mathematics as well as numerical mathematics. In particular, sympy can work with rational numbers (fractions) and do exact arithmetic with them. sympy.Rational()  Rational() sympy.Rational() Consider these three ways to represent : The first is our usual floating point approximation. The second attempts to approximate that approximation with a rational number. (Sometimes this works better; try s.Rational(1\/2) , for example.) The third represents  exactly by storing the numerator and denominator.  Create the augmented matrix for this linear system in the three corresponding ways. Call them A , B , and C . With each, have sympy compute the reduce row echelon form and use it to find the solutoin the original linear system.   Most computers do arithmetic using either 32 or 64 bits. sympy.evalf()  evalf() sympy.evalf() We can exagerate the effect by rounding the matrix more heavily. Solve the linear system again using What does Python give for the solution now? Compare this to the exact solution that you found previously.    Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior. (See .)         The exact solution is while the 3-digit approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we first round to 3 digits, we find the approximate solution      This solution to the first system is and . However, with just a small change in the system, we find the solution and .      "
},
{
  "id": "exercise-29",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-29",
  "type": "Exercise",
  "number": "2.3.6.1",
  "title": "",
  "body": " Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .    "
},
{
  "id": "exercise-30",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-30",
  "type": "Exercise",
  "number": "2.3.6.2",
  "title": "",
  "body": " Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,    "
},
{
  "id": "exercise-31",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-31",
  "type": "Exercise",
  "number": "2.3.6.3",
  "title": "",
  "body": " Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.   "
},
{
  "id": "exercise-32",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-32",
  "type": "Exercise",
  "number": "2.3.6.4",
  "title": "",
  "body": " The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .   "
},
{
  "id": "exercise-33",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-33",
  "type": "Exercise",
  "number": "2.3.6.5",
  "title": "",
  "body": "inverse of a matrix matrix inverse  In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the . Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .   "
},
{
  "id": "exercise-34",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-34",
  "type": "Exercise",
  "number": "2.3.6.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .   "
},
{
  "id": "exercise-35",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-35",
  "type": "Exercise",
  "number": "2.3.6.7",
  "title": "",
  "body": " Suppose that is an matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.   "
},
{
  "id": "exercise-36",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-36",
  "type": "Exercise",
  "number": "2.3.6.8",
  "title": "",
  "body": " Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .   "
},
{
  "id": "exercise-37",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-37",
  "type": "Exercise",
  "number": "2.3.6.9",
  "title": "",
  "body": " Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.   "
},
{
  "id": "exercise-38",
  "level": "2",
  "url": "sec-python-introduction.html#exercise-38",
  "type": "Exercise",
  "number": "2.3.6.10",
  "title": "",
  "body": " This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so    "
},
{
  "id": "sec-pivots",
  "level": "1",
  "url": "sec-pivots.html",
  "type": "Section",
  "number": "2.4",
  "title": "Pivots and their relationship to solution spaces",
  "body": " Pivots and their relationship to solution spaces   By now, we have seen several examples illustrating how the reduced row echelon matrix leads to a convenient description of the solution space to a linear system. In this section, we will use this understanding to make some general observations about how certain features of the reduced row echelon matrix reflect the nature of the solution space.  Remember that a leading entry in a reduced row echelon matrix is the leftmost nonzero entry in a row of the matrix. As we'll see, the positions of these leading entries encode a lot of information about the solution space of the corresponding linear system. For this reason, we make the following definition.   Pivot position  pivot position   A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .    For instance, in this reduced row echelon matrix, the pivot positions are indicated in bold: We can refer to pivot positions by their row and column number saying, for instance, that there is a pivot position in the third row and fourth column.   Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.       Finding pivots in Python  When we compute the reduced row echelon form of a matrix using sympy.rref() , the second part of the returned result tells us where the pivots positions are.   Executing this code shows us that the pivot positions (in Python 0-based indexing) are . Mathematically, using 1-based indexing, we would call these .   When we have looked at solution spaces of linear systems, we have frequently asked whether there are infinitely many solutions, exactly one solution, or no solutions. We will now break this question into two separate questions.   Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?      These two questions represent two sides of a coin that appear in many variations throughout our explorations. In this section, we will study how the location of the pivots influence the answers to these two questions. We begin by considering the first question concerning the existence of solutions.    The existence of solutions       Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.             The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.      Let's summarize the results of this activity by considering the following reduced row echelon matrix: . In terms of variables , , and , the final equation says . If we evaluate the left-hand side with any values of , , and , we get 0, which means that the equation always holds. Therefore, its presence has no effect on the solution space defined by the other three equations.  The third equation, however, says that . Again, if we evaluate the left-hand side with any values of , , and , we get 0 so this equation cannot be satisfied for any . This means that the entire linear system has no solution and is therefore inconsistent.  An equation like this appears in the reduced row echelon matrix as . The pivot positions make this condition clear: the system is inconsistent if there is a pivot position in the rightmost column of the corresponding augmented matrix.   In fact, we will soon see that the system is consistent if there is not a pivot in the rightmost column of the corresponding augmented matrix. This leaves us with the following    A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.    This also says something about the pivot positions of the coefficient matrix. Consider an example of an inconsistent system corresponding to the reduced row echelon form of the following augmented matrix . The Augmentation Principle says that that the reduced row echelon form of the coefficient matrix is which shows that the coefficient matrix has a row without a pivot position. To turn this around, we see that if every row of the coefficient matrix has a pivot position, then the system must be consistent. For instance, if our linear system has a coefficient matrix whose reduced row echelon form is , then we can guarantee that the linear system is consistent because there is no way to obtain a pivot in the rightmost column of the augmented matrix.    If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.      The uniqueness of solutions  Now that we have studied the role that pivot positions play in the existence of solutions, let's turn to the question of uniqueness.     Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.             The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.      Let's consider what we've learned in this activity. Since we are interested in the question of whether a consistent linear system has a unique solution or infinitely many, we will only consider consistent systems. By the results of the previous section, this means that there is not a pivot in the rightmost column of the augmented matrix. Here are two possible examples:          In the first example, we have the equations demonstrating the fact that there is a unique solution .  In the second example, we have the equations that we may rewrite in parametric form as . Here we see that and are basic variables that may be expressed in terms of the free variable . In this case, the presence of the free variable leads to infinitely many solutions.  Remember that every column of the coefficient matrix corresponds to a variable in our linear system. In the first example, we see that every column of the coefficient contains a pivot position, which means that every variable is uniquely determined. In the second example, the column of the coefficient matrix corresponding to does not contain a pivot position, which results in appearing as a free variable. This illustrates the following principle.    Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.      When a linear system has a unique solution, every column of the coefficient matrix has a pivot position. Since every row contains at most one pivot position, there must be at least as many rows as columns in the coefficient matrix. Therefore, the linear system has at least as many equations as variables, which is something we intuitively suspected in .  It is reasonable to ask how we choose the free variables. For instance, if we have a single equation , then we may write or, equivalently, . Clearly, either variable may be considered as a free variable in this case.  As we'll see in the future, we are more interested in the number of free variables rather than in their choice. For convenience, we will adopt the convention that free variables correspond to columns without a pivot position, which allows us to quickly identify them. For example, the variables and appear as free variables in the following linear system: .    Summary  We have seen how the locations of pivot positions, in both the augmented and coefficient matrices, give vital information about the existence and uniqueness of solutions to linear systems. More specifically,     A linear system is inconsistent exactly when a pivot position appears in the rightmost column of the augmented matrix.   If a linear system is consistent, the solution is unique when every column of the coefficient matrix contains a pivot position. There are infinitely solutions when there is a column of the coefficient matrix without a pivot position.   If a linear system is consistent, the columns of the coefficient matrix containing pivot positions correspond to basic variables and the columns without pivot positions correspond to free variables.       For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.      For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.       Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.      Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This is true as illustrated by the matrix   This statement is false as illustrated by the matrix       We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes are described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.       The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .      Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.      A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using what we have learned about pivots, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.      In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Python cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Python cell below and verifying that it passes through the four points.    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.      "
},
{
  "id": "definition-9",
  "level": "2",
  "url": "sec-pivots.html#definition-9",
  "type": "Definition",
  "number": "2.4.1",
  "title": "Pivot position.",
  "body": " Pivot position  pivot position   A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .   "
},
{
  "id": "exploration-4",
  "level": "2",
  "url": "sec-pivots.html#exploration-4",
  "type": "Preview Activity",
  "number": "2.4.1",
  "title": "Some basic observations about pivots.",
  "body": " Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.     "
},
{
  "id": "example-14",
  "level": "2",
  "url": "sec-pivots.html#example-14",
  "type": "Example",
  "number": "2.4.2",
  "title": "Finding pivots in Python.",
  "body": " Finding pivots in Python  When we compute the reduced row echelon form of a matrix using sympy.rref() , the second part of the returned result tells us where the pivots positions are.   Executing this code shows us that the pivot positions (in Python 0-based indexing) are . Mathematically, using 1-based indexing, we would call these .  "
},
{
  "id": "fundamental-questions",
  "level": "2",
  "url": "sec-pivots.html#fundamental-questions",
  "type": "Question",
  "number": "2.4.3",
  "title": "Two Fundamental Questions.",
  "body": " Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?     "
},
{
  "id": "activity-14",
  "level": "2",
  "url": "sec-pivots.html#activity-14",
  "type": "Activity",
  "number": "2.4.2",
  "title": "",
  "body": "     Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.             The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.     "
},
{
  "id": "thm-pivot-inconsistency",
  "level": "2",
  "url": "sec-pivots.html#thm-pivot-inconsistency",
  "type": "Proposition",
  "number": "2.4.4",
  "title": "",
  "body": "  A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.   "
},
{
  "id": "prop-pivot-consistent",
  "level": "2",
  "url": "sec-pivots.html#prop-pivot-consistent",
  "type": "Proposition",
  "number": "2.4.5",
  "title": "",
  "body": "  If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.   "
},
{
  "id": "activity-15",
  "level": "2",
  "url": "sec-pivots.html#activity-15",
  "type": "Activity",
  "number": "2.4.3",
  "title": "",
  "body": "   Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.             The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.     "
},
{
  "id": "principle-1",
  "level": "2",
  "url": "sec-pivots.html#principle-1",
  "type": "Principle",
  "number": "2.4.6",
  "title": "",
  "body": "  Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.     "
},
{
  "id": "exercise-39",
  "level": "2",
  "url": "sec-pivots.html#exercise-39",
  "type": "Exercise",
  "number": "2.4.4.1",
  "title": "",
  "body": " For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.    "
},
{
  "id": "exercise-40",
  "level": "2",
  "url": "sec-pivots.html#exercise-40",
  "type": "Exercise",
  "number": "2.4.4.2",
  "title": "",
  "body": " For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.     "
},
{
  "id": "exercise-41",
  "level": "2",
  "url": "sec-pivots.html#exercise-41",
  "type": "Exercise",
  "number": "2.4.4.3",
  "title": "",
  "body": " Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.    "
},
{
  "id": "exercise-42",
  "level": "2",
  "url": "sec-pivots.html#exercise-42",
  "type": "Exercise",
  "number": "2.4.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This is true as illustrated by the matrix   This statement is false as illustrated by the matrix     "
},
{
  "id": "exercise-43",
  "level": "2",
  "url": "sec-pivots.html#exercise-43",
  "type": "Exercise",
  "number": "2.4.4.5",
  "title": "",
  "body": " We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes are described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.     "
},
{
  "id": "exercise-44",
  "level": "2",
  "url": "sec-pivots.html#exercise-44",
  "type": "Exercise",
  "number": "2.4.4.6",
  "title": "",
  "body": " The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .    "
},
{
  "id": "exercise-45",
  "level": "2",
  "url": "sec-pivots.html#exercise-45",
  "type": "Exercise",
  "number": "2.4.4.7",
  "title": "",
  "body": " Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.    "
},
{
  "id": "exercise-46",
  "level": "2",
  "url": "sec-pivots.html#exercise-46",
  "type": "Exercise",
  "number": "2.4.4.8",
  "title": "",
  "body": " A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using what we have learned about pivots, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.    "
},
{
  "id": "exercise-poly-fit",
  "level": "2",
  "url": "sec-pivots.html#exercise-poly-fit",
  "type": "Exercise",
  "number": "2.4.4.9",
  "title": "",
  "body": " In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Python cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Python cell below and verifying that it passes through the four points.    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.    "
},
{
  "id": "sec-span",
  "level": "1",
  "url": "sec-span.html",
  "type": "Section",
  "number": "3.1",
  "title": "The span of a set of vectors",
  "body": " The span of a set of vectors   Matrix multiplication allows us to rewrite a linear system in the form . Besides being a more compact way of expressing a linear system, this form allows us to think about linear systems geometrically since matrix multiplication is defined in terms of linear combinations of vectors.  We now return to our two fundamental questions, rephrased here in terms of matrix multiplication.   Existence: Is there a solution to the equation ?    Uniqueness: If there is a solution to the equation , is it unique?  In this section, we focus on the existence question and see how it leads to the concept of the span of a set of vectors.   The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.     If , is the equation consistent? If so, find a solution.     Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.       The span of a set of vectors  In the preview activity, we considered a matrix and found that the equation has a solution for some vectors in and has no solution for others. We will introduce a concept called span that describes the vectors for which there is a solution.  We can write an matrix in terms of its columns . Remember that says that the equation is consistent if and only if we can express as a linear combination of .   span   The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.      Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  Equivalently, since we know that is in the span of and :   To determine whether the vector is in the span of and , we form the matrix and consider the equation .   We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .      Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .      This activity aims to convey the geometric meaning of span. Remember that we can think of a linear combination of the two vectors and as a recipe for walking in the plane . We first move a prescribed amount in the direction of and then a prescribed amount in the direction of . The span consists of all the places we can walk to.    Let's consider the vectors and as shown in .    The vectors and and some linear combinations they create.     The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by solving the matrix equation whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and . The solution provides the weights.  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.      Now let's consider the vectors and as shown in .    The vectors and and some linear combinations they create.     From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.     We will denote the span of the set of vectors by .   In , we saw that . However, for the vectors in , we saw that is simply a line.    Pivot positions and span  A set of vectors naturally defines a matrix whose columns are the given vectors. As we've seen, a vector is in precisely when the linear system is consistent.  The previous examples point to the fact that the span is related to the pivot positions of . While and develop this idea more fully, we will now examine the possibilities in .    In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?     Is the vector in ?    Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form.   What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.     The types of sets that appear as the span of a set of vectors in are relatively simple.   First, with a single nonzero vector, all linear combinations are simply scalar multiples of that vector so that the span of this vector is a line, as shown in .   The span of a single nonzero vector is a line.      Notice that the matrix formed by this vector has one pivot position. For example, .    The span of two vectors in that do not lie on the same line will be a plane, as seen in .   The span of these two vectors in is a plane.      For example, the vectors lead to the matrix with two pivot positions.   Finally, a set of three vectors, such as may form a matrix having three pivot positions one in every row. When this happens, no matter how we augment this matrix, it is impossible to obtain a pivot position in the rightmost column: Therefore, any linear system is consistent, which tells us that .    To summarize, we looked at the pivot positions in a matrix whose columns are the three-dimensional vectors . We found that with   one pivot position, the span was a line.    two pivot positions, the span was a plane.   three pivot positions, the span was .  Though we will return to these ideas later, for now take note of the fact that the span of a set of vectors in is a relatively simple, familiar geometric object.  The reasoning we led us to conclude that the span of a set of vectors is when the associated matrix has a pivot position in every row applies more generally.    Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.    This tells us something important about the number of vectors needed to span . Suppose we have vectors that span . The proposition tells us that the matrix has a pivot position in every row, such as in this reduced row echelon matrix. Since a matrix can have at most one pivot position in a column, there must be at least as many columns as there are rows, which implies that . For instance, if we have a set of vectors that span , there must be at least 632 vectors in the set.    A set of vectors whose span is contains at least vectors.    We have thought about a linear combination of a set of vectors as the result of walking a certain distance in the direction of , followed by walking a certain distance in the direction of , and so on. If , this means that we can walk to every point in using the directions . Intuitively, this proposition is telling us that we need at least directions to have the flexibility needed to reach every point in .   Terminology  Because span is a concept that is connected to a set of vectors, we say, The span of the set of vectors is .... While it may be tempting to say, The span of the matrix is ..., we should instead say The span of the columns of the matrix is ....      Summary  We defined the span of a set of vectors and developed some intuition for this concept through a series of examples.   The span of a set of vectors is the set of linear combinations of the vectors. We denote the span by .   A vector is in if an only if the linear system is consistent.   If the matrix has a pivot position in every row, then the span of these vectors is ; that is, .  Any set of vectors that spans must have at least vectors.      In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .      Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .     For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .     Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .     Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If can be expressed as a linear combination of , then is in .  If is a matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .     This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .     Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .     The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .     Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .     Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.     "
},
{
  "id": "p-1386",
  "level": "2",
  "url": "sec-span.html#p-1386",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "span "
},
{
  "id": "exploration-5",
  "level": "2",
  "url": "sec-span.html#exploration-5",
  "type": "Preview Activity",
  "number": "3.1.1",
  "title": "The existence of solutions.",
  "body": " The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.     If , is the equation consistent? If so, find a solution.     Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.    "
},
{
  "id": "p-1401",
  "level": "2",
  "url": "sec-span.html#p-1401",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "span "
},
{
  "id": "definition-10",
  "level": "2",
  "url": "sec-span.html#definition-10",
  "type": "Definition",
  "number": "3.1.1",
  "title": "",
  "body": " span   The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.   "
},
{
  "id": "example-15",
  "level": "2",
  "url": "sec-span.html#example-15",
  "type": "Example",
  "number": "3.1.2",
  "title": "",
  "body": "  Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  Equivalently, since we know that is in the span of and :   To determine whether the vector is in the span of and , we form the matrix and consider the equation .   We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .   "
},
{
  "id": "activity-intro-span",
  "level": "2",
  "url": "sec-span.html#activity-intro-span",
  "type": "Activity",
  "number": "3.1.2",
  "title": "",
  "body": "  Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .     "
},
{
  "id": "example-span-linear-indep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-indep",
  "type": "Example",
  "number": "3.1.5",
  "title": "",
  "body": "  Let's consider the vectors and as shown in .    The vectors and and some linear combinations they create.     The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by solving the matrix equation whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and . The solution provides the weights.  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.   "
},
{
  "id": "example-span-linear-dep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-dep",
  "type": "Example",
  "number": "3.1.7",
  "title": "",
  "body": "  Now let's consider the vectors and as shown in .    The vectors and and some linear combinations they create.     From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.   "
},
{
  "id": "convention-1",
  "level": "2",
  "url": "sec-span.html#convention-1",
  "type": "Notation",
  "number": "3.1.9",
  "title": "",
  "body": " We will denote the span of the set of vectors by .  "
},
{
  "id": "activity-span-r3",
  "level": "2",
  "url": "sec-span.html#activity-span-r3",
  "type": "Activity",
  "number": "3.1.3",
  "title": "",
  "body": "  In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?     Is the vector in ?    Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form.   What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.    "
},
{
  "id": "fig-span-line",
  "level": "2",
  "url": "sec-span.html#fig-span-line",
  "type": "Figure",
  "number": "3.1.12",
  "title": "",
  "body": " The span of a single nonzero vector is a line.     "
},
{
  "id": "fig-span-plane",
  "level": "2",
  "url": "sec-span.html#fig-span-plane",
  "type": "Figure",
  "number": "3.1.13",
  "title": "",
  "body": " The span of these two vectors in is a plane.     "
},
{
  "id": "prop-pivot-row",
  "level": "2",
  "url": "sec-span.html#prop-pivot-row",
  "type": "Proposition",
  "number": "3.1.14",
  "title": "",
  "body": "  Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.   "
},
{
  "id": "prop-span-bound",
  "level": "2",
  "url": "sec-span.html#prop-span-bound",
  "type": "Proposition",
  "number": "3.1.15",
  "title": "",
  "body": "  A set of vectors whose span is contains at least vectors.   "
},
{
  "id": "exercise-48",
  "level": "2",
  "url": "sec-span.html#exercise-48",
  "type": "Exercise",
  "number": "3.1.4.1",
  "title": "",
  "body": " In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .    "
},
{
  "id": "exercise-49",
  "level": "2",
  "url": "sec-span.html#exercise-49",
  "type": "Exercise",
  "number": "3.1.4.2",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .   "
},
{
  "id": "exercise-50",
  "level": "2",
  "url": "sec-span.html#exercise-50",
  "type": "Exercise",
  "number": "3.1.4.3",
  "title": "",
  "body": " For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .   "
},
{
  "id": "exercise-51",
  "level": "2",
  "url": "sec-span.html#exercise-51",
  "type": "Exercise",
  "number": "3.1.4.4",
  "title": "",
  "body": " Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .   "
},
{
  "id": "exercise-52",
  "level": "2",
  "url": "sec-span.html#exercise-52",
  "type": "Exercise",
  "number": "3.1.4.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If can be expressed as a linear combination of , then is in .  If is a matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .   "
},
{
  "id": "exercise-53",
  "level": "2",
  "url": "sec-span.html#exercise-53",
  "type": "Exercise",
  "number": "3.1.4.6",
  "title": "",
  "body": " This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .   "
},
{
  "id": "exercise-54",
  "level": "2",
  "url": "sec-span.html#exercise-54",
  "type": "Exercise",
  "number": "3.1.4.7",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .   "
},
{
  "id": "exercise-55",
  "level": "2",
  "url": "sec-span.html#exercise-55",
  "type": "Exercise",
  "number": "3.1.4.8",
  "title": "",
  "body": " The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .   "
},
{
  "id": "exercise-56",
  "level": "2",
  "url": "sec-span.html#exercise-56",
  "type": "Exercise",
  "number": "3.1.4.9",
  "title": "",
  "body": " Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .   "
},
{
  "id": "exercise-57",
  "level": "2",
  "url": "sec-span.html#exercise-57",
  "type": "Exercise",
  "number": "3.1.4.10",
  "title": "",
  "body": " Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .   "
},
{
  "id": "exercise-58",
  "level": "2",
  "url": "sec-span.html#exercise-58",
  "type": "Exercise",
  "number": "3.1.4.11",
  "title": "",
  "body": " As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.   "
},
{
  "id": "sec-linear-dep",
  "level": "1",
  "url": "sec-linear-dep.html",
  "type": "Section",
  "number": "3.2",
  "title": "Linear independence",
  "body": " Linear independence   In the previous section, questions about the existence of solutions of a linear system led to the concept of the span of a set of vectors. In particular, the span of a set of vectors is the set of vectors for which a solution to the linear system exists.  In this section, we turn to the uniqueness of solutions of a linear system, the second of our two fundamental questions . This will lead us to the concept of linear independence.    Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .    Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .    Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .         Linear dependence  We have seen examples where the span of a set of three vectors in is and other examples where the span of three vectors is a plane. We would like to understand the difference between these two situations.    Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .      Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.    Before exploring this type of behavior more generally, let's think about it from a geometric point of view. Suppose that we begin with the two vectors and in . The span of these two vectors is a plane in , as seen on the left of .   The span of the vectors , , and .       Because the vector is not a linear combination of and , it provides a direction to move that is independent of and . Adding this third vector therefore forms a set whose span is , as seen on the right of .  Similarly, the span of the vectors and in is also a plane. However, the third vector is a linear combination of and , which means that it already lies in the plane formed by and , as seen in . Since we can already move in this direction using just and , adding to the set does not change the span. As a result, it remains a plane.   The span of the vectors , , and .       What distinguishes these two examples is whether one of the vectors is a linear combination of the others, an observation that leads to the following definition.   linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .    For the sake of completeness, we say that a set of vectors containing only one nonzero vector is linearly independent.    How to recognize linear dependence    We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can at most be 10 pivot positions so there are at least two columns without pivot positions.     By now, we should expect that the pivot positions play an important role in determining whether the columns of a matrix are linearly dependent. For instance, suppose we have four vectors and their associated matrix Since the third column does not contain a pivot position, let's just focus on the first three columns and view them as an augmented matrix: This says that which tells us that the set of vectors is linearly dependent. Moreover, we see that   More generally, the same reasoning implies that a set of vectors is linearly dependent if the associated matrix has a column without a pivot position. Indeed, as illustrated here, a vector corresponding to a column without a pivot position can be expressed as a linear combination of the vectors whose columns do contain pivot positions.  Suppose instead that the matrix associated to a set of vectors has a pivot position in every column. Viewing this as an augmented matrix again, we see that the linear system is inconsistent since there is a pivot in the rightmost column, which means that cannot be expressed as a linear combination of the other vectors. Similarly, cannot be expressed as a linear combination of and . In fact, none of the vectors can be written as a linear combination of the others so this set of vectors is linearly independent.  The following proposition summarizes these findings.    The columns of a matrix are linearly independent if and only if every column contains a pivot position.    This condition imposes a constraint on how many vectors we can have in a linearly independent set. Here is an example of the reduced row echelon form of a matrix whose columns form a set of three linearly independent vectors in : Notice that there are at least as many rows as columns, which must be the case if every column is to have a pivot position.  More generally, if is a linearly independent set of vectors in , the associated matrix must have a pivot position in every column. Since every row contains at most one pivot position, the number of columns can be no greater than the number of rows. This means that the number of vectors in a linearly independent set can be no greater than the number of dimensions.    A linearly independent set of vectors in contains at most vectors.    This says, for instance, that any linearly independent set of vectors in can contain no more three vectors. We usually imagine three independent directions, such as up\/down, front\/back, left\/right, in our three-dimensional world. This proposition tells us that there can be no more independent directions.  The proposition above says that a set of vectors in that is linearly independent has at most vectors. By comparison, says that a set of vectors whose span is has at least vectors.  This implies that a set of linearly independent vectors that spans must have exactly vectors. Such a set is called a basis for . We will have more to say about bases in     Homogeneous equations  If is a matrix, we call the equation a homogeneous equation. As we'll see, the uniqueness of solutions to this equation reflects on the linear independence of the columns of .   Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.    Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.      This activity shows how the solution space of the homogeneous equation indicates whether the columns of are linearly dependent or independent. First, we know that the equation always has at least one solution, the vector . Any other solution is a nonzero solution.    Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.    As this example demonstrates, there are many ways we can view the question of linear independence, some of which are recorded in the following proposition.    For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .        Summary  This section developed the concept of linear dependence of a set of vectors. More specifically, we saw that:  A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.  A set of vectors is linearly independent if and only if the vectors form a matrix that has a pivot position in every column.  A set of linearly independent vectors in contains no more than vectors.  The columns of the matrix are linearly dependent if the homogeneous equation has a nonzero solution.  A set of vectors is linearly dependent if there are weights , not all of which are zero, such that .   At the beginning of the section, we said that this concept addressed the second of our two fundamental questions concerning the uniqueness of solutions to a linear system. It is worth comparing the results of this section with those of the previous one so that the parallels between them become clear.  As usual, we will write a matrix as a collection of vectors,    Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the only solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.         Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Find a nonzero solution to the homogenous equation where .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.     Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.     Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.     Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.     Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.     Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.     Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .     When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .     Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .     Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .     "
},
{
  "id": "exploration-6",
  "level": "2",
  "url": "sec-linear-dep.html#exploration-6",
  "type": "Preview Activity",
  "number": "3.2.1",
  "title": "",
  "body": "  Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .    Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .    Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .      "
},
{
  "id": "example-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-r3",
  "type": "Example",
  "number": "3.2.1",
  "title": "",
  "body": "  Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .   "
},
{
  "id": "example-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-plane",
  "type": "Example",
  "number": "3.2.2",
  "title": "",
  "body": "  Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.   "
},
{
  "id": "figure-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-r3",
  "type": "Figure",
  "number": "3.2.3",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "figure-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-plane",
  "type": "Figure",
  "number": "3.2.4",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "definition-11",
  "level": "2",
  "url": "sec-linear-dep.html#definition-11",
  "type": "Definition",
  "number": "3.2.5",
  "title": "",
  "body": " linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .   "
},
{
  "id": "activity-18",
  "level": "2",
  "url": "sec-linear-dep.html#activity-18",
  "type": "Activity",
  "number": "3.2.2",
  "title": "",
  "body": "  We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can at most be 10 pivot positions so there are at least two columns without pivot positions.    "
},
{
  "id": "proposition-9",
  "level": "2",
  "url": "sec-linear-dep.html#proposition-9",
  "type": "Proposition",
  "number": "3.2.6",
  "title": "",
  "body": "  The columns of a matrix are linearly independent if and only if every column contains a pivot position.   "
},
{
  "id": "prop-linear-indep-bound",
  "level": "2",
  "url": "sec-linear-dep.html#prop-linear-indep-bound",
  "type": "Proposition",
  "number": "3.2.7",
  "title": "",
  "body": "  A linearly independent set of vectors in contains at most vectors.   "
},
{
  "id": "p-1707",
  "level": "2",
  "url": "sec-linear-dep.html#p-1707",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "basis "
},
{
  "id": "activity-19",
  "level": "2",
  "url": "sec-linear-dep.html#activity-19",
  "type": "Activity",
  "number": "3.2.3",
  "title": "Linear independence and homogeneous equations.",
  "body": " Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.    Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.     "
},
{
  "id": "example-20",
  "level": "2",
  "url": "sec-linear-dep.html#example-20",
  "type": "Example",
  "number": "3.2.8",
  "title": "",
  "body": "  Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.   "
},
{
  "id": "proposition-11",
  "level": "2",
  "url": "sec-linear-dep.html#proposition-11",
  "type": "Proposition",
  "number": "3.2.9",
  "title": "",
  "body": "  For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .     "
},
{
  "id": "table-1",
  "level": "2",
  "url": "sec-linear-dep.html#table-1",
  "type": "Table",
  "number": "3.2.10",
  "title": "Span and Linear Independence",
  "body": " Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the only solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.     "
},
{
  "id": "exercise-59",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-59",
  "type": "Exercise",
  "number": "3.2.5.1",
  "title": "",
  "body": " Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Find a nonzero solution to the homogenous equation where .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.   "
},
{
  "id": "exercise-60",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-60",
  "type": "Exercise",
  "number": "3.2.5.2",
  "title": "",
  "body": " Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.   "
},
{
  "id": "exercise-61",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-61",
  "type": "Exercise",
  "number": "3.2.5.3",
  "title": "",
  "body": " Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.   "
},
{
  "id": "exercise-62",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-62",
  "type": "Exercise",
  "number": "3.2.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.   "
},
{
  "id": "exercise-63",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-63",
  "type": "Exercise",
  "number": "3.2.5.5",
  "title": "",
  "body": " Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.   "
},
{
  "id": "exercise-64",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-64",
  "type": "Exercise",
  "number": "3.2.5.6",
  "title": "",
  "body": " Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.   "
},
{
  "id": "exercise-65",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-65",
  "type": "Exercise",
  "number": "3.2.5.7",
  "title": "",
  "body": " Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .   "
},
{
  "id": "exercise-66",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-66",
  "type": "Exercise",
  "number": "3.2.5.8",
  "title": "",
  "body": " When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .   "
},
{
  "id": "exercise-67",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-67",
  "type": "Exercise",
  "number": "3.2.5.9",
  "title": "",
  "body": " Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .   "
},
{
  "id": "exercise-68",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-68",
  "type": "Exercise",
  "number": "3.2.5.10",
  "title": "",
  "body": " Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .   "
},
{
  "id": "sec-linear-trans",
  "level": "1",
  "url": "sec-linear-trans.html",
  "type": "Section",
  "number": "3.3",
  "title": "Matrix transformations",
  "body": " Matrix transformations   The past few sections introduced us to matrix-vector multiplication as a means of thinking geometrically about the solutions to a linear system. In particular, we rewrote a linear system as a matrix equation and developed the concepts of span and linear independence in response to our two fundamental questions.   In this section, we will explore how matrix-vector multiplication defines certain types of functions, which we call matrix transformations . We will develop some algebraic tools for thinking about matrix transformations and look at some motivating examples. In the next section, we will see how matrix transformations describe important geometric operations and how they are used in computer animation.    We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in .   Graph the function above.       We will now consider functions having the form . Draw a graph of the function on the left in .   Graphs of the function and .        Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.       The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .       Matrix transformations  In the preview activity, we considered familiar linear functions of a single variable, such as . We construct a function like this by choosing a number ; when given an input , the output is formed by multiplying by .  In this section, we will consider functions whose inputs are vectors and whose outputs are vectors defined through matrix-vector multiplication. That is, if is a matrix and is a vector, the function forms the product as its output. Such a function is called a matrix transformation.   matrix transformation   The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .      The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.      Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix       In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is      Let's discuss a few of the issues that appear in this activity. First, notice that the shape of the matrix and the dimension of the input vector must be compatible if the product is to be defined. In particular, if is an matrix, needs to be an -dimensional vector, and the resulting product will be an -dimensional vector. For the associated matrix transformation, we therefore write meaning takes vectors in as inputs and produces vectors in as outputs. We will say that is the domain of the matrix transformation and is the codomain . domain  codomain For instance, if , then .  While every vector in the domain of a matrix transformation can be provided as in input, it may or may not be the case that every vector in the codomain is a possible output of the transformation. We will call the set of vectors that are posible outputs (that is, for which there is a vector in the domain with ) the range of the matrix transformation. range of a transformation . If the range is equal to all of the codomain, then we say that the matrix transformation is onto . onto transformation This is the same as saying that has a solution for any in the codomain.  Third, we can often reconstruct the matrix if we only know some output values from its associated linear transformation by remembering that matrix-vector multiplication constructs linear combinations. For instance, if is an matrix , then . That is, we can find the first column of by evaluating . Similarly, the second column of is found by evaluating .  More generally, we will write the columns of the identity matrix as identity matrix  columns ( )    Colunn of an identity matrix. This column vector has a 1 in row and 0s everywhere else.   so that . This means that the column of is found by evaluating . We record this fact in the following proposition.   matrix transforamtion  transforamtion  matrix   If is a matrix transformation given by , then the matrix has columns ; that is, .      Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .          In these examples, we glided over an important point: how do we know these functions can be expressed as matrix transformations? We will take up this question in detail in the next section and not worry about it for now.    Composing matrix transformations  It sometimes happens that we want to combine matrix transformations by performing one and then another. In the last activity, for instance, we considered the matrix transformation where is the result of rotating the two-dimensional vector by . Now suppose we are interested in rotating that vector twice; that is, we take a vector , rotate it by to obtain , and then rotate the result by again to obtain .  This process is called function composition and likely appeared in an earlier algebra course. function composition  composition of functions    The coposition of functions and . .  For instance, if and , the composition of these functions obtained by first performing and then performing is denoted by   Composing matrix transformations is similar. Suppose that we have two matrix transformations, and . Their associated matrices will be denoted by and so that and . If we apply to a vector to obtain and then apply to the result, we have Notice that this implies that the composition is itself a matrix transformation and that the associated matrix is the product .    If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .    Notice that the matrix transformations must be compatible if they are to be composed. In particular, the vector , an -dimensional vector, must be a suitable input vector for , which means that the inputs to must be -dimensional. In fact, this is the same condition we need to form the product of their associated matrices, namely, that the number of columns of is the same as the number of rows of .    We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .            Discrete Dynamical Systems  In , we will give considerable attention to a specific type of matrix transformation, which is illustrated in the next activity.    Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.     Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .     As we will see later, this type of situation occurs frequently. We have a vector that describes the state of some system; in this case, describes the distribution of trucks between the two locations at a particular time. Then there is a matrix transformation that describes the state at some later time. We call the state vector and the transition function, as it describes the transition of the state vector from one time to the next. state vector  transition function   Beginning with an initial state , we would like to know how the state evolves over time. For instance, and so on.   dynamical system  discrete  discrete dynamical system  We call this situation where the state of a system evolves from one time to the next according to the rule a discrete dynamical system . In , we will develop a theory that enables us to make long-term predictions about the evolution of the state vector.    Summary  This section introduced matrix transformations , functions that are defined by matrix-vector multiplication, such as for some matrix .  If is an matrix, then .  The columns of the matrix are given by evaluating the transformation on the vectors ; that is, .  The composition of matrix transformations corresponds to matrix multiplication.  A discrete dynamical system consists of a state vector along with a transition function that describes how the state vector evolves from one time to the next. Powers of the matrix determine the long-term behavior of the state vector.       Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .     This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .     Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .     Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .     In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .          Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.     Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .     Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .     Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .     Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.     "
},
{
  "id": "p-1892",
  "level": "2",
  "url": "sec-linear-trans.html#p-1892",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix transformations "
},
{
  "id": "exploration-7",
  "level": "2",
  "url": "sec-linear-trans.html#exploration-7",
  "type": "Preview Activity",
  "number": "3.3.1",
  "title": "",
  "body": "  We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in .   Graph the function above.       We will now consider functions having the form . Draw a graph of the function on the left in .   Graphs of the function and .        Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.       The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .    "
},
{
  "id": "definition-12",
  "level": "2",
  "url": "sec-linear-trans.html#definition-12",
  "type": "Definition",
  "number": "3.3.3",
  "title": "",
  "body": " matrix transformation   The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .   "
},
{
  "id": "example-matrix-to-mt",
  "level": "2",
  "url": "sec-linear-trans.html#example-matrix-to-mt",
  "type": "Example",
  "number": "3.3.4",
  "title": "",
  "body": "  The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.   "
},
{
  "id": "example-mt-to-matrix",
  "level": "2",
  "url": "sec-linear-trans.html#example-mt-to-matrix",
  "type": "Example",
  "number": "3.3.5",
  "title": "",
  "body": "  Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix    "
},
{
  "id": "activity-20",
  "level": "2",
  "url": "sec-linear-trans.html#activity-20",
  "type": "Activity",
  "number": "3.3.2",
  "title": "",
  "body": "  In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is     "
},
{
  "id": "p-1942",
  "level": "2",
  "url": "sec-linear-trans.html#p-1942",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "domain codomain "
},
{
  "id": "p-1943",
  "level": "2",
  "url": "sec-linear-trans.html#p-1943",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "range onto "
},
{
  "id": "prop-linear-trans-columns",
  "level": "2",
  "url": "sec-linear-trans.html#prop-linear-trans-columns",
  "type": "Proposition",
  "number": "3.3.6",
  "title": "",
  "body": " matrix transforamtion  transforamtion  matrix   If is a matrix transformation given by , then the matrix has columns ; that is, .   "
},
{
  "id": "activity-mt-intro",
  "level": "2",
  "url": "sec-linear-trans.html#activity-mt-intro",
  "type": "Activity",
  "number": "3.3.3",
  "title": "",
  "body": "  Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .         "
},
{
  "id": "p-1979",
  "level": "2",
  "url": "sec-linear-trans.html#p-1979",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "composition "
},
{
  "id": "proposition-13",
  "level": "2",
  "url": "sec-linear-trans.html#proposition-13",
  "type": "Proposition",
  "number": "3.3.8",
  "title": "",
  "body": "  If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .   "
},
{
  "id": "activity-22",
  "level": "2",
  "url": "sec-linear-trans.html#activity-22",
  "type": "Activity",
  "number": "3.3.4",
  "title": "",
  "body": "  We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .         "
},
{
  "id": "activity-23",
  "level": "2",
  "url": "sec-linear-trans.html#activity-23",
  "type": "Activity",
  "number": "3.3.5",
  "title": "",
  "body": "  Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.     Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .    "
},
{
  "id": "p-2040",
  "level": "2",
  "url": "sec-linear-trans.html#p-2040",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "state transition "
},
{
  "id": "p-2042",
  "level": "2",
  "url": "sec-linear-trans.html#p-2042",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "discrete dynamical system "
},
{
  "id": "p-2043",
  "level": "2",
  "url": "sec-linear-trans.html#p-2043",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix transformations "
},
{
  "id": "exercise-69",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-69",
  "type": "Exercise",
  "number": "3.3.5.1",
  "title": "",
  "body": " Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .   "
},
{
  "id": "exercise-70",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-70",
  "type": "Exercise",
  "number": "3.3.5.2",
  "title": "",
  "body": " This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .   "
},
{
  "id": "exercise-71",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-71",
  "type": "Exercise",
  "number": "3.3.5.3",
  "title": "",
  "body": " Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .   "
},
{
  "id": "exercise-72",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-72",
  "type": "Exercise",
  "number": "3.3.5.4",
  "title": "",
  "body": " Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .   "
},
{
  "id": "exercise-73",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-73",
  "type": "Exercise",
  "number": "3.3.5.5",
  "title": "",
  "body": " In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .        "
},
{
  "id": "exercise-74",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-74",
  "type": "Exercise",
  "number": "3.3.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.   "
},
{
  "id": "exercise-75",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-75",
  "type": "Exercise",
  "number": "3.3.5.7",
  "title": "",
  "body": " Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .   "
},
{
  "id": "exercise-76",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-76",
  "type": "Exercise",
  "number": "3.3.5.8",
  "title": "",
  "body": " Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .   "
},
{
  "id": "exercise-77",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-77",
  "type": "Exercise",
  "number": "3.3.5.9",
  "title": "",
  "body": " Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .   "
},
{
  "id": "exercise-78",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-78",
  "type": "Exercise",
  "number": "3.3.5.10",
  "title": "",
  "body": " Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.   "
},
{
  "id": "sec-transforms-geom",
  "level": "1",
  "url": "sec-transforms-geom.html",
  "type": "Section",
  "number": "3.4",
  "title": "The geometry of matrix transformations",
  "body": " The geometry of matrix transformations   Matrix transformations, which we explored in the last section, allow us to describe certain functions . In this section, we will demonstrate how matrix transformations provide a convenient way to describe geometric operations, such as rotations, reflections, and scalings. We will then explore how matrix transformations are used in computer animation.    We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .   A vector and its reflection across the horizontal axis.       If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .       The geometry of matrix transformations  We have now seen how a few geometric operations, such as rotations and reflections, can be described using matrix transformations. The following activity shows, more generally, that matrix transformations can perform a variety of important geometric operations.   Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component. shear   This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis. projection   This transformation is called the identity ; it causes no change. identity transformation   This transformation pushes vectors onto the line defined by the vector .     The previous activity presented some examples showing that matrix transformations can perform interesting geometric operations, such as rotations, scalings, and reflections. Before we go any further, we should explain why it is possible to represent these operations by matrix transformations. In fact, we ask more generally: what types of functions are represented as matrix transformations?  The linearity of matrix-vector multiplication provides the key to answering this question. Remember that if is a matrix, and vectors, and a scalar, then . This means that a matrix transformation satisfies the corresponding linearity property:   Linearity of Matrix Transformations  linearity of matrix tranformations  matrix transformations linearity of      It turns out that, if satisfies these two linearity properties, then we can find a matrix such that . In fact, tells us how to form ; we simply write . We will now check that using the linearity of : .  The result is the following proposition.   linearity of matrix transformations  matrix transformations linearity of   The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .    Said simply, this proposition means says that if we have a function and can verify the two linearity properties stated in the proposition, then we know that is a matrix transformation. Let's see how this works in practice.    We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .   The function rotates a vector counterclockwise by .      We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .   We see that the vector is a scalar multiple to so that .      Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .   We see that the vector is the sum of and so that .      Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .   The matrix transformation rotates and by .      Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .   The vector has length 1 and is the hypotenuse of a right isosceles triangle.      This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .    In this example, we found that , a function describing a rotation in the plane, was in fact a matrix transformation by checking that The same kind of thinking applies more generally to show that all rotations, reflections, and scalings are matrix transformations. Similarly, we could revisit the functions in and verify that they are matrix transformations.    In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .       Matrix transformations and computer animation  Linear algebra plays a significant role in computer animation. We will now illustrate how matrix transformations and some of the ideas we have developed in this section are used by computer animators to create the illusion of motion in their characters.   shows a test character used by Pixar animators. On the left is the original definition of the character; on the right, we see that the character has been moved into a different pose. To make it appear that the character is moving, animators create a sequence of frames in which the character's pose is modified slightly from one frame to the next often using matrix transformations.   Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar         Of course, realistic characters will be drawn in three-dimensions. To keep things a little more simple, however, we will look at this two-dimensional character and devise matrix transformations that move them into different poses.    The first thing we may wish to do is simply move them to a different position in the plane, such as that shown in . Motions like this are called translations . translation    Translating our character to a new position in the plane.      This presents a problem because a matrix transformation has the property that . This means that a matrix transformation cannot move the origin of the coordinate plane. To address this restriction, animators use homogeneous coordinates , which are formed by placing the two-dimensional coordinate plane inside as the plane , as shown in . homogeneous coordinates    Include the two-dimensional coordinate plane in as the plane so that we can translate the character.      As a result, rather than describing points in the plane as vectors , we describe them as three-dimensional vectors . As we see in the next activity, this allows us to translate our character in the plane.    In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in    Translating to a new position.        As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.   Waving with the other hand.        Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.   Performing a cartwheel.              Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?   Zooming in on our characters' face.              We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.   Casting a shadow.              Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .       Summary  This section explored how geometric operations are performed by matrix transformations.  A function is a matrix transformation if and only if these properties are satisfied:   Geometric operations, such as rotations, reflections, and scalings, can be represented as matrix transformations.  Composing geometric operations corresponds to matrix multiplication.  Computer animators use homogeneous coordinates and matrix transformations to create the illusion of motion.       For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .     This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.     Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.     If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .     We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .     Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .     This exericse concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effort of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that      "
},
{
  "id": "exploration-8",
  "level": "2",
  "url": "sec-transforms-geom.html#exploration-8",
  "type": "Preview Activity",
  "number": "3.4.1",
  "title": "",
  "body": "  We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .   A vector and its reflection across the horizontal axis.       If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .    "
},
{
  "id": "activity-linear-trans-geom",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-linear-trans-geom",
  "type": "Activity",
  "number": "3.4.2",
  "title": "Using matrix transformations to describe geometric operations.",
  "body": " Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component. shear   This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis. projection   This transformation is called the identity ; it causes no change. identity transformation   This transformation pushes vectors onto the line defined by the vector .    "
},
{
  "id": "prop-linear-trans-characterization",
  "level": "2",
  "url": "sec-transforms-geom.html#prop-linear-trans-characterization",
  "type": "Proposition",
  "number": "3.4.3",
  "title": "",
  "body": " linearity of matrix transformations  matrix transformations linearity of   The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .   "
},
{
  "id": "example-23",
  "level": "2",
  "url": "sec-transforms-geom.html#example-23",
  "type": "Example",
  "number": "3.4.4",
  "title": "",
  "body": "  We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .   The function rotates a vector counterclockwise by .      We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .   We see that the vector is a scalar multiple to so that .      Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .   We see that the vector is the sum of and so that .      Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .   The matrix transformation rotates and by .      Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .   The vector has length 1 and is the hypotenuse of a right isosceles triangle.      This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .   "
},
{
  "id": "activity-25",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-25",
  "type": "Activity",
  "number": "3.4.3",
  "title": "",
  "body": "  In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .    "
},
{
  "id": "fig-blob-man",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-blob-man",
  "type": "Figure",
  "number": "3.4.10",
  "title": "",
  "body": " Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar      "
},
{
  "id": "p-2291",
  "level": "2",
  "url": "sec-transforms-geom.html#p-2291",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "translations "
},
{
  "id": "fig-animate-translate",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-translate",
  "type": "Figure",
  "number": "3.4.11",
  "title": "",
  "body": " Translating our character to a new position in the plane.     "
},
{
  "id": "p-2292",
  "level": "2",
  "url": "sec-transforms-geom.html#p-2292",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "homogeneous coordinates "
},
{
  "id": "fig-animate-homogeneous",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-homogeneous",
  "type": "Figure",
  "number": "3.4.12",
  "title": "",
  "body": " Include the two-dimensional coordinate plane in as the plane so that we can translate the character.     "
},
{
  "id": "activity-26",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-26",
  "type": "Activity",
  "number": "3.4.4",
  "title": "",
  "body": "  In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in    Translating to a new position.        As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.   Waving with the other hand.        Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.   Performing a cartwheel.              Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?   Zooming in on our characters' face.              We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.   Casting a shadow.              Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .    "
},
{
  "id": "exercise-79",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-79",
  "type": "Exercise",
  "number": "3.4.4.1",
  "title": "",
  "body": " For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .   "
},
{
  "id": "ex-compose-reflections",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-compose-reflections",
  "type": "Exercise",
  "number": "3.4.4.2",
  "title": "",
  "body": " This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.   "
},
{
  "id": "exercise-81",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-81",
  "type": "Exercise",
  "number": "3.4.4.3",
  "title": "",
  "body": " Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.   "
},
{
  "id": "exercise-82",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-82",
  "type": "Exercise",
  "number": "3.4.4.4",
  "title": "",
  "body": " If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .   "
},
{
  "id": "exercise-83",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-83",
  "type": "Exercise",
  "number": "3.4.4.5",
  "title": "",
  "body": " We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .   "
},
{
  "id": "exercise-84",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-84",
  "type": "Exercise",
  "number": "3.4.4.6",
  "title": "",
  "body": " Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .   "
},
{
  "id": "exercise-85",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-85",
  "type": "Exercise",
  "number": "3.4.4.7",
  "title": "",
  "body": " This exericse concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effort of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .   "
},
{
  "id": "ex-reflection-compose-general",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-reflection-compose-general",
  "type": "Exercise",
  "number": "3.4.4.8",
  "title": "",
  "body": " We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that    "
},
{
  "id": "sec-matrix-inverse",
  "level": "1",
  "url": "sec-matrix-inverse.html",
  "type": "Section",
  "number": "4.1",
  "title": "Invertibility",
  "body": " Invertibility   Up to this point, we have used the Gaussian elimination algorithm to find solutions to linear systems, which is equivalent to solving matrix equations of the form , where is a vector of constants, a matrix of coefficients, and a vector of variables, or unknowns. We now investigate another way to find solutions to the equation when the matrix is square. To get started, let's look at some familiar examples.      Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .   Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.    Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then        Invertible matrices  The preview activity began with a familiar type of equation, , and asked for a strategy to solve it. One possible response is to divide both sides by 3. Instead, let's rephrase this as multiplying by , the multiplicative inverse of 3.  Now that we are interested in solving equations of the form , we might try to find a similar approach. Is there a matrix that plays the role of the multiplicative inverse of ? Of course, the real number does not have a multiplicative inverse so we probably shouldn't expect every matrix to have a multiplicative inverse. We will see, however, that many do.   invertible  matrix, inverse   An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .     matrix, square Notice that we only define invertibility for matrices that have the same number of rows and columns in which case we say that the matrix is square .    Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.      This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.       We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :     In fact, this reformulation will always work. Suppose that is an invertible matrix with inverse . Suppose furthermore that is any -dimensional vector and consider the equation . We know that is a solution because     If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .    Notice that this is similar to saying that the solution to is , as we saw in the preview activity.  Now since is consistent for every vector , the columns of must span so there is a pivot position in every row. Since is also square, this means that the reduced row echelon form of is the identity matrix.    The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:     You may have noticed that says that the solution to the equation is . Indeed, we know that this equation has a unique solution because has a pivot position in every column.  It is important to remember that the product of two matrices depends on the order in which they are multiplied. That is, if and are matrices, then it sometimes happens that . However, something fortunate happens when we consider invertibility. It turns that if is an matrix and that , then it is also true that . We have verified this in a few examples so far, and explains why it always happens. This leads to the following proposition.    If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,       Solving equations with an inverse  If is an invertible matrix, then shows us how to use to solve equations involving . In particular, the solution to is .    We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Python, you can find it's inverse as np.linalg.inv(B) . Use Python to find the inverse of the matrix and use the inverse to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Python tells us that .    Python helps us see that , which tells us that is invertible.   We find that .   We see that       The next proposition summarizes much of what we have found about invertible matrices.   Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .       Formulas for inverse matrices  There is a simple formula for finding the inverse of a matrix: , which can be easily checked. The condition that be invertible is, in this case, reduced to the condition that . We will understand this condition better once we have explored determinants in . There is a similar formula for the inverse of a matrix, but there is not a good reason to write it here.     Summary  In this section, we found conditions guaranteeing that a matrix has an inverse. When these conditions hold, we also found an algorithm for finding the inverse.  A square matrix is invertible if there is a matrix , known as the inverse of , such that . We usually write .  The matrix is invertible if and only if it is row equivalent to , the identity matrix.  If a matrix is invertible, we can use Gaussian elimination to find its inverse: .  If a matrix is invertible, then the solution to the equation is .       Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .     In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as     diagonal matrix matrix diagonal  Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.         left inverse of a matrix matrix inverse left  Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .     Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .     Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.    similar matrices matrix similar  We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .       Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.          We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have       "
},
{
  "id": "exploration-9",
  "level": "2",
  "url": "sec-matrix-inverse.html#exploration-9",
  "type": "Preview Activity",
  "number": "4.1.1",
  "title": "",
  "body": "    Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .   Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.    Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then     "
},
{
  "id": "definition-13",
  "level": "2",
  "url": "sec-matrix-inverse.html#definition-13",
  "type": "Definition",
  "number": "4.1.1",
  "title": "",
  "body": " invertible  matrix, inverse   An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .   "
},
{
  "id": "example-24",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-24",
  "type": "Example",
  "number": "4.1.2",
  "title": "",
  "body": "  Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.   "
},
{
  "id": "activity-27",
  "level": "2",
  "url": "sec-matrix-inverse.html#activity-27",
  "type": "Activity",
  "number": "4.1.2",
  "title": "",
  "body": "  This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.    "
},
{
  "id": "example-inverse-augment-I",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-inverse-augment-I",
  "type": "Example",
  "number": "4.1.3",
  "title": "",
  "body": "  We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :    "
},
{
  "id": "proposition-inverse-solve",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-solve",
  "type": "Proposition",
  "number": "4.1.4",
  "title": "",
  "body": "  If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .   "
},
{
  "id": "proposition-invertible-rref",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-rref",
  "type": "Proposition",
  "number": "4.1.5",
  "title": "",
  "body": "  The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:    "
},
{
  "id": "proposition-inverse-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-inverse",
  "type": "Proposition",
  "number": "4.1.6",
  "title": "",
  "body": "  If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,    "
},
{
  "id": "activity-28",
  "level": "2",
  "url": "sec-matrix-inverse.html#activity-28",
  "type": "Activity",
  "number": "4.1.3",
  "title": "",
  "body": "  We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Python, you can find it's inverse as np.linalg.inv(B) . Use Python to find the inverse of the matrix and use the inverse to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Python tells us that .    Python helps us see that , which tells us that is invertible.   We find that .   We see that      "
},
{
  "id": "proposition-invertible-properties",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-properties",
  "type": "Proposition",
  "number": "4.1.7",
  "title": "Properties of invertible matrices.",
  "body": " Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .     "
},
{
  "id": "remark-3",
  "level": "2",
  "url": "sec-matrix-inverse.html#remark-3",
  "type": "Remark",
  "number": "4.1.8",
  "title": "Formulas for inverse matrices.",
  "body": " Formulas for inverse matrices  There is a simple formula for finding the inverse of a matrix: , which can be easily checked. The condition that be invertible is, in this case, reduced to the condition that . We will understand this condition better once we have explored determinants in . There is a similar formula for the inverse of a matrix, but there is not a good reason to write it here.  "
},
{
  "id": "exercise-87",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-87",
  "type": "Exercise",
  "number": "4.1.4.1",
  "title": "",
  "body": " Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .   "
},
{
  "id": "exercise-88",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-88",
  "type": "Exercise",
  "number": "4.1.4.2",
  "title": "",
  "body": " In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as    "
},
{
  "id": "exercise-89",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-89",
  "type": "Exercise",
  "number": "4.1.4.3",
  "title": "",
  "body": "diagonal matrix matrix diagonal  Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.        "
},
{
  "id": "exercise-90",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-90",
  "type": "Exercise",
  "number": "4.1.4.4",
  "title": "",
  "body": "left inverse of a matrix matrix inverse left  Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .   "
},
{
  "id": "exercise-91",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-91",
  "type": "Exercise",
  "number": "4.1.4.5",
  "title": "",
  "body": " Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .   "
},
{
  "id": "exercise-92",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-92",
  "type": "Exercise",
  "number": "4.1.4.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .   "
},
{
  "id": "exercise-93",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-93",
  "type": "Exercise",
  "number": "4.1.4.7",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.   "
},
{
  "id": "exercise-94",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-94",
  "type": "Exercise",
  "number": "4.1.4.8",
  "title": "",
  "body": "similar matrices matrix similar  We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .     "
},
{
  "id": "exercise-95",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-95",
  "type": "Exercise",
  "number": "4.1.4.9",
  "title": "",
  "body": " Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.        "
},
{
  "id": "ex-right-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#ex-right-inverse",
  "type": "Exercise",
  "number": "4.1.4.10",
  "title": "",
  "body": " We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have     "
},
{
  "id": "subsec-triangular-invertible",
  "level": "1",
  "url": "subsec-triangular-invertible.html",
  "type": "Section",
  "number": "4.2",
  "title": "Triangular matrices and Gaussian elimination",
  "body": " Triangular matrices and Gaussian elimination   With some of the ideas we've developed, we can recast the Gaussian elimination algorithm in terms of matrix multiplication and invertibility. These ideas will be especially helpful later when we consider determinants and LU factorizations. Triangular matrices will play an important role.    Triangular matrices   lower triangular matrix  upper triangular matrix  diagonal matrix   We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero. A matrix that is both upper and lower triangular is called diagonal because all of its non-zero entries are along the diagonal of the matrix.    For example, the matrix below is a lower triangular matrix while is upper triangular and is diagonal. .  We can develop a simple test to determine whether an lower triangular matrix is invertible. Let's use Gaussian elimination to find the reduced row echelon form of the lower triangular matrix Because the entries on the diagonal are nonzero, we find a pivot position in every row, which tells us that the matrix is invertible.  If, however, there is a zero entry on the diagonal, the matrix cannot be invertible. Considering the matrix below, we see that having a zero on the diagonal leads to a row without a pivot position.     An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.      Elementary matrices   Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .    Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?    Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?         Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .      matrix, elementary The following are examples of matrices, known as elementary matrices , that perform the row operations on a matrix having three rows.  Replacement  Multiplying the second row by 3 and adding it to the third row is performed by These matrices are always have 1's along the diagonal and exactly one additional non-zero entry. Notice that these matrices are always triangular. During the forward phase of Gaussian elimination, they will be lower diagonal, so we will sometimes use to emphasize this.  Scaling  Multiplying the third row by 2 is performed by Scaling matrices are diagonal with at most one entry that is not a 1.  Interchange  Interchanging the first two rows is performed by We use rather than because we reserve for the identity matrix. P stands for permutation since an interchange permutes (changes the order of) the rows. These matrices are formed by swapping two rows of the identity matrix.      Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. We can see this by simple algebra. That is the effect of , which is obtained by negating the off diagonal entry of .  Notice that we now have , which gives where is the lower triangular matrix  factorization  factorization This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .      Summary  In this section, we learned that the elementary row operations of replacement, scaling, and interchange can be performed via multiplication by elementary matrices . Elementary matrices can be formed as slight modifications of identity matrices.    Replacement : The elementary matrices for replacement differ from an identity matrix in exactly one entry. These matrices are triangular with one off-diagonal entry that is non-zero. (For the replacements needed during the forward phase of Gaussian elimination, these will all be lower triangular .)     Scaling : The elementary matrices for scaling also differ from an identity matrix in exactly one entry. These matrices are diagonal with one diagonal entry replaced by the scaling factor.     Interchange : The elementary matrices for interchange are formed by swapping two rows of an identity matrix. (This changes two 0s to 1s and two 1s to 0s.)     The inverse of any elementary matrix also has a simple form that is easily determined.  elementary matrices  inverses of      Replacement : Flip the sign on the off-diagonal entry.     Scaling by : Scale by to undo the scaling.     Interchange : Interchange matrices are their own inverses.      Along they way we saw that triangular matrices also have a nice property, namely   Square triangular matrices are invertible if and only if all the diagonal entries are non-zero. Furthermore, their inverses are easily computed via back substitution.        If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .     Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .   .     "
},
{
  "id": "definition-14",
  "level": "2",
  "url": "subsec-triangular-invertible.html#definition-14",
  "type": "Definition",
  "number": "4.2.1",
  "title": "",
  "body": " lower triangular matrix  upper triangular matrix  diagonal matrix   We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero. A matrix that is both upper and lower triangular is called diagonal because all of its non-zero entries are along the diagonal of the matrix.   "
},
{
  "id": "proposition-triangular-invertibility",
  "level": "2",
  "url": "subsec-triangular-invertible.html#proposition-triangular-invertibility",
  "type": "Proposition",
  "number": "4.2.2",
  "title": "",
  "body": "  An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.   "
},
{
  "id": "activity-29",
  "level": "2",
  "url": "subsec-triangular-invertible.html#activity-29",
  "type": "Activity",
  "number": "4.2.1",
  "title": "Gaussian elimination and matrix multiplication.",
  "body": " Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .    Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?    Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?         Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .    "
},
{
  "id": "p-2716",
  "level": "2",
  "url": "subsec-triangular-invertible.html#p-2716",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "elementary matrices "
},
{
  "id": "example-26",
  "level": "2",
  "url": "subsec-triangular-invertible.html#example-26",
  "type": "Example",
  "number": "4.2.3",
  "title": "",
  "body": "  Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. We can see this by simple algebra. That is the effect of , which is obtained by negating the off diagonal entry of .  Notice that we now have , which gives where is the lower triangular matrix  factorization  factorization This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .   "
},
{
  "id": "p-2724",
  "level": "2",
  "url": "subsec-triangular-invertible.html#p-2724",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "elementary matrices Replacement triangular lower triangular Scaling diagonal Interchange "
},
{
  "id": "p-2728",
  "level": "2",
  "url": "subsec-triangular-invertible.html#p-2728",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Replacement Scaling Interchange "
},
{
  "id": "exercise-97",
  "level": "2",
  "url": "subsec-triangular-invertible.html#exercise-97",
  "type": "Exercise",
  "number": "4.2.4.1",
  "title": "",
  "body": " If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .   "
},
{
  "id": "exercise-98",
  "level": "2",
  "url": "subsec-triangular-invertible.html#exercise-98",
  "type": "Exercise",
  "number": "4.2.4.2",
  "title": "",
  "body": " Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .   .   "
},
{
  "id": "sec-bases",
  "level": "1",
  "url": "sec-bases.html",
  "type": "Section",
  "number": "4.3",
  "title": "Bases and coordinate systems",
  "body": " Bases and coordinate systems   Standard Cartesian coordinates are commonly used to describe points in the plane. If we mention the point , we know that we arrive at this point from the origin by moving four units to the right and three units up.  Sometimes, however, it is more natural to work in a different coordinate system. Suppose that you live in the city whose map is shown in and that you would like to give a guest directions for getting from your house to the store. You would probably say something like, \"Go four blocks up Maple. Then turn left on Main for three blocks.\" The grid of streets in the city gives a more natural coordinate system than standard north-south, east-west coordinates.   A city map.      In this section, we will develop the concept of a basis through which we will create new coordinate systems in . We will see that the right choice of a coordinate system provides a more natural way to approach some problems.    Consider the vectors in , which are shown in .   Linear combinations of and .        Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.      In the preview activity, we worked with a set of two vectors in and found that we could express any vector in in two different ways: in the usual way where the components of the vector describe horizontal and vertical changes, and in a new way as a linear combination of and . We could also translate between these two descriptions. This example illustrates the central idea of this section.    Bases  In the preview activity, we created a new coordinate system for using linear combinations of a set of two vectors. More generally, the following definition will guide us.   basis   A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.      We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .    Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.     We can develop a test to determine if a set of vectors forms a basis for by considering the matrix . To be a basis, this set of vectors must span and be linearly independent.  We know that the span of the set of vectors is if and only if has a pivot position in every row. We also know that the set of vectors is linearly independent if and only if has a pivot position in every column. This means that a set of vectors forms a basis if and only if has a pivot in every row and every column. Therefore, must be row equivalent to the identify matrix : .  In addition to helping identify bases, this fact tells us something important about the number of vectors in a basis. Since the matrix has a pivot position in every row and every column, it must have the same number of rows as columns. Therefore, the number of vectors in a basis for must be . For example, a basis for must have exactly 23 vectors.    A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .      Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard       Coordinate systems  A basis for forms a coordinate system for , as we will describe. Rather than continuing to write a list of vectors, we will find it convenient to denote a basis using a single symbol, such as     In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .    This example illustrates how a basis in provides a new coordinate system for and shows how we may translate between this coordinate system and the standard one.  More generally, suppose that is a basis for . We know that the span of the vectors is , which implies that any vector in can be written as a linear combination of the vectors. In addition, we know that the vectors are linearly independent, which means that we can write as a linear combination of the vectors in exactly one way. Therefore, we have where the weights are unique. In this case, we write the coordinate description of in the basis as .    Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,      This activity demonstrates how we can efficiently convert between coordinate systems defined by different bases. Let's consider a basis and a vector . We know that If we use to denote the matrix whose columns are the basis vectors, then we find that where . This means that the matrix converts coordinates in the basis into standard coordinates.  Since the columns of are the basis vectors , we know that , and is therefore invertible. Since we have , we must also have .    If is a basis and the matrix whose columns are the basis vectors, then     If we have another basis , we find, in the same way, that for the conversion between coordinates in the basis into standard coordinates. We then have . Therefore, is the matrix that converts -coordinates into -coordinates.    Examples of bases  We will now look at some examples of bases that illustrate how it can be useful to study a problem using a different coordinate system.    Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .      Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in    A representation of the basis elements of .      To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis , and the change of basis is known as a Haar wavelet transform . Haar wavelet basis  Haar wavelet transform In the next section, we will see how this basis provides a useful way to store digital images.     Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artist effects.   A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.       We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .    Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.       Summary  We defined a basis to be a set of vectors that is linearly independent and whose span is .  A set of vectors forms a basis for if and only if the matrix . This means there must be vectors in a basis for .  If forms a basis for , then any vector in can be written as a linear combination of the vectors in exactly one way.  We used the basis to define a coordinate system in which , the coordinates of in the basis , are defined by .   Forming the matrix whose columns are the basis vectors, we can convert between coordinate systems: .       Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .     Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .     Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .     Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .     This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .     Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .     Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .     Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.     Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .     "
},
{
  "id": "fig-city-map",
  "level": "2",
  "url": "sec-bases.html#fig-city-map",
  "type": "Figure",
  "number": "4.3.1",
  "title": "",
  "body": " A city map.     "
},
{
  "id": "p-2764",
  "level": "2",
  "url": "sec-bases.html#p-2764",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "basis "
},
{
  "id": "exploration-10",
  "level": "2",
  "url": "sec-bases.html#exploration-10",
  "type": "Preview Activity",
  "number": "4.3.1",
  "title": "",
  "body": "  Consider the vectors in , which are shown in .   Linear combinations of and .        Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.     "
},
{
  "id": "definition-15",
  "level": "2",
  "url": "sec-bases.html#definition-15",
  "type": "Definition",
  "number": "4.3.3",
  "title": "",
  "body": " basis   A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.   "
},
{
  "id": "activity-30",
  "level": "2",
  "url": "sec-bases.html#activity-30",
  "type": "Activity",
  "number": "4.3.2",
  "title": "",
  "body": "  We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .    Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.    "
},
{
  "id": "proposition-20",
  "level": "2",
  "url": "sec-bases.html#proposition-20",
  "type": "Proposition",
  "number": "4.3.4",
  "title": "",
  "body": "  A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .   "
},
{
  "id": "example-27",
  "level": "2",
  "url": "sec-bases.html#example-27",
  "type": "Example",
  "number": "4.3.5",
  "title": "",
  "body": "  Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard    "
},
{
  "id": "example-28",
  "level": "2",
  "url": "sec-bases.html#example-28",
  "type": "Example",
  "number": "4.3.6",
  "title": "",
  "body": "  In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .   "
},
{
  "id": "activity-31",
  "level": "2",
  "url": "sec-bases.html#activity-31",
  "type": "Activity",
  "number": "4.3.3",
  "title": "",
  "body": "  Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,     "
},
{
  "id": "proposition-coordinate-transform",
  "level": "2",
  "url": "sec-bases.html#proposition-coordinate-transform",
  "type": "Proposition",
  "number": "4.3.7",
  "title": "",
  "body": "  If is a basis and the matrix whose columns are the basis vectors, then    "
},
{
  "id": "example-29",
  "level": "2",
  "url": "sec-bases.html#example-29",
  "type": "Example",
  "number": "4.3.8",
  "title": "",
  "body": "  Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .   "
},
{
  "id": "example-wavelet-basis",
  "level": "2",
  "url": "sec-bases.html#example-wavelet-basis",
  "type": "Example",
  "number": "4.3.9",
  "title": "",
  "body": "  Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in    A representation of the basis elements of .      To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis , and the change of basis is known as a Haar wavelet transform . Haar wavelet basis  Haar wavelet transform In the next section, we will see how this basis provides a useful way to store digital images.   "
},
{
  "id": "activity-32",
  "level": "2",
  "url": "sec-bases.html#activity-32",
  "type": "Activity",
  "number": "4.3.4",
  "title": "Edge detection.",
  "body": " Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artist effects.   A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.       We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .    Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.    "
},
{
  "id": "exercise-99",
  "level": "2",
  "url": "sec-bases.html#exercise-99",
  "type": "Exercise",
  "number": "4.3.5.1",
  "title": "",
  "body": " Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .   "
},
{
  "id": "exercise-100",
  "level": "2",
  "url": "sec-bases.html#exercise-100",
  "type": "Exercise",
  "number": "4.3.5.2",
  "title": "",
  "body": " Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .   "
},
{
  "id": "exercise-101",
  "level": "2",
  "url": "sec-bases.html#exercise-101",
  "type": "Exercise",
  "number": "4.3.5.3",
  "title": "",
  "body": " Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .   "
},
{
  "id": "exercise-102",
  "level": "2",
  "url": "sec-bases.html#exercise-102",
  "type": "Exercise",
  "number": "4.3.5.4",
  "title": "",
  "body": " Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .   "
},
{
  "id": "exercise-103",
  "level": "2",
  "url": "sec-bases.html#exercise-103",
  "type": "Exercise",
  "number": "4.3.5.5",
  "title": "",
  "body": " This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .   "
},
{
  "id": "exercise-104",
  "level": "2",
  "url": "sec-bases.html#exercise-104",
  "type": "Exercise",
  "number": "4.3.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .   "
},
{
  "id": "exercise-105",
  "level": "2",
  "url": "sec-bases.html#exercise-105",
  "type": "Exercise",
  "number": "4.3.5.7",
  "title": "",
  "body": " Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .   "
},
{
  "id": "exercise-106",
  "level": "2",
  "url": "sec-bases.html#exercise-106",
  "type": "Exercise",
  "number": "4.3.5.8",
  "title": "",
  "body": " Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.   "
},
{
  "id": "exercise-107",
  "level": "2",
  "url": "sec-bases.html#exercise-107",
  "type": "Exercise",
  "number": "4.3.5.9",
  "title": "",
  "body": " Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .   "
},
{
  "id": "sec-jpeg",
  "level": "1",
  "url": "sec-jpeg.html",
  "type": "Section",
  "number": "4.4",
  "title": "Image compression",
  "body": " Image compression   Digital images, such as the photographs taken on your phone, are displayed as a rectangular array of pixels. For example, the photograph in is 1440 pixels wide and 1468 pixels high. If we were to zoom in on the photograph, we would be able to see individual pixels, such as those shown on the right.   An image stored as a array of pixels along with a close-up of a smaller array.       A lot of data is required to display this image. A quantity of digital data is frequently measured in bytes, where one byte is the amount of storage needed to record an integer between 0 and 255. As we will see shortly, each pixel requires three bytes to record that pixel's color. This means the amount of data required to display this image is bytes or about 6.3 megabytes.  Of course, we would like to store this image on a phone or computer and perhaps transmit it through our data plan to share it with others. If possible, we would like to find a way to represent this image using a smaller amount of data so that we don't run out of memory on our phone and quickly exhaust our data plan.  As we will see in this section, the JPEG compression algorithm provides a means for doing just that. This image, when stored in the JPEG format, requires only 467,359 bytes of data, which is about 7% of the 6.3 megabytes required to display the image. That is, when we display this image, we are reconstructing it from only 7% of the original data. This isn't too surprising since there is quite a bit of redundancy in the image; the left half of the image is almost uniformly blue. The JPEG algorithm detects this redundancy by representing the data using bases that are well-suited to the task.    Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .    If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .       Color models  A color is represented digitally by a vector in . There are different ways in which we can represent colors, however, depending on whether a computer or a human will be processing the color. We will describe two of these representations, called color models , and demonstrate how they are used in the JPEG compression algorithm. color model   Digital displays typically create colors by blending together various amounts of red, green, and blue. We can therefore describe a color by putting its constituent amounts of red, green, and blue into a vector . The quantities , , and are stored with one byte of information so they are integers between 0 and 255. This is called the RGB color model . RGB color model   We define a basis where to define a new coordinate system with coordinates we denote , , and : . luminance  chrominance The coordinate is called luminance while and are called blue and red chrominance , respectively. In this coordinate system, luminance will vary from 0 to 255, while the chrominances vary between -127.5 and 127.5. This is known as the color model , also denoted YCbCr. (To be completely accurate, we should add 127.5 to the chrominance values so that they lie between 0 and 255, but we won't worry about that here.) YCbCr color model     This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.     These two color models provide us with two ways to represent colors, each of which is useful in a certain context. Digital displays, such as those in phones and computer monitors, create colors by combining various amounts of red, green, and blue. The model is therefore most relevant in digital applications.  By contrast, the color model was created based on research into human vision and aims to concentrate the most visually important data into a single coordinate, the luminance, to which our eyes are most sensitive. Of course, any basis of must have three vectors so we need two more coordinates, blue and red chrominance, if we want to represent all colors.  To see this explicitly, shown in is the original image and the image as rendered with only the luminance. That is, on the right, the color of each pixel is represented by only one byte, which is the luminance. This image essentially looks like a grayscale version of the original image with all its visual detail. In fact, before digital television became the standard, television signals were broadcast using the color model. When a signal was displayed on a black-and-white television, the luminance was displayed and the two chrominance values simply ignored.   The original image rendered with only the luminance values.       For comparison, shown in are the corresponding images created using only the blue chrominance and the red chrominance. Notice that the amount of visual detail is considerably less in these images.   The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.       The aim of the JPEG compression algorithm is to represent an image using the smallest amount of data possible. By converting from the color model to the color model, we are concentrating the most visually important data into the luminance values. This is helpful because we can safely ignore some of the data in the chrominance values since that data is not as visually important.    The JPEG compression algorithm  The key to representing the image using a smaller amount of data is to detect redundancies in the data. To begin, we first break the image, which is composed of pixels, into small blocks of pixels. For example, we will consider the block of pixels outlined in green in the original image, shown on the left of . The image on the right zooms in on the block.   An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.       Notice that this block, as seen in the original image, is very small. If we were to change some of the colors in this block slightly, our eyes would probably not notice.   The block under consideration.   Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.     Following our earlier work, we will change the representation of colors from the color model to the model. This separates the colors into luminance and chrominance values that we will consider separately. In , we see the luminance values of this block. Again, notice how these values do not vary significantly over the block.   The luminance values in this block.       Our strategy in the compression algorithm is to perform a change of basis to take advantage of the fact that the luminance values do not change significantly over the block. Rather than recording the luminance of each of the pixels, this change of basis will allow us to record the average luminance along with some information about how the individual colors vary from the average.  Let's look at the first column of luminance values, which is a vector in : . We will perform a change of basis and describe this vector by the average of the luminance values and information about variations from the average.   Discrete Fourier Transform The JPEG compression algorithm uses the Discrete Fourier Transform , which is defined using the basis whose basis vectors are   On first glance, this probably looks intimidating, but we can make sense of it by looking at these vectors graphically. Shown in are four of these basis vectors. Notice that is constantly 1, varies relatively slowly, varies a little more rapidly, and varies quite rapidly. The main thing to notice is that the basis vectors vary at different rates with the first vectors varying relatively slowly and the later vectors varying more rapidly.   Four of the basis vectors , , , and .       These vectors form the basis for . Remember that is the vector of luminance values in the first column as seen on the right. We will write in the new coordinates . The coordinates are called the Fourier coefficients of the vector .      We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Python cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Python cell and notice that it prints the matrix .   Now look at the form of and explain why is the average of the luminance values in the vector .  The Python cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell.   Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector .    How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Python cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.    Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.         Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.     Up to this point, we have been working with the luminance values in one block of our image. We formed the Fourier coefficients for each of the columns of this block. Once we notice that the Fourier coefficients across a row are relatively constant, it seems reasonable to find the Fourier coefficients of the rows of the matrix of Fourier coefficients. Doing so leads to the matrix .  If we were to look inside a JPEG image file, we would see lots of matrices like this. For each block, there would be three matrices of Fourier coefficients of the rows of Fourier coefficients, one matrix for each of the luminance, blue chrominance, and red chrominance values. However, we store these Fourier coefficients as integers inside the JPEG file so we need to round off the coefficients to the nearest integer, as shown here: .  There are many zeroes in this matrix, and we can save space in a JPEG image file by only recording the nonzero Fourier coefficients.  In fact, when a JPEG file is created, there is a quality parameter that can be set, such as that shown in . When the quality parameter is high, we will store many of the Fourier coefficients; when it is low, we will ignore more of them.   When creating a JPEG file, we choose a value of the quality parameter.      To see how this works, suppose the quality setting is relatively high. After rounding off the Fourier coefficients, we will set all of the coefficients whose absolute value is less than 2 to zero, which creates the matrix: Notice that there are 12 nonzero Fourier coefficients, out of 64, that we need to record. Consequently, we only save of the data.  If instead, the quality setting is relatively low, we set all of the Fourier coefficients whose absolute value is less than 4 to zero, creating the matrix: . Notice that there are only 5 nonzero Fourier coefficients that we need to record now, meaning we save only of the data. This will result in a smaller JPEG file describing the image.  With a lower quality setting, we have thrown away more information about the Fourier coefficients so the image will not be reconstructed as accurately. To see this, we can reconstruct the luminance values from the Fourier coefficients by converting back into the standard coordinate system. Rather than showing the luminance values themselves, we will show the difference in the original luminance values and the reconstructed luminance values. When the quality setting was high and we stored 12 Fourier coefficients, we find this difference to be . When the quality setting is lower and we store only 5 Fourier coefficients, the difference is .  This demonstrates the trade off. With a high quality setting, we require more storage to save more of the data, but the reconstructed image is closer to the original. With the lower quality setting, we require less storage, but the reconstructed image differs more from the original.  If we remember that the visual information stored by the blue and red chrominance values is not as important as that contained in the luminance values, we feel safer in discarding more of the Fourier coefficients for the chrominance values resulting in an even greater savings.  Shown in is the original image compared to a version stored with a very low quality setting. If you look carefully, you can individual blocks.   The original image and the result of storing the image with a low quality setting.       This discussion of the JPEG compression algorithm is meant to explore the ideas that underlie its construction and demonstrate the importance of a choice of basis and its accompanying coordinate system. There are a few details, most notably about the rounding of the Fourier coefficients, that are not strictly accurate. The actual implementation is a little more complicated, but the presentation here conveys the spirit of the algorithm.  The JPEG compression algorithm allows us to store image files using only a fraction of the data. Similar ideas are used to efficiently store digital music and video files.    Summary  This section has explored how appropriate changes in bases help us reconstruct an image using only a fraction of its data. This is known as image compression.  There are several ways of representing colors, all of which use vectors in . We explored the color model, which is appropriate in digital applications, and the model, in which the most important visual information is conveyed by the component, known as luminance.  We also explored a change of basis called the Discrete Fourier Transform. In the coordinate system that results, the first coefficient measures the average of the components of a vector. Other coefficients measure variations in the components away from the average.  We put both of these ideas to use in demonstrating the JPEG compression algorithm. An image is broken into blocks, and the colors into luminance, blue chrominance, and red chrominance. Applying the Discrete Fourier Transform allows us to reconstruct a good approximation of the image using only a fraction of the original data.       Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.     There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     "
},
{
  "id": "fig-jpeg-orig",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-orig",
  "type": "Figure",
  "number": "4.4.1",
  "title": "",
  "body": " An image stored as a array of pixels along with a close-up of a smaller array.      "
},
{
  "id": "exploration-11",
  "level": "2",
  "url": "sec-jpeg.html#exploration-11",
  "type": "Preview Activity",
  "number": "4.4.1",
  "title": "",
  "body": "  Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .    If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .    "
},
{
  "id": "p-3047",
  "level": "2",
  "url": "sec-jpeg.html#p-3047",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "color models "
},
{
  "id": "p-3048",
  "level": "2",
  "url": "sec-jpeg.html#p-3048",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "RGB color model "
},
{
  "id": "p-3049",
  "level": "2",
  "url": "sec-jpeg.html#p-3049",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "luminance chrominance color model "
},
{
  "id": "activity-33",
  "level": "2",
  "url": "sec-jpeg.html#activity-33",
  "type": "Activity",
  "number": "4.4.2",
  "title": "",
  "body": "  This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.    "
},
{
  "id": "fig-jpeg-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-luminance",
  "type": "Figure",
  "number": "4.4.4",
  "title": "",
  "body": " The original image rendered with only the luminance values.      "
},
{
  "id": "fig-jpeg-chrominance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-chrominance",
  "type": "Figure",
  "number": "4.4.5",
  "title": "",
  "body": " The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.      "
},
{
  "id": "fig-jpeg-block",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block",
  "type": "Figure",
  "number": "4.4.6",
  "title": "",
  "body": " An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.      "
},
{
  "id": "fig-jpeg-block-zoom",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-zoom",
  "type": "Figure",
  "number": "4.4.7",
  "title": "",
  "body": " The block under consideration.   Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.    "
},
{
  "id": "fig-jpeg-block-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-luminance",
  "type": "Figure",
  "number": "4.4.8",
  "title": "",
  "body": " The luminance values in this block.      "
},
{
  "id": "p-3123",
  "level": "2",
  "url": "sec-jpeg.html#p-3123",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Discrete Fourier Transform "
},
{
  "id": "fig-jpeg-fourier-basis",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-fourier-basis",
  "type": "Figure",
  "number": "4.4.9",
  "title": "",
  "body": " Four of the basis vectors , , , and .     "
},
{
  "id": "p-3125",
  "level": "2",
  "url": "sec-jpeg.html#p-3125",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Fourier coefficients "
},
{
  "id": "activity-34",
  "level": "2",
  "url": "sec-jpeg.html#activity-34",
  "type": "Activity",
  "number": "4.4.3",
  "title": "",
  "body": "  We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Python cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Python cell and notice that it prints the matrix .   Now look at the form of and explain why is the average of the luminance values in the vector .  The Python cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell.   Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector .    How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Python cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.    Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.         Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.    "
},
{
  "id": "fig-jpeg-quality",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-quality",
  "type": "Figure",
  "number": "4.4.13",
  "title": "",
  "body": " When creating a JPEG file, we choose a value of the quality parameter.     "
},
{
  "id": "fig-jpeg-image-low",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-image-low",
  "type": "Figure",
  "number": "4.4.14",
  "title": "",
  "body": " The original image and the result of storing the image with a low quality setting.      "
},
{
  "id": "exercise-108",
  "level": "2",
  "url": "sec-jpeg.html#exercise-108",
  "type": "Exercise",
  "number": "4.4.4.1",
  "title": "",
  "body": " Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.   "
},
{
  "id": "exercise-109",
  "level": "2",
  "url": "sec-jpeg.html#exercise-109",
  "type": "Exercise",
  "number": "4.4.4.2",
  "title": "",
  "body": " There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.   "
},
{
  "id": "exercise-110",
  "level": "2",
  "url": "sec-jpeg.html#exercise-110",
  "type": "Exercise",
  "number": "4.4.4.3",
  "title": "",
  "body": " The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .   "
},
{
  "id": "exercise-111",
  "level": "2",
  "url": "sec-jpeg.html#exercise-111",
  "type": "Exercise",
  "number": "4.4.4.4",
  "title": "",
  "body": " In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.   "
},
{
  "id": "exercise-112",
  "level": "2",
  "url": "sec-jpeg.html#exercise-112",
  "type": "Exercise",
  "number": "4.4.4.5",
  "title": "",
  "body": " In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .   "
},
{
  "id": "sec-determinants",
  "level": "1",
  "url": "sec-determinants.html",
  "type": "Section",
  "number": "4.5",
  "title": "Determinants",
  "body": " Determinants   As invertibility plays a central role in this chapter, we need a criterion that tells us when a matrix is invertible. We already know that a square matrix is invertible if and only if it is row equivalent to the identity matrix. In this section, we will develop a second, numerical criterion that tells us when a square matrix is invertible.  To begin, let's consider a matrix whose columns are vectors and . We have frequently drawn the vectors and studied the linear combinations they form using a figure such as .   Linear combinations of two vectors and form a collection of congruent parallelograms.      Notice how the linear combinations form a set of congruent parallelograms in the plane. In this section, we will use the area of these parallelograms to define a numerical quantity called the determinant that tells us whether the matrix is invertible.   To recall, the area of parallelogram is found by multiplying the length of one side by the perpendicular distance to its parallel side. Using the notation in the figure, the area of the parallelogram is .      We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.       Determinants of matrices  We will begin by defining the determinant of a matrix . First, however, we need to define the orientation of an ordered pair of vectors. As shown in , an ordered pair of vectors and is called positively oriented if the angle, measured in the counterclockwise direction, from to is less than ; we say the pair is negatively oriented if it is more than .  orientation  of a pair of vectors     The vectors on the left are positively oriented while the ones on the right are negatively oriented.       determinant   Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is minus the area of the parallelogram.      Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have    The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.      Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .      In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .     Later in this section, we will learn an algebraic technique for computing determinants. In the meantime, we will simply note that we can define determinants for matrices by measuring the volume of a box defined by the columns of the matrix, even if this box resides in for some very large .   For example, the columns of a matrix will form a parallelpiped, like the one shown here, and there is a means by which we can classify sets of such vectors as either positively or negatively oriented. Therefore, we can define the determinant in terms of the volume of the parallelpiped, but we will not worry about the details here.    Though the previous activity deals with determinants of matrices, it illustrates some important properties of determinants that are true more generally.  If is a triangular matrix, then equals the product of the entries on the diagonal. For example, , since the two parallelograms in have equal area.   The determinant of a triangular matrix equals the product of its diagonal entries.        We also saw that because the columns form a negatively oriented pair. You may remember from that a matrix such as this is obtained by interchanging two rows of the identity matrix.  The determinant satisfies a multiplicative property, which says that Rather than simply thinking of the determinant as the area of a parallelogram, we may also think of it as a factor by which areas are scaled under the matrix transformation defined by the matrix. Applying the matrix transformation defined by will scale area by . If we then compose with the matrix transformation defined by , area will scale a second time by the factor . The net effect is that the matrix transformation defined by scales area by so that .      The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .         Determinants and invertibility  Perhaps the most important property of determinants also appeared in the previous activity. We saw that when the columns of the matrix are linearly dependent, the parallelogram formed by those vectors folds down onto a line. For instance, if , then the resulting parallelogram, as shown in , has zero area, which means that .   When the columns of are linearly dependent, we find that .      The condition that the columns of are linearly dependent is precisely the same as the condition that is not invertible. This leads us to believe that is not invertible if and only if its determinant is zero. The following proposition expresses this thought.    The matrix is invertible if and only if .    To understand this proposition more fully, let's remember that the matrix is invertible if and only if it is row equivalent to the identity matrix . We will therefore consider how the determinant changes when we perform row operations on a matrix. Along the way, we will discover an effective means to compute the determinant.  In , we saw how to describe the three row operations, scaling, interchange, and replacement, using matrix multiplication. If we perform a row operation on the matrix to obtain the matrix , we would like to relate and . To do so, remember that   Scalings are performed by multiplying a matrix by a diagonal matrix, such as which has the effect of multiplying the second row of by to obtain . Since is diagonal, we know that its determinant is the product of its diagonal entries so that . This means that and therefore In general, if we scale a row of by , we have .  Interchanges are performed by matrices such as which has the effect of interchanging the first and second rows of . As we saw in , . Therefore, when , we have In other words, when we perform an interchange.  Row replacement operations are performed by matrices such as which multiplies the first row by and adds the result to the third row. Since this is a lower triangular matrix, we know that the determinant is the product of the diagonal entries, which says that . This means that when , we have . In other words, a row replacement does not change the determinant.     The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a replacement operation, then .         We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether invertible?  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row.  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains not zero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.     As seen in this activity, row operations can be used to compute the determinant of a matrix. More specifically, applying the forward substitution phase of Gaussian elimination to the matrix leads us to an upper triangular matrix so that .  We know that is invertible when all of its diagonal entries are nonzero. We also know that under the same condition. This tells us is invertible if and only if .  Now if , we also have since applying a sequence of row operations to only multiplies the determinant by a nonzero number. It then follows that is invertible so . Therefore, we also know that and so must also be invertible.  This explains and so we know that is invertible if and only if .  Finally, notice that if is invertible, we have , which tells us that Therefore, .    If is an invertible matrix, then .      Cofactor expansions  cofactor expansion  determinant via cofactor expansion  We now have a technique for computing the determinant of a matrix using row operations. There is another way to compute determinants, using what are called cofactor expansions , that will be important for us in the next chapter. We will describe this method here.  To begin, let's show how to compute the determinant of a matrix.   Determinant of a matrix  determinant of a matrix  Let . We can create an equivalent triangular matrix using . This gives us .   Note that the derivation of a formula for the determinant of a matrix relied on two facts: (a) replacement doesn't change the determinant, and (b) the determinant of a triangular matrix is the product of the diagonal elements. This same general approach could be used on any square matrix: keep applying row operations until we obtain a triangular matrix. If we use scaling or or interchange, we need to keep track of how our determinant changes. But we can obtain a triangular matrix without using scaling, so this just amounts to keeping track of a possible sign change when we use interchange.  Now that we have a formula for the determinant of a matrix in hand, we can develop the method of cofactor expansion. Using a cofactor expansion to find the determinant of a more general matrix is a little more work so we will demonstrate it with an example.    We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the .      We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . (Which row should you choose to make your work especially easy?) Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  NumPy or SciPy will compute the determinant of a matrix A with the command numpy.linalg.det() or scipy.linal.det . Use Python to find the determinant of the matrix numpy.linalg.det()  linalg.det()  scipy.linalg.det()  linalg.det()  linalg.det()  determinant  in Python  .        We will using a cofactor expansion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Python tells us that .       Summary  In this section, we associated a numerical quantity, the determinant, to a square matrix and showed how it tells us whether the matrix is invertible.   The determinant of a matrix has a geometric interpretation. In particular, when , the determinant is the signed area of the parallelogram formed by the two columns of the matrix.    The determinant satisfies many properties, including the facts that    , and    the determinant of a triangular matrix is equal to the product of its diagonal entries.       These properties helped us compute the determinant of a matrix using row operations. This also led to the important observation that the determinant of a matrix is nonzero if and only if the matrix is invertible.    Finally, we learned how to compute the determinant of a matrix using cofactor expansions , which will be a valuable tool for us in the next chapter.    We have seen three ways to compute the determinant: by interpreting the determinant as a signed area or volume; by applying appropriate row operations; and by using a cofactor expansion. It's worth spending a moment to think about the relative merits of these approaches.  The geometric definition of the determinant tells us that the determinant is measuring a natural geometric quantity, an insight that does not easily come through the other two approaches. The intuition we gain by thinking about the determinant geometrically makes it seem reasonable that the determinant should be zero for matrices that are not invertible: if the columns are linearly dependent, the vectors cannot create a positive volume.  Approaching the determinant through row operations provides an effective means of computing the determinant. In fact, this is what most computer programs do behind the scenes when they compute a determinant. This approach is also a useful theoretical tool for explaining why the determinant tells us whether a matrix is invertible.  The cofactor expansion method will be useful to us in the next chapter when we look at eigenvalues and eigenvectors. It is not, however, a practical way to compute a determinant. To see why, consider the fact that the determinant of a matrix, written as , requires us to compute two terms, and . To compute the determinant of a matrix, we need to compute three determinants, which involves terms. For a matrix, we need to compute four determinants, which produces terms. Continuing in this way, we see that the cofactor expansion of a matrix would involve terms.  By contrast, we have seen that the number of steps required to perform Gaussian elimination on an matrix is proportional to . When , we have , which points to the fact that finding the determinant using Gaussian elimination is considerably less work.     Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .    This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .     In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .     Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .     Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.     Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .     Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.     Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.     In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .     Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .    Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .     The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have      "
},
{
  "id": "fig-intro-dets",
  "level": "2",
  "url": "sec-determinants.html#fig-intro-dets",
  "type": "Figure",
  "number": "4.5.1",
  "title": "",
  "body": " Linear combinations of two vectors and form a collection of congruent parallelograms.     "
},
{
  "id": "exploration-12",
  "level": "2",
  "url": "sec-determinants.html#exploration-12",
  "type": "Preview Activity",
  "number": "4.5.1",
  "title": "",
  "body": "  We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.    "
},
{
  "id": "p-3302",
  "level": "2",
  "url": "sec-determinants.html#p-3302",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orientation positively oriented negatively oriented "
},
{
  "id": "fig-det-orientation",
  "level": "2",
  "url": "sec-determinants.html#fig-det-orientation",
  "type": "Figure",
  "number": "4.5.2",
  "title": "",
  "body": " The vectors on the left are positively oriented while the ones on the right are negatively oriented.     "
},
{
  "id": "definition-16",
  "level": "2",
  "url": "sec-determinants.html#definition-16",
  "type": "Definition",
  "number": "4.5.3",
  "title": "",
  "body": " determinant   Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is minus the area of the parallelogram.   "
},
{
  "id": "example-det-identity",
  "level": "2",
  "url": "sec-determinants.html#example-det-identity",
  "type": "Example",
  "number": "4.5.4",
  "title": "",
  "body": "  Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have    The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.      Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .   "
},
{
  "id": "activity-35",
  "level": "2",
  "url": "sec-determinants.html#activity-35",
  "type": "Activity",
  "number": "4.5.2",
  "title": "",
  "body": "  In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .    "
},
{
  "id": "fig-parallelogram-f",
  "level": "2",
  "url": "sec-determinants.html#fig-parallelogram-f",
  "type": "Figure",
  "number": "4.5.7",
  "title": "",
  "body": " The determinant of a triangular matrix equals the product of its diagonal entries.      "
},
{
  "id": "proposition-det-properties",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-properties",
  "type": "Proposition",
  "number": "4.5.8",
  "title": "",
  "body": "  The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .      "
},
{
  "id": "figure-linear-dep-det",
  "level": "2",
  "url": "sec-determinants.html#figure-linear-dep-det",
  "type": "Figure",
  "number": "4.5.9",
  "title": "",
  "body": " When the columns of are linearly dependent, we find that .     "
},
{
  "id": "prop-invertible-det",
  "level": "2",
  "url": "sec-determinants.html#prop-invertible-det",
  "type": "Proposition",
  "number": "4.5.10",
  "title": "",
  "body": "  The matrix is invertible if and only if .   "
},
{
  "id": "proposition-det-row-operations",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-row-operations",
  "type": "Proposition",
  "number": "4.5.11",
  "title": "The effect of row operations on the determinant.",
  "body": " The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a replacement operation, then .      "
},
{
  "id": "act-determinant-gaussian-elimination",
  "level": "2",
  "url": "sec-determinants.html#act-determinant-gaussian-elimination",
  "type": "Activity",
  "number": "4.5.3",
  "title": "",
  "body": "  We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether invertible?  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row.  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains not zero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.    "
},
{
  "id": "proposition-25",
  "level": "2",
  "url": "sec-determinants.html#proposition-25",
  "type": "Proposition",
  "number": "4.5.12",
  "title": "",
  "body": "  If is an invertible matrix, then .   "
},
{
  "id": "p-3371",
  "level": "2",
  "url": "sec-determinants.html#p-3371",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "cofactor expansions "
},
{
  "id": "example-32",
  "level": "2",
  "url": "sec-determinants.html#example-32",
  "type": "Example",
  "number": "4.5.13",
  "title": "",
  "body": "  We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the .   "
},
{
  "id": "activity-37",
  "level": "2",
  "url": "sec-determinants.html#activity-37",
  "type": "Activity",
  "number": "4.5.4",
  "title": "",
  "body": "  We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . (Which row should you choose to make your work especially easy?) Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  NumPy or SciPy will compute the determinant of a matrix A with the command numpy.linalg.det() or scipy.linal.det . Use Python to find the determinant of the matrix numpy.linalg.det()  linalg.det()  scipy.linalg.det()  linalg.det()  linalg.det()  determinant  in Python  .        We will using a cofactor expansion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Python tells us that .    "
},
{
  "id": "p-3395",
  "level": "2",
  "url": "sec-determinants.html#p-3395",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "cofactor expansions "
},
{
  "id": "exercise-113",
  "level": "2",
  "url": "sec-determinants.html#exercise-113",
  "type": "Exercise",
  "number": "4.5.5.1",
  "title": "",
  "body": " Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .  "
},
{
  "id": "exercise-114",
  "level": "2",
  "url": "sec-determinants.html#exercise-114",
  "type": "Exercise",
  "number": "4.5.5.2",
  "title": "",
  "body": " This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .   "
},
{
  "id": "exercise-115",
  "level": "2",
  "url": "sec-determinants.html#exercise-115",
  "type": "Exercise",
  "number": "4.5.5.3",
  "title": "",
  "body": " In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .   "
},
{
  "id": "exercise-116",
  "level": "2",
  "url": "sec-determinants.html#exercise-116",
  "type": "Exercise",
  "number": "4.5.5.4",
  "title": "",
  "body": " Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .   "
},
{
  "id": "exercise-117",
  "level": "2",
  "url": "sec-determinants.html#exercise-117",
  "type": "Exercise",
  "number": "4.5.5.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.   "
},
{
  "id": "exercise-118",
  "level": "2",
  "url": "sec-determinants.html#exercise-118",
  "type": "Exercise",
  "number": "4.5.5.6",
  "title": "",
  "body": " Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .   "
},
{
  "id": "exercise-119",
  "level": "2",
  "url": "sec-determinants.html#exercise-119",
  "type": "Exercise",
  "number": "4.5.5.7",
  "title": "",
  "body": " Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.   "
},
{
  "id": "exercise-120",
  "level": "2",
  "url": "sec-determinants.html#exercise-120",
  "type": "Exercise",
  "number": "4.5.5.8",
  "title": "",
  "body": " Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.   "
},
{
  "id": "exercise-121",
  "level": "2",
  "url": "sec-determinants.html#exercise-121",
  "type": "Exercise",
  "number": "4.5.5.9",
  "title": "",
  "body": " Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.   "
},
{
  "id": "exercise-122",
  "level": "2",
  "url": "sec-determinants.html#exercise-122",
  "type": "Exercise",
  "number": "4.5.5.10",
  "title": "",
  "body": " In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .   "
},
{
  "id": "exercise-123",
  "level": "2",
  "url": "sec-determinants.html#exercise-123",
  "type": "Exercise",
  "number": "4.5.5.11",
  "title": "",
  "body": " Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .  "
},
{
  "id": "exercise-124",
  "level": "2",
  "url": "sec-determinants.html#exercise-124",
  "type": "Exercise",
  "number": "4.5.5.12",
  "title": "",
  "body": " Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .   "
},
{
  "id": "exercise-125",
  "level": "2",
  "url": "sec-determinants.html#exercise-125",
  "type": "Exercise",
  "number": "4.5.5.13",
  "title": "",
  "body": " The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have    "
},
{
  "id": "sec-subspaces",
  "level": "1",
  "url": "sec-subspaces.html",
  "type": "Section",
  "number": "4.6",
  "title": "Subspaces",
  "body": " Subspaces   In this chapter, we have been looking at bases for , sets of vectors that are linearly independent and span . Frequently, however, we focus on only a subset of . In particular, if we are given an matrix , we have been interested in both the span of the columns of and the solution space to the homogeneous equation . In this section, we will expand the concept of basis to describe sets like these.    Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and .      The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.       Subspaces  Our goal is to develop a common framework for describing subsets like the span of the columns of a matrix and the solution space to a homogeneous equation. That leads us to the following definition.   subspace   A subspace of is a subset of that is the span of a set of vectors.    Since we have explored the concept of span in some detail, this definition just gives us a new word to describe something familiar. Let's look at some examples.   Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .     We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .       As the activity shows, it is possible to represent some subspaces as the span of more than one set of vectors. We are particularly interested in representing a subspace as the span of a linearly independent set of vectors.    dimension  basis, of a subspace  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.     A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .   Subspaces of are either   0-dimensional, consisting of the single vector ,    a 1-dimensional line,    a 2-dimensional plane, or    the 3-dimensional subspace .   There is no 4-dimensional subspace of because there is no linearly independent set of four vectors in .  There are two important subspaces associated to any matrix, each of which springs from one of our two fundamental questions, as we will now see.    The column space of   The first subspace associated to a matrix that we'll consider is its column space.    column space  If is an matrix, we call the span of its columns the column space of and denote it as .    Notice that the columns of are vectors in , which means that any linear combination of the columns is also in . Since the column space is described as the span of a set of vectors, we see that is a subspace of .    We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.      Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .   In general, a column without a pivot position can be written as a linear combination of the columns that have pivot positions. This means that a basis for will always be given by the columns of having pivot positions. This leads us to the following definition and proposition.   rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .      If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .    For example, the rank of the matrix in is two because there are two pivot positions. A basis for is given by the first two columns of since those columns have pivot positions.   Caution  Remember, we determine the pivot positions by looking at the reduced row echelon form of . However, we form a basis of from the columns of rather than the columns of the reduced row echelon matrix.     The null space of   The second subspace associated to a matrix is its null space.     null space    kernel  null space   If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .    Remember that a subspace is a subset that can be represented as the span of a set of vectors. The column space of , which is simply the span of the columns of , fits this definition. It may not be immediately clear how the null space of , which is the solution space of the equation , does, but we will see that is a subspace of .    We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .      Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .   As illustrated in this example, the dimension of is equal to the number of free variables in the equation , which equals the number of columns of without pivot positions or the number of columns of minus the number of pivot positions.    If is an matrix, then is a subspace of whose dimension is     Combining and shows that    If is an matrix, then       Summary  Once again, we find ourselves revisiting our two fundamental questions concerning the existence and uniqueness of solutions to linear systems. The column space contains all the vectors for which the equation is consistent. The null space is the solution space to the equation , which reflects on the uniqueness of solutions to this and other equations.    A subspace of is a subset of that can be represented as the span of a set of vectors. A basis of is a linearly independent set of vectors whose span is .  If is an matrix, the column space is the span of the columns of and forms a subspace of .  A basis for is found from the columns of that have pivot positions. The dimension is therefore .  The null space is the solution space to the homogeneous equation and is a subspace of .  A basis for is found through a parametric description of the solution space of , and we have that .       Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is      Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .     Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.     Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .     Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .     Consider the vectors and suppose that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives      Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .    Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .    In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .     "
},
{
  "id": "exploration-13",
  "level": "2",
  "url": "sec-subspaces.html#exploration-13",
  "type": "Preview Activity",
  "number": "4.6.1",
  "title": "",
  "body": "  Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and .      The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.    "
},
{
  "id": "definition-17",
  "level": "2",
  "url": "sec-subspaces.html#definition-17",
  "type": "Definition",
  "number": "4.6.1",
  "title": "",
  "body": " subspace   A subspace of is a subset of that is the span of a set of vectors.   "
},
{
  "id": "example-33",
  "level": "2",
  "url": "sec-subspaces.html#example-33",
  "type": "Example",
  "number": "4.6.2",
  "title": "Subspaces of <span class=\"process-math\">\\(\\real^3\\)<\/span>.",
  "body": " Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .  "
},
{
  "id": "activity-38",
  "level": "2",
  "url": "sec-subspaces.html#activity-38",
  "type": "Activity",
  "number": "4.6.2",
  "title": "",
  "body": "  We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .      "
},
{
  "id": "definition-18",
  "level": "2",
  "url": "sec-subspaces.html#definition-18",
  "type": "Definition",
  "number": "4.6.4",
  "title": "",
  "body": "  dimension  basis, of a subspace  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.   "
},
{
  "id": "example-34",
  "level": "2",
  "url": "sec-subspaces.html#example-34",
  "type": "Example",
  "number": "4.6.5",
  "title": "A subspace of <span class=\"process-math\">\\(\\real^4\\)<\/span>.",
  "body": " A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .  "
},
{
  "id": "definition-19",
  "level": "2",
  "url": "sec-subspaces.html#definition-19",
  "type": "Definition",
  "number": "4.6.6",
  "title": "",
  "body": "  column space  If is an matrix, we call the span of its columns the column space of and denote it as .   "
},
{
  "id": "activity-39",
  "level": "2",
  "url": "sec-subspaces.html#activity-39",
  "type": "Activity",
  "number": "4.6.3",
  "title": "",
  "body": "  We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.    "
},
{
  "id": "example-col-basis",
  "level": "2",
  "url": "sec-subspaces.html#example-col-basis",
  "type": "Example",
  "number": "4.6.7",
  "title": "",
  "body": " Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .  "
},
{
  "id": "definition-20",
  "level": "2",
  "url": "sec-subspaces.html#definition-20",
  "type": "Definition",
  "number": "4.6.8",
  "title": "",
  "body": " rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .   "
},
{
  "id": "proposition-col-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-col-dim",
  "type": "Proposition",
  "number": "4.6.9",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .   "
},
{
  "id": "definition-21",
  "level": "2",
  "url": "sec-subspaces.html#definition-21",
  "type": "Definition",
  "number": "4.6.10",
  "title": "",
  "body": "   null space    kernel  null space   If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .   "
},
{
  "id": "activity-40",
  "level": "2",
  "url": "sec-subspaces.html#activity-40",
  "type": "Activity",
  "number": "4.6.4",
  "title": "",
  "body": "  We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .    "
},
{
  "id": "example-null-intro",
  "level": "2",
  "url": "sec-subspaces.html#example-null-intro",
  "type": "Example",
  "number": "4.6.11",
  "title": "",
  "body": " Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .  "
},
{
  "id": "proposition-nul-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-nul-dim",
  "type": "Proposition",
  "number": "4.6.12",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension is    "
},
{
  "id": "proposition-28",
  "level": "2",
  "url": "sec-subspaces.html#proposition-28",
  "type": "Proposition",
  "number": "4.6.13",
  "title": "",
  "body": "  If is an matrix, then    "
},
{
  "id": "exercise-126",
  "level": "2",
  "url": "sec-subspaces.html#exercise-126",
  "type": "Exercise",
  "number": "4.6.5.1",
  "title": "",
  "body": " Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is    "
},
{
  "id": "exercise-127",
  "level": "2",
  "url": "sec-subspaces.html#exercise-127",
  "type": "Exercise",
  "number": "4.6.5.2",
  "title": "",
  "body": " Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .   "
},
{
  "id": "exercise-128",
  "level": "2",
  "url": "sec-subspaces.html#exercise-128",
  "type": "Exercise",
  "number": "4.6.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.   "
},
{
  "id": "exercise-129",
  "level": "2",
  "url": "sec-subspaces.html#exercise-129",
  "type": "Exercise",
  "number": "4.6.5.4",
  "title": "",
  "body": " Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .   "
},
{
  "id": "exercise-130",
  "level": "2",
  "url": "sec-subspaces.html#exercise-130",
  "type": "Exercise",
  "number": "4.6.5.5",
  "title": "",
  "body": " For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .   "
},
{
  "id": "exercise-131",
  "level": "2",
  "url": "sec-subspaces.html#exercise-131",
  "type": "Exercise",
  "number": "4.6.5.6",
  "title": "",
  "body": " Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .   "
},
{
  "id": "exercise-132",
  "level": "2",
  "url": "sec-subspaces.html#exercise-132",
  "type": "Exercise",
  "number": "4.6.5.7",
  "title": "",
  "body": " Consider the vectors and suppose that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives    "
},
{
  "id": "exercise-133",
  "level": "2",
  "url": "sec-subspaces.html#exercise-133",
  "type": "Exercise",
  "number": "4.6.5.8",
  "title": "",
  "body": " Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .  "
},
{
  "id": "exercise-134",
  "level": "2",
  "url": "sec-subspaces.html#exercise-134",
  "type": "Exercise",
  "number": "4.6.5.9",
  "title": "",
  "body": " Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .  "
},
{
  "id": "exercise-135",
  "level": "2",
  "url": "sec-subspaces.html#exercise-135",
  "type": "Exercise",
  "number": "4.6.5.10",
  "title": "",
  "body": " In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .   "
},
{
  "id": "sec-gaussian-revisited",
  "level": "1",
  "url": "sec-gaussian-revisited.html",
  "type": "Section",
  "number": "4.7",
  "title": "Partial pivoting and LU factorizations",
  "body": " Partial pivoting and LU factorizations   Our principal tool for finding solutions to linear systems has been Gaussian elimination, which we first met back in . When presented with a linear system, this method finds the reduced row echelon form of the system's augmented matrix and uses it to read off the solution (with the help of a little back substitution).  While this is a convenient approach for learning linear algebra, people rarely use the reduced row echelon form of a matrix. In fact, many linear algebra software packages (including numpy and scipy ) do not include functions for finding the reduced row echelon form.  In this section, we will describe why this is the case and then explore some alternatives. matrix factorization The intent of this section is to demonstrate how linear algebraic computations are handled in practice and to introduce the important idea of matrix factorization , writing a matrix as the product of matrices, each of which has a specified form.      To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .       Partial pivoting  The first issue that we will address is the fact that computers do not perform arithemtic operations exactly. For instance, if we ask Python to evaluate 0.1 + 0.2 , it reports 0.30000000000000004 though we know that the true value is 0.3.   There are a couple of reasons for this. First, computers perform arithmetic using base 2 numbers, which means that numbers we enter in decimal form, such as , must be converted to base 2. Even though 0.1 has a simple decimal form, its representation in base 2 is the repeating decimal , To accurately represent this number inside a computer would require infinitely many digits. Since a computer can only hold a finite number of digits, we are necessarily using an approximation just by representing this number in a computer.  In addition, arithmetic operations, such as addition, are prone to error. To keep things simple, suppose we have a computer that represents numbers using only three decimal digits. For instance, the number 1.023 would be represented as 1.02 while 0.023421 would be 0.0234 . If we add these numbers, we have 1.023 + 0.023421 = 1.046421; the computer reports this sum as 1.02 + 0.0234 = 1.04 , whose last digit is not correctly rounded. Generally speaking, we will see this problem, which is called round off error , whenever we add numbers of signficantly different magnitudes. round off error   Remember that Gaussian elimination, when applied to an matrix, requires approximately operations. If we have a matrix, performing Gaussian elimination requires roughly a billion operations, and the errors introduced in each operation could accumulate. How can we have confidence in the final result? We can never completely avoid these errors, but we can take steps to mitigate them. The next activity will introduce one such technique.    Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.     This activity demonstrates how the practical aspects of computing differ from the theoretical. We know that the order in which we write the equations has no effect on the solution space; row interchange is one of our three allowed row operations in the Gaussian elimination algorithm. However, when we are only able to perform arithmetic operations approximately, applying row interchanges can dramatically improve the accuracy of our approximations.  If we could compute the solution exactly, we find Since our hypothetical computer represents numbers using only three decimal digits, our computer finds This is the best we can hope to do with our computer since it is impossible to represent the solution exactly.  When the equations are written in their original order and we multiply the first equation by 10000 and subtract from the second, we find   In fact, we find the same value for when we interchange the equations. Here we multiply the first equation by 0.0001 and subtract from the second equation. We then find   The difference occurs when we substitute into the first equation. When the equations are written in their original order, we have When the equations are written in their original order, we find the solution .  When we write the equation in the opposite order, however, substituting into the first equation gives In this case, we find the approximate solution , which is the most accurate solution that our hypothetical computer can find. Simply interchanging the order of the equation produces a much more accurate solution.   We can understand why this works graphically. Each equation represents a line in the plane, and the solution is the intersection point. Notice that the slopes of these lines differ considerably.    When the equations are written in their original order, we substitute into the equation , which is a nearly horizontal line. Along this line, a small change in leads to a large change in . The slight difference in our approximation from the exact value leads to a large difference in the approximation from the exact value .  If we exchange the order in which the equations are written, we substitute our approximation into the equation . Notice that the slope of the associated line is . On this line, a small change in leads to a relatively small change in as well. Therefore, the difference in our approximation from the exact value leads to only a small difference in the approximation from the exact value.  This example motivates the technique that computers usually use to perform Gaussian elimation. We only need to perform a row interchange when a zero occurs in a pivot position, such as . However, we will perform a row interchange to put the entry having the largest possible absolute value into the pivot position. For instance, when performing Gaussian elimination on the following matrix, we begin by interchanging the first and third rows so that the upper left entry has the largest possible absolute value.  partial pivoting This technique is called partial pivoting , and it means that, in practice, we will perform many more row interchange operations than we typically do when computing exactly by hand.    factorization   factorizations  In , we saw that the number of arithmetic operations needed to perform Gaussian elimination on an matrix is about . This means that a matrix, requires about two thirds of a billion operations.  Suppose that we have two equations, and , that we would like to solve. Usually, we would form augmented matrices and and apply Gaussian elimination. Of course, the steps we perform in these two computations are nearly identical. Is there a way to store some of the computation we perform in reducing and reuse it in solving subsequent equations? The next activity will point us in the right direction.    We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .     This activity introduces a method for factoring a matrix as a product of two triangular matrices, , where is lower triangular and is upper triangular. The key to finding this factorization is to represent the row operations that we apply in the Gaussian elimination algorithm through multiplication by elementary matrices.   Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.   An factorization allow us to trade in one equation for two simpler equations For instance, the equation in our example has the form Because is a lower-triangular matrix, we can read off the first component of directly from the equations: . We then have , which gives , and , which gives . Solving a triangular system is simplified because we only need to perform a sequence of substitutions.  In fact, solving an equation with an triangular matrix requires approximately operations. Once we have the factorization , we solve the equation by solving two equations involving triangular matrices, which requires about operations. For example, if is a matrix, we solve the equation using about one million steps. This compares with roughly a billion operations needed to perform Gaussian elimination, which represents a significant savings. Of course, we have to first find the factorization of and this requires roughly the same amount of work as performing Gaussian elimination. However, once we have the factorization, we can use it to solve for different right hand sides .  Our discussion so far has ignored one issue, however. Remember that we sometimes have to perform row interchange operations in addition to row replacement. A typical row interchange is represented by multiplication by a matrix such as which has the effect of interchanging the first and third rows. Notice that this matrix is not triangular so performing a row interchange will disrupt the structure of the factorization we seek. Without giving the details, we simply note that linear algebra software packages compute a matrix that describes how the rows are permuted in the Gaussian elimination process.  Once we have , where is a (row) permutation matrix, is lower triangular, and is upper triangular, we can use this to solve the equation using backsubstitution twice. Notice that . We can determine by solving to find , and then we find by solving .   factorization: factorization with partial pivoting   For any matrix there are matrices , , and such that    is an (row) permutation matrix.     is an lower triangular matrix.     is an uppter triangular matrix.     .   Matrices , , and can by found by keeping track of the elementary matrices corresponding to Gaussian elimination with row permutations to put numbers with the largest absolute value into the pivot positions.  We can solve using this factorization as follows:   Rewrite as .    Solve for using backsubstitution.    Solve for backsubstitution.    This implies that , so since is invertible.         The scipy.linalg package can compute factorizations; we write P, L, U = scipy.linalg.LU(A) to obtain the matrices , , and such that . Here is an example.   In , we found the factorization Using Python, define the matrix and then ask for the factorization. What are the matrices , , and ?  Notice that scipy.linalg.LI() finds a different factorization than we found in the previous activity because it is using partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Python and compute .  Use the matrices L and U to solve and . You should find the same solution that you found in the previous activity.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Python to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Python gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Python tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .       Summary  We returned to Gaussian elimination, which we have used as a primary tool for finding solutions to linear systems, and explored its practicality, both in terms of numerical accuracy and computational effort.  We saw that the accuracy of computations implemented on a computer could be improved using partial pivoting , a technique that performs row interchanges so that the entry in a pivot position has the largest possible magnitude.  Beginning with a matrix , we used the Gaussian elimination algorithm to write , where is a permutation matrix, is lower triangular, and is upper triangular.  Finding this factorization involves roughly as much work as performing Gaussian elimination. However, once we have the factorization, we are able to quickly solve equations of the form by first solving and then .      In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exericse.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .     In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.    In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .     Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .    Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Python tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.     In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exericse demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives      Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .     In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.     Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.     Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .     "
},
{
  "id": "p-3810",
  "level": "2",
  "url": "sec-gaussian-revisited.html#p-3810",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix factorization "
},
{
  "id": "exploration-14",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exploration-14",
  "type": "Preview Activity",
  "number": "4.7.1",
  "title": "",
  "body": "  To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .    "
},
{
  "id": "p-3825",
  "level": "2",
  "url": "sec-gaussian-revisited.html#p-3825",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "round off error "
},
{
  "id": "activity-41",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-41",
  "type": "Activity",
  "number": "4.7.2",
  "title": "",
  "body": "  Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.    "
},
{
  "id": "p-3854",
  "level": "2",
  "url": "sec-gaussian-revisited.html#p-3854",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "partial pivoting "
},
{
  "id": "activity-42",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-42",
  "type": "Activity",
  "number": "4.7.3",
  "title": "",
  "body": "  We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .    "
},
{
  "id": "example-LU",
  "level": "2",
  "url": "sec-gaussian-revisited.html#example-LU",
  "type": "Example",
  "number": "4.7.1",
  "title": "",
  "body": " Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.  "
},
{
  "id": "prop-LU",
  "level": "2",
  "url": "sec-gaussian-revisited.html#prop-LU",
  "type": "Proposition",
  "number": "4.7.2",
  "title": "<span class=\"process-math\">\\(PLU\\)<\/span> factorization: <span class=\"process-math\">\\(LU\\)<\/span> factorization with partial pivoting.",
  "body": " factorization: factorization with partial pivoting   For any matrix there are matrices , , and such that    is an (row) permutation matrix.     is an lower triangular matrix.     is an uppter triangular matrix.     .   Matrices , , and can by found by keeping track of the elementary matrices corresponding to Gaussian elimination with row permutations to put numbers with the largest absolute value into the pivot positions.  We can solve using this factorization as follows:   Rewrite as .    Solve for using backsubstitution.    Solve for backsubstitution.    This implies that , so since is invertible.      "
},
{
  "id": "activity-43",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-43",
  "type": "Activity",
  "number": "4.7.4",
  "title": "",
  "body": "  The scipy.linalg package can compute factorizations; we write P, L, U = scipy.linalg.LU(A) to obtain the matrices , , and such that . Here is an example.   In , we found the factorization Using Python, define the matrix and then ask for the factorization. What are the matrices , , and ?  Notice that scipy.linalg.LI() finds a different factorization than we found in the previous activity because it is using partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Python and compute .  Use the matrices L and U to solve and . You should find the same solution that you found in the previous activity.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Python to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Python gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Python tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .    "
},
{
  "id": "p-3910",
  "level": "2",
  "url": "sec-gaussian-revisited.html#p-3910",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "partial pivoting "
},
{
  "id": "exercise-136",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-136",
  "type": "Exercise",
  "number": "4.7.4.1",
  "title": "",
  "body": " In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exericse.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .   "
},
{
  "id": "exercise-137",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-137",
  "type": "Exercise",
  "number": "4.7.4.2",
  "title": "",
  "body": " In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.  "
},
{
  "id": "exercise-138",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-138",
  "type": "Exercise",
  "number": "4.7.4.3",
  "title": "",
  "body": " In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .   "
},
{
  "id": "exercise-139",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-139",
  "type": "Exercise",
  "number": "4.7.4.4",
  "title": "",
  "body": " Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .  "
},
{
  "id": "exercise-eigenvector-approx",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-eigenvector-approx",
  "type": "Exercise",
  "number": "4.7.4.5",
  "title": "",
  "body": " Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Python tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.   "
},
{
  "id": "exercise-141",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-141",
  "type": "Exercise",
  "number": "4.7.4.6",
  "title": "",
  "body": " In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exericse demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives    "
},
{
  "id": "exercise-142",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-142",
  "type": "Exercise",
  "number": "4.7.4.7",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .   "
},
{
  "id": "exercise-143",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-143",
  "type": "Exercise",
  "number": "4.7.4.8",
  "title": "",
  "body": " In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.   "
},
{
  "id": "exercise-144",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-144",
  "type": "Exercise",
  "number": "4.7.4.9",
  "title": "",
  "body": " Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.   "
},
{
  "id": "exercise-145",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-145",
  "type": "Exercise",
  "number": "4.7.4.10",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .   "
},
{
  "id": "sec-eigen-intro",
  "level": "1",
  "url": "sec-eigen-intro.html",
  "type": "Section",
  "number": "5.1",
  "title": "An introduction to eigenvalues and eigenvectors",
  "body": " An introduction to eigenvalues and eigenvectors   This section introduces the concept of eigenvalues and eigenvectors and offers an example that motivates our interest in them. The point here is to develop an intuitive understanding of eigenvalues and eigenvectors and explain how they can be used to simplify some problems that we have previously encountered. In the rest of this chapter, we will develop this concept into a richer theory and illustrate its use with more meaningful examples.    Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .         A few examples  We will now introduce the definition of eigenvalues and eigenvectors and then look at a few simple examples.     eigenvalue    eigenvector   Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .    At first glance, there is a lot going on in this definition so let's look at an example.    Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .      This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.     Let's consider the ideas we saw in the activity in some more depth. To be an eigenvector of , the vector must satisfy for some scalar . This means that and are scalar multiples of each other so they must lie on the same line.  Consider now the matrix . On the left of , we see that is not an eigenvector of since the vectors and do not lie on the same line. On the right, however, we see that is an eigenvector. In fact, is obtained from by stretching by a factor of . Therefore, is an eigenvector of with eigenvalue .       On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .   It is not difficult to see that any multiple of is also an eigenvector of with eigenvalue . Indeed, we will see later that all the eigenvectors associated to a given eigenvalue form a subspace of .  In , we see that is also an eigenvector with eigenvalue .      Here we see another eigenvector with eigenvalue .   The interactive diagram we used in the activity is meant to convey the fact that the eigenvectors of a matrix are special vectors. Most of the time, the vectors and appear visually unrelated. For certain vectors, however, and line up with one another. Something important is going on when that happens so we call attention to these vectors by calling them eigenvectors. For these vectors, the operation of multiplying by reduces to the much simpler operation of scalar multiplying by . The reason eigenvectors are important is because it is extremely convenient to be able to replace matrix multiplication by scalar multiplication.    The usefulness of eigenvalues and eigenvectors  In the next section, we will introduce an algebraic technique for finding the eigenvalues and eigenvectors of a matrix. Before doing that, however, we would like to discuss why eigenvalues and eigenvectors are so useful.  Let's continue looking at the example . We have seen that is an eigenvector with eigenvalue and is an eigenvector with eigenvalue . This means that and . By the linearity of matrix multiplication, we can determine what happens when we multiply a linear combination of and by : .   For instance, if we consider the vector , we find that as seen in the figure.    In other words, multiplying by has the effect of stretching a vector in the direction by a factor of and flipping in direction.  We can draw an analogy with the more familiar example of the diagonal matrix . As we have seen, the matrix transformation defined by combines a horizontal stretching by a factor of 3 with a reflection across the horizontal axis, as is illustrated in .      The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.   The matrix has a similar effect when viewed in the basis defined by the eigenvectors and , as seen in .      The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.   In a sense that will be made precise later, having a set of eigenvectors of that forms a basis of enables us to think of as being equivalent to a diagonal matrix . Of course, as the other examples in the previous activity show, it may not always be possible to form a basis from the eigenvectors of a matrix. For example, the only eigenvectors of the matrix , which represents a shear, have the form . In this example, we are not able to create a basis for consisting of eigenvectors of the matrix. This is also true for the matrix , which represents a rotation.    Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.    This activity is important and motivates much of our work with eigenvalues and eigenvectors so it's worth reviewing to make sure we have a clear understanding of the concepts.  First, we compute This shows that is an eigenvector of with eigenvalue and is an eigenvector of with eigenvalue .  By the linearity of matrix matrix multiplication, we have . Therefore, we will write the vector describing the initial distribution of cars as a linear combination of and ; that is, . To do, we form the augmented matrix and row reduce: . Therefore, .  To determine the distribution of cars on subsequent days, we will repeatedly multiply by . We find that .  In particular, this shows us that . Taking notice of the pattern, we may write . Multiplying a number by is the same as taking 20% of that number. As each day goes by, the second term is multiplied by so the coefficient of in the expression for will eventually become extremely small. We therefore see that the distribution of cars will stabilize at .  Notice how our understanding of the eigenvectors of the matrix allows us to replace matrix multiplication with the simpler operation of scalar multiplication. As a result, we can look far into the future without having to repeatedly perform matrix multiplication.  Furthermore, notice how this example relies on the fact that we can express the initial vector as a linear combination of eigenvectors. For this reason, we would like, when given an matrix, to be able to create a basis of that consists of its eigenvectors. We will frequently return to this question in later sections.    If is an matrix, can we form a basis of consisting of eigenvectors of ?      Summary  We defined an eigenvector of a square matrix to be a nonzero vector such that for some scalar , which is called the eigenvalue associated to .  If is an eigenvector, then matrix multiplication by reduces to the simpler operation of scalar multiplication by .  Scalar multiples of an eigenvector are also eigenvectors. In fact, we will see that the eigenvectors associated to an eigenvalue form a subspace.  If we can form a basis for consisting of eigenvectors of , then is, in some sense, equivalent to a diagonal matrix.  Rewriting a vector as a linear combination of eigenvectors of simplifies the process of repeatedly multiplying by .       Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have      Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have      Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .                For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .       Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.     Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .        Explain why how the definition of eigenvalue and eigenvector implies that a matrix must be square to have eigenvalues and eigenvectors.    How could we modify the definition of eigenvalue and eigenvector so that it would work for non-square matrices?        If is not square, then and have different dimensions, so they can't be the same.    The big idea here is that by part a we can't put on both sides of . But we could use different vectors on the left and right sides. This only become interesting if we add some constraints on the vectors, like requiring them to form bases of the row and column space of . In fact, we can require even more: we can require that the bases be othogonal (each pair of vectors forms a right angle.). We will return to this idea in .        If and , then is associated with a linear transformation since both and are -dimensional.    If is , then is associated with a linear transformation , so we can't use the same vector on both sides of since the dimensions don't work out. But would could look for a and a pair of vectors and such that . In isolation is this is meaningless, since we can take and . It becomes more interesting, however, if we look for two orthogonal bases (bases where the angles between any pair of basis vectors is 90 degrees), an orthogonal basis of and an orthogonal basis of .  It turns out that every matrix has such a set of bases. This will be our topic of discussion in and .       For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.      Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.     "
},
{
  "id": "exploration-15",
  "level": "2",
  "url": "sec-eigen-intro.html#exploration-15",
  "type": "Preview Activity",
  "number": "5.1.1",
  "title": "",
  "body": "  Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .      "
},
{
  "id": "definition-22",
  "level": "2",
  "url": "sec-eigen-intro.html#definition-22",
  "type": "Definition",
  "number": "5.1.1",
  "title": "",
  "body": "   eigenvalue    eigenvector   Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .   "
},
{
  "id": "example-38",
  "level": "2",
  "url": "sec-eigen-intro.html#example-38",
  "type": "Example",
  "number": "5.1.2",
  "title": "",
  "body": "  Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .   "
},
{
  "id": "activity-eigen-geom",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-geom",
  "type": "Activity",
  "number": "5.1.2",
  "title": "",
  "body": "  This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.    "
},
{
  "id": "fig-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro",
  "type": "Figure",
  "number": "5.1.4",
  "title": "",
  "body": "     On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-2",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-2",
  "type": "Figure",
  "number": "5.1.5",
  "title": "",
  "body": "    Here we see another eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-diagonal",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-diagonal",
  "type": "Figure",
  "number": "5.1.6",
  "title": "",
  "body": "    The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.  "
},
{
  "id": "fig-eigen-intro-A",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-A",
  "type": "Figure",
  "number": "5.1.7",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.  "
},
{
  "id": "activity-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-intro",
  "type": "Activity",
  "number": "5.1.3",
  "title": "",
  "body": "  Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.   "
},
{
  "id": "question-eigen-basis",
  "level": "2",
  "url": "sec-eigen-intro.html#question-eigen-basis",
  "type": "Question",
  "number": "5.1.8",
  "title": "",
  "body": "  If is an matrix, can we form a basis of consisting of eigenvectors of ?   "
},
{
  "id": "exercise-146",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-146",
  "type": "Exercise",
  "number": "5.1.4.1",
  "title": "",
  "body": " Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have    "
},
{
  "id": "exercise-147",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-147",
  "type": "Exercise",
  "number": "5.1.4.2",
  "title": "",
  "body": " Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have    "
},
{
  "id": "exercise-148",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-148",
  "type": "Exercise",
  "number": "5.1.4.3",
  "title": "",
  "body": " Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .   "
},
{
  "id": "exercise-149",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-149",
  "type": "Exercise",
  "number": "5.1.4.4",
  "title": "",
  "body": " Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .              "
},
{
  "id": "exercise-150",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-150",
  "type": "Exercise",
  "number": "5.1.4.5",
  "title": "",
  "body": " For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.   "
},
{
  "id": "exercise-151",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-151",
  "type": "Exercise",
  "number": "5.1.4.6",
  "title": "",
  "body": " Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .     "
},
{
  "id": "exercise-152",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-152",
  "type": "Exercise",
  "number": "5.1.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.   "
},
{
  "id": "exercise-153",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-153",
  "type": "Exercise",
  "number": "5.1.4.8",
  "title": "",
  "body": " Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .   "
},
{
  "id": "exercise-154",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-154",
  "type": "Exercise",
  "number": "5.1.4.9",
  "title": "",
  "body": "    Explain why how the definition of eigenvalue and eigenvector implies that a matrix must be square to have eigenvalues and eigenvectors.    How could we modify the definition of eigenvalue and eigenvector so that it would work for non-square matrices?        If is not square, then and have different dimensions, so they can't be the same.    The big idea here is that by part a we can't put on both sides of . But we could use different vectors on the left and right sides. This only become interesting if we add some constraints on the vectors, like requiring them to form bases of the row and column space of . In fact, we can require even more: we can require that the bases be othogonal (each pair of vectors forms a right angle.). We will return to this idea in .        If and , then is associated with a linear transformation since both and are -dimensional.    If is , then is associated with a linear transformation , so we can't use the same vector on both sides of since the dimensions don't work out. But would could look for a and a pair of vectors and such that . In isolation is this is meaningless, since we can take and . It becomes more interesting, however, if we look for two orthogonal bases (bases where the angles between any pair of basis vectors is 90 degrees), an orthogonal basis of and an orthogonal basis of .  It turns out that every matrix has such a set of bases. This will be our topic of discussion in and .     "
},
{
  "id": "exercise-155",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-155",
  "type": "Exercise",
  "number": "5.1.4.10",
  "title": "",
  "body": " For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.    "
},
{
  "id": "exercise-156",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-156",
  "type": "Exercise",
  "number": "5.1.4.11",
  "title": "",
  "body": " Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.   "
},
{
  "id": "sec-eigen-find",
  "level": "1",
  "url": "sec-eigen-find.html",
  "type": "Section",
  "number": "5.2",
  "title": "Finding eigenvalues and eigenvectors",
  "body": " Finding eigenvalues and eigenvectors   The last section introduced eigenvalues and eigenvectors, presented the underlying geometric intuition behind their definition, and demonstrated their use in understanding the long-term behavior of certain systems. We will now develop a more algebraic understanding of eigenvalues and eigenvectors. In particular, we will find an algebraic method for determining the eigenvalues and eigenvectors of a square matrix.    Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .       characteristic polynomial  The characteristic polynomial  We will first see that the eigenvalues of a square matrix appear as the roots of a particular polynomial. To begin, notice that we originally defined an eigenvector as a nonzero vector that satisfies the equation . We will rewrite this as In other words, an eigenvector is a solution of the homogeneous equation . This puts us in the familiar territory explored in the next activity.    The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.       This activity demonstrates a technique that enables us to find the eigenvalues of a square matrix . Since an eigenvalue is a scalar for which the equation has a nonzero solution, it must be the case that is not invertible. Therefore, its determinant is zero. This gives us the equation whose solutions are the eigenvalues of . This equation is called the characteristic equation of .  characteristic equation     If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .   In general, the expression is a polynomial in , which is called the characteristic polynomial of .  characteristic polynomial    polynomial characteristic  If is an matrix, the degree of the characteristic polynomial is . For instance, if is a matrix, then is a quadratic polynomial; if is a matrix, then is a cubic polynomial.  The matrix in has a characteristic polynomial with two real and distinct roots. This will not always be the case, as demonstrated in the next two examples.   Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .    To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .   is important enough to present as the following proposition.  eigenvalue of a triangular matrix   If is a square triangular matrix, then the eigenvaltues of are the diagonal entries of .      Finding eigenvectors  Now that we can find the eigenvalues of a square matrix by solving the characteristic equation , we will turn to the question of finding the eigenvectors associated to an eigenvalue . The key, as before, is to note that an eigenvector is a nonzero solution to the homogeneous equation . In other words, the eigenvectors associated to an eigenvalue form the null space .  This shows that the eigenvectors associated to an eigenvalue form a subspace of . We will denote the subspace of eigenvectors of a matrix associated to the eigenvalue by and note that . We say that is the eigenspace of associated to the eigenvalue .  eigenspace      In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .     Once we find an eigenvalue of a matrix , describing the associated eigenspace amounts to the familiar task of describing the null space .   Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,     Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .    If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .   These last two examples illustrate two types of behavior when there is a single eigenvalue. In one case, we are able to construct a basis of using eigenvectors; in the other, we are not. We will explore this behavior more in the next subsection.   A check on our work  When finding eigenvalues and their associated eigenvectors in this way, we first find eigenvalues by solving the characteristic equation. If is a solution to the characteristic equation, then is not invertible and, consequently, must contain a row without a pivot position.  This serves as a check on our work. If we row reduce and find the identity matrix, then we have made an error either in solving the characteristic equation or in finding .     The characteristic polynomial and the dimension of eigenspaces  Given a square matrix , we saw in the previous section the value of being able to express any vector in as a linear combination of eigenvectors of . For this reason, asks when we can construct a basis of consisting of eigenvectors. We will explore this question more fully now.  As we saw above, the eigenvalues of are the solutions of the characteristic equation . The examples we have considered demonstrate some different types of behavior. For instance, we have seen the characteristic equations    , which has real and distinct roots,     , which has repeated roots, and     , which has complex roots.     If is an matrix, then the characteristic polynomial is a degree polynomial, and this means that it has roots. Therefore, the characteristic equation can be written as giving eigenvalues . As we have seen, some of the eigenvalues may be complex. Moreover, some of the eigenvalues may appear in this list more than once. However, we can always write the characteristic equation in the form The number of times that appears as a factor in the characteristic polynomial, is called the multiplicity of the eigenvalue .  multiplicity of an eigenvalue    eignevalue multiplicity of     We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .      If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.    The multiplicities of the eigenvalues are important because they reflect the dimension of the eigenspaces. We know that the dimension of an eigenspace must be at least one; the following proposition also tells us the dimension of an eigenspace can be no larger than the multiplicity of its associated eigenvalue.    If is a real eigenvalue of the matrix with multiplicity , then .      The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .      The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .      We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.      If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.    Fortunately, if we have an matrix, it frequently happens that the characteristic equation has the form where there are distinct real eigenvalues, each of which has multiplicity . In this case, the dimension of each of the eigenspaces . With a little work, it can be seen that choosing a basis vector for each of the eigenspaces produces a basis for . We therefore have the following proposition.    If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .    This proposition provides one answer to our . The next activity explores this question further.      Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .         Using Python to find eigenvalues and eigenvectors  We can use Python to find the characteristic polynomial, eigenvalues, and eigenvectors of a matrix. As we will see, however, some care is required when dealing with matrices whose entries include decimals.    We will use Python to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of a sympy.Matrix  A by writing A.charpoly('lambda') . sympy.Matrix.charpoly()  charpoly() sympy.Matrix.charpoly()  characteristic polynomial sympy.Matrix.charpoly() Notice that we have to give Python a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with sympy.factor() .    If we only want the eigenvalues, we can use numpy.linalg.eigvals() or scipy.linalg.eigvals() .  np.linalg.eigvals()  scipy.linalg.eigvals()   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find both eigenvalues and (right) eigenvectors using numpy.linalg.eig(A) or scipy.linalg.eig(A) . eigenvector  eigenvector left  eigenvector right  eigenvector np.linalg.eig()  numpy.linalg.eig()  numpy.linalg.eig() scipy.linalg.eig()  scipy.linalg.eigvals()  numpy.linalg.eigvals()    Left and right eigenvectors  Technically, we are finding right eigenvectors since the vector appears to the right of in the definition . That's most often what we want. But we can also get left eigenvectors with scipy.linalg.eig(A, left = True) . Feel free to try it in the code chunk below. NumPy does not provide this option.    The first item in the tuple returned gives the eigenvalues, the second the eigenvalues.   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This can be a problem when we wish to find the eigenvectors of a matrix. To illustrate, consider the matrix .  Without using Python, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Python to determine the reduced row echelon form of :   What result does Python report for the reduced row echelon form? Why is this result not correct? (Hint: compute row and column sums.)    reduced row echelon form not numerically stable  Despite the error in finding the reduce row echelon form, NumPy and SciPy can compute these eigenvalues and eigenvectors for us. As you might expect, this implies that the method being used does not rely on using RREF along the way. RREF is not numerically stable, this is why it is not included in numpy and scipy.    If we provide sympy with rational values instead of floating point, then it can compute the RREF of our matrix exactly and the error from above goes away. In we will see other methods for computing eigevectors that do not rely on RREF.         The characteristic polynomial is .   linalg.eig(A) returns [-3, -3] . This means we have one eigenvalue ( ) with multiplicity 2.  The eigenvectors are and . Notice that , so we really only have one independent eigevector here and the eigenspace is 1-dimensional (the x-axis).  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Python returns , which is not correct because cannot be invertible if is an eigenvalue of . The issue is round-off errors that make things that should be exactly zero slightly different from 0.   Here we find the correct eigenvalues, with basis vector for and with basis vector for .         Summary  In this section, we developed a technique for finding the eigenvalues and eigenvectors of an matrix .  The expression is a degree polynomial, known as the characteristic polynomial of . The eigenvalues of are the roots of the characteristic polynomial found by solving the characteristic equation .  The set of eigenvectors associated to the eigenvalue forms a subspace of , the eigenspace .  If the factor appears times in the characteristic polynomial, we say that the eigenvalue has multiplicity and note that .  If each of the eigenvalues is real and has multiplicity , then we can form a basis of consisting of eigenvectors of .  We can use Python to find the eigenvalues and eigenvalues of matrices. However, we need to be careful working with floating point numbers since floating point arithmetic is only an approximation.      For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .     Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .      Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .     Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .     For each of the following matrices, use Python to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Python tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is      There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.     Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .     Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .     "
},
{
  "id": "exploration-16",
  "level": "2",
  "url": "sec-eigen-find.html#exploration-16",
  "type": "Preview Activity",
  "number": "5.2.1",
  "title": "",
  "body": "  Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .    "
},
{
  "id": "activity-46",
  "level": "2",
  "url": "sec-eigen-find.html#activity-46",
  "type": "Activity",
  "number": "5.2.2",
  "title": "",
  "body": "  The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.      "
},
{
  "id": "p-4281",
  "level": "2",
  "url": "sec-eigen-find.html#p-4281",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "characteristic equation "
},
{
  "id": "example-eigenvalues-poly",
  "level": "2",
  "url": "sec-eigen-find.html#example-eigenvalues-poly",
  "type": "Example",
  "number": "5.2.1",
  "title": "",
  "body": " If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .  "
},
{
  "id": "p-4283",
  "level": "2",
  "url": "sec-eigen-find.html#p-4283",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "characteristic polynomial "
},
{
  "id": "example-40",
  "level": "2",
  "url": "sec-eigen-find.html#example-40",
  "type": "Example",
  "number": "5.2.2",
  "title": "",
  "body": " Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .  "
},
{
  "id": "example-41",
  "level": "2",
  "url": "sec-eigen-find.html#example-41",
  "type": "Example",
  "number": "5.2.3",
  "title": "",
  "body": " To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .  "
},
{
  "id": "prop-triangular-eigenvalues",
  "level": "2",
  "url": "sec-eigen-find.html#prop-triangular-eigenvalues",
  "type": "Proposition",
  "number": "5.2.4",
  "title": "",
  "body": " eigenvalue of a triangular matrix   If is a square triangular matrix, then the eigenvaltues of are the diagonal entries of .   "
},
{
  "id": "p-4289",
  "level": "2",
  "url": "sec-eigen-find.html#p-4289",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "eigenspace "
},
{
  "id": "activity-47",
  "level": "2",
  "url": "sec-eigen-find.html#activity-47",
  "type": "Activity",
  "number": "5.2.3",
  "title": "",
  "body": "  In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .    "
},
{
  "id": "example-42",
  "level": "2",
  "url": "sec-eigen-find.html#example-42",
  "type": "Example",
  "number": "5.2.5",
  "title": "",
  "body": " Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,   "
},
{
  "id": "example-43",
  "level": "2",
  "url": "sec-eigen-find.html#example-43",
  "type": "Example",
  "number": "5.2.6",
  "title": "",
  "body": " Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .  "
},
{
  "id": "example-44",
  "level": "2",
  "url": "sec-eigen-find.html#example-44",
  "type": "Example",
  "number": "5.2.7",
  "title": "",
  "body": " If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .  "
},
{
  "id": "p-4320",
  "level": "2",
  "url": "sec-eigen-find.html#p-4320",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "multiplicity "
},
{
  "id": "example-45",
  "level": "2",
  "url": "sec-eigen-find.html#example-45",
  "type": "Example",
  "number": "5.2.8",
  "title": "",
  "body": "  We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .   "
},
{
  "id": "example-46",
  "level": "2",
  "url": "sec-eigen-find.html#example-46",
  "type": "Example",
  "number": "5.2.9",
  "title": "",
  "body": "  If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.   "
},
{
  "id": "prop-eigen-basis",
  "level": "2",
  "url": "sec-eigen-find.html#prop-eigen-basis",
  "type": "Proposition",
  "number": "5.2.10",
  "title": "",
  "body": "  If is a real eigenvalue of the matrix with multiplicity , then .   "
},
{
  "id": "example-47",
  "level": "2",
  "url": "sec-eigen-find.html#example-47",
  "type": "Example",
  "number": "5.2.11",
  "title": "",
  "body": "  The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .   "
},
{
  "id": "example-48",
  "level": "2",
  "url": "sec-eigen-find.html#example-48",
  "type": "Example",
  "number": "5.2.12",
  "title": "",
  "body": "  The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .   "
},
{
  "id": "example-49",
  "level": "2",
  "url": "sec-eigen-find.html#example-49",
  "type": "Example",
  "number": "5.2.13",
  "title": "",
  "body": "  We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.   "
},
{
  "id": "example-50",
  "level": "2",
  "url": "sec-eigen-find.html#example-50",
  "type": "Example",
  "number": "5.2.14",
  "title": "",
  "body": "  If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.   "
},
{
  "id": "proposition-32",
  "level": "2",
  "url": "sec-eigen-find.html#proposition-32",
  "type": "Proposition",
  "number": "5.2.15",
  "title": "",
  "body": "  If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "activity-48",
  "level": "2",
  "url": "sec-eigen-find.html#activity-48",
  "type": "Activity",
  "number": "5.2.4",
  "title": "",
  "body": "    Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .      "
},
{
  "id": "activity-49",
  "level": "2",
  "url": "sec-eigen-find.html#activity-49",
  "type": "Activity",
  "number": "5.2.5",
  "title": "",
  "body": "  We will use Python to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of a sympy.Matrix  A by writing A.charpoly('lambda') . sympy.Matrix.charpoly()  charpoly() sympy.Matrix.charpoly()  characteristic polynomial sympy.Matrix.charpoly() Notice that we have to give Python a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with sympy.factor() .    If we only want the eigenvalues, we can use numpy.linalg.eigvals() or scipy.linalg.eigvals() .  np.linalg.eigvals()  scipy.linalg.eigvals()   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find both eigenvalues and (right) eigenvectors using numpy.linalg.eig(A) or scipy.linalg.eig(A) . eigenvector  eigenvector left  eigenvector right  eigenvector np.linalg.eig()  numpy.linalg.eig()  numpy.linalg.eig() scipy.linalg.eig()  scipy.linalg.eigvals()  numpy.linalg.eigvals()    Left and right eigenvectors  Technically, we are finding right eigenvectors since the vector appears to the right of in the definition . That's most often what we want. But we can also get left eigenvectors with scipy.linalg.eig(A, left = True) . Feel free to try it in the code chunk below. NumPy does not provide this option.    The first item in the tuple returned gives the eigenvalues, the second the eigenvalues.   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This can be a problem when we wish to find the eigenvectors of a matrix. To illustrate, consider the matrix .  Without using Python, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Python to determine the reduced row echelon form of :   What result does Python report for the reduced row echelon form? Why is this result not correct? (Hint: compute row and column sums.)    reduced row echelon form not numerically stable  Despite the error in finding the reduce row echelon form, NumPy and SciPy can compute these eigenvalues and eigenvectors for us. As you might expect, this implies that the method being used does not rely on using RREF along the way. RREF is not numerically stable, this is why it is not included in numpy and scipy.    If we provide sympy with rational values instead of floating point, then it can compute the RREF of our matrix exactly and the error from above goes away. In we will see other methods for computing eigevectors that do not rely on RREF.         The characteristic polynomial is .   linalg.eig(A) returns [-3, -3] . This means we have one eigenvalue ( ) with multiplicity 2.  The eigenvectors are and . Notice that , so we really only have one independent eigevector here and the eigenspace is 1-dimensional (the x-axis).  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Python returns , which is not correct because cannot be invertible if is an eigenvalue of . The issue is round-off errors that make things that should be exactly zero slightly different from 0.   Here we find the correct eigenvalues, with basis vector for and with basis vector for .      "
},
{
  "id": "exercise-157",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-157",
  "type": "Exercise",
  "number": "5.2.6.1",
  "title": "",
  "body": " For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .   "
},
{
  "id": "exercise-158",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-158",
  "type": "Exercise",
  "number": "5.2.6.2",
  "title": "",
  "body": " Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .    "
},
{
  "id": "exercise-159",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-159",
  "type": "Exercise",
  "number": "5.2.6.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .   "
},
{
  "id": "exercise-160",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-160",
  "type": "Exercise",
  "number": "5.2.6.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .   "
},
{
  "id": "exercise-161",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-161",
  "type": "Exercise",
  "number": "5.2.6.5",
  "title": "",
  "body": " For each of the following matrices, use Python to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Python tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is    "
},
{
  "id": "exercise-162",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-162",
  "type": "Exercise",
  "number": "5.2.6.6",
  "title": "",
  "body": " There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.   "
},
{
  "id": "exercise-163",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-163",
  "type": "Exercise",
  "number": "5.2.6.7",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "exercise-164",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-164",
  "type": "Exercise",
  "number": "5.2.6.8",
  "title": "",
  "body": " Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "sec-eigen-diag",
  "level": "1",
  "url": "sec-eigen-diag.html",
  "type": "Section",
  "number": "5.3",
  "title": "Diagonalization, similarity, and powers of a matrix",
  "body": " Diagonalization, similarity, and powers of a matrix   The first example we considered in this chapter was the matrix , which has eigenvectors and and associated eigenvalues and . In , we described how is, in some sense, equivalent to the diagonal matrix .  This equivalence is summarized by . The diagonal matrix has the geometric effect of stretching vectors horizontally by a factor of and flipping vectors vertically. The matrix has the geometric effect of stretching vectors by a factor of in the direction and flipping them in the direction. That is, the geometric effect of is the same as that of when viewed in a basis of eigenvectors of .      The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.   Our goal in this section is to express this geometric observation in algebraic terms. In doing so, we will make precise the sense in which and are equivalent.    In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .        Diagonalization of matrices  When working with an matrix , demonstrated the value of having a basis of consisting of eigenvectors of . In fact, tells us that if the eigenvalues of are real and distinct, then there is a such a basis. As we'll see later, there are other conditions on that guarantee a basis of eigenvectors. For now, suffice it to say that we can find a basis of eigenvectors for many matrices. With this assumption, we will see how the matrix is equivalent to a diagonal matrix .    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Python cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Python.    More generally, suppose that we have an matrix and that there is a basis of consisting of eigenvectors of with associated eigenvalues . If we use the eigenvectors to form the matrix and the eigenvalues to form the diagonal matrix and apply the same reasoning demonstrated in the activity, we find that and hence   We have now seen the following proposition.    If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .     We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .     diagonalizable matrix   matrix  diagonalizable   We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that       We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .      We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.    In fact, if we only know that , we can say that the columns of are eigenvectors of and that the diagonal entries of are the associated eigenvalues.    An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .      Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .        Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .         Powers of a diagonalizable matrix  In several earlier examples, we have been interested in computing powers of a given matrix. For instance, in , we had the matrix and an initial vector , and we wanted to compute In particular, we wanted to find and determine what happens as becomes very large. If a matrix is diagonalizable, writing can help us understand powers of more easily.      Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.     If is diagonalizable, the activity demonstrates that any power of is as well.    If , then . When is invertible, we also have .     Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.      Similarity and complex eigenvalues  We have been interested in diagonalizing a matrix because doing so relates a matrix to a simpler diagonal matrix . In particular, the effect of multiplying a vector by , viewed in the basis defined by the columns of , is the same as the effect of multiplying by in the standard basis.  While many matrices are diagonalizable, there are some that are not. For example, if a matrix has complex eigenvalues, it is not possible to find a basis of consisting of eigenvectors, which means that the matrix is not diagonalizable. In this case, however, we can still relate the matrix to a simpler form that explains the geometric effect this matrix has on vectors.     similar matrices    matrix  similar   We say that is similar to if there is an invertible matrix such that .    Notice that a matrix is diagonalizable if and only if it is similar to a diagonal matrix. In case a matrix has complex eigenvalues, we will find a simpler matrix that is similar to and note that has the same effect, when viewed in the basis defined by the columns of , as , when viewed in the standard basis.  To begin, suppose that is a matrix having a complex eigenvalue . It turns out that is similar to .   The next activity shows that has a simple geometric effect on . First, however, we will use polar coordinates to rewrite . As shown in the figure, the point defines , the distance from the origin, and , the angle formed with the positive horizontal axis. We then have Notice that the Pythagorean theorem says that .      We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Python will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .     If the matrix has a complex eigenvalue , it turns out that is always similar to the matrix whose geometric effect on vectors can be described in terms of a rotation and a scaling. There is, in fact, a method for finding the matrix so that that we'll see in . For now, we note that has the same geometric effect as , when viewed in the basis provided by the columns of . We will put this fact to use in the next section to understand certain dynamical systems.    If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .      Summary  Our goal in this section has been to use the eigenvalues and eigenvectors of a matrix to relate to a simpler matrix.  We said that is diagonalizable if we can write where is a diagonal matrix.   An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of . In that case, we can find a diagonalization by letting the columns of consist of eigenvectors of and the diagonal entries of be the associated eigenvalues.  We said that and are similar if there is an invertible matrix such that . In this case, .  If is a matrix with complex eigenvalue , then is similar to . Writing the point in polar coordinates and , we see that rotates vectors through an angle and scales them by a factor of .       Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .     Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .     Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  We can determine the inverse of as follows. and , so .  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.     Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .     Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.     When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .     For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.       For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.     "
},
{
  "id": "fig-eigen-diag-A",
  "level": "2",
  "url": "sec-eigen-diag.html#fig-eigen-diag-A",
  "type": "Figure",
  "number": "5.3.1",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.  "
},
{
  "id": "exploration-17",
  "level": "2",
  "url": "sec-eigen-diag.html#exploration-17",
  "type": "Preview Activity",
  "number": "5.3.1",
  "title": "",
  "body": "  In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .     "
},
{
  "id": "activity-50",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-50",
  "type": "Activity",
  "number": "5.3.2",
  "title": "",
  "body": "  Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Python cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Python.   "
},
{
  "id": "prop-diagonalizable",
  "level": "2",
  "url": "sec-eigen-diag.html#prop-diagonalizable",
  "type": "Proposition",
  "number": "5.3.2",
  "title": "",
  "body": "  If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .   "
},
{
  "id": "example-51",
  "level": "2",
  "url": "sec-eigen-diag.html#example-51",
  "type": "Example",
  "number": "5.3.3",
  "title": "",
  "body": " We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .  "
},
{
  "id": "definition-23",
  "level": "2",
  "url": "sec-eigen-diag.html#definition-23",
  "type": "Definition",
  "number": "5.3.4",
  "title": "",
  "body": "  diagonalizable matrix   matrix  diagonalizable   We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that    "
},
{
  "id": "example-52",
  "level": "2",
  "url": "sec-eigen-diag.html#example-52",
  "type": "Example",
  "number": "5.3.5",
  "title": "",
  "body": "  We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .   "
},
{
  "id": "example-53",
  "level": "2",
  "url": "sec-eigen-diag.html#example-53",
  "type": "Example",
  "number": "5.3.6",
  "title": "",
  "body": "  We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.   "
},
{
  "id": "proposition-34",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-34",
  "type": "Proposition",
  "number": "5.3.7",
  "title": "",
  "body": "  An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "example-54",
  "level": "2",
  "url": "sec-eigen-diag.html#example-54",
  "type": "Example",
  "number": "5.3.8",
  "title": "",
  "body": "  Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .   "
},
{
  "id": "activity-51",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-51",
  "type": "Activity",
  "number": "5.3.3",
  "title": "",
  "body": "    Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .      "
},
{
  "id": "activity-52",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-52",
  "type": "Activity",
  "number": "5.3.4",
  "title": "",
  "body": "    Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.    "
},
{
  "id": "proposition-35",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-35",
  "type": "Proposition",
  "number": "5.3.9",
  "title": "",
  "body": "  If , then . When is invertible, we also have .   "
},
{
  "id": "example-55",
  "level": "2",
  "url": "sec-eigen-diag.html#example-55",
  "type": "Example",
  "number": "5.3.10",
  "title": "",
  "body": " Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.   "
},
{
  "id": "definition-24",
  "level": "2",
  "url": "sec-eigen-diag.html#definition-24",
  "type": "Definition",
  "number": "5.3.11",
  "title": "",
  "body": "   similar matrices    matrix  similar   We say that is similar to if there is an invertible matrix such that .   "
},
{
  "id": "activity-53",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-53",
  "type": "Activity",
  "number": "5.3.5",
  "title": "",
  "body": "  We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Python will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .    "
},
{
  "id": "proposition-36",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-36",
  "type": "Proposition",
  "number": "5.3.12",
  "title": "",
  "body": "  If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .   "
},
{
  "id": "p-4622",
  "level": "2",
  "url": "sec-eigen-diag.html#p-4622",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diagonalizable "
},
{
  "id": "exercise-165",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-165",
  "type": "Exercise",
  "number": "5.3.5.1",
  "title": "",
  "body": " Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .   "
},
{
  "id": "exercise-166",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-166",
  "type": "Exercise",
  "number": "5.3.5.2",
  "title": "",
  "body": " Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .   "
},
{
  "id": "exercise-167",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-167",
  "type": "Exercise",
  "number": "5.3.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  We can determine the inverse of as follows. and , so .  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.   "
},
{
  "id": "exercise-168",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-168",
  "type": "Exercise",
  "number": "5.3.5.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .   "
},
{
  "id": "exercise-169",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-169",
  "type": "Exercise",
  "number": "5.3.5.5",
  "title": "",
  "body": " Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .   "
},
{
  "id": "exercise-170",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-170",
  "type": "Exercise",
  "number": "5.3.5.6",
  "title": "",
  "body": " We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.   "
},
{
  "id": "exercise-171",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-171",
  "type": "Exercise",
  "number": "5.3.5.7",
  "title": "",
  "body": " Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.   "
},
{
  "id": "exercise-complex-eigenvector",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-complex-eigenvector",
  "type": "Exercise",
  "number": "5.3.5.8",
  "title": "",
  "body": " When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .   "
},
{
  "id": "exercise-173",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-173",
  "type": "Exercise",
  "number": "5.3.5.9",
  "title": "",
  "body": " For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.     "
},
{
  "id": "exercise-174",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-174",
  "type": "Exercise",
  "number": "5.3.5.10",
  "title": "",
  "body": " For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.   "
},
{
  "id": "sec-dynamical",
  "level": "1",
  "url": "sec-dynamical.html",
  "type": "Section",
  "number": "5.4",
  "title": "Dynamical systems",
  "body": " Dynamical systems   The last section demonstrated ways in which we may relate a matrix, and the effect that multiplication has on vectors, to a simpler form. For instance, if there is a basis of consisting of eigenvectors of , we saw that is similar to a diagonal matrix . As a result, the effect of multiplying vectors by , when expressed using the basis of eigenvectors, is the same as multiplying by .  In this section, we will put these ideas to use as we explore discrete dynamical systems, first encountered in . Recall that we used a state vector to characterize the state of some system, such as the distribution of delivery trucks between two locations, at a particular time. A matrix described the transition of the state vector with characterizing the state of the system at a later time. Since we would like to understand how the state vector evolves over time, we are interested in studying the sequence of vectors .  Our goal in this section is to describe the types of behaviors that dynamical systems exhibit and to develop a means of detecting these behaviors.    Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .       A first example  We will begin with a dynamical system that illustrates how the ideas we've been developing can help us understand the populations of two interacting species. There are several possible ways in which two species may interact. For example, wolves on Isle Royale in northern Michigan prey on moose so this interaction is often called a predator-prey relationship. Other interactions between species, such as bees and flowering plants, are mutually beneficial for both species.    Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .     This activity demonstrates the type of systems we will be considering. In particular, we will have vectors that describe the state of the system at time and a matrix that describes how the state evolves from one time to the next: . The eigenvalues and eigenvectors of provide the key that helps us understand how the vectors evolve and that enables us to make long-range predictions.  Let's look at the specific example in the previous activity more carefully. We see that and that the matrix has eigenvectors and with associated eigenvalues and .  With initial populations , we have   Let's shift our perspective slightly. The eigenvectors and form a basis of , which says that is diagonalizable; that is, where   The coordinate system defined by the basis can be used to express the state vectors. For instance, we can write the initial state vector , which means that . Moreover, so that In the same way,   More generally, we have which is a restatement of the fact that is similar to .   Thinking about this geometrically, we begin with the vector . Subsequent vectors are obtained by scaling horizontally by a factor of and scaling vertically by a factor . Notice how the points move along a curve away from the origin becoming ever closer to the horizontal axis. After a very long time, .     To recover the behavior of the sequence , we change coordinate systems using the basis defined by and . Here, the points move along a curve away from the origin becoming ever closer to the line defined by .    Eventually, the vectors become practically indistinguishable from a scalar multiple of since . This means that This shows that so that we expect the population of species to eventually be about twice that of species .  In addition, so that and , which tells us that both populations are multiplied by 1.3 every year meaning the annual growth rate for both populations is about 30%.  In the same way, we can consider other possible initial populations as shown in . Regardless of , the population vectors, in the coordinates defined by , are scaled horizontally by a factor of and vertically by a factor of . The sequence of points , called trajectories , move along the curves, as shown on the left. trajectory In the standard coordinate system, we see that the trajectories converge to the eigenspace .      The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.   We conclude that, regardless of the initial populations, the ratio of the populations will approach 2 to 1 and that the growth rate for both populations approaches 30%. This example demonstrates the power of using eigenvalues and eigenvectors to rewrite the problem in terms of a new coordinate system. By doing so, we are able to predict the long-term behavior of the populations independently of the initial populations.  Diagrams like those shown in are called phase portraits.  phase portrait On the left of is the phase portrait of the diagonal matrix while the right of that figure shows the phase portrait of . The phase portrait of is relatively easy to understand because it is determined only by the two eigenvalues. Once we have the phase portrait of , however, the phase portrait of has a similar appearance with the eigenvectors replacing the standard basis vectors .    Classifying dynamical systems  In the previous example, we were able to make predictions about the behavior of trajectories by considering the eigenvalues and eigenvectors of the matrix . The next activity looks at a collection of matrices that demonstrate the types of behavior a dynamical system can exhibit.    We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                           This activity demonstrates six possible types of dynamical systems, which are determined by the eigenvalues of .  Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves away from the origin so we say that the origin is a repellor . repellor This is illustrated in .      The origin is a repellor when .    Suppose that has two real eigenvalues and and that . In this case, most nonzero vectors form trajectories that converge to the eigenspace . saddle In this case, we say that the origin is a saddle as illustrated in .      The origin is a saddle when .    Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves into the origin so we say that the origin is an attractor . attractor This is illustrated in .      The origin is an attractor when .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals away from the origin. repellor spiral  spiral repellor We say that the origin is a spiral repellor , as illustrated in .      The origin is a spiral repellor when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that moves on a closed curve around the origin. We say that the origin is a center , as illustrated in .      The origin is a center when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals into the origin. spiral attractor  attractor spiral We say that the origin is a spiral attractor , as illustrated in .      The origin is a spiral attractor when has an eigenvalue with .     This list includes many types of expected behavior, but there are other possibilities if, for instance, one of the eigenvalues is 0. The next section explores the situation when one of the eigenvalues is 1.    In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.       A system  Up to this point, we have focused on systems. In fact, the general case is quite similar. As an example, consider a system where the matrix has eigenvalues , , and . Since the eigenvalues are real and distinct, there is a basis consisting of eigenvectors of so we can look at the trajectories in the coordinate system defined by . The phase portraits in show how some representative trajectories will evolve. We see that all the trajectories will converge into the eigenspace .       In a system with , , and , the trajectories move along the curves shown above.   In the same way, suppose we have a system with complex eigenvalues and . Since the complex eigenvalues satisfy , there is a two-dimensional subspace in which the trajectories spiral in toward the origin. The phase portraits in show some of the trajectories. Once again, we see that all the trajectories converge into the eigenspace .       In a system with complex eigenvalues with and , the trajectories move along the curves shown above.     The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.       Summary  We have been exploring discrete dynamical systems in which an initial state vector evolves over time according to the rule . The eigenvalues and eigenvectors of help us understand the behavior of the state vectors. In the case, we saw that   produces an attractor so that trajectories are pulled in toward the origin.   and produces a saddle in which most trajectories are pushed away from the origin and in the direction of .   produces a repellor in which trajectories are pushed away from the origin.  The same kind of reasoning allows us to analyze systems as well.     For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.        We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .     Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.      Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which ?  Find the eigenvalues of .  Then find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .     Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.     Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.     The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .     This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .     Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .     Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Python tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.     "
},
{
  "id": "exploration-18",
  "level": "2",
  "url": "sec-dynamical.html#exploration-18",
  "type": "Preview Activity",
  "number": "5.4.1",
  "title": "",
  "body": "  Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .    "
},
{
  "id": "activity-54",
  "level": "2",
  "url": "sec-dynamical.html#activity-54",
  "type": "Activity",
  "number": "5.4.2",
  "title": "",
  "body": "  Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .    "
},
{
  "id": "p-4845",
  "level": "2",
  "url": "sec-dynamical.html#p-4845",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "trajectories "
},
{
  "id": "fig-eigen-dynam-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-dynam-saddle",
  "type": "Figure",
  "number": "5.4.1",
  "title": "",
  "body": "    The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.  "
},
{
  "id": "p-4847",
  "level": "2",
  "url": "sec-dynamical.html#p-4847",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "phase portraits. "
},
{
  "id": "activity-55",
  "level": "2",
  "url": "sec-dynamical.html#activity-55",
  "type": "Activity",
  "number": "5.4.3",
  "title": "",
  "body": "  We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                          "
},
{
  "id": "p-4857",
  "level": "2",
  "url": "sec-dynamical.html#p-4857",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "repellor saddle attractor spiral repellor center spiral attractor "
},
{
  "id": "activity-56",
  "level": "2",
  "url": "sec-dynamical.html#activity-56",
  "type": "Activity",
  "number": "5.4.4",
  "title": "",
  "body": "  In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.    "
},
{
  "id": "fig-eigen-3d-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-saddle",
  "type": "Figure",
  "number": "5.4.8",
  "title": "",
  "body": "     In a system with , , and , the trajectories move along the curves shown above.  "
},
{
  "id": "fig-eigen-3d-spiral",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-spiral",
  "type": "Figure",
  "number": "5.4.9",
  "title": "",
  "body": "     In a system with complex eigenvalues with and , the trajectories move along the curves shown above.  "
},
{
  "id": "activity-57",
  "level": "2",
  "url": "sec-dynamical.html#activity-57",
  "type": "Activity",
  "number": "5.4.5",
  "title": "",
  "body": "  The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.    "
},
{
  "id": "exercise-175",
  "level": "2",
  "url": "sec-dynamical.html#exercise-175",
  "type": "Exercise",
  "number": "5.4.5.1",
  "title": "",
  "body": " For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.      "
},
{
  "id": "exercise-176",
  "level": "2",
  "url": "sec-dynamical.html#exercise-176",
  "type": "Exercise",
  "number": "5.4.5.2",
  "title": "",
  "body": " We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .   "
},
{
  "id": "exercise-177",
  "level": "2",
  "url": "sec-dynamical.html#exercise-177",
  "type": "Exercise",
  "number": "5.4.5.3",
  "title": "",
  "body": " Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.    "
},
{
  "id": "exercise-178",
  "level": "2",
  "url": "sec-dynamical.html#exercise-178",
  "type": "Exercise",
  "number": "5.4.5.4",
  "title": "",
  "body": " Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which ?  Find the eigenvalues of .  Then find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .   "
},
{
  "id": "exercise-179",
  "level": "2",
  "url": "sec-dynamical.html#exercise-179",
  "type": "Exercise",
  "number": "5.4.5.5",
  "title": "",
  "body": " Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.   "
},
{
  "id": "exercise-180",
  "level": "2",
  "url": "sec-dynamical.html#exercise-180",
  "type": "Exercise",
  "number": "5.4.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.   "
},
{
  "id": "exercise-181",
  "level": "2",
  "url": "sec-dynamical.html#exercise-181",
  "type": "Exercise",
  "number": "5.4.5.7",
  "title": "",
  "body": " The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .   "
},
{
  "id": "exercise-182",
  "level": "2",
  "url": "sec-dynamical.html#exercise-182",
  "type": "Exercise",
  "number": "5.4.5.8",
  "title": "",
  "body": " This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .   "
},
{
  "id": "exercise-183",
  "level": "2",
  "url": "sec-dynamical.html#exercise-183",
  "type": "Exercise",
  "number": "5.4.5.9",
  "title": "",
  "body": " Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .   "
},
{
  "id": "exercise-184",
  "level": "2",
  "url": "sec-dynamical.html#exercise-184",
  "type": "Exercise",
  "number": "5.4.5.10",
  "title": "",
  "body": " Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Python tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.   "
},
{
  "id": "sec-stochastic",
  "level": "1",
  "url": "sec-stochastic.html",
  "type": "Section",
  "number": "5.5",
  "title": "Markov chains and Google’s PageRank algorithm",
  "body": " Markov chains and Google's PageRank algorithm   In the last section, we used our understanding of eigenvalues and eigenvectors to describe the long-term behavior of some discrete dynamical systems. The state of the system, which could record, say, the populations of a few interacting species, at one time is described by a vector . The state vector then evolves according to a linear rule .  This section continues this exploration by looking at Markov chains , which form a specific type of discrete dynamical system. Markov chain For instance, we could be interested in a rental car company that rents cars from several locations. From one day to the next, the number of cars at different locations can change, but the total number of cars stays the same. Once again, an understanding of eigenvalues and eigenvectors will help us make predictions about the long-term behavior of the system.    Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .       A first example  In the preview activity, the distribution of rental cars was described by the discrete dynamical system . This matrix has some special properties. First, each entry represents the probability that a car rented at one location is returned to another. For instance, there is an 80% chance that a car rented at is returned to , which explains the entry of 0.8 in the upper left corner. Therefore, the entries of the matrix are between 0 and 1.  Second, a car rented at one location must be returned to one of the locations. For example, since 80% of the cars rented at are returned to , it follows that the other 20% of cars rented at are returned to . This implies that the entries in each column must add to 1. This will occur frequently in our discussion so we introduce the following definitions.   probability vector  vector  probability  stochastic matrix  matrix  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix .     Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.    The previous activity illustrates some important points that we wish to emphasize.  First, to determine , we note that in election , party retains 60% of its voters from the previous election and adds 20% of those who voted for party . In this way, we see that We therefore define the matrix and note that .  If we consider the first column of , we see that the entries represent the percentages of party 's voters in the last election who vote for each of the three parties in the next election. Since everyone who voted for party previously votes for one of the three parties in the next election, the sum of these percentages must be 1. This is true for each of the columns of , which explains why is a stochastic matrix.  We begin with the vector , the entries of which represent the percentage of voters voting for each of the three parties. Since every voter votes for one of the three parties, the sum of these entries must be 1, which means that is a probability vector. We then find that . Notice that the vectors are also probability vectors and that the sequence seems to be converging to . It is this behavior that we would like to understand more fully by investigating the eigenvalues and eigenvectors of .  We find that the eigenvalues of are . Notice that if is an eigenvector of with associated eigenvalue , then . That is, is unchanged when we multiply it by .   Otherwise, we have where Notice that so the trajectories spiral into the eigenspace as indicated in the figure.    This tells us that the sequence converges to a vector in . In the usual way, we see that is a basis vector for because so we expect that will converge to a scalar multiple of . Indeed, since the vectors are probability vectors, we expect them to converge to a probability vector in .  We can find the probability vector in by finding the appropriate scalar multiple of . Notice that is a probability vector when , which implies that . Therefore, is the unique probability vector in . Since the sequence converges to a probability vector in , we see that converges to , which agrees with the computations we showed above.  The role of the eigenvalues is important in this example. Since , we can find a probability vector that is unchanged by multiplication by . Also, the other eigenvalues satisfy , which means that all the trajectories get pulled in to the eigenspace . Since is a sequence of probability vectors, these vectors converge to the probability vector as they are pulled into .    Markov chains  If we have a stochastic matrix and a probability vector , we can form the sequence where . We call this sequence of vectors a Markov chain . explains why we can guarantee that the vectors are probability vectors.  Markov chain    In the example that studied voting patterns, we constructed a Markov chain that described how the percentages of voters choosing different parties changed from one election to the next. We saw that the Markov chain converges to , a probability vector in the eigenspace . In other words, is a probability vector that is unchanged under multiplication by ; that is, . This implies that, after a long time, 20% of voters choose party , 40% choose , and 40% choose .   steady-state vector   stationary vector   vector  stationary  vector  steady-state  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .   An important question that arises from our previous example is   If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?     Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Python cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.     As this activity implies, the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. Here are a few important facts about the eigenvalues of a stochastic matrix.  As is demonstrated in , is an eigenvalue of any stochastic matrix. We usually order the eigenvalues so it is the first eigenvalue meaning that .   All other eigenvalues satisfy the property that .  Any stochastic matrix has at least one steady-state vector .   As illustrated in the activity, a Markov chain could fail to converge to a steady-state vector if . This happens for the matrix , whose eigenvalues are and .  However, if all but the first eigenvalue satisfy , then there is a unique steady-state vector and any Markov chain will converge to . This was the case for the matrix , whose eigenvalues are and . In this case, any Markov chain will converge to the unique steady-state vector .  In this way, we see that the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. However, it is somewhat inconvenient to compute the eigenvalues to answer this question. Is there some way to conclude that every Markov chain will converge to a steady-state vector without actually computing the eigenvalues? It turns out that there is a simple condition on the matrix that guarantees this.    positive matrix    matrix positive   We say that a matrix is positive if either or some power has all positive entries.     The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.    Positive matrices are important because of the following theorem.   Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .      We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Python cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.       Google's PageRank algorithm  Markov chains and the Perron-Frobenius theorem are the central ingredients in Google's PageRank algorithm, developed by Google to assess the quality of web pages.  Suppose we enter linear algebra into Google's search engine. Google responds by telling us there are 138 million web pages containing those terms. On the first page, however, there are links to ten web pages that Google judges to have the highest quality and to be the ones we are most likely to be interested in. How does Google assess the quality of web pages?  At the time this is being written, Google is tracking 35 trillion web pages. Clearly, this is too many for humans to evaluate. Plus, human evaluators may inject their own biases into their evaluations, perhaps even unintentionally. Google's idea is to use the structure of the Internet to assess the quality of web pages without any human intervention. For instance, if a web page has quality content, other web pages will link to it. This means that the number of links to a page reflect the quality of that page. In addition, we would expect a page to have even higher quality content if those links are coming from pages that are themselves assessed to have high quality. Simply said, if many quality pages link to a page, that page must itself be of high quality. This is the essence of the PageRank algorithm, which we introduce in the next activity.     We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .     This activity shows us two ways to find the PageRank vector. In the first, we determine a steady-state vector directly by finding a description of the eigenspace and then finding the appropriate scalar multiple of a basis vector that gives us the steady-state vector. To find a description of the eigenspace , however, we need to find the null space . Remember that the real Internet has 35 trillion pages so finding requires us to row reduce a matrix with 35 trillion rows and columns. As we saw in , that is not computationally feasible.  As suggested by the activity, the second way to find the PageRank vector is to use a Markov chain that converges to the PageRank vector. Since multiplying a vector by a matrix is significantly less work than row reducing the matrix, this approach is computationally feasible, and it is, in fact, how Google computes the PageRank vector.    Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.     The Perron-Frobenius theorem tells us that a Markov chain converges to a unique steady-state vector when the matrix is positive. This means that or some power of should have only positive entries. Clearly, this is not the case for the matrix formed from the Internet in .  We can understand the problem with the Internet shown in by adding a box around some of the pages as shown in . Here we see that the pages outside of the box give up all of their PageRank to the pages inside the box. This is not desirable because the PageRanks of the pages outside of the box are found to be zero. Once again, the Google matrix is not a positive matrix.      The pages outside the box give up all of their PageRank to the pages inside the box.   Google solves this problem by slightly modifying the Google matrix to obtain a positive matrix . To understand this, think of the entries in the Google matrix as giving the probability that an Internet user follows a link from one page to another. To create a positive matrix, we will allow that user to randomly jump to any other page on the Internet with a small probability.  To make sense of this, suppose that there are pages on our internet. The matrix is a positive stochastic matrix describing a process where we can move from any page to another with equal probability. To form the modified Google matrix , we choose a parameter that is used to mix and together; that is, is the positive stochastic matrix . In practice, it is thought that Google uses a value of (Google doesn't publish this number as it is a trade secret) so that we have . Intuitively, this means that an Internet user will randomly follow a link from one page to another 85% of the time and will randomly jump to any other page on the Internet 15% of the time. Since the matrix is positive, the Perron-Frobenius theorem tells us that any Markov chain will converge to a unique steady-state vector that we call the PageRank vector.    The following Python cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Python cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.     The ability to access almost anything we want to know through the Internet is something we take for granted in today's society. Without Google's PageRank algorithm, however, the Internet would be a chaotic place indeed; imagine trying to find a useful web page among the 30 trillion available pages without it. (There are, of course, other search algorithms, but Google's is the most widely used.) The fundamental role that Markov chains and the Perron-Frobenius theorem play in Google's algorithm demonstrates the vast power that mathematics has to shape our society.    Summary  This section explored stochastic matrices and Markov chains.  A probability vector is one whose entries are nonnegative and sum to 1. A stochastic matrix is a square matrix whose columns are probability vectors.  A Markov chain is formed from a stochastic matrix and an initial probability vector using the rule . We may think of the sequence as describing the evolution of some conserved quantity, such as the number of rental cars or voters, among a number of possible states over time.  A steady-state vector for a stochastic matrix is a probability vector that satisfies .  The Perron-Frobenius theorem tells us that, if is a positive stochastic matrix, then every Markov chain defined by converges to a unique, positive steady-state vector.  Google's PageRank algorithm uses Markov chains and the Perron-Frobenius theorem to assess the relative quality of web pages on the Internet.      Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.     Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.     Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .      Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.    In the next few exercises, we will consider the matrix .   Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.     Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .     This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .     We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.     This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .       "
},
{
  "id": "p-5108",
  "level": "2",
  "url": "sec-stochastic.html#p-5108",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Markov chains "
},
{
  "id": "exploration-19",
  "level": "2",
  "url": "sec-stochastic.html#exploration-19",
  "type": "Preview Activity",
  "number": "5.5.1",
  "title": "",
  "body": "  Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .    "
},
{
  "id": "definition-25",
  "level": "2",
  "url": "sec-stochastic.html#definition-25",
  "type": "Definition",
  "number": "5.5.1",
  "title": "",
  "body": " probability vector  vector  probability  stochastic matrix  matrix  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix .  "
},
{
  "id": "activity-58",
  "level": "2",
  "url": "sec-stochastic.html#activity-58",
  "type": "Activity",
  "number": "5.5.2",
  "title": "",
  "body": "  Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.   "
},
{
  "id": "p-5146",
  "level": "2",
  "url": "sec-stochastic.html#p-5146",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Markov chain "
},
{
  "id": "definition-26",
  "level": "2",
  "url": "sec-stochastic.html#definition-26",
  "type": "Definition",
  "number": "5.5.2",
  "title": "",
  "body": " steady-state vector   stationary vector   vector  stationary  vector  steady-state  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .  "
},
{
  "id": "question-3",
  "level": "2",
  "url": "sec-stochastic.html#question-3",
  "type": "Question",
  "number": "5.5.3",
  "title": "",
  "body": " If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?  "
},
{
  "id": "activity-59",
  "level": "2",
  "url": "sec-stochastic.html#activity-59",
  "type": "Activity",
  "number": "5.5.3",
  "title": "",
  "body": "  Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Python cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.    "
},
{
  "id": "definition-27",
  "level": "2",
  "url": "sec-stochastic.html#definition-27",
  "type": "Definition",
  "number": "5.5.4",
  "title": "",
  "body": "  positive matrix    matrix positive   We say that a matrix is positive if either or some power has all positive entries.  "
},
{
  "id": "example-56",
  "level": "2",
  "url": "sec-stochastic.html#example-56",
  "type": "Example",
  "number": "5.5.5",
  "title": "",
  "body": "  The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.   "
},
{
  "id": "theorem-perron",
  "level": "2",
  "url": "sec-stochastic.html#theorem-perron",
  "type": "Theorem",
  "number": "5.5.6",
  "title": "Perron-Frobenius.",
  "body": " Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .   "
},
{
  "id": "activity-60",
  "level": "2",
  "url": "sec-stochastic.html#activity-60",
  "type": "Activity",
  "number": "5.5.4",
  "title": "",
  "body": "  We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Python cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.    "
},
{
  "id": "activity-61",
  "level": "2",
  "url": "sec-stochastic.html#activity-61",
  "type": "Activity",
  "number": "5.5.5",
  "title": "",
  "body": "   We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .    "
},
{
  "id": "activity-62",
  "level": "2",
  "url": "sec-stochastic.html#activity-62",
  "type": "Activity",
  "number": "5.5.6",
  "title": "",
  "body": "  Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.    "
},
{
  "id": "fig-google-reducible-box",
  "level": "2",
  "url": "sec-stochastic.html#fig-google-reducible-box",
  "type": "Figure",
  "number": "5.5.11",
  "title": "",
  "body": "    The pages outside the box give up all of their PageRank to the pages inside the box.  "
},
{
  "id": "activity-63",
  "level": "2",
  "url": "sec-stochastic.html#activity-63",
  "type": "Activity",
  "number": "5.5.7",
  "title": "",
  "body": "  The following Python cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Python cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.    "
},
{
  "id": "exercise-185",
  "level": "2",
  "url": "sec-stochastic.html#exercise-185",
  "type": "Exercise",
  "number": "5.5.5.1",
  "title": "",
  "body": " Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.   "
},
{
  "id": "exercise-186",
  "level": "2",
  "url": "sec-stochastic.html#exercise-186",
  "type": "Exercise",
  "number": "5.5.5.2",
  "title": "",
  "body": " Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.   "
},
{
  "id": "exercise-187",
  "level": "2",
  "url": "sec-stochastic.html#exercise-187",
  "type": "Exercise",
  "number": "5.5.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.   "
},
{
  "id": "exercise-188",
  "level": "2",
  "url": "sec-stochastic.html#exercise-188",
  "type": "Exercise",
  "number": "5.5.5.4",
  "title": "",
  "body": " Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .   "
},
{
  "id": "exercise-189",
  "level": "2",
  "url": "sec-stochastic.html#exercise-189",
  "type": "Exercise",
  "number": "5.5.5.5",
  "title": "",
  "body": " Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.   "
},
{
  "id": "exercise-stochastic-probability",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-probability",
  "type": "Exercise",
  "number": "5.5.5.6",
  "title": "",
  "body": " Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.   "
},
{
  "id": "exercise-191",
  "level": "2",
  "url": "sec-stochastic.html#exercise-191",
  "type": "Exercise",
  "number": "5.5.5.7",
  "title": "",
  "body": " Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .   "
},
{
  "id": "exercise-stochastic-eigenvalue",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-eigenvalue",
  "type": "Exercise",
  "number": "5.5.5.8",
  "title": "",
  "body": " This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .   "
},
{
  "id": "exercise-193",
  "level": "2",
  "url": "sec-stochastic.html#exercise-193",
  "type": "Exercise",
  "number": "5.5.5.9",
  "title": "",
  "body": " We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.   "
},
{
  "id": "exercise-194",
  "level": "2",
  "url": "sec-stochastic.html#exercise-194",
  "type": "Exercise",
  "number": "5.5.5.10",
  "title": "",
  "body": " This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .     "
},
{
  "id": "sec-power-method",
  "level": "1",
  "url": "sec-power-method.html",
  "type": "Section",
  "number": "5.6",
  "title": "Finding eigenvectors numerically",
  "body": " Finding eigenvectors numerically   We have typically found eigenvalues of a square matrix as the roots of the characteristic polynomial and the associated eigenvectors as the null space . Unfortunately, this approach is not practical when we are working with large matrices. First, finding the charactertic polynomial of a large matrix requires considerable computation, as does finding the roots of that polynomial. Second, finding the null space of a singular matrix is plagued by numerical problems, as we will see in the preview activity.  For this reason, we will explore a technique called the power method that finds numerical approximations to the eigenvalues and eigenvectors of a square matrix.    Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sympy, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .       The power method  power method  Our goal is to find a technique that produces numerical approximations to the eigenvalues and associated eigenvectors of a matrix . We begin by searching for the eigenvalue having the largest absolute value, which is called the dominant eignevalue .  eigenvalue  dominant   dominant eigenvalue The next two examples demonstrate this technique.   stochastic matrix  matrix  stochastic  Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which is essentially how the Google PageRank algorithm works.    Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method        Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Python cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.     Notice that the power method gives us not only an eigenvector but also its associated eigenvalue. As in the activity, consider the matrix , which has eigenvector . The first component has the largest absolute value so we multiply by to obtain . When we multiply by , we have . Notice that the first component still has the largest absolute value so that the multiplier is the eigenvalue corresponding to the eigenvector. This demonstrates the fact that the multipliers approach the eigenvalue having the largest absolute value.  Notice that the power method requires us to choose an initial vector . For most choices, this method will find the eigenvalue having the largest absolute value. However, an unfortunate choice of may not. For instance, if we had chosen in our example above, the vectors in the sequence will not detect the eigenvector . However, it usually happens that our initial guess has some contribution from that enables us to find it.  The power method, as presented here, will fail for certain unlucky matrices. This is examined in along with a means to improve the power method to work for all matrices.    Finding other eigenvalues  The power method gives a technique for finding the dominant eigenvalue of a matrix. We can modify the method to find the other eigenvalues as well.    The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Python cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.    The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .     With the power method and the inverse power method, we can now find the eigenvalues of a matrix having the largest and smallest absolute values. With one more modification, we can find all the eigenvalues of .    Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvalue of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Python cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .     There are some restrictions on the matrices to which this technique applies as we have assumed that the eigenvalues of are real and distinct. If has repeated or complex eigenvalues, this technique will need to be modified, as explored in some of the exercises.    Summary  We have explored the power method as a tool for numerically approximating the eigenvalues and eigenvectors of a matrix.  After choosing an initial vector , we define the sequence . As grows larger, the direction of the vectors closely approximates the direction of the eigenspace corresponding to the eigenvalue having the largest absolute value.  We normalize the vectors by multiplying by , where is the component having the largest absolute value. In this way, the vectors approach an eigenvector associated to , and the multipliers approach the eigenvalue .  To find the eigenvalue having the smallest absolute value, we apply the power method using the matrix .  To find the eigenvalue closest to some number , we apply the power method using the matrix .     This Sage cell has the commands power , inverse_power , and find_closest_eigenvalue that we have developed in this section. After evaluating this cell, these commands will be available in any other cell on this page.    Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .     Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .     Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .    Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .     We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.     Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .    Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .    The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.    Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     "
},
{
  "id": "p-5434",
  "level": "2",
  "url": "sec-power-method.html#p-5434",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "power method "
},
{
  "id": "exploration-20",
  "level": "2",
  "url": "sec-power-method.html#exploration-20",
  "type": "Preview Activity",
  "number": "5.6.1",
  "title": "",
  "body": "  Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sympy, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .    "
},
{
  "id": "p-5449",
  "level": "2",
  "url": "sec-power-method.html#p-5449",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dominant eignevalue "
},
{
  "id": "example-57",
  "level": "2",
  "url": "sec-power-method.html#example-57",
  "type": "Example",
  "number": "5.6.1",
  "title": "",
  "body": " stochastic matrix  matrix  stochastic  Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which is essentially how the Google PageRank algorithm works.  "
},
{
  "id": "example-58",
  "level": "2",
  "url": "sec-power-method.html#example-58",
  "type": "Example",
  "number": "5.6.2",
  "title": "",
  "body": " Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method     "
},
{
  "id": "activity-64",
  "level": "2",
  "url": "sec-power-method.html#activity-64",
  "type": "Activity",
  "number": "5.6.2",
  "title": "",
  "body": "  Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Python cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.    "
},
{
  "id": "activity-65",
  "level": "2",
  "url": "sec-power-method.html#activity-65",
  "type": "Activity",
  "number": "5.6.3",
  "title": "",
  "body": "  The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Python cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.    The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .    "
},
{
  "id": "activity-66",
  "level": "2",
  "url": "sec-power-method.html#activity-66",
  "type": "Activity",
  "number": "5.6.4",
  "title": "",
  "body": "  Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvalue of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Python cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .    "
},
{
  "id": "exercise-195",
  "level": "2",
  "url": "sec-power-method.html#exercise-195",
  "type": "Exercise",
  "number": "5.6.4.1",
  "title": "",
  "body": " Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .   "
},
{
  "id": "exercise-196",
  "level": "2",
  "url": "sec-power-method.html#exercise-196",
  "type": "Exercise",
  "number": "5.6.4.2",
  "title": "",
  "body": " Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .   "
},
{
  "id": "exercise-197",
  "level": "2",
  "url": "sec-power-method.html#exercise-197",
  "type": "Exercise",
  "number": "5.6.4.3",
  "title": "",
  "body": " Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .  "
},
{
  "id": "exercise-198",
  "level": "2",
  "url": "sec-power-method.html#exercise-198",
  "type": "Exercise",
  "number": "5.6.4.4",
  "title": "",
  "body": " Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .   "
},
{
  "id": "exercise-power-method",
  "level": "2",
  "url": "sec-power-method.html#exercise-power-method",
  "type": "Exercise",
  "number": "5.6.4.5",
  "title": "",
  "body": " We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.   "
},
{
  "id": "exercise-200",
  "level": "2",
  "url": "sec-power-method.html#exercise-200",
  "type": "Exercise",
  "number": "5.6.4.6",
  "title": "",
  "body": " Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .  "
},
{
  "id": "exercise-201",
  "level": "2",
  "url": "sec-power-method.html#exercise-201",
  "type": "Exercise",
  "number": "5.6.4.7",
  "title": "",
  "body": " Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .  "
},
{
  "id": "exercise-202",
  "level": "2",
  "url": "sec-power-method.html#exercise-202",
  "type": "Exercise",
  "number": "5.6.4.8",
  "title": "",
  "body": " The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.  "
},
{
  "id": "exercise-203",
  "level": "2",
  "url": "sec-power-method.html#exercise-203",
  "type": "Exercise",
  "number": "5.6.4.9",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .   "
},
{
  "id": "sec-dot-product",
  "level": "1",
  "url": "sec-dot-product.html",
  "type": "Section",
  "number": "6.1",
  "title": "The dot product",
  "body": " The dot product   In this section, we introduce a new operation on vectors, known as the dot product . Geometrically, the dot product is related to the lengths of vectors and the angles between pairs of vectors. Importantly, the dot product is also easily computed from the list-of-numbers representation of vectors. It also has many nice algebraic properties. So this is another situation where each of our three presepectives of vectors provides valuable insight.    Projections and dot products  We will motivate the defintion of the dot product by investigating the projection of a vector in the direction of a vector , which we will denote as .     the projection of in the direction of  The projection vector is the scalar multiple of that is as close as possible to . That means to find the projection vector, we must find the scalar that makes the vector as short as possible. As is illustrated in , this happens when and form a 90 degree angle.   Illustrating the projection . We are looking for a vector along the blue line that is as close as possible to . This will occur (upper right) when and form a right angle. If is shorter (lower left) or longer (lower right), then the angle will not be a right angle, and so a closer vector exists (because the legs of a right triangle are shorter than the hypotenuse).            projection By the definition of cosine, this means that where denotes the (Euclidean) length of the vector and is the angle between the two vectors. Solving for , we find that . So .  Notice that . This leads to our geometric definition of the dot product, which is similar to the peojection, but is (a) symmetric in and , (b) avoids the denominator, and (c) results in a scalar rather than a vector. The result will be easier to work with algebraically and also easier to compute.      The dot product measures the angle .    dot product   For any two non-zero vectors and in with an angle between them, we define their dot product , denoted , as . If either or is the zero vector, then .    Note that the dot product of two vectors is a scalar . For this reason, the dot product is sometimes called the scalar product .  We can easily re-express the projection in terms of the dot product.   projection   For any two vectors and in , .    The formulas for the dot product and for projection become simpler for unit vectors , vectors with length 1. If and are unit vectors in and and are any vectors in , then unit vector  vector unit unit vector  , and . And since is a unit vector, this gives us another way to think about , namely, . In other words, we obtain the length of the projection by taking the dot product of with a unit vector in the same direction as .  A number of properties of the dot product follow from the definition.    For any vectors and scalar , the following properties hold.   Commutativity       Scalar multiples       Length       Distributivity       Linear Combinations                     The first two statements follow directly from the definition. The third follows by observing that for two vectors that point in the same direction.  The fourth property deserves some explanation. First, we observe that a similar property holds for projections, namely, . This is illustrated in . The orange vector is the sum of the red and blue vectors, and the projections onto one of the axes is indicated. You can drag the background to get a different perspective and to see that this holds even if , , and the axis of projection do not lie in a plane. Projecting onto an axis makes the visualization a little bit simpler, but changing the perspective demonstrates that this is not essential. It will always be the case that the projection of a sum is the sum of the projections.  The final property follows by (repeated) application of scalar multiples and distributivity.    The projection of a sum is the sum of projections. Drag the arrow heads to change the vectors. Drag on the background change the perspective. Projections are illustrated with thinner, lighter lines of the same color as the vectors being projected. Each of the red, blue, and orange triangles is a right triangle. The blue projection vector is indicated twice, once as part of the blue triangle, and a second time translated so that it begins where the red projection vector ends.     From this it follows that     These properties of the dot product allow us to algebraically rearrange or simplify many expressions involving dot products.    Suppose that and . Then .    As we move forward, it will be important for us to recognize when vectors are perpendicular to one another. For instance, when vectors and are perpendicular, the angle between them and we have . Therefore, the dot product between perpendicular vectors must be zero. This leads to the following definition.    orthogonal vectors    the vectors and are orthogonal  We say that vectors and are orthogonal if . We can denote this with .   In practical terms, two perpendicular vectors are orthogonal. However, the concept of orthogonality is somewhat more general because it allows one or both of the vectors to be the zero vector .  We turn next to the important question of how to compute dot products.    Computing dot products  The geometric definition of the dot product motivates many of its applications, but it can be difficult to compute the dot product from this definition because may be difficult to obtain, espcially in high dimensions. Fortunately, there is a computational shortcut that follows directly from .  Let be two vectors in . Then we can write From this it follows that because , so the only terms in the sum that are not 0 are the ones where and match.  In other words, the dot product can be computed as the sum of products of corresponding entries in the two vectors. It will be useful, whenever you see a sum of products to ask how it might be interpreted as a dot product.    Let and be two vectors in . Then .      For two-dimensional vectors and , their dot product is . For instance, .      We compute the dot product between two four-dimensional vectors as .      From the we know that . We can also compute the length of a vector using the Pythagorean Theorem. When using our computationa shortcut, we see that these are the same calculation.  For example, consider the vector as shown in .      The vector .   We may find the length of this vector using the Pythagorean theorem since the vector forms the hypotenuse of a right triangle having a horizontal leg of length 3 and a vertical leg of length 2, so . Now notice that the dot product of with itself performs the identical arithmetic: .        Compute the dot product .  Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .           .    The length of is 5.     , which is the square of the length of .          .     .       >       Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Python can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then np.linalg.norm() gives the length of v and v @ w gives .  np.linalg.norm()  norm Euclidean  Suppose that . Use the Python cell below to find , , , and the angle between and . You may use math.acos() to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .       As we have seen, we can use the dot product to compute the angle between two vectors. This angle (or the cosine of the angle) is often taken as a measure of \"similarity\" between two vectors. For example, consider the vectors , , and , shown in . The vectors and seem somewhat similar as the directions they define are nearly the same. By comparison, appears rather dissimilar to both and . We will measure the similarity of vectors by finding the angle between them; the smaller the angle, the more similar the vectors. This is especially useful in contexts where the direction of a vector conveys more important information than its magnitude.      Which of the vectors are most similar?     This activity explores two uses of the dot product as a way to compute the \"similarity\" of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. document vector In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector, called the document vector for each article.  For example, evaluate the following cell that loads a matrix constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. Each row of the matrix represents one of 604 words and each column represents one of the articles as a document vector. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article.      Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Python cell below to find the angle between the document vector for the Veteran's Day article and the other four articles. To express the angle in degrees, multiply radians by 180.0 \/ math.pi .     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series.  time series Evaluate the Python cell below to see a representation of time series for four different stocks over 10 days. Notices that although one stock has a higher value, stocks 0 and 3 appear to be related since they seem to rise and fall at roughly similar ways. We often say that such series are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean  demean each time series; that is, for each time series, we will subtract its average value to obtain a new time series.     If the demeaned series are and , then the correlation between and is defined to be correlation  where is the angle between and . That is, the correlation equals the cosine of the angle between the demeaned time series. Among other things, this implies that is always between -1 and 1.  Find the correlation between each pair of stocks.     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Which pair of stocks had the largest correlation? How is this reflected in the plot?    Which pair of stocks had the smallest (most negative) correlation? How is this reflected in the plot?    Which pair of stocks had a correlation closest to 0? How is this reflected in the plot?    The correlation is important enough that numpy can do all the work of computing correlations for each pair of rows (default) or columns (what we want here) in a matrix. The result is a matrix containing all the pairwise correlations. np.corrcoef()      Explain why the shape of the correlation matrix is what it is. What shape would it have been if we had not used rowvar = False ?    Explain why this matrix is symmetric.    Explain why the diagonal values are what they are.                  If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    See below for all of the correlations.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The largest correlation is for stocks 0 and 3, which are most similar.    The smallest correlation is for stocks 1 and 3, which are least similar. When one goes up, the other tends to go down.    The closest correlation to 0 is for stocks 3 and 4. These stocks seem unrelated. When one goes up, the other sometimes goes up, sometimes down.    Correlation matrices are square and have a row and column for each of the vectors we computed correlations with. Since we have 4 column vectors here, we get a matrix. It would be if we did row-wise correlations. These matrices are always symmetric because the definition of correlation is symmetric. The diagonal entries are 1 because .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                              We conclude this section by listing once more the important properties of the dot produt.   Properties of the dot product  Let , , and be vectors in , let be the angle between and , and let and be scalars. Then    .     .     .     .     .     .     .         -means clustering  clustering  A typical problem in data science is to find some underlying patterns in a dataset. Suppose, for instance, that we have the set of 177 data points plotted in . Notice that the points are not scattered around haphazardly; instead, they seem to form clusters. Our goal here is to develop a strategy for detecting the clusters.      A set of 177 data points.   To see how this could be useful, suppose we have medical data describing a group of patients, some of whom have been diagnosed with a specific condition, such as diabetes. Perhaps we have a record of age, weight, blood sugar, cholesterol, and other attributes for each patient. It could be that the data points for the group diagnosed as having the condition form a cluster that is somewhat distinct from the rest of the data. Suppose that we are able to identify that cluster and that we are then presented with a new patient that has not been tested for the condition. If the attributes for that patient place them in that cluster, we might identify them as being at risk for the condition and prioritize them for appropriate screenings.  If there are many attributes for each patient, the data may be high-dimensional and not easily visualized. We would therefore like to develop an algorithm that separates the data points into clusters without human intervention. clustering We call the result a clustering .  The next activity introduces a technique, called -means clustering, that helps us find clusterings. To do so, we will view the data points as vectors so that the distance between two data points equals the length of the vector joining them. That is, if two points are represented by the vectors and , then the distance between the points is .     centroid To begin, we identify the centroid , or the average, of a set of vectors as .   Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.       This activity demonstrates our algorithm for finding a clustering. We first choose a value and seek to break the data points into clusters. The algorithm proceeds in the following way:   Choose points at random from our data set.    Construct the cluster as the set of data points closest to , as the set of data points closest to , and so forth.    Repeat the following until the clusters no longer change:   Find the centroids of the current clusters.    Update the clusters .        The clusterings we find depend on the initial random choice of points . For instance, in the previous activity, we arrived, with the initial choice and , at the clustering: .  If we instead choose the initial points to be and , we eventually find the clustering: .  Is there a way that we can determine which clustering is the better of the two? It seems like a better clustering will be one for which the points in a cluster are, on average, closer to the centroid of their cluster. If we have a clustering, we therefore define a function, called the objective , which measures the average of the square of the distance from each point to the centroid of the cluster to which that point belongs. A clustering with a smaller objective will have clusters more tightly centered around their centroids, which should result in a better clustering.  For example, when we obtain the clustering: . with centroids and , we find the objective to be .    We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our data set is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a data set consisting of 177 data points. This data set has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is .    The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .          We have now seen how our algorithm and the objective identify a reasonable value for , the number of the clusters, and produce a good clustering having clusters. Notice that we don't claim to have found the best clustering as the true test of any clustering will be in how it helps us understand the dataset and helps us make predictions about any new data that we may encounter.    Summary  This section introduced the dot product and the ability to investigate geometric relationships between vectors.   The dot product of two vectors and satisfies these properties: where is the angle between and .    The vectors and are orthogonal when .    We explored some applications of the dot product to the similarity of vectors, correlation of time series, and -means clustering.        Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .       Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                                 Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .       Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so        Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .              Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.       Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so        Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.       Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.       In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .       Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .       "
},
{
  "id": "p-5587",
  "level": "2",
  "url": "sec-dot-product.html#p-5587",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dot product "
},
{
  "id": "p-5588",
  "level": "2",
  "url": "sec-dot-product.html#p-5588",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "projection "
},
{
  "id": "fig-projection-illustrated",
  "level": "2",
  "url": "sec-dot-product.html#fig-projection-illustrated",
  "type": "Figure",
  "number": "6.1.1",
  "title": "",
  "body": " Illustrating the projection . We are looking for a vector along the blue line that is as close as possible to . This will occur (upper right) when and form a right angle. If is shorter (lower left) or longer (lower right), then the angle will not be a right angle, and so a closer vector exists (because the legs of a right triangle are shorter than the hypotenuse).          "
},
{
  "id": "fig-dot-angle",
  "level": "2",
  "url": "sec-dot-product.html#fig-dot-angle",
  "type": "Figure",
  "number": "6.1.2",
  "title": "",
  "body": "    The dot product measures the angle .  "
},
{
  "id": "def-geometric-dot-product",
  "level": "2",
  "url": "sec-dot-product.html#def-geometric-dot-product",
  "type": "Definition",
  "number": "6.1.3",
  "title": "",
  "body": " dot product   For any two non-zero vectors and in with an angle between them, we define their dot product , denoted , as . If either or is the zero vector, then .   "
},
{
  "id": "note-7",
  "level": "2",
  "url": "sec-dot-product.html#note-7",
  "type": "Note",
  "number": "6.1.4",
  "title": "",
  "body": "Note that the dot product of two vectors is a scalar . For this reason, the dot product is sometimes called the scalar product . "
},
{
  "id": "prop-projection-in-terms-of-dot-product",
  "level": "2",
  "url": "sec-dot-product.html#prop-projection-in-terms-of-dot-product",
  "type": "Proposition",
  "number": "6.1.5",
  "title": "",
  "body": " projection   For any two vectors and in , .   "
},
{
  "id": "p-5594",
  "level": "2",
  "url": "sec-dot-product.html#p-5594",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "unit vectors "
},
{
  "id": "prop-properties-of-dot-product",
  "level": "2",
  "url": "sec-dot-product.html#prop-properties-of-dot-product",
  "type": "Proposition",
  "number": "6.1.6",
  "title": "",
  "body": "  For any vectors and scalar , the following properties hold.   Commutativity       Scalar multiples       Length       Distributivity       Linear Combinations                     The first two statements follow directly from the definition. The third follows by observing that for two vectors that point in the same direction.  The fourth property deserves some explanation. First, we observe that a similar property holds for projections, namely, . This is illustrated in . The orange vector is the sum of the red and blue vectors, and the projections onto one of the axes is indicated. You can drag the background to get a different perspective and to see that this holds even if , , and the axis of projection do not lie in a plane. Projecting onto an axis makes the visualization a little bit simpler, but changing the perspective demonstrates that this is not essential. It will always be the case that the projection of a sum is the sum of the projections.  The final property follows by (repeated) application of scalar multiples and distributivity.    The projection of a sum is the sum of projections. Drag the arrow heads to change the vectors. Drag on the background change the perspective. Projections are illustrated with thinner, lighter lines of the same color as the vectors being projected. Each of the red, blue, and orange triangles is a right triangle. The blue projection vector is indicated twice, once as part of the blue triangle, and a second time translated so that it begins where the red projection vector ends.     From this it follows that    "
},
{
  "id": "example-59",
  "level": "2",
  "url": "sec-dot-product.html#example-59",
  "type": "Example",
  "number": "6.1.8",
  "title": "",
  "body": "  Suppose that and . Then .   "
},
{
  "id": "definition-29",
  "level": "2",
  "url": "sec-dot-product.html#definition-29",
  "type": "Definition",
  "number": "6.1.9",
  "title": "",
  "body": "  orthogonal vectors    the vectors and are orthogonal  We say that vectors and are orthogonal if . We can denote this with .  "
},
{
  "id": "prop-computing-dot-product",
  "level": "2",
  "url": "sec-dot-product.html#prop-computing-dot-product",
  "type": "Proposition",
  "number": "6.1.10",
  "title": "",
  "body": "  Let and be two vectors in . Then .   "
},
{
  "id": "example-60",
  "level": "2",
  "url": "sec-dot-product.html#example-60",
  "type": "Example",
  "number": "6.1.11",
  "title": "",
  "body": "  For two-dimensional vectors and , their dot product is . For instance, .   "
},
{
  "id": "example-61",
  "level": "2",
  "url": "sec-dot-product.html#example-61",
  "type": "Example",
  "number": "6.1.12",
  "title": "",
  "body": "  We compute the dot product between two four-dimensional vectors as .   "
},
{
  "id": "example-62",
  "level": "2",
  "url": "sec-dot-product.html#example-62",
  "type": "Example",
  "number": "6.1.13",
  "title": "",
  "body": "  From the we know that . We can also compute the length of a vector using the Pythagorean Theorem. When using our computationa shortcut, we see that these are the same calculation.  For example, consider the vector as shown in .      The vector .   We may find the length of this vector using the Pythagorean theorem since the vector forms the hypotenuse of a right triangle having a horizontal leg of length 3 and a vertical leg of length 2, so . Now notice that the dot product of with itself performs the identical arithmetic: .   "
},
{
  "id": "activity-67",
  "level": "2",
  "url": "sec-dot-product.html#activity-67",
  "type": "Activity",
  "number": "6.1.1",
  "title": "",
  "body": "    Compute the dot product .  Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .           .    The length of is 5.     , which is the square of the length of .          .     .       "
},
{
  "id": "activity-68",
  "level": "2",
  "url": "sec-dot-product.html#activity-68",
  "type": "Activity",
  "number": "6.1.2",
  "title": "",
  "body": "     Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Python can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then np.linalg.norm() gives the length of v and v @ w gives .  np.linalg.norm()  norm Euclidean  Suppose that . Use the Python cell below to find , , , and the angle between and . You may use math.acos() to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .      "
},
{
  "id": "fig-similar-vectors",
  "level": "2",
  "url": "sec-dot-product.html#fig-similar-vectors",
  "type": "Figure",
  "number": "6.1.17",
  "title": "",
  "body": "    Which of the vectors are most similar?  "
},
{
  "id": "activity-69",
  "level": "2",
  "url": "sec-dot-product.html#activity-69",
  "type": "Activity",
  "number": "6.1.3",
  "title": "",
  "body": "  This activity explores two uses of the dot product as a way to compute the \"similarity\" of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. document vector In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector, called the document vector for each article.  For example, evaluate the following cell that loads a matrix constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. Each row of the matrix represents one of 604 words and each column represents one of the articles as a document vector. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article.      Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Python cell below to find the angle between the document vector for the Veteran's Day article and the other four articles. To express the angle in degrees, multiply radians by 180.0 \/ math.pi .     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series.  time series Evaluate the Python cell below to see a representation of time series for four different stocks over 10 days. Notices that although one stock has a higher value, stocks 0 and 3 appear to be related since they seem to rise and fall at roughly similar ways. We often say that such series are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean  demean each time series; that is, for each time series, we will subtract its average value to obtain a new time series.     If the demeaned series are and , then the correlation between and is defined to be correlation  where is the angle between and . That is, the correlation equals the cosine of the angle between the demeaned time series. Among other things, this implies that is always between -1 and 1.  Find the correlation between each pair of stocks.     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Which pair of stocks had the largest correlation? How is this reflected in the plot?    Which pair of stocks had the smallest (most negative) correlation? How is this reflected in the plot?    Which pair of stocks had a correlation closest to 0? How is this reflected in the plot?    The correlation is important enough that numpy can do all the work of computing correlations for each pair of rows (default) or columns (what we want here) in a matrix. The result is a matrix containing all the pairwise correlations. np.corrcoef()      Explain why the shape of the correlation matrix is what it is. What shape would it have been if we had not used rowvar = False ?    Explain why this matrix is symmetric.    Explain why the diagonal values are what they are.                  If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    See below for all of the correlations.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The largest correlation is for stocks 0 and 3, which are most similar.    The smallest correlation is for stocks 1 and 3, which are least similar. When one goes up, the other tends to go down.    The closest correlation to 0 is for stocks 3 and 4. These stocks seem unrelated. When one goes up, the other sometimes goes up, sometimes down.    Correlation matrices are square and have a row and column for each of the vectors we computed correlations with. Since we have 4 column vectors here, we get a matrix. It would be if we did row-wise correlations. These matrices are always symmetric because the definition of correlation is symmetric. The diagonal entries are 1 because .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                             "
},
{
  "id": "fig-clusters",
  "level": "2",
  "url": "sec-dot-product.html#fig-clusters",
  "type": "Figure",
  "number": "6.1.19",
  "title": "",
  "body": "    A set of 177 data points.  "
},
{
  "id": "p-5721",
  "level": "2",
  "url": "sec-dot-product.html#p-5721",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "clustering "
},
{
  "id": "activity-70",
  "level": "2",
  "url": "sec-dot-product.html#activity-70",
  "type": "Activity",
  "number": "6.1.4",
  "title": "",
  "body": "   centroid To begin, we identify the centroid , or the average, of a set of vectors as .   Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.      "
},
{
  "id": "p-5755",
  "level": "2",
  "url": "sec-dot-product.html#p-5755",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "objective "
},
{
  "id": "activity-71",
  "level": "2",
  "url": "sec-dot-product.html#activity-71",
  "type": "Activity",
  "number": "6.1.5",
  "title": "",
  "body": "  We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our data set is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a data set consisting of 177 data points. This data set has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is .    The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .         "
},
{
  "id": "exercise-204",
  "level": "2",
  "url": "sec-dot-product.html#exercise-204",
  "type": "Exercise",
  "number": "6.1.5.1",
  "title": "",
  "body": " Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .     "
},
{
  "id": "exercise-205",
  "level": "2",
  "url": "sec-dot-product.html#exercise-205",
  "type": "Exercise",
  "number": "6.1.5.2",
  "title": "",
  "body": " Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                               "
},
{
  "id": "exercise-206",
  "level": "2",
  "url": "sec-dot-product.html#exercise-206",
  "type": "Exercise",
  "number": "6.1.5.3",
  "title": "",
  "body": " Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .     "
},
{
  "id": "exercise-207",
  "level": "2",
  "url": "sec-dot-product.html#exercise-207",
  "type": "Exercise",
  "number": "6.1.5.4",
  "title": "",
  "body": " Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so      "
},
{
  "id": "exercise-208",
  "level": "2",
  "url": "sec-dot-product.html#exercise-208",
  "type": "Exercise",
  "number": "6.1.5.5",
  "title": "",
  "body": " Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .            "
},
{
  "id": "exercise-209",
  "level": "2",
  "url": "sec-dot-product.html#exercise-209",
  "type": "Exercise",
  "number": "6.1.5.6",
  "title": "",
  "body": " Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.     "
},
{
  "id": "exercise-210",
  "level": "2",
  "url": "sec-dot-product.html#exercise-210",
  "type": "Exercise",
  "number": "6.1.5.7",
  "title": "",
  "body": " Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so      "
},
{
  "id": "exercise-211",
  "level": "2",
  "url": "sec-dot-product.html#exercise-211",
  "type": "Exercise",
  "number": "6.1.5.8",
  "title": "",
  "body": " Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.     "
},
{
  "id": "exercise-212",
  "level": "2",
  "url": "sec-dot-product.html#exercise-212",
  "type": "Exercise",
  "number": "6.1.5.9",
  "title": "",
  "body": " Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.     "
},
{
  "id": "exercise-213",
  "level": "2",
  "url": "sec-dot-product.html#exercise-213",
  "type": "Exercise",
  "number": "6.1.5.10",
  "title": "",
  "body": " In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .     "
},
{
  "id": "exercise-214",
  "level": "2",
  "url": "sec-dot-product.html#exercise-214",
  "type": "Exercise",
  "number": "6.1.5.11",
  "title": "",
  "body": " Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .     "
},
{
  "id": "sec-transpose",
  "level": "1",
  "url": "sec-transpose.html",
  "type": "Section",
  "number": "6.2",
  "title": "Orthogonal complements and the matrix transpose",
  "body": " Orthogonal complements and the matrix transpose   We've now seen how the dot product enables us to determine the angle between two vectors and, more specifically, when two vectors are orthogonal. Moving forward, we will explore how the orthogonality condition simplifies many common tasks, such as expressing a vector as a linear combination of a given set of vectors.  This section introduces the notion of an orthogonal complement, the set of vectors each of which is orthogonal to a prescribed subspace. We'll also find a way to describe dot products using matrix products, which allows us to study orthogonality using many of the tools for understanding linear systems that we developed earlier.     Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.         Orthogonal complements  The preview activity presented us with a vector and led us through the process of describing all the vectors orthogonal to . Notice that the set of scalar multiples of describes a line , a 1-dimensional subspace of . We then described a second line consisting of all the vectors orthogonal to . Notice that every vector on this line is orthogonal to every vector on the line . orthogonal complement We call this new line the orthogonal complement of and denote it by . The lines and are illustrated on the left of .       On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .   The next definition places this example into a more general context.   orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .    A typical example appears on the right of . Here we see a plane , a two-dimensional subspace of , and its orthogonal complement , which is a line in .  As the next activity demonstrates, the orthogonal complement of a subspace is itself a subspace of .    Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .         If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .      Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .      The matrix transpose  The previous activity and examples show how we can describe the orthogonal complement of a subspace as the solution set of a particular linear system. We will make this connection more explicit by defining a new matrix operation called the transpose .   transpose   The transpose of the matrix is the matrix whose rows are the columns of .      If , then      This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.       The previous activity demonstrates an important connection between the matrix transpose and dot products. More specifically, the components of the product are simply the dot products of the columns of with . We will make frequent use of this observation so let's record it as a proposition.    If is the matrix whose columns are , then       Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .    To place this example in a slightly more general context, note that and , the columns of , form a basis of . Since , the column space of is the subspace of linear combinations of the columns of , we have .  This example also shows that the orthogonal complement is described by the solution set of . This solution set is what we have called , the null space of . In this way, we see the following proposition, which is visually represented in .    For any matrix , the orthogonal complement of is ; that is,         The orthogonal complement of the column space of is the null space of .     Properties of the matrix transpose  The transpose is a simple algebraic operation performed on a matrix. The next activity explores some of its properties.    In numpy, the transpose of a matrix A is given by np.transpose(A) . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .       In spite of the fact that we are looking at some specific examples, this activity demonstrates the following general properties of the transpose, which may be verified with a little effort.   Properties of the transpose  Here are some properties of the matrix transpose, expressed in terms of general matrices , , and . We assume that is a square matrix.  If is defined, then .   .   .   .  If is defined, then . Notice that the order of the multiplication is reversed.  If is invertible, then so is , and .    There is one final property we wish to record though we will wait until to explain why it is true.   For any matrix , we have    This proposition is important because it implies a relationship between the dimensions of a subspace and its orthogonal complement. For instance, if is an matrix, we saw in that and .  Now suppose that is an -dimensional subspace of with basis . If we form the matrix , then so that   The transpose is an matrix having . Since , we have This explains the following proposition.    If is a subspace of , then       In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.      In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .         Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the command scipy.linalg.null_space() computes a basis for . scipy.linalg.null_space()  null space scipy.linalg.null_space()  null space      Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.            Summary  This section introduced the matrix transpose, its connection to dot products, and its use in describing the orthogonal complement of a subspace.   The columns of the matrix are the rows of the matrix transpose .    The components of the product are the dot products of with the columns of .    The orthogonal complement of the column space of equals the null space of ; that is, .    If is a subspace of , then         Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .       Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .       Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is             Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .       Apply properties of matrix operations to simplify the following expressions.                                                                            A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.          If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.            This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.       In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .       Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .       "
},
{
  "id": "exploration-21",
  "level": "2",
  "url": "sec-transpose.html#exploration-21",
  "type": "Preview Activity",
  "number": "6.2.1",
  "title": "",
  "body": "   Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.      "
},
{
  "id": "p-5941",
  "level": "2",
  "url": "sec-transpose.html#p-5941",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal complement "
},
{
  "id": "fig-orthog-comps",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comps",
  "type": "Figure",
  "number": "6.2.2",
  "title": "",
  "body": "     On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .  "
},
{
  "id": "definition-30",
  "level": "2",
  "url": "sec-transpose.html#definition-30",
  "type": "Definition",
  "number": "6.2.3",
  "title": "",
  "body": " orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .   "
},
{
  "id": "activity-72",
  "level": "2",
  "url": "sec-transpose.html#activity-72",
  "type": "Activity",
  "number": "6.2.2",
  "title": "",
  "body": "  Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .      "
},
{
  "id": "example-orthog-comp-line",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-line",
  "type": "Example",
  "number": "6.2.4",
  "title": "",
  "body": "  If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .   "
},
{
  "id": "example-orthog-comp-gen",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-gen",
  "type": "Example",
  "number": "6.2.5",
  "title": "",
  "body": "  Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .   "
},
{
  "id": "p-5972",
  "level": "2",
  "url": "sec-transpose.html#p-5972",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "transpose "
},
{
  "id": "definition-31",
  "level": "2",
  "url": "sec-transpose.html#definition-31",
  "type": "Definition",
  "number": "6.2.6",
  "title": "",
  "body": " transpose   The transpose of the matrix is the matrix whose rows are the columns of .   "
},
{
  "id": "example-65",
  "level": "2",
  "url": "sec-transpose.html#example-65",
  "type": "Example",
  "number": "6.2.7",
  "title": "",
  "body": "  If , then   "
},
{
  "id": "activity-73",
  "level": "2",
  "url": "sec-transpose.html#activity-73",
  "type": "Activity",
  "number": "6.2.3",
  "title": "",
  "body": "  This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.      "
},
{
  "id": "prop-transpose-multiplication",
  "level": "2",
  "url": "sec-transpose.html#prop-transpose-multiplication",
  "type": "Proposition",
  "number": "6.2.8",
  "title": "",
  "body": "  If is the matrix whose columns are , then    "
},
{
  "id": "example-66",
  "level": "2",
  "url": "sec-transpose.html#example-66",
  "type": "Example",
  "number": "6.2.9",
  "title": "",
  "body": "  Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .   "
},
{
  "id": "prop-col-orthog",
  "level": "2",
  "url": "sec-transpose.html#prop-col-orthog",
  "type": "Proposition",
  "number": "6.2.10",
  "title": "",
  "body": "  For any matrix , the orthogonal complement of is ; that is,    "
},
{
  "id": "fig-orthog-comp",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comp",
  "type": "Figure",
  "number": "6.2.11",
  "title": "",
  "body": "    The orthogonal complement of the column space of is the null space of .  "
},
{
  "id": "activity-74",
  "level": "2",
  "url": "sec-transpose.html#activity-74",
  "type": "Activity",
  "number": "6.2.4",
  "title": "",
  "body": "  In numpy, the transpose of a matrix A is given by np.transpose(A) . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .      "
},
{
  "id": "prop-col-row-rank",
  "level": "2",
  "url": "sec-transpose.html#prop-col-row-rank",
  "type": "Proposition",
  "number": "6.2.12",
  "title": "",
  "body": " For any matrix , we have   "
},
{
  "id": "prop-orthog-dim",
  "level": "2",
  "url": "sec-transpose.html#prop-orthog-dim",
  "type": "Proposition",
  "number": "6.2.13",
  "title": "",
  "body": "  If is a subspace of , then    "
},
{
  "id": "example-67",
  "level": "2",
  "url": "sec-transpose.html#example-67",
  "type": "Example",
  "number": "6.2.14",
  "title": "",
  "body": "  In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.   "
},
{
  "id": "example-68",
  "level": "2",
  "url": "sec-transpose.html#example-68",
  "type": "Example",
  "number": "6.2.15",
  "title": "",
  "body": "  In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .   "
},
{
  "id": "activity-75",
  "level": "2",
  "url": "sec-transpose.html#activity-75",
  "type": "Activity",
  "number": "6.2.5",
  "title": "",
  "body": "     Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the command scipy.linalg.null_space() computes a basis for . scipy.linalg.null_space()  null space scipy.linalg.null_space()  null space      Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.         "
},
{
  "id": "exercise-215",
  "level": "2",
  "url": "sec-transpose.html#exercise-215",
  "type": "Exercise",
  "number": "6.2.5.1",
  "title": "",
  "body": " Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .     "
},
{
  "id": "exercise-216",
  "level": "2",
  "url": "sec-transpose.html#exercise-216",
  "type": "Exercise",
  "number": "6.2.5.2",
  "title": "",
  "body": " Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .     "
},
{
  "id": "exercise-217",
  "level": "2",
  "url": "sec-transpose.html#exercise-217",
  "type": "Exercise",
  "number": "6.2.5.3",
  "title": "",
  "body": " Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is           "
},
{
  "id": "exercise-218",
  "level": "2",
  "url": "sec-transpose.html#exercise-218",
  "type": "Exercise",
  "number": "6.2.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .     "
},
{
  "id": "exercise-219",
  "level": "2",
  "url": "sec-transpose.html#exercise-219",
  "type": "Exercise",
  "number": "6.2.5.5",
  "title": "",
  "body": " Apply properties of matrix operations to simplify the following expressions.                                                                          "
},
{
  "id": "exercise-220",
  "level": "2",
  "url": "sec-transpose.html#exercise-220",
  "type": "Exercise",
  "number": "6.2.5.6",
  "title": "",
  "body": " A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.        "
},
{
  "id": "exercise-221",
  "level": "2",
  "url": "sec-transpose.html#exercise-221",
  "type": "Exercise",
  "number": "6.2.5.7",
  "title": "",
  "body": " If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.          "
},
{
  "id": "exercise-222",
  "level": "2",
  "url": "sec-transpose.html#exercise-222",
  "type": "Exercise",
  "number": "6.2.5.8",
  "title": "",
  "body": " This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.     "
},
{
  "id": "exercise-223",
  "level": "2",
  "url": "sec-transpose.html#exercise-223",
  "type": "Exercise",
  "number": "6.2.5.9",
  "title": "",
  "body": " In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .     "
},
{
  "id": "exercise-224",
  "level": "2",
  "url": "sec-transpose.html#exercise-224",
  "type": "Exercise",
  "number": "6.2.5.10",
  "title": "",
  "body": " Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .     "
},
{
  "id": "sec-orthogonal-bases",
  "level": "1",
  "url": "sec-orthogonal-bases.html",
  "type": "Section",
  "number": "6.3",
  "title": "Orthogonal bases and projections",
  "body": " Orthogonal bases and projections   We know that a linear system is inconsistent when is not in , the column space of . Later in this chapter, we'll develop a strategy for dealing with inconsistent systems by finding , the vector in that minimizes the distance to (i.e., minimizes ). The equation is therefore consistent and its solution set can provide us with useful information about the original system .  In this section and the next, we'll develop some techniques that enable us to find , the vector in a given subspace that is closest to a given vector .    For this activity, it will be helpful to recall the distributive property of dot products: . We'll work with the basis of formed by the vectors .   Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that . To find the weight , dot both sides of this expression with : , and apply the distributive property.    In a similar fashion, find the weight .    Now look at the expression for and express it in terms of a projection.    Verify that using the weights you have found.          We can compute that .     and .     .     .     .     .       We frequently ask to write a given vector as a linear combination of given basis vectors. In the past, we have done this by solving a linear system. The preview activity illustrates how this task can be simplified when the basis vectors are orthogonal to each other. We'll explore this and other uses of orthogonal bases in this section.    Orthogonal sets  The preview activity dealt with a basis of formed by two orthogonal vectors. More generally, we will consider a set of orthogonal vectors, as described in the next definition.   orthogonal set of vectors    By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.      The 3-dimensional vectors form an orthogonal set, which can be verified by computing . Notice that this set of vectors forms a basis for .      The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .    Suppose that a vector is a linear combination of an orthogonal set of vectors ; that is, suppose that Just as in the preview activity, we can find the weight by dotting both sides with and applying the distributive property of dot products: Notice how the presence of an orthogonal set causes most of the terms in the sum to vanish. In the same way, we find that so that   We'll record this fact in the following proposition.    If a vector is a linear combination of an orthogonal set of vectors , then .     Using this proposition, we can see that an orthogonal set of vectors must be linearly independent. Suppose, for instance, that is a set of nonzero orthogonal vectors and that one of the vectors is a linear combination of the others, say, . We therefore know that , which cannot happen since we know that is nonzero. This tells us that    An orthogonal set of vectors is linearly independent.    If vectors in an orthogonal set in , they form a linearly independent set in and are therefore a basis for the subspace . This means that . If , then the vectors form an orthogonal basis for .    Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that             This activity introduces an important way of modifying an orthogonal set so that the vectors in the set have unit length. Since for any vector and scalar , . That is, if we divide each vector in an orthogonal set by its length, the result is an orthogonal set of vectors each of which is a unit vector.  Orthogonal sets of unit vectors are called orthonormal and are especially convenient.   othonormal set   An orthonormal set is an orthogonal set of unit vectors.      The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix , we find that since tells us that     The previous activity and example illustrate the next proposition.    If the columns of the matrix form an orthonormal set, then , the identity matrix.      Orthogonal projections  We now turn to an important problem that will appear in many forms in the rest of our explorations. Suppose, as shown in , that we have a subspace of and a vector that is not in that subspace. We would like to find the vector in that is closest to , meaning the distance between and is as small as possible.      Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .   To get started, let's consider a simpler problem where we have a line in , defined by the vector , and another vector that is not on the line, as shown on the left of . We wish to find , the vector on the line that is closest to , as illustrated in the right of .    Given a line and a vector , we seek the vector on that is closest to .   We have already seen how to find . Our goal now is to expand on this idea to determine the orthogonal projection of a vector onto any subspace of .     This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      We already know the formula for . But let's rederive it using properties of the dot product. First notice that for some scalar . Since is orthogonal to , what do we know about the dot product ?    Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is .       The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then . Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector .    Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is .    If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that .             0     .               0                    We require that be orthogonal to every vector .         Use the fact that        In all the cases considered in the activity, we are looking for , the vector in a subspace closest to a vector , which is found by requiring that be orthogonal to . This means that for any vector in .  If we have an orthogonal basis for , then . Therefore, This leads to the projection formula:   Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is .      Caution  Remember that the projection formula given in applies only when the basis of is orthogonal .   If we have an orthonormal basis for , the projection formula simplifies to . If we then form the matrix , this expression may be succintly written   This leads to the following proposition.    If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where .      In the previous activity, we looked at the plane defined by the two orthogonal vectors . We can form an orthonormal basis by scalar multiplying these vectors to have unit length: . Using these vectors, we form the matrix . The projection onto the plane is then given by the matrix .  Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that .         Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     Find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                     This activity demonstrates one issue of note. We found , the orthogonal projection of onto , by requiring that be orthogonal to . In other words, is a vector in the orthogonal complement , which we may denote . This explains the following proposition, which is illustrated in    If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .       A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .   Let's summarize what we've found. If is a matrix whose columns form an orthonormal set in , then    , the identity matrix, because this product computes the dot products between the columns of .     is the matrix the projects vectors orthogonally onto , the subspace of spanned by .   As we've said before, matrix multiplication depends on the order in which we multiply the matrices, and we see this clearly here.  Because , there is a temptation to say that is invertible. This is usually not the case, however. Remember that an invertible matrix must be a square matrix, and the matrix will only be square if . In this case, there are vectors in the orthonormal set so the subspace spanned by the vectors is . If is a vector in , then is the orthogonal projection of onto . In other words, is the closest vector in to , and this closest vector must be itself. Therefore, , which means that . In this case, is an invertible matrix.    Consider the orthonormal set of vectors and the matrix they define . In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.      Now consider the orthonormal set of vectors and the matrix they define . Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.      orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .    This terminology can be a little confusing. We call a basis orthogonal if the basis vectors are orthogonal to one another. However, a matrix is orthogonal if the columns are orthogonal to one another and have unit length. It pays to keep this in mind when reading statements about orthogonal bases and orthogonal matrices. In the meantime, we record the following proposition.    An orthogonal matrix is invertible and its inverse .      Summary  This section introduced orthogonal sets and the projection formula that allows us to project vectors orthogonally onto a subspace.   Given an orthogonal set that spans an -dimensional subspace of , the orthogonal projection of onto is the vector in closest to and may be written as .    If is an orthonormal basis of and is the matrix whose columns are , then the matrix projects vectors orthogonally onto .    If the columns of form an orthonormal basis for an -dimensional subspace of , then .    An orthogonal matrix  is a square matrix whose columns form an orthonormal basis. In this case, so that .        Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have        Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .       Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .       Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .       Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exericse.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .       Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .       Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .      Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so        Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .       This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.       "
},
{
  "id": "preview-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#preview-orthogonal-basis",
  "type": "Preview Activity",
  "number": "6.3.1",
  "title": "",
  "body": "  For this activity, it will be helpful to recall the distributive property of dot products: . We'll work with the basis of formed by the vectors .   Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that . To find the weight , dot both sides of this expression with : , and apply the distributive property.    In a similar fashion, find the weight .    Now look at the expression for and express it in terms of a projection.    Verify that using the weights you have found.          We can compute that .     and .     .     .     .     .      "
},
{
  "id": "definition-32",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-32",
  "type": "Definition",
  "number": "6.3.1",
  "title": "",
  "body": " orthogonal set of vectors    By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.   "
},
{
  "id": "example-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-basis",
  "type": "Example",
  "number": "6.3.2",
  "title": "",
  "body": "  The 3-dimensional vectors form an orthogonal set, which can be verified by computing . Notice that this set of vectors forms a basis for .   "
},
{
  "id": "example-orthogonal-set",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-set",
  "type": "Example",
  "number": "6.3.3",
  "title": "",
  "body": "  The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .   "
},
{
  "id": "prop-orthog-lincomb",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lincomb",
  "type": "Proposition",
  "number": "6.3.4",
  "title": "",
  "body": "  If a vector is a linear combination of an orthogonal set of vectors , then .    "
},
{
  "id": "prop-orthog-lin-indep",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lin-indep",
  "type": "Proposition",
  "number": "6.3.5",
  "title": "",
  "body": "  An orthogonal set of vectors is linearly independent.   "
},
{
  "id": "p-6254",
  "level": "2",
  "url": "sec-orthogonal-bases.html#p-6254",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal basis "
},
{
  "id": "activity-76",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-76",
  "type": "Activity",
  "number": "6.3.2",
  "title": "",
  "body": "  Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that            "
},
{
  "id": "p-6278",
  "level": "2",
  "url": "sec-orthogonal-bases.html#p-6278",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthonormal "
},
{
  "id": "definition-33",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-33",
  "type": "Definition",
  "number": "6.3.6",
  "title": "",
  "body": " othonormal set   An orthonormal set is an orthogonal set of unit vectors.   "
},
{
  "id": "example-71",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-71",
  "type": "Example",
  "number": "6.3.7",
  "title": "",
  "body": "  The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix , we find that since tells us that    "
},
{
  "id": "prop-orthonormal-QTQ",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthonormal-QTQ",
  "type": "Proposition",
  "number": "6.3.8",
  "title": "",
  "body": "  If the columns of the matrix form an orthonormal set, then , the identity matrix.   "
},
{
  "id": "fig-3d-orthog-proj",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-3d-orthog-proj",
  "type": "Figure",
  "number": "6.3.9",
  "title": "",
  "body": "    Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .  "
},
{
  "id": "fig-projection-line-a",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-projection-line-a",
  "type": "Figure",
  "number": "6.3.10",
  "title": "",
  "body": "  Given a line and a vector , we seek the vector on that is closest to .  "
},
{
  "id": "activity-77",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-77",
  "type": "Activity",
  "number": "6.3.3",
  "title": "",
  "body": "  This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      We already know the formula for . But let's rederive it using properties of the dot product. First notice that for some scalar . Since is orthogonal to , what do we know about the dot product ?    Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is .       The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then . Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector .    Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is .    If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that .             0     .               0                    We require that be orthogonal to every vector .         Use the fact that       "
},
{
  "id": "prop-proj-formula",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-formula",
  "type": "Proposition",
  "number": "6.3.13",
  "title": "Projection formula.",
  "body": " Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is .    "
},
{
  "id": "p-6330",
  "level": "2",
  "url": "sec-orthogonal-bases.html#p-6330",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal "
},
{
  "id": "prop-proj-orthonormal",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-orthonormal",
  "type": "Proposition",
  "number": "6.3.14",
  "title": "",
  "body": "  If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where .   "
},
{
  "id": "example-projection-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-projection-matrix",
  "type": "Example",
  "number": "6.3.15",
  "title": "",
  "body": "  In the previous activity, we looked at the plane defined by the two orthogonal vectors . We can form an orthonormal basis by scalar multiplying these vectors to have unit length: . Using these vectors, we form the matrix . The projection onto the plane is then given by the matrix .  Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that .   "
},
{
  "id": "activity-78",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-78",
  "type": "Activity",
  "number": "6.3.4",
  "title": "",
  "body": "     Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     Find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                    "
},
{
  "id": "prop-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-decomp",
  "type": "Proposition",
  "number": "6.3.16",
  "title": "",
  "body": " If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .  "
},
{
  "id": "fig-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-orthog-decomp",
  "type": "Figure",
  "number": "6.3.17",
  "title": "",
  "body": "    A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .  "
},
{
  "id": "example-73",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-73",
  "type": "Example",
  "number": "6.3.18",
  "title": "",
  "body": "  Consider the orthonormal set of vectors and the matrix they define . In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.   "
},
{
  "id": "example-74",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-74",
  "type": "Example",
  "number": "6.3.19",
  "title": "",
  "body": "  Now consider the orthonormal set of vectors and the matrix they define . Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.   "
},
{
  "id": "definition-34",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-34",
  "type": "Definition",
  "number": "6.3.20",
  "title": "",
  "body": "  orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .   "
},
{
  "id": "prop-orthog-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-matrix",
  "type": "Proposition",
  "number": "6.3.21",
  "title": "",
  "body": "  An orthogonal matrix is invertible and its inverse .   "
},
{
  "id": "p-6387",
  "level": "2",
  "url": "sec-orthogonal-bases.html#p-6387",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal matrix "
},
{
  "id": "exercise-225",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-225",
  "type": "Exercise",
  "number": "6.3.4.1",
  "title": "",
  "body": " Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have      "
},
{
  "id": "exercise-226",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-226",
  "type": "Exercise",
  "number": "6.3.4.2",
  "title": "",
  "body": " Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .     "
},
{
  "id": "exercise-227",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-227",
  "type": "Exercise",
  "number": "6.3.4.3",
  "title": "",
  "body": " Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .     "
},
{
  "id": "exercise-228",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-228",
  "type": "Exercise",
  "number": "6.3.4.4",
  "title": "",
  "body": " Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .     "
},
{
  "id": "exercise-229",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-229",
  "type": "Exercise",
  "number": "6.3.4.5",
  "title": "",
  "body": " Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exericse.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .     "
},
{
  "id": "exercise-230",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-230",
  "type": "Exercise",
  "number": "6.3.4.6",
  "title": "",
  "body": " Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .     "
},
{
  "id": "exercise-231",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-231",
  "type": "Exercise",
  "number": "6.3.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .    "
},
{
  "id": "exercise-232",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-232",
  "type": "Exercise",
  "number": "6.3.4.8",
  "title": "",
  "body": " Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so      "
},
{
  "id": "exercise-233",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-233",
  "type": "Exercise",
  "number": "6.3.4.9",
  "title": "",
  "body": " Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .     "
},
{
  "id": "exercise-234",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-234",
  "type": "Exercise",
  "number": "6.3.4.10",
  "title": "",
  "body": " This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.     "
},
{
  "id": "sec-gram-schmidt",
  "level": "1",
  "url": "sec-gram-schmidt.html",
  "type": "Section",
  "number": "6.4",
  "title": "Finding orthogonal bases",
  "body": " Finding orthogonal bases   The last section demonstrated the value of working with orthogonal, and especially orthonormal, sets. If we have an orthogonal basis for a subspace , the Projection Formula tells us that the orthogonal projection of a vector onto is . An orthonormal basis is even more convenient: after forming the matrix , we have .  In the examples we've seen so far, however, orthogonal bases were given to us. What we need now is a way to form orthogonal bases. In this section, we'll explore an algorithm that begins with any basis for a subspace and creates an orthogonal basis. Once we have an orthogonal basis, we can scale each of the vectors appropriately to produce an orthonormal basis.    Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .         Gram-Schmidt orthogonalization   Gram-Schmidt The preview activity illustrates the main idea behind an algorithm, known as Gram-Schmidt orthogonalization , that begins with a basis for some subspace of and produces an orthogonal or orthonormal basis. The algorithm relies on our construction of the orthogonal projection. Remember that we formed the orthogonal projection of onto a subspace by requiring that is orthogonal to as shown in .      If is the orthogonal projection of onto , then is orthogonal to .   This observation guides our construction of an orthogonal basis for it allows us to create a vector that is orthogonal to a given subspace. Let's see how the Gram-Schmidt algorithm works.    Suppose that is a three-dimensional subspace of with basis: . We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by : .      Find the vector .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as . In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .            As this activity illustrates, Gram-Schmidt orthogonalization begins with a basis for a subspace of and creates an orthogonal basis for . Let's work through a second example.    Let's start with the basis , which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by : This gives .  Notice that we found . Therefore, we can rewrite any linear combination of and as , a linear combination of and . This tells us that . In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto : .  We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis: .    More generally, if we have a basis for a subspace of , the Gram-Schmidt algorithm creates an orthogonal basis for in the following way:   From here, we may form an orthonormal basis by constructing a unit vector parallel to each vector in the orthogonal basis: .    Python can automate these computations for us, and we'll learn a standard numpy way of approaching this shortly. But first let's see how we can code up the Gram-Schmidt process directly. The code below allows vectors to be given as 1-d arrays or as 1-column matrices. flatten() is used to convert either to the 1-d representation. The basis is provided as a python list of vectors (in either representation).    Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                         factorizations  factorization  Now that we've seen how the Gram-Schmidt algorithm forms an orthonormal basis for a given subspace, we will explore how the algorithm leads to an important matrix factorization known as the factorization . The factorization can be computed in most any computer package that does linear algebra.    Suppose that is the matrix whose columns are . These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using That is, Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have . This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .       When the columns of a matrix are linearly independent, they form a basis for so that we can perform the Gram-Schmidt algorithm. The previous activity shows how this leads to a factorization of as the product of a matrix whose columns are an orthonormal basis for and an upper triangular matrix .    factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.     We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization .    The value of the factorization will become clear in the next section where we use it to solve least squares problems.    As before, we would like to use Python to automate the process of finding and using the factorization of a matrix . Q, R = numpy.linalg.qr(A) will do the trick. numpy.linalg.qr()  factorization  factorization numpy.linalg.qr()    Suppose that is the following matrix whose columns are linearly independent.    If , what are the shapes of and of ? What is special about the form of ?    Find the factorization of and verify that (a) they havce the anticipated shapes and (b) .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .     Here is some Python code that performs the necessary calculations.         is and is a upper triangular matrix.    We see that                .          Summary  This section explored the Gram-Schmidt orthogonalization algorithm and how it leads to the matrix factorization when the columns of are linearly independent.   Beginning with a basis for a subspace of , the vectors form an orthogonal basis for .     We may scale each vector appropriately to obtain an orthonormal basis .    Expressing the Gram-Schmidt algorithm in matrix form shows that, if the columns of are linearly independent, then we can write , where the columns of form an orthonormal basis for and is upper triangular. is upper triangular because each only depends on for .     numpy.linalg.qr() will compute factorizations for us.        Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                  Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .    Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.               Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                  Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .       Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .       Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.       Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.       "
},
{
  "id": "exploration-23",
  "level": "2",
  "url": "sec-gram-schmidt.html#exploration-23",
  "type": "Preview Activity",
  "number": "6.4.1",
  "title": "",
  "body": "  Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .      "
},
{
  "id": "fig-proj-orthog",
  "level": "2",
  "url": "sec-gram-schmidt.html#fig-proj-orthog",
  "type": "Figure",
  "number": "6.4.3",
  "title": "",
  "body": "    If is the orthogonal projection of onto , then is orthogonal to .  "
},
{
  "id": "activity-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-gram-schmidt",
  "type": "Activity",
  "number": "6.4.2",
  "title": "",
  "body": "  Suppose that is a three-dimensional subspace of with basis: . We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by : .      Find the vector .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as . In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .           "
},
{
  "id": "example-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#example-gram-schmidt",
  "type": "Example",
  "number": "6.4.4",
  "title": "",
  "body": "  Let's start with the basis , which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by : This gives .  Notice that we found . Therefore, we can rewrite any linear combination of and as , a linear combination of and . This tells us that . In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto : .  We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis: .   "
},
{
  "id": "activity-80",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-80",
  "type": "Activity",
  "number": "6.4.3",
  "title": "",
  "body": "  Python can automate these computations for us, and we'll learn a standard numpy way of approaching this shortly. But first let's see how we can code up the Gram-Schmidt process directly. The code below allows vectors to be given as 1-d arrays or as 1-column matrices. flatten() is used to convert either to the 1-d representation. The basis is provided as a python list of vectors (in either representation).    Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                     "
},
{
  "id": "p-6607",
  "level": "2",
  "url": "sec-gram-schmidt.html#p-6607",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "factorization "
},
{
  "id": "activity-81",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-81",
  "type": "Activity",
  "number": "6.4.4",
  "title": "",
  "body": "  Suppose that is the matrix whose columns are . These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using That is, Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have . This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .      "
},
{
  "id": "prop-qr",
  "level": "2",
  "url": "sec-gram-schmidt.html#prop-qr",
  "type": "Proposition",
  "number": "6.4.5",
  "title": "<span class=\"process-math\">\\(QR\\)<\/span> factorization.",
  "body": "  factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.  "
},
{
  "id": "example-76",
  "level": "2",
  "url": "sec-gram-schmidt.html#example-76",
  "type": "Example",
  "number": "6.4.6",
  "title": "",
  "body": "  We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization .   "
},
{
  "id": "activity-82",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-82",
  "type": "Activity",
  "number": "6.4.5",
  "title": "",
  "body": "  As before, we would like to use Python to automate the process of finding and using the factorization of a matrix . Q, R = numpy.linalg.qr(A) will do the trick. numpy.linalg.qr()  factorization  factorization numpy.linalg.qr()    Suppose that is the following matrix whose columns are linearly independent.    If , what are the shapes of and of ? What is special about the form of ?    Find the factorization of and verify that (a) they havce the anticipated shapes and (b) .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .     Here is some Python code that performs the necessary calculations.         is and is a upper triangular matrix.    We see that                .      "
},
{
  "id": "exercise-235",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-235",
  "type": "Exercise",
  "number": "6.4.4.1",
  "title": "",
  "body": " Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                "
},
{
  "id": "exercise-236",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-236",
  "type": "Exercise",
  "number": "6.4.4.2",
  "title": "",
  "body": " Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .  "
},
{
  "id": "exercise-237",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-237",
  "type": "Exercise",
  "number": "6.4.4.3",
  "title": "",
  "body": " Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.             "
},
{
  "id": "exercise-238",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-238",
  "type": "Exercise",
  "number": "6.4.4.4",
  "title": "",
  "body": " Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                "
},
{
  "id": "exercise-239",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-239",
  "type": "Exercise",
  "number": "6.4.4.5",
  "title": "",
  "body": " Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .     "
},
{
  "id": "exercise-240",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-240",
  "type": "Exercise",
  "number": "6.4.4.6",
  "title": "",
  "body": " Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .     "
},
{
  "id": "exercise-241",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-241",
  "type": "Exercise",
  "number": "6.4.4.7",
  "title": "",
  "body": " Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.     "
},
{
  "id": "exercise-242",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-242",
  "type": "Exercise",
  "number": "6.4.4.8",
  "title": "",
  "body": " Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.     "
},
{
  "id": "sec-least-squares",
  "level": "1",
  "url": "sec-least-squares.html",
  "type": "Section",
  "number": "6.5",
  "title": "Least squares methods",
  "body": " Least squares methods   Suppose we collect some data when performing an experiment and plot it as shown on the left of . Notice that there is no line on which all the points lie; in fact, it would be surprising if there were since we can expect some uncertainty in the measurements recorded. There does, however, appear to be a line, as shown on the right, on which the points almost lie.       A collection of points and a line approximating the linear relationship implied by them.   In this section, we'll explore how the techniques developed in this chapter enable us to find the line that best approximates the data.  More generally, that whenever is inconsistent, we can instead seek an approximate solution -- a solution to where is a close as possible to . Orthogonal projection gives us just the right tool for doing this.       Is there a solution to the equation where and are such that .     We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .         A first example  When we've encountered inconsistent systems in the past, we've simply said there is no solution and moved on. The preview activity, however, shows how we can find approximate solutions to an inconsistent system: if there are no solutions to , we instead solve the consistent system , the orthogonal projection of onto . As we'll see, this solution is, in a specific sense, the best possible.    Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. Statisticans prefer the notation , and we're going to adopt statistical preferences for most of the remainder of this chapter since least squares is such an important method in statistics.  To begin, we will try to find and so that the three points lie on the line with equation . The first data point gives an equation for and . In particular, we know that when , then so we have or .  Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector specifies the intercept and slope of the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection .    Since is in , the equation is consistent. Find its solution, which we will denote , and sketch the line in . This line is called the least squares regression line . least squares regression  regression least squares regression That \"hat\" on indicates that these coefficients (most likely) do not fit the data exactly, but come as close as we can to doing so (in the sense of minimizing the distance between and ).          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.        This activity illustrates the idea behind a technique known as least squares regression  least squares regression , which we have been working toward throughout this chapter. If the data points are denoted as , we construct the matrix and vector as . With the vector representing the line with equation , we see that the equation describes a line passing through all the data points. In our activity, it is visually apparent that there is no such line, which agrees with the fact that the equation is inconsistent.  Remember that is the closest vector in to . Therefore, when we solve the equation , we are finding the vector so that is as close to as possible. Let's think about what this means within the context of this problem.  The difference so that the square of the distance between and is  least squares regression Our approach finds the values for and that make this sum of squares as small as possible, which is why we call this a least squares problem.  Usually the least squares regression line does not pass through all of the data points. Statisticians call the residual for observation . As shown in , residuals measure the vertical distance between the observed response value and the predicted response value : . Seen in this way, the square of the distance is a measure of how much the line defined by the vector misses the data points. The solution to the least squares problem is the line that misses the data points by the smallest amount possible (when measured in this way).      The solution of the least squares problem and the vertical distances between the line and the data points.     linear model  The linear model framework  The previous example is an example of simple linear regression . In simple linear regression there is a single quantitative predictor and the model proposes a linear relationship between the explanatory variable and the response. linear model Least squares regression can be used with multiple explanatory variables just as easily as with one -- at least if we are willing to let the computer take care of the tedious arithmetic involved. In this section we describe a general framework called linear models . Linear models and their generalizations are arguably the most important and commonly used method of data analysis.  Suppose we are looking for a relationship of the form . We mean for this to hold for every subject in the data, each of whom has (likely different) values of and the 's. But we want the values of the to be the same for everyone. Let's rewrite our equation to emphasize that we are really dealing with vectors here. . Notice that we snuck in a vector of 1's to make the \"constant\" term look like all the others.  Now let's go one more step and express this model using matrices. data matrix The vector of ones and the vectors form the columns of the matrix . This matrix is usually refered to as the model matrix . coefficients of a regression equation  model matrix The coefficients  are arranged into a column vector. Then the entire relationship is expressed as a simple equation of the form , but with different letters. And typically statisticians prefer to swap the left and right sides of the equation as well. So the model is exprssed like this: This equation will almost never have an exact solution because in typical applications will have many more rows than columns.  Since there is typically no exact solution, we seek an approximate solution, a solution to . model space In this context, is called the model space . It will be important below to note that this means that .  Whether we express our equation as , as we have mostly done to this point, or as , as we will typically do in statistical applications, or using some other letters, the linear algebra is the same (and familiar): orthogonal projection, solving sytems of linear equations, etc.    Add example of setting up a multiple regression problem. Include at least a binary categorical predictor.    We conclude this section with a note about the intercept term.   Models without an intercept  It is possible to fit models without an intecept term. In this case the column of 1's will be omitted from the model matrix . Algebraically, a few things, like the defnition and interpretation of below do not work out as well in that case. And statistically, omitting the intercept makes a strong assumption about the nature of the relationship. In most statistical software, the default it to always include an intercept, but there are options to fit models without the intercept term if so desired.     Solving least squares problems  Now that we've discussed least squares approximate solutions to and seen an important application of this method in linear models, usually expressed as , it is time to turn our attention to some of the details involved in solving least squares problems. We'll continue with the statistical notation for this.  We already know one way to solve a least squares problem , namely   Project into the model space  Compute .    Solve the new equation for  Because , we know this equation is consistent. If the columns of are linearly independent (as will usually be the case in linear model applications), then there is exactly one solution. We denote the solution to this as . The entries in are called the (estiamted) coeffients of the model.  Because we can measure how close is to using an expression that involves a sum of squares, and makes this expression as small as possible, is called a least squares approximate solution to the original equation .     That is the method we have outlined above. But there are other methods that are usually used in practice, because they are more efficient and more stable numerically.  We begin by describing a method for finding that does not involve first finding the orthogonal projection . Remember that , so is orthogonal to . In other words, is in the orthogonal complement , which tells us is the same as . Since is in , it follows that . This is just another way of writing down that is orthogonal to each column of .  Because the least squares approximate solution is the vector such that , we can rearrange this equation to see that   normal equation  This equation is called the normal equation , and we have the following proposition.    If the columns of are linearly independent, then there is a unique least squares approximate solution to the equation given by the normal equation .    The next example demonstrates how we can use the normal equation to find the least squares approximate solution.   Consider the equation . Since this equation is inconsistent, we will find the least squares approximate solution by solving the normal equation , which has the form . Solving this yields .   You may wonder why the approach in is better than the original appraoch. Here's one reason why. Suppose we have a larger example and is with much larger than , as is often the case. Then is , which is small. But is , which is large -- much larger than . So computing is expensive. Working with is comparatively much less computationally intensive.    The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data:   We would like to represent this relationship by a linear function .   Use the first data point to write an equation involving and .    Suppose that we represent the coefficients using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the desired relationship.     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least squares approximate solution to the equation . Call your solution .   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  Plot the data and your least squares regression line.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.        Once we have the linear function that best fits the data, we can make predictions about situations that we haven't encountered in the data. If we're going to use our function to make predictions, it's natural to ask how much confidence we have in these predictions. This is a statistical question that leads to a rich and well-developed theory, which we won't explore in much detail here. However, there is one simple measure of how well our linear function fits the data that is known as the coefficient of determination and denoted by .  We have seen that the predicted values from our model are given by and that the square of the distance measures the amount by which the line fails to pass through the data points. When the line is close to the data points, we expect this number to be small. However, the size of this measure depends on the scale of the data. For instance, the two lines shown in seem to fit the data equally well, but is 100 times larger on the right.       The lines appear to fit equally well in spite of the fact that differs by a factor of 100.   We can create a measure of fit that is indpendent of scale if we consider the relationship among three important vectors:    holds the differences between the observed response values and their mean value. The (square of the) length of this vector is a measure of the total variability in the response variable.     holds the differences between the observed response values and model fitted values. These differences are called residuals This vector is orthogonal to the model space , .     holds the differences between the fitted values and the mean response. Importantly, this vector is in the model space . We can see this as follows. The vector is in the model space by definition. If our model includes an intercept (so the first column of is ), then , so is also in the model space. This implies that is in the model space.  Because is in the model space and is orthogonal to the model space, we know that these vectors are orthogonal.   Putting this together we see that The second equation is the just the Pythagorean Theorem applied to the triangle formed by our three vectors.  This provides a natural way to measure how well our model fits the data:    Coefficient of determination  coefficient of determination     coefficient of determination     the coefficient of determination    For any model that determines predictions for a response variable , we can define the coefficient of determination as  For a linear model, , but the definition can be applied to other models as well.    It is clear from the definition that for linear models with an intercept, . So one way to interpret is as the proportion of the variation in that is explained by the model (i.e., by ). If , the fit is perfect and . At the other extreme, if our model predicts (everyone is average), then .  A more complete explanation of this definition relies on the concept of variance, which we explore in and the next chapter. For the time being, it's enough to know that and that the closer is to 1, the better the line fits the data. In our original example, illustrated in , we find that , and in our study of cricket chirp rates, we have . However, assessing the confidence we have in predictions made by solving a least squares problem can require considerable thought, and it would be naive to rely only on the value of .  There is also a connection between the correlation coefficient and the coefficient of of determination. For a simple linear model , . For this model, an orthogonal basis for the model space is and     Using factorizations  As we've seen, the least squares approximate solution to may be found by solving the normal equation , and this can be a practical strategy for some problems. However, this approach can be problematic as small rounding errors can accumulate and lead to inaccurate final results.  As the next activity demonstrates, there is an third method for finding the least squares approximate solution using a factorization of the matrix , and this method is preferable as it is both computatinoally efficient and numerically more reliable. This is the method implemented in most statistical software packages.       Suppose we are interested in finding the least squares approximate solution to the equation and that we have the factorization . Explain why the least squares approximation solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating        To summarize, we have seen that    If the columns of are linearly independent and we have the factorization , then the least squares approximate solution to the equation is given by .      Polynomial Regression  In the examples we've seen so far, we have fit a linear function to a dataset. Sometimes, however, a polynomial, such as a quadratic function, may be more appropriate. It turns out that the techniques we've developed in this section are still useful as the next activity demonstrates.       Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated.   Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. Creat a plot that incluedes the raw data and the quadratic fit.     What is your predicted value when .    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.       The matrices that you created in the last activity when fitting a quadratic and cubic function to a dataset have a special form. In particular, if the data points are labeled and we seek a degree polynomial, then  Vandermonde matrix This is called a Vandermonde matrix of degree . You can use numpy.polynomial.polynomial.polyvander() to create  numpy.polynomial.polynomial.polyvander()    Vandermonde matrix  numpy.polynomial.polynomial.polyvander()   these matrices for a specified vector and degree. Notice that is treated as for the purposes of this matrix.    This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 1980, 2012, and 2017. We will focus primarily on 2012.      Find the vector , the least squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     Plot the data along with the fitted polynomial model.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise , and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much influence over the fit of the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a function that is too flexible and fits the training data better than it can be expected to fit new data high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. What we really need is a method for selecting a good value for and a better way to measure how well we should expect the model to fit new data, not the data used to train the model. That discussion would take us too far afield for the moment, but it is an important discussion.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.         Fitting linear models with standard tools    Summary  This section introduced some types of least squares problems and a framework for working with them.   Given an inconsistent system , we find , the least squares approximate solution by requiring that be as close to as possible. In other words, we solve where     One important application of this is fitting linear models to data. In that context, we typically use different letters. Instead of , you are more likely to see . Here    represents the response variable .  the variable we are trying to predict or estimate from other available data.     represents the data matrix .  Each row of represents an observation unit. Each column represents a data variable. Often we include a column of 1's in . This allows us to model an intercept which represents a baseline amount that is part of every prediction. may include the results of applying a function to some of the \"raw data\", after all, that's just another variable.     represents the coefficients of the model.       One way to find with is by solving the normal equations . This is not our preferred method since numerical problems can arise.  The statistical version of the normal equation is .    A second way to find with uses a factorization of . If , then and finding is computationally feasible since is upper triangular. Alternatively, we can use backsubstitution to solve .  The statistical version of this is and .    This technique may be applied widely and is useful for modeling data. We saw examples in this section where linear functions of several input variables and polynomials provided effective models for different datasets.    A simple measure of the quality of the fit is the coefficient of determination though some care must be used in interpreting this number in context. In particular, as models become more complex, generally increases because more flexible models can fit the data better. But they may be prone to overfitting. Our goal is generally not to fit the data at hand but to learn something of value about other data.       Evaluating the following cell loads in some commands that will be helpful in the following exercises. In particular, there are commands    QR(A) that returns the factorization of A as Q, R = QR(A) ,     onesvec(n) that returns the -dimensional vector whose entries are all 1,     demean(v) that demeans the vector v ,     vandermonde(x, k) that returns the Vandermonde matrix of degree formed from the components of the vector x , and     plot_model(xhat, data) that plots the data and the model xhat .       Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .       Consider the data in .  A data set with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least squares approximate solution to .    Find the least squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .            Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .            Consider the data in .  A simple data set               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .            Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the approximate least squares solution to that is more reliable than solving the normal equations.    A solution to is the least squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least squares approximate solution.       Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .       The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the data set, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.       This problem concerns a data set describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the data set and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.       Evaluating the following cell loads a data set describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.       The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .       We observed that if the columns of are linearly independent, then there is a unique least squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.       This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,        "
},
{
  "id": "lst-squares-intro",
  "level": "2",
  "url": "sec-least-squares.html#lst-squares-intro",
  "type": "Figure",
  "number": "6.5.1",
  "title": "",
  "body": "     A collection of points and a line approximating the linear relationship implied by them.  "
},
{
  "id": "exploration-24",
  "level": "2",
  "url": "sec-least-squares.html#exploration-24",
  "type": "Preview Activity",
  "number": "6.5.1",
  "title": "",
  "body": "     Is there a solution to the equation where and are such that .     We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .      "
},
{
  "id": "activity-83",
  "level": "2",
  "url": "sec-least-squares.html#activity-83",
  "type": "Activity",
  "number": "6.5.2",
  "title": "",
  "body": "  Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. Statisticans prefer the notation , and we're going to adopt statistical preferences for most of the remainder of this chapter since least squares is such an important method in statistics.  To begin, we will try to find and so that the three points lie on the line with equation . The first data point gives an equation for and . In particular, we know that when , then so we have or .  Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector specifies the intercept and slope of the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection .    Since is in , the equation is consistent. Find its solution, which we will denote , and sketch the line in . This line is called the least squares regression line . least squares regression  regression least squares regression That \"hat\" on indicates that these coefficients (most likely) do not fit the data exactly, but come as close as we can to doing so (in the sense of minimizing the distance between and ).          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.       "
},
{
  "id": "p-6813",
  "level": "2",
  "url": "sec-least-squares.html#p-6813",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares regression "
},
{
  "id": "p-6815",
  "level": "2",
  "url": "sec-least-squares.html#p-6815",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares "
},
{
  "id": "p-6816",
  "level": "2",
  "url": "sec-least-squares.html#p-6816",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "residual "
},
{
  "id": "fig-least-squares-def",
  "level": "2",
  "url": "sec-least-squares.html#fig-least-squares-def",
  "type": "Figure",
  "number": "6.5.5",
  "title": "",
  "body": "    The solution of the least squares problem and the vertical distances between the line and the data points.  "
},
{
  "id": "p-6817",
  "level": "2",
  "url": "sec-least-squares.html#p-6817",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "simple linear regression linear models "
},
{
  "id": "p-6819",
  "level": "2",
  "url": "sec-least-squares.html#p-6819",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "model matrix coefficients "
},
{
  "id": "p-6820",
  "level": "2",
  "url": "sec-least-squares.html#p-6820",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "model space "
},
{
  "id": "example-77",
  "level": "2",
  "url": "sec-least-squares.html#example-77",
  "type": "Example",
  "number": "6.5.6",
  "title": "",
  "body": "  Add example of setting up a multiple regression problem. Include at least a binary categorical predictor.   "
},
{
  "id": "note-8",
  "level": "2",
  "url": "sec-least-squares.html#note-8",
  "type": "Note",
  "number": "6.5.7",
  "title": "Models without an intercept.",
  "body": " Models without an intercept  It is possible to fit models without an intecept term. In this case the column of 1's will be omitted from the model matrix . Algebraically, a few things, like the defnition and interpretation of below do not work out as well in that case. And statistically, omitting the intercept makes a strong assumption about the nature of the relationship. In most statistical software, the default it to always include an intercept, but there are options to fit models without the intercept term if so desired.  "
},
{
  "id": "p-6826",
  "level": "2",
  "url": "sec-least-squares.html#p-6826",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares approximate solution "
},
{
  "id": "p-6832",
  "level": "2",
  "url": "sec-least-squares.html#p-6832",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "normal equation "
},
{
  "id": "proposition-52",
  "level": "2",
  "url": "sec-least-squares.html#proposition-52",
  "type": "Proposition",
  "number": "6.5.8",
  "title": "",
  "body": "  If the columns of are linearly independent, then there is a unique least squares approximate solution to the equation given by the normal equation .   "
},
{
  "id": "example-toy-normal-equation",
  "level": "2",
  "url": "sec-least-squares.html#example-toy-normal-equation",
  "type": "Example",
  "number": "6.5.9",
  "title": "",
  "body": " Consider the equation . Since this equation is inconsistent, we will find the least squares approximate solution by solving the normal equation , which has the form . Solving this yields .  "
},
{
  "id": "activity-crickets",
  "level": "2",
  "url": "sec-least-squares.html#activity-crickets",
  "type": "Activity",
  "number": "6.5.3",
  "title": "",
  "body": "  The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data:   We would like to represent this relationship by a linear function .   Use the first data point to write an equation involving and .    Suppose that we represent the coefficients using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the desired relationship.     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least squares approximate solution to the equation . Call your solution .   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  Plot the data and your least squares regression line.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.      "
},
{
  "id": "fig-regression-scale",
  "level": "2",
  "url": "sec-least-squares.html#fig-regression-scale",
  "type": "Figure",
  "number": "6.5.10",
  "title": "",
  "body": "     The lines appear to fit equally well in spite of the fact that differs by a factor of 100.  "
},
{
  "id": "p-6860",
  "level": "2",
  "url": "sec-least-squares.html#p-6860",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "residuals model space "
},
{
  "id": "def-rsquared",
  "level": "2",
  "url": "sec-least-squares.html#def-rsquared",
  "type": "Definition",
  "number": "6.5.11",
  "title": "Coefficient of determination.",
  "body": " Coefficient of determination  coefficient of determination     coefficient of determination     the coefficient of determination    For any model that determines predictions for a response variable , we can define the coefficient of determination as  For a linear model, , but the definition can be applied to other models as well.   "
},
{
  "id": "activity-BFI",
  "level": "2",
  "url": "sec-least-squares.html#activity-BFI",
  "type": "Activity",
  "number": "6.5.4",
  "title": "",
  "body": "     Suppose we are interested in finding the least squares approximate solution to the equation and that we have the factorization . Explain why the least squares approximation solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating       "
},
{
  "id": "proposition-53",
  "level": "2",
  "url": "sec-least-squares.html#proposition-53",
  "type": "Proposition",
  "number": "6.5.12",
  "title": "",
  "body": "  If the columns of are linearly independent and we have the factorization , then the least squares approximate solution to the equation is given by .   "
},
{
  "id": "activity-86",
  "level": "2",
  "url": "sec-least-squares.html#activity-86",
  "type": "Activity",
  "number": "6.5.5",
  "title": "",
  "body": "     Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated.   Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. Creat a plot that incluedes the raw data and the quadratic fit.     What is your predicted value when .    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.      "
},
{
  "id": "p-6940",
  "level": "2",
  "url": "sec-least-squares.html#p-6940",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Vandermonde matrix "
},
{
  "id": "activity-87",
  "level": "2",
  "url": "sec-least-squares.html#activity-87",
  "type": "Activity",
  "number": "6.5.6",
  "title": "",
  "body": "  This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 1980, 2012, and 2017. We will focus primarily on 2012.      Find the vector , the least squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     Plot the data along with the fitted polynomial model.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise , and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much influence over the fit of the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a function that is too flexible and fits the training data better than it can be expected to fit new data high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. What we really need is a method for selecting a good value for and a better way to measure how well we should expect the model to fit new data, not the data used to train the model. That discussion would take us too far afield for the moment, but it is an important discussion.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.      "
},
{
  "id": "p-6966",
  "level": "2",
  "url": "sec-least-squares.html#p-6966",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares approximate solution linear models response variable data matrix intercept "
},
{
  "id": "exercise-243",
  "level": "2",
  "url": "sec-least-squares.html#exercise-243",
  "type": "Exercise",
  "number": "6.5.8.1",
  "title": "",
  "body": " Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .     "
},
{
  "id": "ex-lst-squares-line",
  "level": "2",
  "url": "sec-least-squares.html#ex-lst-squares-line",
  "type": "Exercise",
  "number": "6.5.8.2",
  "title": "",
  "body": " Consider the data in .  A data set with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least squares approximate solution to .    Find the least squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .          "
},
{
  "id": "exercise-245",
  "level": "2",
  "url": "sec-least-squares.html#exercise-245",
  "type": "Exercise",
  "number": "6.5.8.3",
  "title": "",
  "body": " Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .          "
},
{
  "id": "exercise-246",
  "level": "2",
  "url": "sec-least-squares.html#exercise-246",
  "type": "Exercise",
  "number": "6.5.8.4",
  "title": "",
  "body": " Consider the data in .  A simple data set               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .          "
},
{
  "id": "exercise-247",
  "level": "2",
  "url": "sec-least-squares.html#exercise-247",
  "type": "Exercise",
  "number": "6.5.8.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the approximate least squares solution to that is more reliable than solving the normal equations.    A solution to is the least squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least squares approximate solution.     "
},
{
  "id": "exercise-248",
  "level": "2",
  "url": "sec-least-squares.html#exercise-248",
  "type": "Exercise",
  "number": "6.5.8.6",
  "title": "",
  "body": " Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .     "
},
{
  "id": "exercise-249",
  "level": "2",
  "url": "sec-least-squares.html#exercise-249",
  "type": "Exercise",
  "number": "6.5.8.7",
  "title": "",
  "body": " The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the data set, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.     "
},
{
  "id": "exercise-250",
  "level": "2",
  "url": "sec-least-squares.html#exercise-250",
  "type": "Exercise",
  "number": "6.5.8.8",
  "title": "",
  "body": " This problem concerns a data set describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the data set and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.     "
},
{
  "id": "exercise-251",
  "level": "2",
  "url": "sec-least-squares.html#exercise-251",
  "type": "Exercise",
  "number": "6.5.8.9",
  "title": "",
  "body": " Evaluating the following cell loads a data set describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.     "
},
{
  "id": "exercise-252",
  "level": "2",
  "url": "sec-least-squares.html#exercise-252",
  "type": "Exercise",
  "number": "6.5.8.10",
  "title": "",
  "body": " The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .     "
},
{
  "id": "exercise-253",
  "level": "2",
  "url": "sec-least-squares.html#exercise-253",
  "type": "Exercise",
  "number": "6.5.8.11",
  "title": "",
  "body": " We observed that if the columns of are linearly independent, then there is a unique least squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.     "
},
{
  "id": "ex-r2-meaning",
  "level": "2",
  "url": "sec-least-squares.html#ex-r2-meaning",
  "type": "Exercise",
  "number": "6.5.8.12",
  "title": "",
  "body": " This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,      "
},
{
  "id": "sec-symmetric-matrices",
  "level": "1",
  "url": "sec-symmetric-matrices.html",
  "type": "Section",
  "number": "7.1",
  "title": "Symmetric matrices and variance",
  "body": " Symmetric matrices and variance   In this section, we will revisit the theory of eigenvalues and eigenvectors for the special class of matrices that are symmetric , meaning that the matrix equals its transpose. This understanding of symmetric matrices will enable us to form singular value decompositions later in the chapter. We'll also begin studying variance in this section as it provides an important context that motivates some of our later work.  To begin, remember that if is a square matrix, we say that is an eigenvector of with associated eigenvalue if . In other words, for these special vectors, the operation of matrix multiplication simplifies to scalar multiplication.    This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.       The preview activity asks us to compare the matrix transformations defined by two matrices, a diagonal matrix and a matrix whose eigenvectors are given to us. The transformation defined by stretches horizontally by a factor of 3 and reflects in the horizontal axis, as shown in       The matrix transformation defined by .   By contrast, the transformation defined by stretches the plane by a factor of 3 in the direction of and reflects in the line defined by , as seen in .      The matrix transformation defined by .   In this way, we see that the matrix transformations defined by these two matrices are equivalent after a rotation. similar matrices  matrix similar This notion of equivalence is what we called similarity in . There we considered a square matrix that provided enough eigenvectors to form a basis of . For example, suppose we can construct a basis for using eigenvectors having associated eigenvalues . Forming the matrices, enables us to write . diagonalizable matrix  matrix diagonalizable This is what it means for to be diagonalizable .  For the example in the preview activity, we are led to form which tells us that .  Notice that the matrix has eigenvectors and that not only form a basis for but, in fact, form an orthogonal basis for . Given the prominent role played by orthogonal bases in the last chapter, we would like to understand what conditions on a matrix enable us to form an orthogonal basis of eigenvectors.    Symmetric matrices and orthogonal diagonalization  Let's begin by looking at some examples in the next activity.    Remember that the Python command scipy.linalg.eig(A) attempts to find a basis for consisting of eigenvectors of . If successful, e, E = linalg.aig(A) provides a vector of eigen values e and a matrix E containing the associated eigenvectors as columns.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and such that .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                      The examples in this activity illustrate a range of possibilities. First, a matrix may have complex eigenvalues, in which case it will not be diagonalizable. Second, even if all the eigenvalues are real, there may not be a basis of eigenvalues if the dimension of one of the eigenspaces is less than the algebraic multiplicity of the associated eigenvalue.  We are interested in matrices for which there is an orthogonal basis of eigenvectors. When this happens, we can create an orthonormal basis of eigenvectors by scaling each eigenvector in the basis so that its length is 1. Putting these orthonormal vectors into a matrix produces an orthogonal matrix, which means that . We then have In this case, we say that is orthogonally diagonalizable .    orthogonal diagonalization    diagonalization orthogonal    If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.    When is orthogonally diagonalizable, notice that That is, when is orthogonally diagonalizable, and we say that is symmetric .    symmetric matrix    matrix symmetric    A symmetric matrix is one for which .      Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .      If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.    We have seen that a matrix that is orthogonally diagonalizable must be symmetric. In fact, it turns out that any symmetric matrix is orthogonally diagonalizable. We record this fact in the next theorem.   The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.      Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                  As the examples in illustrate, the Spectral Theorem implies a number of things. Namely, if is a symmetric matrix, then   the eigenvalues of are real.    there is a basis of consisting of eigenvectors.    two eigenvectors that are associated to different eigenvalues are orthogonal.     We won't justify the first two facts here since that would take us rather far afield. However, it will be helpful to explain the third fact. To begin, notice the following: This is a useful fact that we'll employ quite a bit in the future so let's summarize it in the following proposition.    For any matrix , we have In particular, if is symmetric, then       Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that any two eigenvectors of any symmetric matrix associated to distinct eigenvalues must be orthogonal.      If is symmetric, then any pair of eigenvectors for with distinct eigenvalues are orthogonal.  That is, if and are eigenvectors associated with distinct eigenvalues , then .      Variance  Many of the ideas we'll encounter in this chapter, such as orthogonal diagonalizations, can be applied to the study of data. In fact, it can be useful to understand these applications because they provide an important context in which mathematical ideas have a more concrete meaning and their motivation appears more clearly. For that reason, we will now introduce the statistical concept of variance as a way to gain insight into the significance of orthogonal diagonalizations.  Given a set of data points, their variance measures how spread out the points are. The next activity looks at some examples.    We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                          Notice that variance enjoys an additivity property. Consider, for instance, the situation where our data points are two-dimensional and suppose that the demeaned points are . We have If we take the average over all data points, we find that the total variance is the sum of the variances in the and directions:   More generally, suppose that we have an orthonormal basis and . If we project the demeaned points onto the line defined by , we obtain the points so that   For each of our demeaned data points, the Projection Formula tells us that We then have since . When we average over all the data points, we find that the total variance is the sum of the variances in the and directions. This leads to the following proposition, in which this observation is expressed more generally.   Additivity of Variance   If is a subspace with orthonormal basis , then the variance of the points projected onto is the sum of the variances in the directions:     The next activity demonstrates a more efficient way to find the variance in a particular direction and connects our discussion of variance with symmetric matrices.    Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. covariance matrix  matrix covariance Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .            This activity introduced the covariance matrix of a dataset, which is defined to be where is the matrix of demeaned data points. Notice that which tells us that is symmetric. In particular, we know that it is orthogonally diagonalizable, an observation that will play an important role in the future.  This activity also demonstrates the significance of the covariance matrix, which is recorded in the following proposition.    If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is     Our goal in the future will be to find directions where the variance is as large as possible and directions where it is as small as possible. The next activity demonstrates why this is useful.       Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.       This activity illustrates how variance can identify a line along which the data are concentrated. When the data primarily lie along a line defined by a vector , then the variance in that direction will be large while the variance in an orthogonal direction will be small.  Remember that variance is additive, according to , so that if and are orthogonal unit vectors, then the total variance is Therefore, if we choose to be the direction where is a maximum, then will be a minimum.  In the next section, we will use an orthogonal diagonalization of the covariance matrix to find the directions having the greatest and smallest variances. In this way, we will be able to determine when data are concentrated along a line or subspace.    Summary  This section explored both symmetric matrices and variance. In particular, we saw that   A matrix is orthogonally diagonalizable if there is an orthonormal basis of eigenvectors. In particular, we can write , where is a diagonal matrix of eigenvalues and is an orthogonal matrix of eigenvectors.    The Spectral Theorem tells us that a matrix is orthogonally diagonalizable if and only if it is symmetric; that is, .    The variance of a dataset can be computed using the covariance matrix , where is the matrix of demeaned data points. In particular, the variance of the demeaned data points projected onto the line defined by the unit vector is .    Variance is additive so that if is a subspace with orthonormal basis , then         For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.     .                        Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.        Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and             Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.            Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .       Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.      Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and        Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .                 Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.       Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to     Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and        "
},
{
  "id": "p-7181",
  "level": "2",
  "url": "sec-symmetric-matrices.html#p-7181",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "symmetric "
},
{
  "id": "exploration-25",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exploration-25",
  "type": "Preview Activity",
  "number": "7.1.1",
  "title": "",
  "body": "  This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.      "
},
{
  "id": "fig-eigen-diag-D",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-D",
  "type": "Figure",
  "number": "7.1.2",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "fig-eigen-diag-general",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-general",
  "type": "Figure",
  "number": "7.1.3",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "p-7204",
  "level": "2",
  "url": "sec-symmetric-matrices.html#p-7204",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "similarity diagonalizable "
},
{
  "id": "activity-88",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-88",
  "type": "Activity",
  "number": "7.1.2",
  "title": "",
  "body": "  Remember that the Python command scipy.linalg.eig(A) attempts to find a basis for consisting of eigenvectors of . If successful, e, E = linalg.aig(A) provides a vector of eigen values e and a matrix E containing the associated eigenvectors as columns.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and such that .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                     "
},
{
  "id": "p-7239",
  "level": "2",
  "url": "sec-symmetric-matrices.html#p-7239",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonally diagonalizable "
},
{
  "id": "def-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#def-orthog-diag",
  "type": "Definition",
  "number": "7.1.4",
  "title": "",
  "body": "  orthogonal diagonalization    diagonalization orthogonal    If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.   "
},
{
  "id": "p-7241",
  "level": "2",
  "url": "sec-symmetric-matrices.html#p-7241",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "symmetric "
},
{
  "id": "definition-37",
  "level": "2",
  "url": "sec-symmetric-matrices.html#definition-37",
  "type": "Definition",
  "number": "7.1.5",
  "title": "",
  "body": "  symmetric matrix    matrix symmetric    A symmetric matrix is one for which .   "
},
{
  "id": "example-79",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-79",
  "type": "Example",
  "number": "7.1.6",
  "title": "",
  "body": "  Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .   "
},
{
  "id": "example-80",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-80",
  "type": "Example",
  "number": "7.1.7",
  "title": "",
  "body": "  If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.   "
},
{
  "id": "theorem-3",
  "level": "2",
  "url": "sec-symmetric-matrices.html#theorem-3",
  "type": "Theorem",
  "number": "7.1.9",
  "title": "The Spectral Theorem.",
  "body": " The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.   "
},
{
  "id": "activity-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-orthog-diag",
  "type": "Activity",
  "number": "7.1.3",
  "title": "",
  "body": "  Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                 "
},
{
  "id": "prop-symmetric-dot",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-symmetric-dot",
  "type": "Proposition",
  "number": "7.1.10",
  "title": "",
  "body": "  For any matrix , we have In particular, if is symmetric, then    "
},
{
  "id": "example-81",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-81",
  "type": "Example",
  "number": "7.1.11",
  "title": "",
  "body": "  Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that any two eigenvectors of any symmetric matrix associated to distinct eigenvalues must be orthogonal.   "
},
{
  "id": "prop-symmetric-implies-orthogonal-eigenvectors",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-symmetric-implies-orthogonal-eigenvectors",
  "type": "Proposition",
  "number": "7.1.12",
  "title": "",
  "body": "  If is symmetric, then any pair of eigenvectors for with distinct eigenvalues are orthogonal.  That is, if and are eigenvectors associated with distinct eigenvalues , then .   "
},
{
  "id": "activity-90",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-90",
  "type": "Activity",
  "number": "7.1.4",
  "title": "",
  "body": "  We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                         "
},
{
  "id": "prop-variance-additivity",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-variance-additivity",
  "type": "Proposition",
  "number": "7.1.17",
  "title": "Additivity of Variance.",
  "body": " Additivity of Variance   If is a subspace with orthonormal basis , then the variance of the points projected onto is the sum of the variances in the directions:    "
},
{
  "id": "activity-91",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-91",
  "type": "Activity",
  "number": "7.1.5",
  "title": "",
  "body": "  Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. covariance matrix  matrix covariance Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .           "
},
{
  "id": "prop-covariance",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-covariance",
  "type": "Proposition",
  "number": "7.1.18",
  "title": "",
  "body": "  If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is    "
},
{
  "id": "activity-92",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-92",
  "type": "Activity",
  "number": "7.1.6",
  "title": "",
  "body": "     Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.      "
},
{
  "id": "exercise-255",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-255",
  "type": "Exercise",
  "number": "7.1.4.1",
  "title": "",
  "body": " For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.     .                        Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.      "
},
{
  "id": "exercise-256",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-256",
  "type": "Exercise",
  "number": "7.1.4.2",
  "title": "",
  "body": " Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and           "
},
{
  "id": "exercise-257",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-257",
  "type": "Exercise",
  "number": "7.1.4.3",
  "title": "",
  "body": " Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.          "
},
{
  "id": "exercise-258",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-258",
  "type": "Exercise",
  "number": "7.1.4.4",
  "title": "",
  "body": " Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .     "
},
{
  "id": "exercise-259",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-259",
  "type": "Exercise",
  "number": "7.1.4.5",
  "title": "",
  "body": " Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.    "
},
{
  "id": "exercise-260",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-260",
  "type": "Exercise",
  "number": "7.1.4.6",
  "title": "",
  "body": " Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and      "
},
{
  "id": "exercise-261",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-261",
  "type": "Exercise",
  "number": "7.1.4.7",
  "title": "",
  "body": " Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .               "
},
{
  "id": "exercise-262",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-262",
  "type": "Exercise",
  "number": "7.1.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.     "
},
{
  "id": "exercise-263",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-263",
  "type": "Exercise",
  "number": "7.1.4.9",
  "title": "",
  "body": " Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to   "
},
{
  "id": "exercise-264",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-264",
  "type": "Exercise",
  "number": "7.1.4.10",
  "title": "",
  "body": " Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and      "
},
{
  "id": "sec-quadratic-forms",
  "level": "1",
  "url": "sec-quadratic-forms.html",
  "type": "Section",
  "number": "7.2",
  "title": "Quadratic forms",
  "body": " Quadratic forms   With our understanding of symmetric matrices and variance in hand, we'll now explore how to determine the directions in which the variance of a dataset is as large as possible and where it is as small as possible. This is part of a much larger story involving a type of function, called a quadratic form , that we'll introduce here.    Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .              Quadratic forms  Given a matrix of demeaned data points, the symmetric covariance matrix determines the variance in a particular direction where is a unit vector defining the direction.  More generally, a symmetric matrix defines a function by Notice that this expression is similar to the one we use to find the variance in terms of the covariance matrix . The only difference is that we allow to be any vector rather than requiring it to be a unit vector.    Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.     quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .      Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and             Linear algebra is principally about things that are linear. However, quadratic forms, as the name implies, have a distinctly non-linear character. First, if , is a symmetric matrix, then the associated quadratic form is Notice how the variables and are multiplied together, which tells us this isn't a linear function.  This expression assumes an especially simple form when is a diagonal matrix. In particular, if , then . This is special because there is no cross-term involving .  Remember that matrix transformations have the property that . Quadratic forms behave differently: For instance, when we multiply by the scalar 2, then . Also, notice that since the scalar is squared.  Finally, evaluating a quadratic form on an eigenvector has a particularly simple form. Suppose that is an eigenvector of with associated eigenvalue . We then have   Let's now return to our motivating question: in which direction is the variance of a dataset as large as possible and in which is it as small as possible. Remembering that the vector is a unit vector, we can now state a more general form of this question: If is a quadratic form, for which unit vectors is as large as possible and for which is it as small as possible? Since a unit vector specifies a direction, we will often ask for the directions in which the quadratic form is at its maximum or minimum value.    We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur?    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .       This activity demonstrates how the eigenvalues of determine the maximum and minimum values of the quadratic form when evaluated on unit vectors and how the associated eigenvectors determine the directions in which the maximum and minimum values occur. Let's look at another example so that this connection is clear.    Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.    More generally, we have    Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .      Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .      Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .    We've been focused on finding the directions in which a quadratic form attains its maximum and minimum values, but there's another important observation to make after this activity. Recall how we used the fact that a symmetric matrix is orthogonally diagonalizable: if , then where .  More generally, if we define , we have Remembering that the quadratic form associated to a diagonal form has no cross terms, we obtain In other words, after a change of coordinates, the quadratic form can be written without cross terms. This is known as the Principle Axes Theorem.   Principle Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as     We will put this to use in the next section.    Definite symmetric matrices  While our questions about variance provide some motivation for exploring quadratic forms, these functions appear in a variety of other contexts so it's worth spending some more time with them. For example, quadratic forms appear in multivariable calculus when describing the behavior of a function of several variables near a critical point and in physics when describing the kinetic energy of a rigid body.  The following definition will be important in this section.    A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.      This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.       As seen in this activity, it is straightforward to determine the definiteness of a diagonal matrix. For instance, if , then This shows that when either or is not zero so we conclude that is positive definite. In the same way, we see that is positive semidefinite if all the diagonal entries are nonnegative.  Understanding this behavior for diagonal matrices enables us to understand more general symmetric matrices. As we saw previously, the quadratic form for a symmetric matrix agrees with the quadratic form for the diagonal matrix after a change of coordinates. In particular, where . Now the diagonal entries of are the eigenvalues of from which we conclude that if all the eigenvalues of are positive. Likewise, if all the eigenvalues are nonnegative.    A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.    We will now apply what we've learned about quadratic forms to study the nature of critical points in multivariable calculus. The rest of this section assumes that the reader is familiar with ideas from multivariable calculus and can be skipped by others.  First, suppose that is a differentiable function. We will use and to denote the partial derivatives of with respect to and . Similarly, , , and denote the second partial derivatives. You may recall that the mixed partials, and are equal under a mild assumption on the function . A typical question in calculus is to determine where this function has its maximum and minimum values.  Any local maximum or minimum of appears at a critical point where Near a critical point, the quadratic approximation of tells us that     Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .       Near a critical point of a function , we can write where and . If is positive definite, then , which tells us that and that the critical point is therefore a local minimum.  The matrix is called the Hessian of , and we see now that the eigenvalues of this symmetric matrix determine the nature of the critical point . In particular, if the eigenvalues are both positive, then is positive definite, and the critical point is a local minimum.  This observation leads to the Second Derivative Test for multivariable functions.   Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.       Most multivariable calculus texts assume that the reader is not familiar with linear algebra and so write the second derivative test for functions of two variables in terms of . If    and , then is a local minimum.     and , then is a local maximum.     , then is neither a local maximum nor minimum.   The conditions in this version of the second derivative test are simply algebraic criteria that tell us about the definiteness of the Hessian matrix .    Summary  This section explored quadratic forms, functions that are defined by symmetric matrices.   If is a symmetric matrix, then the quadratic form defined by is the function .  Quadratic forms appear when studying the variance of a dataset. If is the covariance matrix, then the variance in the direction defined by a unit vector is .  Similarly, quadratic forms appear in multivariable calculus when analyzing the behavior of a function of several variables near a critical point.    If is the largest eigenvalue of a symmetric matrix and the smallest, then the maximum value of among unit vectors , is , and this maximum value occurs in the direction of , a unit eigenvector associated to .  Similarly, the minimum value of is , which appears in the direction of , an eigenvector associated to .    A symmetric matrix is positive definite if its eigenvalues are all positive, positive semidefinite if its eigenvalues are all nonnegative, and indefinite if it has both positive and negative eigenvalues.    If the Hessian of a multivariable function is positive definite at a critical point, then the critical point is a local minimum. Likewise, if the Hessian is negative definite, the critical point is a local maximum.        Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .       Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .       Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .       Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .       Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.       Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .       Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, explain why is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.       Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a data set, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.       Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.       Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.       "
},
{
  "id": "preview-quadforms",
  "level": "2",
  "url": "sec-quadratic-forms.html#preview-quadforms",
  "type": "Preview Activity",
  "number": "7.2.1",
  "title": "",
  "body": "  Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .           "
},
{
  "id": "example-82",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-82",
  "type": "Example",
  "number": "7.2.2",
  "title": "",
  "body": "  Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.   "
},
{
  "id": "definition-38",
  "level": "2",
  "url": "sec-quadratic-forms.html#definition-38",
  "type": "Definition",
  "number": "7.2.3",
  "title": "",
  "body": " quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .   "
},
{
  "id": "activity-93",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-93",
  "type": "Activity",
  "number": "7.2.2",
  "title": "",
  "body": "  Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and            "
},
{
  "id": "activity-94",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-94",
  "type": "Activity",
  "number": "7.2.3",
  "title": "",
  "body": "  We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur?    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .      "
},
{
  "id": "example-83",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-83",
  "type": "Example",
  "number": "7.2.4",
  "title": "",
  "body": "  Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.   "
},
{
  "id": "prop-quadform-extrema",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-quadform-extrema",
  "type": "Proposition",
  "number": "7.2.5",
  "title": "",
  "body": "  Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .   "
},
{
  "id": "example-84",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-84",
  "type": "Example",
  "number": "7.2.6",
  "title": "",
  "body": "  Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .   "
},
{
  "id": "example-85",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-85",
  "type": "Example",
  "number": "7.2.7",
  "title": "",
  "body": "  Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .   "
},
{
  "id": "theorem-4",
  "level": "2",
  "url": "sec-quadratic-forms.html#theorem-4",
  "type": "Theorem",
  "number": "7.2.10",
  "title": "Principle Axes Theorem.",
  "body": " Principle Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as    "
},
{
  "id": "definition-39",
  "level": "2",
  "url": "sec-quadratic-forms.html#definition-39",
  "type": "Definition",
  "number": "7.2.11",
  "title": "",
  "body": "  A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.   "
},
{
  "id": "activity-95",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-95",
  "type": "Activity",
  "number": "7.2.4",
  "title": "",
  "body": "  This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.      "
},
{
  "id": "prop-definite-matrices",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-definite-matrices",
  "type": "Proposition",
  "number": "7.2.12",
  "title": "",
  "body": "  A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.   "
},
{
  "id": "activity-96",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-96",
  "type": "Activity",
  "number": "7.2.5",
  "title": "",
  "body": "  Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .      "
},
{
  "id": "proposition-60",
  "level": "2",
  "url": "sec-quadratic-forms.html#proposition-60",
  "type": "Proposition",
  "number": "7.2.13",
  "title": "Second Derivative Test.",
  "body": " Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.      "
},
{
  "id": "exercise-265",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-265",
  "type": "Exercise",
  "number": "7.2.4.1",
  "title": "",
  "body": " Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .     "
},
{
  "id": "exercise-266",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-266",
  "type": "Exercise",
  "number": "7.2.4.2",
  "title": "",
  "body": " Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .     "
},
{
  "id": "exercise-267",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-267",
  "type": "Exercise",
  "number": "7.2.4.3",
  "title": "",
  "body": " Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .     "
},
{
  "id": "exercise-268",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-268",
  "type": "Exercise",
  "number": "7.2.4.4",
  "title": "",
  "body": " Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .     "
},
{
  "id": "exercise-269",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-269",
  "type": "Exercise",
  "number": "7.2.4.5",
  "title": "",
  "body": " Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.     "
},
{
  "id": "exercise-270",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-270",
  "type": "Exercise",
  "number": "7.2.4.6",
  "title": "",
  "body": " Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .     "
},
{
  "id": "exercise-271",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-271",
  "type": "Exercise",
  "number": "7.2.4.7",
  "title": "",
  "body": " Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, explain why is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.     "
},
{
  "id": "exercise-272",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-272",
  "type": "Exercise",
  "number": "7.2.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a data set, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.     "
},
{
  "id": "exercise-273",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-273",
  "type": "Exercise",
  "number": "7.2.4.9",
  "title": "",
  "body": " Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.     "
},
{
  "id": "exercise-274",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-274",
  "type": "Exercise",
  "number": "7.2.4.10",
  "title": "",
  "body": " Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.     "
},
{
  "id": "sec-pca",
  "level": "1",
  "url": "sec-pca.html",
  "type": "Section",
  "number": "7.3",
  "title": "Principal Component Analysis",
  "body": " Principal Component Analysis   We are sometimes presented with a dataset having many data points that live in a high dimensional space. For instance, we looked at a dataset describing body fat index (BFI) in where each data point is six-dimensional. Developing an intuitive understanding of the data is hampered by the fact that it cannot be visualized.  This section explores a technique called principal component analysis , which enables us to reduce the dimension of a dataset so that it may be visualized or studied in a way so that interesting features more readily stand out. Our previous work with variance and the orthogonal diagonalization of symmetric matrices provides the key ideas.    We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .       Here are some ideas we've seen previously that will be particularly useful for us in this section. Remember that the covariance matrix of a dataset is where is the matrix of demeaned data points.   When is a unit vector, the variance of the demeaned data after projecting onto the line defined by is given by the quadratic form .    In particular, if is a unit eigenvector of with associated eigenvalue , then .    Moreover, variance is additive, as we recorded in : if is a subspace having an orthonormal basis , then the variance        Principal Component Analysis  Let's begin by looking at an example that illustrates the central theme of this technique.    Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .       This activity demonstrates how the eigenvalues of the covariance matrix can tell us when data are clustered around, or even wholly contained within, a smaller dimensional subspace. In particular, the original data is 5-dimensional, but we see that it actually lies in a 3-dimensional subspace of . Later in this section, we'll see how to use this observation to work with the data as if it were three-dimensional, an idea known as dimensional reduction .   principal components The eigenvectors of the covariance matrix are called principal components , and we will order them so that their associated eigenvalues decrease. Generally speaking, we hope that the first few principal components retain most of the variance, as the example in the activity demonstrates. In that example, we have the sequence of subspaces    , the 1-dimensional subspace spanned by , which retains of the total variance,     , the 2-dimensional subspace spanned by and , which retains of the variance, and     , the 3-dimensional subspace spanned by , , and , which retains all of the variance.     Notice how we retain more of the total variance as we increase the dimension of the subspace onto which the data are projected. Eventually, projecting the data onto retains all the variance, which tells us the data must lie in , a smaller dimensional subspace of .  In fact, these subspaces are the best possible. We know that the first principal component is the eigenvector of associated to the largest eigenvalue. This means that the variance is as large as possible in the direction. In other words, projecting onto any other line will retain a smaller amount of variance. Similarly, projecting onto any other 2-dimensional subspace besides will retain less variance than projecting onto . The principal components have the wonderful ability to pick out the best possible subspaces to retain as much variance as possible.  Of course, this is a contrived example. Typically, the presence of noise in a dataset means that we do not expect all the points to be wholly contained in a smaller dimensional subspace. In fact, the 2-dimensional subspace retains of the variance. Depending on the situation, we may want to write off the remaining of the variance as noise in exchange for the convenience of working with a smaller dimensional subspace. As we'll see later, we will seek a balance using a number of principal components large enough to retain most of the variance but small enough to be easy to work with.    We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.       This example is a more realistic illustration of principal component analysis. The plot of the 3-dimensional data appears to show that the data lies close to a plane, and the principal components will identify this plane. Starting with the matrix of demeaned data , we construct the covariance matrix and study its eigenvalues. Notice that the first two principal components account for more than 98% of the variance, which means we can expect the points to lie close to , the two-dimensional subspace spanned by and .  Since is a subspace of , projecting the data points onto gives a list of 100 points in . In order to visualize them more easily, we instead consider the coordinates of the projections in the basis defined by and . For instance, we know that the projection of a data point is which is a three-dimensional vector. Instead, we can record the coordinates and plot them in the two-dimensional coordinate plane, as illustrated in .       The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .   If we form the matrix , then we have This means that the columns of represent the coordinates of the projected points, which may now be plotted in the plane.  In this plot, the first coordinate, represented by the horizontal coordinate, represents the projection of a data point onto the line defined by while the second coordinate represents the projection onto the line defined by . Since is the first principal component, the variance in the direction is greater than the variance in the direction. For this reason, the plot will be more spread out in the horizontal direction than in the vertical.    Using Principal Component Analysis  Now that we've explored the ideas behind principal component analysis, we will look at a few examples that illustrate its use.    The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.       This activity demonstrates how principal component analysis enables us to extract information from a dataset that may not be easily obtained otherwise. As in our previous example, we see that the data points lie quite close to a two-dimensional subspace of . In fact, , the subspace spanned by the first two principal components, accounts for more than 96% of the variance. More importantly, when we project the data onto , it becomes apparent that Northern Ireland is fundamentally different from the other three nations.  With some additional thought, we can determine more specific ways in which Northern Ireland is different. On the -dimensional plot, Northern Ireland lies far to the right compared to the other three nations. Since the data has been demeaned, the origin in this plot corresponds to the average of the four nations. The coordinates of the point representing Northern Ireland are about , meaning that the projected data point differs from the mean by about .  Let's just focus on the contribution from . We see that the ninth component of , the one that describes Fresh Fruit, is about . This means that the ninth component of differs from the mean by about grams per person per week. So roughly speaking, people in Northern Ireland are eating about 300 fewer grams of Fresh Fruit than the average across the four nations. This is borne out by looking at the original data, which show that the consumption of Fresh Fruit in Northern Ireland is significantly less than the other nations. Examing the other components of shows other ways in which Northern Ireland differs from the other three nations.    In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free DOcumetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .         Summary  This section has explored principal component analysis as a technique to reduce the dimension of a dataset. From the demeaned data matrix , we form the covariance matrix , where is the number of data points.   The eigenvectors , of are called the principal components. We arrange them so that their corresponding eigenvalues are in decreasing order.    If is the subspace spanned by the first principal components, then the variance of the demeaned data projected onto is the sum of the first eigenvalues of . No other -dimensional subspace retains more variance when the data is projected onto it.    If is the matrix whose columns are the first principal components, then the columns of hold the coordinates, expressed in the basis , of the data once projected onto .    Our goal is to use a number of principal components that is large enough to retain most of the variance in the dataset but small enough to be manageable.        Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .       Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .       Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .       Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.       In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.       This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.       "
},
{
  "id": "exploration-27",
  "level": "2",
  "url": "sec-pca.html#exploration-27",
  "type": "Preview Activity",
  "number": "7.3.1",
  "title": "",
  "body": "  We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .      "
},
{
  "id": "activity-97",
  "level": "2",
  "url": "sec-pca.html#activity-97",
  "type": "Activity",
  "number": "7.3.2",
  "title": "",
  "body": "  Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .      "
},
{
  "id": "activity-98",
  "level": "2",
  "url": "sec-pca.html#activity-98",
  "type": "Activity",
  "number": "7.3.3",
  "title": "",
  "body": "  We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.      "
},
{
  "id": "fig-pca-coords",
  "level": "2",
  "url": "sec-pca.html#fig-pca-coords",
  "type": "Figure",
  "number": "7.3.1",
  "title": "",
  "body": "     The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .  "
},
{
  "id": "activity-99",
  "level": "2",
  "url": "sec-pca.html#activity-99",
  "type": "Activity",
  "number": "7.3.4",
  "title": "",
  "body": "  The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.      "
},
{
  "id": "activity-pca-iris",
  "level": "2",
  "url": "sec-pca.html#activity-pca-iris",
  "type": "Activity",
  "number": "7.3.5",
  "title": "",
  "body": "  In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free DOcumetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .      "
},
{
  "id": "exercise-275",
  "level": "2",
  "url": "sec-pca.html#exercise-275",
  "type": "Exercise",
  "number": "7.3.4.1",
  "title": "",
  "body": " Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .     "
},
{
  "id": "exercise-276",
  "level": "2",
  "url": "sec-pca.html#exercise-276",
  "type": "Exercise",
  "number": "7.3.4.2",
  "title": "",
  "body": " Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .     "
},
{
  "id": "exercise-277",
  "level": "2",
  "url": "sec-pca.html#exercise-277",
  "type": "Exercise",
  "number": "7.3.4.3",
  "title": "",
  "body": " Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .     "
},
{
  "id": "exercise-278",
  "level": "2",
  "url": "sec-pca.html#exercise-278",
  "type": "Exercise",
  "number": "7.3.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.     "
},
{
  "id": "exercise-279",
  "level": "2",
  "url": "sec-pca.html#exercise-279",
  "type": "Exercise",
  "number": "7.3.4.5",
  "title": "",
  "body": " In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.     "
},
{
  "id": "exercise-280",
  "level": "2",
  "url": "sec-pca.html#exercise-280",
  "type": "Exercise",
  "number": "7.3.4.6",
  "title": "",
  "body": " This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.     "
},
{
  "id": "sec-svd-intro",
  "level": "1",
  "url": "sec-svd-intro.html",
  "type": "Section",
  "number": "7.4",
  "title": "Singular Value Decompositions",
  "body": " Singular Value Decompositions   The Spectral Theorem has motivated the past few sections. In particular, we applied the fact that symmetric matrices can be orthogonally diagonalized to simplify quadratic forms, which enabled us to use principal component analysis to reduce the dimension of a dataset.  But what can we do with matrices that are not symmetric or even square? For instance, the following matrices are not diagonalizable, much less orthogonally so: In this section, we will develop a description of matrices called the singular value decomposition that is, in many ways, analogous to an orthogonal diagonalization. For example, we have seen that any symmetric matrix can be written in the form where is an orthogonal matrix and is diagonal. A singular value decomposition will have the form where and are orthogonal and is diagonal. Most notably, we will see that every matrix has a singular value decomposition whether it's symmetric or not.    Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of and in which direction does it occur?     What is the minimum value of and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.         Finding singular value decompositions  We will begin by explaining what a singular value decomposition is and how we can find one for a given matrix .  Recall how the orthogonal diagonalization of a symmetric matrix is formed: if is symmetric, we write where the diagonal entries of are the eigenvalues of and the columns of are the associated eigenvectors. Moreover, the eigenvalues are related to the maximum and minimum values of the associated quadratic form among all unit vectors.  A general matrix, particularly a matrix that is not square, may not have eigenvalues and eigenvectors, but we can discover analogous features, called singular values and singular vectors , by studying a function somewhat similar to a quadratic form. More specifically, any matrix defines a function which measures the length of . For example, the diagonal matrix gives the function . The presence of the square root means that this function is not a quadratic form. We can, however, define the singular values and vectors by looking for the maximum and minimum of this function among all unit vectors .  While is not itself a quadratic form, it becomes one if we square it:  Gram matrix We call , the Gram matrix associated to and note that This is important in the next activity, which introduces singular values and singular vectors.    The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .       As this activity shows, the singular values of are the maximum and minimum values of among all unit vectors and the right singular vectors and are the directions in which they occur. The key to finding the singular values and vectors is to utilize the Gram matrix and its associated quadratic form . We will illustrate with some more examples.    We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition     To summarize, we find a singular value decomposition of a matrix in the following way:   Construct the Gram matrix and find an orthogonal diagonalization to obtain eigenvalues and an orthonormal basis of eigenvectors.    The singular values of are the squares roots of eigenvalues of ; that is, . By convention, the singular values are listed in decreasing order: . The right singular vectors are the associated eigenvectors of .    The left singular vectors are found by . Because , we know that will be a unit vector.  In fact, the left singular vectors will also form an orthonormal basis. To see this, suppose that the associcated singular values are nonzero. We then have: since the right singular vectors are orthogonal.       Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .      In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .         We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.    As we'll see in the next section, some additional work may be needed to construct the left singular vectors if more of the singular values are zero, but we won't worry about that now. For the time being, let's record our work in the following theorem.   The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.   Notice that a singular value decomposition of gives us a singular value decomposition of . More specifically, if , then     If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.    As we said earlier, a singular value decomposition should be thought of a generalization of an orthogonal diagonalization. For instance, the Spectral Theorem tells us that a symmetric matrix can be written as . Many matrices, however, are not symmetric and so they are not orthogonally diagonalizable. However, every matrix has a singular value decomposition . The price of this generalization is that we usually have two sets of singular vectors that form the orthogonal matrices and whereas a symmetric matrix has a single set of eignevectors that form the orthogonal matrix .    The structure of singular value decompositions  Now that we have an understanding of what a singular value decomposition is and how to construct it, let's explore the ways in which a singular value decomposition reveals the underlying structure of the matrix. As we'll see, the matrices and in a singular value decomposition provide convenient bases for some important subspaces, such as the column and null spaces of the matrix. This observation will provide the key to some of our uses of these decompositions in the next section.    Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.       This activity shows how a singular value decomposition of a matrix encodes important information about its null and column spaces. More specifically, the left and right singular vectors provide orthonormal bases for and . This is one of the reasons that singular value decompositions are so useful.    Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .    Generally speaking, if the rank of an matrix is , then there are nonzero singular values and has the form The first columns of form an orthonormal basis for : and the last columns of form an orthonormal basis for :   Remember that says that and its transpose share the same singular values. Since the rank of a matrix equals its number of nonzero singular values, this means that , a fact that we cited back in .    For any matrix ,     If we have a singular value decomposition of an matrix , also tells us that the left singular vectors of are the right singular vectors of . Therefore, is the matrix whose columns are the right singular vectors of . This means that the last vectors form an orthonormal basis for . Therefore, the columns of provide orthonormal bases for and : This reflects the familiar fact that is the orthogonal complement of .  In the same way, is the matrix whose columns are the left singular vectors of , which means that the first vectors form an orthonormal basis for . Because the columns of are the rows of , this subspace is sometimes called the row space of and denoted . row space While we have yet to have an occasion to use , there are times when it is important to have an orthonormal basis for it, and a singular value decomposition provides just that. To summarize, the columns of provide orthonormal bases for and :   Considered altogether, the subspaces , , , and are called the four fundamental subspaces associated to . fundamental subspaces In addition to telling us the rank of a matrix, a singular value decomposition gives us orthonormal bases for all four fundamental subspaces.    Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .       When we previously outlined a procedure for finding a singular decomposition of an matrix , we found the left singular vectors using the expression . This produces left singular vectors , where . If , however, we still need to find the left singular vectors . tells us how to do that: because those vectors form an orthonormal basis for , we can find them by solving to obtain a basis for and applying the Gram-Schmidt algorithm.  We won't worry about this issue too much, however, as we will frequently use software to find singular value decompositions for us.    Reduced singular value decompositions  As we'll see in the next section, there are times when it is helpful to express a singular value decomposition in a slightly different form.    Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where        We call this a reduced singular value decomposition .   Reduced singular value decomposition  singular value decomposition  reduced   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .         In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is       Summary  This section has explored singular value decompositions, how to find them, and how they organize important information about a matrix.   A singular value decomposition of a matrix is a factorization where . The matrix has the same shape as , and its only nonzero entries are the singular values of , which appear in decreasing order on the diagonal. The matrices and are orthogonal and contain the left and right singular vectors, respectively, as their columns.    To find a singular value decomposition of a matrix, we construct the Gram matrix , which is symmetric. The singular values of are the square roots of the eigenvalues of , and the right singular vectors are the associated eigenvectors of . The left singular vectors are determined from the relationship .    A singular value decomposition reveals fundamental information about a matrix. For instance, the number of nonzero singular values is the rank of the matrix. The first left singular vectors form an orthonormal basis for with the remaining left singular vectors forming an orthonormal basis of . The first right singular vectors form an orthonormal basis for while the remaining right singular vectors form an orthonormal basis of .    If is a rank matrix, we can write a reduced singular value decomposition as where the columns of form an orthonormal basis for , the columns of form an orthonormal basis for , and is an diagonal, invertible matrix.        Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .            Find singular value decompositions for the following matrices:    .     .                                                                   Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.       Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .       Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .       Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exericse to write , where is in and is in .    Find the least squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .       Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.          If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.       If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .       Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.       Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.       "
},
{
  "id": "p-8039",
  "level": "2",
  "url": "sec-svd-intro.html#p-8039",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "singular value decomposition "
},
{
  "id": "exploration-28",
  "level": "2",
  "url": "sec-svd-intro.html#exploration-28",
  "type": "Preview Activity",
  "number": "7.4.1",
  "title": "",
  "body": "  Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of and in which direction does it occur?     What is the minimum value of and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.      "
},
{
  "id": "p-8054",
  "level": "2",
  "url": "sec-svd-intro.html#p-8054",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "singular values singular vectors "
},
{
  "id": "p-8055",
  "level": "2",
  "url": "sec-svd-intro.html#p-8055",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gram matrix "
},
{
  "id": "activity-101",
  "level": "2",
  "url": "sec-svd-intro.html#activity-101",
  "type": "Activity",
  "number": "7.4.2",
  "title": "",
  "body": "  The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .      "
},
{
  "id": "example-86",
  "level": "2",
  "url": "sec-svd-intro.html#example-86",
  "type": "Example",
  "number": "7.4.2",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition    "
},
{
  "id": "example-87",
  "level": "2",
  "url": "sec-svd-intro.html#example-87",
  "type": "Example",
  "number": "7.4.3",
  "title": "",
  "body": "  Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .   "
},
{
  "id": "activity-102",
  "level": "2",
  "url": "sec-svd-intro.html#activity-102",
  "type": "Activity",
  "number": "7.4.3",
  "title": "",
  "body": "  In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .      "
},
{
  "id": "example-svd-nonsquare",
  "level": "2",
  "url": "sec-svd-intro.html#example-svd-nonsquare",
  "type": "Example",
  "number": "7.4.4",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.   "
},
{
  "id": "theorem-svd",
  "level": "2",
  "url": "sec-svd-intro.html#theorem-svd",
  "type": "Theorem",
  "number": "7.4.5",
  "title": "The singular value decomposition.",
  "body": " The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.  "
},
{
  "id": "prop-svd-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-svd-transpose",
  "type": "Proposition",
  "number": "7.4.6",
  "title": "",
  "body": "  If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.   "
},
{
  "id": "activity-103",
  "level": "2",
  "url": "sec-svd-intro.html#activity-103",
  "type": "Activity",
  "number": "7.4.4",
  "title": "",
  "body": "  Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.      "
},
{
  "id": "example-89",
  "level": "2",
  "url": "sec-svd-intro.html#example-89",
  "type": "Example",
  "number": "7.4.7",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .   "
},
{
  "id": "prop-rank-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-rank-transpose",
  "type": "Proposition",
  "number": "7.4.8",
  "title": "",
  "body": "  For any matrix ,    "
},
{
  "id": "p-8173",
  "level": "2",
  "url": "sec-svd-intro.html#p-8173",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row space "
},
{
  "id": "p-8174",
  "level": "2",
  "url": "sec-svd-intro.html#p-8174",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "four fundamental subspaces "
},
{
  "id": "thm-four-subspaces",
  "level": "2",
  "url": "sec-svd-intro.html#thm-four-subspaces",
  "type": "Theorem",
  "number": "7.4.9",
  "title": "",
  "body": "  Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .      "
},
{
  "id": "activity-104",
  "level": "2",
  "url": "sec-svd-intro.html#activity-104",
  "type": "Activity",
  "number": "7.4.5",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where       "
},
{
  "id": "p-8202",
  "level": "2",
  "url": "sec-svd-intro.html#p-8202",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "reduced singular value decomposition "
},
{
  "id": "prop-reduced-svd",
  "level": "2",
  "url": "sec-svd-intro.html#prop-reduced-svd",
  "type": "Proposition",
  "number": "7.4.10",
  "title": "Reduced singular value decomposition.",
  "body": " Reduced singular value decomposition  singular value decomposition  reduced   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .      "
},
{
  "id": "example-90",
  "level": "2",
  "url": "sec-svd-intro.html#example-90",
  "type": "Example",
  "number": "7.4.11",
  "title": "",
  "body": "  In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is    "
},
{
  "id": "ex-7-4-1",
  "level": "2",
  "url": "sec-svd-intro.html#ex-7-4-1",
  "type": "Exercise",
  "number": "7.4.5.1",
  "title": "",
  "body": " Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .          "
},
{
  "id": "exercise-282",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-282",
  "type": "Exercise",
  "number": "7.4.5.2",
  "title": "",
  "body": " Find singular value decompositions for the following matrices:    .     .                                                                 "
},
{
  "id": "exercise-283",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-283",
  "type": "Exercise",
  "number": "7.4.5.3",
  "title": "",
  "body": " Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.     "
},
{
  "id": "exercise-284",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-284",
  "type": "Exercise",
  "number": "7.4.5.4",
  "title": "",
  "body": " Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .     "
},
{
  "id": "exercise-285",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-285",
  "type": "Exercise",
  "number": "7.4.5.5",
  "title": "",
  "body": " Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .     "
},
{
  "id": "exercise-286",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-286",
  "type": "Exercise",
  "number": "7.4.5.6",
  "title": "",
  "body": " Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exericse to write , where is in and is in .    Find the least squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .     "
},
{
  "id": "exercise-287",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-287",
  "type": "Exercise",
  "number": "7.4.5.7",
  "title": "",
  "body": " Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.     "
},
{
  "id": "exercise-288",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-288",
  "type": "Exercise",
  "number": "7.4.5.8",
  "title": "",
  "body": "    If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.     "
},
{
  "id": "exercise-289",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-289",
  "type": "Exercise",
  "number": "7.4.5.9",
  "title": "",
  "body": " If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .     "
},
{
  "id": "exercise-290",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-290",
  "type": "Exercise",
  "number": "7.4.5.10",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.     "
},
{
  "id": "exercise-291",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-291",
  "type": "Exercise",
  "number": "7.4.5.11",
  "title": "",
  "body": " Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.     "
},
{
  "id": "sec-svd-uses",
  "level": "1",
  "url": "sec-svd-uses.html",
  "type": "Section",
  "number": "7.5",
  "title": "Using Singular Value Decompositions",
  "body": " Using Singular Value Decompositions   We've now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces. This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.  Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions. Therefore, we'll take some time in this section to revisit some familiar issues, such as least squares problems and principal component analysis, while also looking at some new applications.    Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .         Least squares problems  Least squares problems, which we explored in , arise when we are confronted with an inconsistent linear system . Since there is no solution to the system, we instead find the vector minimizing the distance between and . That is, we find the vector , the least squares approximate solution, by solving where is the orthogonal projection of onto the column space of .  If we have a singular value decomposition , then the number of nonzero singular values tells us the rank of , and the first columns of form an orthonormal basis for . This basis may be used to project vectors onto and hence to solve least squares problems.  Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions. One new feature is that we need to declare our matrix to consist of floating point entries. We do this by including RDF inside the matrix definition, as illustrated in the following cell.     Consider the equation where    Find a singular value decomposition for using the Python cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing the matrix , consisting of the first columns of , the matrix , consisting of the first columns of , and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Python, then B[:, slice] and B[slice, :] can be used to extract columns or rows from B where slice enumerates the desired columns or rows. For instance, B[0:2, :]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                      This activity demonstrates the power of a singular value decomposition to find a least squares approximate solution for an equation . Because it immediately provides an orthonormal basis for , something that we've had to construct using the Gram-Schmidt process in the past, we can easily project onto , which results in a simple expression for .    If is a reduced singular value decomposition of , then a least squares approximate solution to is given by     If the columns of are linearly independent, then the equation has only one solution so there is a unique least squares approximate solution . Otherwise, the expression in produces the solution to having the shortest length.   Moore-Penrose psuedoinverse The matrix is known as the Moore-Penrose psuedoinverse of . When is invertible, .    Rank approximations  If we have a singular value decomposition for a matrix , we can form a sequence of matrices that approximate with increasing accuracy. This may feel familiar to calculus students who have seen the way in which a function can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.  We'll begin with a singular value decomposition of a rank matrix so that . To create the approximating matrix , we keep the first singular values and set the others to zero. For instance, if , we can form matrices and define and . Because has nonzero singular values, we know that . In fact, there is a sense in which is the closest matrix to among all rank matrices.    Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .       In this activity, the approximating matrix has rank because its singular value decomposition has nonzero singular values. We then saw how the difference between and the approximations decreases as increases, which means that the sequence forms better approximations as increases.  Another way to represent is with a reduced singular value decomposition so that where Notice that the rank matrix then has the form and that we can similarly write:   Given two vectors and , the matrix is called the outer product of and . (The dot product is sometimes called the inner product .) An outer product will always be a rank matrix so we see above how is obtained by adding together rank matrices, each of which gets us one step closer to the original matrix .    Principal component analysis  In , we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix from a demeaned data matrix and saw that the eigenvalues and eigenvectors of tell us about the variance of the dataset in different directions. We referred to the eigenvectors of as principal components and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset. As we added more principal components, we retained more information about the original dataset. This feels similar to the rank approximations we have just seen so let's explore the connection.  Suppose that we have a dataset with points, that represents the demeaned data matrix, that is a singular value decomposition, and that the singular values are are denoted as . It follows that the covariance matrix Notice that is a diagonal matrix whose diagonal entries are . Therefore, it follows that is an orthgonal diagonalization of showing that   the principal components of the dataset, which are the eigenvectors of , are given by the columns of . In other words, the left singular vectors of are the principal components of the dataset.    the variance in the direction of a principal component is the associated eigenvalue of and therefore        Let's revisit the iris data set that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command np.linalg.svd() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .       In our first encounter with principal component analysis, we began with a demeaned data matrix , formed the covariance matrix , and used the eigenvalues and eigenvectors of to project the demeaned data onto a smaller dimensional subspace. In this section, we have seen that a singular value decomposition of provides a more direct route: the left singular vectors of form the principal components and the approximating matrix represents the data points projected onto the subspace spanned by the first principal components. The coordinates of a projected demeaned data point are given by the columns of .    Image compressing and denoising  In addition to principal component analysis, the approximations of a matrix obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in . We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.    Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation          Several examples illustrating how the singular value decomposition compresses images are available at this page from Tim Baumann.     Analyzing Supreme Court cases  As we've seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors. We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices. A similar analysis appears in the paper A pattern analysis of the second Rehnquist U.S. Supreme Court by Lawrence Sirovich.  The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist. This is sometimes called the second Rehnquist court . The justices during this period were:  William Rehnquist  Antonin Scalia  Clarence Thomas  Anthony Kennedy  Sandra Day O'Connor  John Paul Stevens  David Souter  Ruth Bader Ginsburg  Stephen Breyer    During this time, there were 911 cases in which all nine judges voted. We would like to understand patterns in their voting.    Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor         Summary  This section has demonstrated some uses of the singular value decomposition. Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.   Because the first left singular vectors form an orthonormal basis for , a singular value decomposition provides a convenient way to project vectors onto and therefore to solve least squares problems.    A singular value decomposition of a rank matrix leads to a series of approximations of where In each case, is the rank matrix that is closest to .    If is a demeaned data matrix, the left singular vectors give the principal components of and the variance in the direction of a principal component can be simply expressed in terms of the corresponding singular value.    The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.    Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.        Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,        Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then        Remember that the outer product of two vector and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .       Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.       Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.       The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.       Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .       In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .       "
},
{
  "id": "exploration-29",
  "level": "2",
  "url": "sec-svd-uses.html#exploration-29",
  "type": "Preview Activity",
  "number": "7.5.1",
  "title": "",
  "body": "  Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .      "
},
{
  "id": "activity-105",
  "level": "2",
  "url": "sec-svd-uses.html#activity-105",
  "type": "Activity",
  "number": "7.5.2",
  "title": "",
  "body": "  Consider the equation where    Find a singular value decomposition for using the Python cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing the matrix , consisting of the first columns of , the matrix , consisting of the first columns of , and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Python, then B[:, slice] and B[slice, :] can be used to extract columns or rows from B where slice enumerates the desired columns or rows. For instance, B[0:2, :]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                     "
},
{
  "id": "prop-svd-ols",
  "level": "2",
  "url": "sec-svd-uses.html#prop-svd-ols",
  "type": "Proposition",
  "number": "7.5.1",
  "title": "",
  "body": "  If is a reduced singular value decomposition of , then a least squares approximate solution to is given by    "
},
{
  "id": "activity-106",
  "level": "2",
  "url": "sec-svd-uses.html#activity-106",
  "type": "Activity",
  "number": "7.5.3",
  "title": "",
  "body": "  Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .      "
},
{
  "id": "activity-107",
  "level": "2",
  "url": "sec-svd-uses.html#activity-107",
  "type": "Activity",
  "number": "7.5.4",
  "title": "",
  "body": "  Let's revisit the iris data set that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command np.linalg.svd() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .      "
},
{
  "id": "activity-108",
  "level": "2",
  "url": "sec-svd-uses.html#activity-108",
  "type": "Activity",
  "number": "7.5.5",
  "title": "",
  "body": "  Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation         "
},
{
  "id": "activity-109",
  "level": "2",
  "url": "sec-svd-uses.html#activity-109",
  "type": "Activity",
  "number": "7.5.6",
  "title": "",
  "body": "  Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor      "
},
{
  "id": "exercise-292",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-292",
  "type": "Exercise",
  "number": "7.5.7.1",
  "title": "",
  "body": " Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,      "
},
{
  "id": "exercise-293",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-293",
  "type": "Exercise",
  "number": "7.5.7.2",
  "title": "",
  "body": " Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then      "
},
{
  "id": "exercise-294",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-294",
  "type": "Exercise",
  "number": "7.5.7.3",
  "title": "",
  "body": " Remember that the outer product of two vector and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .     "
},
{
  "id": "exercise-295",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-295",
  "type": "Exercise",
  "number": "7.5.7.4",
  "title": "",
  "body": " Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.     "
},
{
  "id": "exercise-296",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-296",
  "type": "Exercise",
  "number": "7.5.7.5",
  "title": "",
  "body": " Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.     "
},
{
  "id": "exercise-297",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-297",
  "type": "Exercise",
  "number": "7.5.7.6",
  "title": "",
  "body": " The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.     "
},
{
  "id": "exercise-298",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-298",
  "type": "Exercise",
  "number": "7.5.7.7",
  "title": "",
  "body": " Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     "
},
{
  "id": "exercise-299",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-299",
  "type": "Exercise",
  "number": "7.5.7.8",
  "title": "",
  "body": " In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .     "
},
{
  "id": "app-notation",
  "level": "1",
  "url": "app-notation.html",
  "type": "Appendix",
  "number": "A",
  "title": "Notation",
  "body": " Notation   "
},
{
  "id": "subsection-138",
  "level": "1",
  "url": "subsection-138.html",
  "type": "Subsection",
  "number": "B.1",
  "title": "Accessing Python",
  "body": " Accessing Python  In addition to the SageMath cellls included throughout the book, there are a number of ways to access Python. Here are just a few.   There is a freely available Sage cell at .    You can save your Sage work by creating an account at and working in a Sage worksheet.    For createing documents that include Python code, jupter notebooks, or quarto documents work well.    VS Code can be configured to not only edit, but also execute Python code (and jupyter notebooks and quarto documents)     "
},
{
  "id": "subsection-139",
  "level": "1",
  "url": "subsection-139.html",
  "type": "Subsection",
  "number": "B.2",
  "title": "Packages and libraries for data science",
  "body": " Packages and libraries for data science  Python packages can be loaded using the import command.   import   We can load entire packages or load submodules or even individual objects. See below for some examples.  There are several add-on pacakges that are useful for linear algebra. The ones used here are part of the SciPy \"ecosystem of open-source software for mathematics, science, and engineering.\" Core packages in this suite include   NumPy ( import numpy as np )    numpy     NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I\/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.   Note: NumPy includes both the 2-dimensional arrays ( np.array() ) and matrices ( np.matrix() ). The numpy.matrix class makes certain things a bit more convenient, but we will follow SciPy advice: Despite its convenience, the use of the numpy.matrix is discouraged, since it adds nothing that cannot be accomplished with 2-D numpy.ndarray objects, and may lead to a confusion of which class is being used.     SciPy Linear Algebra\" ( from scipy import linalg )    scipy.linalg      linalg    scipy.linalg    SciPy  SciPy is a large library with many packages and submodules. We will primarily use scipy.linalg , which contains many linear algebra functions.  Note that np.linalg also exists. Here's what SciPy has to say about that:  \"scipy.linalg contains all the functions in numpy.linalg plus some other more advanced ones not contained in numpy.linalg. Another advantage of using scipy.linalg over numpy.linalg is that it is always compiled with BLAS\/LAPACK support, while for numpy this is optional. Therefore, the scipy version might be faster depending on how numpy was installed.  Therefore, unless you don’t want to add scipy as a dependency to your numpy program, use scipy.linalg instead of numpy.linalg.\"      Matplotlib ( import matplotlib.pyplot as plt )    matplotlib.pyplot     Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.      SymPy ( import sympy )    sympy     SymPy is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python.      pandas ( import pandas as pd )    pandas     pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.   The primary feature of pandas that we will use is its support for data frames . In a data frame, each row represents and observational unit and each column represents a variable. Different variables may be of different types and need not be numeric. pandas allows us to read data frames in from files of various formats (e.g., CSV files) and to perform data wrangling on the result.       seaborn.objects  and  plotly  provide alternatives to matplotlib for plotting.    seaborn.objects is \"a completely new interface for making seaborn plots.   seaborn.objects   It offers a more consistent and flexible API, comprising a collection of composable classes for transforming and plotting data. In contrast to the existing seaborn functions, the new interface aims to support end-to-end plot specification and customization without dropping down to matplotlib (although it will remain possible to do so if necessary).\"    Unlike seaborn.objects , Plotly is built on top of a   plotly   javascript graphics library rather than on top of Matplotlib. Plotly was designed with interactive graphics in mind from the start and provides interfaces for a number of languages, including python, R, and julia.     "
},
{
  "id": "p-8712",
  "level": "2",
  "url": "subsection-139.html#p-8712",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "data frames "
},
{
  "id": "subsec-frequently-used-python",
  "level": "1",
  "url": "subsec-frequently-used-python.html",
  "type": "Subsection",
  "number": "B.3",
  "title": "Frequently used Python commands",
  "body": " Frequently used Python commands  If some code that you write isn't working, check whether you are missing one of the required import statements. The following will be used frequently. Less frequently we will use some of the following:   To make things a bit easier in this book, we will often include a code chunks near the top of a section, like the ones above below, to load the packages we require. In an interactive document, these will be auto-evaluated. You can tell that a cell has already been evaluated by the green output box below the code and the lack of a button to execute the code.    Creating matrices  There are a couple of ways to create matrices. For instance, the matrix can be created in either of the two following ways.              Be aware that Python can treat mathematically equivalent matrices in different ways depending on how they are entered. For instance, the matrix has integer entries while has floating point entries. If any of the entries in a matrix are provided as floating point numbers, then all of the entries will be converted to floating point values.    Special matrices  The identity matrix can be created with the curiously named function np.eye() (or sympy.eye() ). This has nothing to do with eyes, but is a play on the pronunciation of . Notice that sympy creates a matrix with integer entries, but np produces a matrix with floating point entries. A diagonal matrix can be created from a list of its diagonal entries. For instance,     Reduced row echelon form  The reduced row echelon form of a matrix can be obtained using the rref() method after converting our matrix to a sympy.Matrix . For instance,     Vectors  NumPy arrays do not need to be 2-dimensional. A vector is defined by listing its components. Notice that the shape of v is a 1-tuple.     Addition  The + operator performs vector and matrix addition.      Multiplication  From a mathematical perspecitve, the * operator performs scalar multiplication of vectors and matrices.   Computationally, numpy is using something called broadcasting , which is more general than scalar multiplication. When two numpy arrays have different shapes, numpy starts from the end of the shape tuples and compares their values. If they match, that's good. But it is also OK if one of them is 1 (or non-existant, which amounts to nearly the samething) and the other is not. In this case, we can imagine duplicating the array with 1 in that dimension to fill out its shape to match the other. (NumPy does not actually do this duplication, since that would be ineffcient, but it is a good mental model for how the operation behaves.) Working from back to front, each axis is considered, and in the end, if the shapes are compatible, they can be treated as if they had the same shape. At that point, the opreation proceeds element by element in the expanded arrays.  You can find out much more about broadcasting at .  Broadcasting means that * cannot be used for for matrix-vector and matrix-matrix multiplication in the linear algebra sense. Instead we use @ .     Operations on vectors     The length of a vector v is found using scipy.linalg.norm() .   Actually, np.linalg.norm() can compute many different norms of both vectors and matrices.    dot product  The dot product of two vectors v and w is also computed using @         Operations on matrices    The transpose of a matrix A is obtained using np.transpose()      The inverse of a matrix A is obtained using either linalg.inv(A) . But for serious computational work, there is almost always something better than explicitly computing an inverse this way.    The determinant of A is linalg.det(A) .    An orthonormal basis for the null space is found with linalg.nullspace(A) .       Eigenvectors and eigenvalues     The eigenvalues of a matrix A can be found with linalg.eigvals(A) . The number of times that an eigenvalue appears in the list equals its multiplicity.    The eigenvectors of a matrix A can be found with linalg.eig(A) .    If can be diagonalized as , then provides the matrices D and P . Recall the sympy does symbolic computation. We can use evalf() to compute a numerical approximation.    The characteristic polynomial of sympy.Matrix  A is A.charpoly('x') .        Matrix factorizations     The factorization of a matrix gives matrices so that , where is a permutation matrix, is lower diagonal, and is upper diagonal.    A singular value decomposition is obtained with Note that s is not a matric but a 1-d array of singular values. If the matrix is needed, we can construct it with     The factorization of A is linalg.qr(A) provided that A .         "
},
{
  "id": "p-8728",
  "level": "2",
  "url": "subsec-frequently-used-python.html#p-8728",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "broadcasting "
},
{
  "id": "index-1",
  "level": "1",
  "url": "index-1.html",
  "type": "Index",
  "number": "",
  "title": "Index",
  "body": " Index   "
},
{
  "id": "colophon-2",
  "level": "1",
  "url": "colophon-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
