<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<div class="para logical">
<div class="para">This section introduced some types of least squares problems and a framework for working with them.</div>
<ul class="disc">
<li><div class="para">Given an inconsistent system <span class="process-math">\(A\xvec=\bvec\text{,}\)</span> we find <span class="process-math">\(\xhat\text{,}\)</span> the <dfn class="terminology">least squares approximate solution</dfn> by requiring that <span class="process-math">\(A\xhat\)</span> be as close to <span class="process-math">\(\bvec\)</span> as possible.  In other words, we solve <span class="process-math">\(A\xhat = \bhat\)</span> where <span class="process-math">\(\bhat = \proj{\bvec}{\col(A)}\)</span>
</div></li>
<li><div class="para logical">
<div class="para">One important application of this is fitting <dfn class="terminology">linear models</dfn> to data. In that context, we typically use different letters. Instead of <span class="process-math">\(A \xvec = \bvec\text{,}\)</span> you are more likely to see <span class="process-math">\(\yvec = X \betavec\text{.}\)</span> Here</div>
<ol class="decimal">
<li>
<span class="heading"><span class="title"><span class="process-math">\(\yvec\)</span> represents the <dfn class="terminology">response variable</dfn>..</span></span><div class="para">the variable we are trying to predict or estimate from other available data.</div>
</li>
<li>
<span class="heading"><span class="title"><span class="process-math">\(X\)</span> represents the <dfn class="terminology">data matrix</dfn>..</span></span><div class="para">Each row of <span class="process-math">\(X\)</span> represents an observation unit. Each column represents a data variable. Often we include a column of 1’s in <span class="process-math">\(X\text{.}\)</span> This  allows us to model an <dfn class="terminology">intercept</dfn> which represents a baseline amount that is  part of every prediction. <span class="process-math">\(X\)</span> may include the results of applying a function to some of the "raw data", after all, that’s just another variable.</div>
</li>
<li>
<span class="heading"><span class="title"><span class="process-math">\(\betavec = \begin{bmatrix}\beta_0\\\beta_1\\ \vdots \\ \beta_p \\\end{bmatrix}\)</span> represents the coefficients of the model..</span></span><div class="para"></div>
</li>
</ol>
</div></li>
<li>
<div class="para logical">
<div class="para">One way to find <span class="process-math">\(\xhat\)</span> with <span class="process-math">\(A \xhat = \bhat\)</span> is by solving the normal equations</div>
<div class="displaymath process-math">
\begin{equation*}
A^{\transpose}A\xhat = A^{\transpose}\bvec\text{.}
\end{equation*}
</div>
<div class="para">This is not our preferred method since numerical problems can arise.</div>
</div>
<div class="para logical">
<div class="para">The statistical version of the normal equation is</div>
<div class="displaymath process-math">
\begin{equation*}
X^{\transpose}X\betahat = A^{\transpose}\yvec\text{.}
\end{equation*}
</div>
</div>
</li>
<li>
<div class="para">A second way to find <span class="process-math">\(\xhat\)</span> with <span class="process-math">\(A \xhat = \bhat\)</span> uses a <span class="process-math">\(QR\)</span> factorization of <span class="process-math">\(A\text{.}\)</span>  If <span class="process-math">\(A=QR\text{,}\)</span> then <span class="process-math">\(\xhat = R^{-1}Q^{\transpose}\bvec\)</span> and finding <span class="process-math">\(R^{-1}\)</span> is computationally feasible since <span class="process-math">\(R\)</span> is upper triangular.  Alternatively, we can use backsubstitution to solve <span class="process-math">\(R \xhat = Q^{\transpose} \bvec\text{.}\)</span>
</div>
<div class="para">The statistical version of this is <span class="process-math">\(X = QR\)</span> and <span class="process-math">\(R \betahat = Q^{\transpose} \yvec\text{.}\)</span>
</div>
</li>
<li><div class="para">This technique may be applied widely and is useful for modeling data. We saw examples in this section where linear functions of several input variables and polynomials provided effective models for different datasets.</div></li>
<li><div class="para">A simple measure of the quality of the fit is the coefficient of determination <span class="process-math">\(R^2\)</span> though some care must be used in interpreting this number in context.  In particular, as models become more complex, <span class="process-math">\(R^2\)</span> generally increases because more flexible models can fit the data better. But they may be prone to overfitting. Our goal is generally not to fit the data at hand but to learn something of value about other data.</div></li>
</ul>
</div>
<span class="incontext"><a href="sec-least-squares.html#p-7034" class="internal">in-context</a></span>
</body>
</html>
