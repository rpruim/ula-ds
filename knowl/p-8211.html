<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<div class="para logical">
<div class="para">This section has explored <dfn class="terminology">principal component analysis</dfn> (PCA) as a technique to reduce the dimension of a dataset.  From the demeaned column-variate data matrix <span class="process-math">\(X\text{,}\)</span> we form the covariance matrix <span class="process-math">\(S_{XX} = \frac1{n-1} X^{\transpose}X\text{,}\)</span> where <span class="process-math">\(n\)</span> is the number of observational units.</div>
<ul class="disc">
<li><div class="para">The eigenvectors <span class="process-math">\(\uvec_1, \uvec_2, \ldots \uvec_m\text{,}\)</span> of <span class="process-math">\(S_{XX}\)</span> are called the <dfn class="terminology">principal components</dfn>. We arrange them so that their corresponding eigenvalues are in decreasing order.</div></li>
<li><div class="para">If <span class="process-math">\(W_p\)</span> is the subspace spanned by the first <span class="process-math">\(p\)</span> principal components, then the variance of the demeaned data projected onto <span class="process-math">\(W_p\)</span> is the sum of the first <span class="process-math">\(p\)</span> eigenvalues of <span class="process-math">\(S_{XX}\text{.}\)</span>  No other <span class="process-math">\(p\)</span>-dimensional subspace retains more variance when the data are projected onto it.</div></li>
<li><div class="para">If <span class="process-math">\(Q\)</span> is the matrix whose columns are the first <span class="process-math">\(p\)</span> principal components, then <span class="process-math">\(XQ\)</span> contains the column-variate data matrix, expressed in the basis <span class="process-math">\(\uvec_1,\ldots,\uvec_p\text{,}\)</span> of the data once projected onto <span class="process-math">\(W_p\text{.}\)</span>
</div></li>
<li><div class="para">Our goal is to use a number of principal components that is large enough to retain most of the variance in the dataset but small enough to be manageable.</div></li>
<li><div class="para">The advantage of principal components analysis is the resulting data reudction. The primary disadvantage is the prinicpal components may not be easily or naturally interpretable.</div></li>
</ul>
</div>
<span class="incontext"><a href="sec-pca.html#p-8211" class="internal">in-context</a></span>
</body>
</html>
