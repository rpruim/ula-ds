<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<div class="para">Somewhat counterintuitively, it is not always good to fit the data better. In an extreme case, if we have <span class="process-math">\(n\)</span> linearly independent<a href="" class="fn-knowl original" data-knowl="./knowl/fn-4-hidden.html" title="Footnote 3.1.1"><sup> 1 </sup></a> predictors (including <span class="process-math">\(\onevec\)</span>), then the model space is all of <span class="process-math">\(\real^n\text{,}\)</span> so  <span class="process-math">\(\yhat = \yvec\)</span> and the model fits the data perfectly. But a model that fits the data this well is almost surely <dfn class="terminology">overfitting</dfn> -- it is learning both the generalizable features of the data and the idiosyncrasies of the data.  Because of this, a "larger" model (one with a higher-dimensional model space) may be less useful at predicting new data than a "smaller" model that fits the data less well but generalizes better.</div>
<span class="incontext"><a href="sec-span.html#p-1522" class="internal">in-context</a></span>
</body>
</html>
