<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Sample statistics as linear algebra</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" Randall Pruim ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math",
    "renderActions": {
      "findScript": [
        10,
        function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        },
        ""
      ]
    }
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-sage",
  "linked": true,
  "linkKey": "linked-sage",
  "autoeval": false,
  "languages": [
    "sage"
  ],
  "evalButtonText": "Evaluate (Sage)"
});
</script><script>// Make *any* pre with class 'sagecell-python' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-python",
  "linked": true,
  "linkKey": "linked-python",
  "autoeval": false,
  "languages": [
    "python"
  ],
  "evalButtonText": "Evaluate (Python)"
});
</script><script>// Make *any* pre with class 'sagecell-python-autoeval' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-python-autoeval",
  "linked": true,
  "linkKey": "linked-python",
  "autoeval": true,
  "languages": [
    "python"
  ],
  "evalButtonText": "Evaluate (Python)",
  "hide": [
    "evalButton"
  ]
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.3/pretext_search.js"></script><link href="https://pretextbook.org/css/0.7/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.3</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.3/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.3/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.3/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link href="https://pretextbook.org/css/0.7/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra:</span> <span class="subtitle">Data Science Edition</span></a></h1>
<p class="byline">Randall Pruim</p>
</div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" class="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms" class="searchterms"></span>
</h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" aria-label="Show or hide table of contents"><span class="icon">‚ò∞</span><span class="name">Contents</span></button><a class="index-button button" href="index-1.html" title="Index"><span class="name">Index</span></a><button id="user-preferences-button" class="user-preferences-button button" title="Modify user preferences"><span id="avatarbutton" class="avatarbutton name">You!</span><div id="preferences_menu_holder" class="preferences_menu_holder hidden"><ol id="preferences_menu" class="preferences_menu" style="font-family: 'Roboto Serif', serif;">
<li data-env="avatar" tabindex="-1">Choose avatar<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden avatar">
<li data-val="You!" tabindex="-1">
<span id="theYou!" class="avatarcheck">‚úîÔ∏è</span>You!</li>
<li data-val="üò∫" tabindex="-1">
<span id="theüò∫" class="avatarcheck"></span>üò∫</li>
<li data-val="üë§" tabindex="-1">
<span id="theüë§" class="avatarcheck"></span>üë§</li>
<li data-val="üëΩ" tabindex="-1">
<span id="theüëΩ" class="avatarcheck"></span>üëΩ</li>
<li data-val="üê∂" tabindex="-1">
<span id="theüê∂" class="avatarcheck"></span>üê∂</li>
<li data-val="üêº" tabindex="-1">
<span id="theüêº" class="avatarcheck"></span>üêº</li>
<li data-val="üåà" tabindex="-1">
<span id="theüåà" class="avatarcheck"></span>üåà</li>
</ol>
</li>
<li data-env="fontfamily" tabindex="-1">Font family<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fontfamily">
<li data-val="face" data-change="OS" tabindex="-1" style="font-family: 'Open Sans'">
<span id="theOS" class="ffcheck">‚úîÔ∏è</span><span class="name">Open Sans</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
<li data-val="face" data-change="RS" tabindex="-1" style="font-family: 'Roboto Serif'">
<span id="theRS" class="ffcheck"></span><span class="name">Roboto Serif</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
</ol>
</li>
<li data-env="font" tabindex="-1">Adjust font<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fonts">
<li>Size</li>
<li><span id="thesize">12</span></li>
<li data-val="size" data-change="-1" tabindex="-1" style="font-size: 80%">Smaller</li>
<li data-val="size" data-change="1" tabindex="-1" style="font-size: 110%">Larger</li>
<li>Width</li>
<li><span id="thewdth">100</span></li>
<li data-val="wdth" data-change="-5" tabindex="-1" style="font-variation-settings: 'wdth' 60">narrower</li>
<li data-val="wdth" data-change="5" tabindex="-1" style="font-variation-settings: 'wdth' 150">wider</li>
<li>Weight</li>
<li><span id="thewght">400</span></li>
<li data-val="wght" data-change="-50" tabindex="-1" style="font-weight: 200">thinner</li>
<li data-val="wght" data-change="50" tabindex="-1" style="font-weight: 700">heavier</li>
<li>Letter spacing</li>
<li>
<span id="thelspace">0</span><span class="byunits">/200</span>
</li>
<li data-val="lspace" data-change="-1" tabindex="-1">closer</li>
<li data-val="lspace" data-change="1" tabindex="-1">f a r t h e r</li>
<li>Word spacing</li>
<li>
<span id="thewspace">0</span><span class="byunits">/50</span>
</li>
<li data-val="wspace" data-change="-1" tabindex="-1">smaller‚ÄÖgap‚ÄÉ</li>
<li data-val="wspace" data-change="1" tabindex="-1">larger‚ÄÉgap</li>
<li>Line Spacing</li>
<li>
<span id="theheight">135</span><span class="byunits">/100</span>
</li>
<li data-val="height" data-change="-5" tabindex="-1" style="line-height: 1">closer<br>together</li>
<li data-val="height" data-change="5" tabindex="-1" style="line-height: 1.75">further<br>apart</li>
</ol>
</li>
<li data-env="atmosphere" tabindex="-1">Light/dark mode<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden atmosphere">
<li data-val="default" tabindex="-1">
<span id="thedefault" class="atmospherecheck">‚úîÔ∏è</span>default</li>
<li data-val="pastel" tabindex="-1">
<span id="thepastel" class="atmospherecheck"></span>pastel</li>
<li data-val="darktwilight" tabindex="-1">
<span id="thedarktwilight" class="atmospherecheck"></span>twilight</li>
<li data-val="dark" tabindex="-1">
<span id="thedark" class="atmospherecheck"></span>dark</li>
<li data-val="darkmidnight" tabindex="-1">
<span id="thedarkmidnight" class="atmospherecheck"></span>midnight</li>
</ol>
</li>
<li data-env="ruler" tabindex="-1">Reading ruler<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden ruler">
<li data-val="none" tabindex="-1">
<span id="thenone" class="rulercheck">‚úîÔ∏è</span>none</li>
<li data-val="underline" tabindex="-1">
<span id="theunderline" class="rulercheck"></span>underline</li>
<li data-val="lunderline" tabindex="-1">
<span id="thelunderline" class="rulercheck"></span>L-underline</li>
<li data-val="greybar" tabindex="-1">
<span id="thegreybar" class="rulercheck"></span>grey bar</li>
<li data-val="lightbox" tabindex="-1">
<span id="thelightbox" class="rulercheck"></span>light box</li>
<li data-val="sunrise" tabindex="-1">
<span id="thesunrise" class="rulercheck"></span>sunrise</li>
<li data-val="sunriseunderline" tabindex="-1">
<span id="thesunriseunderline" class="rulercheck"></span>sunrise underline</li>
<li class="moveQ">Motion by:</li>
<li data-val="mouse" tabindex="-1">
<span id="themouse" class="motioncheck">‚úîÔ∏è</span>follow the mouse</li>
<li data-val="arrow" tabindex="-1">
<span id="thearrow" class="motioncheck"></span>up/down arrows - not yet</li>
<li data-val="eye" tabindex="-1">
<span id="theeye" class="motioncheck"></span>eye tracking - not yet</li>
</ol>
</li>
</ol></div></button><span class="treebuttons"><a class="previous-button button" href="chap7.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="up-button button" href="chap7.html" title="Up"><span class="icon">^</span><span class="name">Up</span></a><a class="next-button button" href="sec-symmetric-matrices.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a></span><div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" class="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\boldsymbol a}}
\newcommand{\bvec}{{\boldsymbol b}}
\newcommand{\cvec}{{\boldsymbol c}}
\newcommand{\dvec}{{\boldsymbol d}}
\newcommand{\dtil}{\widetilde{\boldsymbol d}}
\newcommand{\evec}{{\boldsymbol e}}
\newcommand{\fvec}{{\boldsymbol f}}
\newcommand{\mvec}{{\boldsymbol m}}
\newcommand{\nvec}{{\boldsymbol n}}
\newcommand{\pvec}{{\boldsymbol p}}
\newcommand{\qvec}{{\boldsymbol q}}
\newcommand{\rvec}{{\boldsymbol r}}
\newcommand{\svec}{{\boldsymbol s}}
\newcommand{\tvec}{{\boldsymbol t}}
\newcommand{\uvec}{{\boldsymbol u}}
\newcommand{\vvec}{{\boldsymbol v}}
\newcommand{\wvec}{{\boldsymbol w}}
\newcommand{\xvec}{{\boldsymbol x}}
\newcommand{\yvec}{{\boldsymbol y}}
\newcommand{\zvec}{{\boldsymbol z}}
\newcommand{\betavec}{{\boldsymbol \beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\zerovec}{{\boldsymbol 0}}
\newcommand{\onevec}{{\boldsymbol 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\atilde}{\tilde{\avec}}
\newcommand{\btilde}{\tilde{\bvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\xmean}{\overline{\xvec}}
\newcommand{\xbar}{\overline{\xvec}}
\newcommand{\xtilde}{\tilde{\xvec}}
\newcommand{\Xtilde}{\tilde{X}}
\newcommand{\yhat}{\widehat{\yvec}}
\newcommand{\ymean}{\overline{\yvec}}
\newcommand{\ybar}{\overline{\yvec}}
\newcommand{\ytilde}{\tilde{\yvec}}
\newcommand{\yperp}{\yvec^\perp}
\newcommand{\betahat}{\widehat{\betavec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\by}{\times}
\newcommand{\transpose}{\top}
\newcommand{\proj}[2]{\operatorname{proj}\left(#1 \to #2\right)}
\newcommand{\projsub}[2]{\operatorname{proj}_{#2}(#1)}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural">
<li>
<div class="toc-item"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="colophon-1.html" class="internal"><span class="title">Colophon</span></a></div></li>
<li><div class="toc-item"><a href="preface-1.html" class="internal"><span class="title">Our goals -- Preface to David Austin‚Äôs original edition</span></a></div></li>
<li><div class="toc-item"><a href="preface-2.html" class="internal"><span class="title">What‚Äôs different in the data science edition?</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap1.html" class="internal"><span class="codenumber">1</span> <span class="title">Scalars, Vectors and Matrices</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-vectors.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsection-1" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Three ways to think about vectors</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-vectors.html#subsection-2" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Vector operations: scalar multiplication and vector addition.</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-scalar-multiplication" class="internal"><span class="codenumber">1.1.2.1</span> <span class="title">Scalar Multiplication</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-addition" class="internal"><span class="codenumber">1.1.2.2</span> <span class="title">Vector addition</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-properties" class="internal"><span class="codenumber">1.1.2.3</span> <span class="title">Mathematical properties of vector operations</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-3" class="internal"><span class="codenumber">1.1.3</span> <span class="title">The (Euclidean) length of a vector</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-4" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Summary</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-vectors-in-python.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Vectors in Python</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-intro-to-python" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Introduction to Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-numpy-vectors" class="internal"><span class="codenumber">1.2.2</span> <span class="title"><code class="code-inline tex2jax_ignore">numpy</code> vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-vector-length-numpy" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Vector length</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsection-8" class="internal"><span class="codenumber">1.2.4</span> <span class="title">Plotting vectors</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-combos-of-vectors.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Linear combinations of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#subsection-9" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#exercises-1" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-matrices.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrices.html#subsec-matrices-and-their-uses" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Matrices and their uses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-11" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Scalar multiplication and addition of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-12" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Matrix-vector multiplication and linear combinations</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-13" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Matrix-vector multiplication and linear systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#sec-matrices-in-python" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Matrices in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-15" class="internal"><span class="codenumber">1.4.6</span> <span class="title">Matrix-matrix products</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsec-special-matrices" class="internal"><span class="codenumber">1.4.7</span> <span class="title">Some special types of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-17" class="internal"><span class="codenumber">1.4.8</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#exercises-2" class="internal"><span class="codenumber">1.4.9</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-tensors.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Tensors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-tensors.html#subsec-numpy-tensors" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Tensors in NumPy</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-axes" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Aggregation and Axes</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-ndarray-append" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expanding an array</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-broadcasting" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Broadcasting</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#exercises-1-5" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap2.html" class="internal"><span class="codenumber">2</span> <span class="title">Systems of equations: Solving <span class="process-math">\(A \xvec = \bvec\)</span></span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-expect.html" class="internal"><span class="codenumber">2.1</span> <span class="title">What can we expect</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-expect.html#subsection-22" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Some simple examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-23" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Systems of linear equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-24" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#exercises-4" class="internal"><span class="codenumber">2.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-finding-solutions.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Finding solutions to linear systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-25" class="internal"><span class="codenumber">2.2.1</span> <span class="title">Gaussian elimination</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-26" class="internal"><span class="codenumber">2.2.2</span> <span class="title">Augmented matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-27" class="internal"><span class="codenumber">2.2.3</span> <span class="title">Reduced row echelon form</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsec-solving-matrix-equations" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Solving matrix equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-29" class="internal"><span class="codenumber">2.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#exercises-5" class="internal"><span class="codenumber">2.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-python-introduction.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Computational Linear Algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-30" class="internal"><span class="codenumber">2.3.1</span> <span class="title">Reduced row echelon form in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-linalg-solve" class="internal"><span class="codenumber">2.3.2</span> <span class="title">np.linalg.solve()</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-compute-effort" class="internal"><span class="codenumber">2.3.3</span> <span class="title">Computational effort</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-33" class="internal"><span class="codenumber">2.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-6" class="internal"><span class="codenumber">2.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pivots.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Pivots and their relationship to solution spaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pivots.html#subsection-34" class="internal"><span class="codenumber">2.4.1</span> <span class="title">The existence of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-35" class="internal"><span class="codenumber">2.4.2</span> <span class="title">The uniqueness of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-36" class="internal"><span class="codenumber">2.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#exercises-7" class="internal"><span class="codenumber">2.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap3.html" class="internal"><span class="codenumber">3</span> <span class="title">Linear combinations and transformations</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-span.html" class="internal"><span class="codenumber">3.1</span> <span class="title">The span of a set of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-span.html#subsection-37" class="internal"><span class="codenumber">3.1.1</span> <span class="title">The span of a set of vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-38" class="internal"><span class="codenumber">3.1.2</span> <span class="title">Pivot positions and span</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsec-span-and-linear-models" class="internal"><span class="codenumber">3.1.3</span> <span class="title">Span and linear models</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-40" class="internal"><span class="codenumber">3.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#exercises-8" class="internal"><span class="codenumber">3.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-dep.html" class="internal"><span class="codenumber">3.2</span> <span class="title">Linear independence</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-41" class="internal"><span class="codenumber">3.2.1</span> <span class="title">Linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-42" class="internal"><span class="codenumber">3.2.2</span> <span class="title">How to recognize linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-43" class="internal"><span class="codenumber">3.2.3</span> <span class="title">Homogeneous equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-44" class="internal"><span class="codenumber">3.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#exercises-9" class="internal"><span class="codenumber">3.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-trans.html" class="internal"><span class="codenumber">3.3</span> <span class="title">Matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-matrix-trans" class="internal"><span class="codenumber">3.3.1</span> <span class="title">Matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-linear-trans" class="internal"><span class="codenumber">3.3.2</span> <span class="title">Linear transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-47" class="internal"><span class="codenumber">3.3.3</span> <span class="title">Composing matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-dynamical-systems" class="internal"><span class="codenumber">3.3.4</span> <span class="title">Discrete Dynamical Systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-49" class="internal"><span class="codenumber">3.3.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#exercises-10" class="internal"><span class="codenumber">3.3.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transforms-geom.html" class="internal"><span class="codenumber">3.4</span> <span class="title">The geometry of matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-50" class="internal"><span class="codenumber">3.4.1</span> <span class="title">The geometry of <span class="process-math">\(2\by2\)</span> matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-51" class="internal"><span class="codenumber">3.4.2</span> <span class="title">Matrix transformations and computer animation</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-52" class="internal"><span class="codenumber">3.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#exercises-11" class="internal"><span class="codenumber">3.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap4.html" class="internal"><span class="codenumber">4</span> <span class="title">Invertibility, bases, and coordinate systems</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-matrix-inverse.html" class="internal"><span class="codenumber">4.1</span> <span class="title">Invertibility</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-53" class="internal"><span class="codenumber">4.1.1</span> <span class="title">Invertible matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-54" class="internal"><span class="codenumber">4.1.2</span> <span class="title">Solving equations with an inverse</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsec-finding-inverses" class="internal"><span class="codenumber">4.1.3</span> <span class="title">Finding inverses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-56" class="internal"><span class="codenumber">4.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#exercises-12" class="internal"><span class="codenumber">4.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="subsec-triangular-invertible.html" class="internal"><span class="codenumber">4.2</span> <span class="title">Triangular matrices and Gaussian elimination</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-triangular-matrices" class="internal"><span class="codenumber">4.2.1</span> <span class="title">Triangular matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-elementary-matrices" class="internal"><span class="codenumber">4.2.2</span> <span class="title">Elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsection-59" class="internal"><span class="codenumber">4.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#exercises-13" class="internal"><span class="codenumber">4.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-bases.html" class="internal"><span class="codenumber">4.3</span> <span class="title">Bases and coordinate systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-bases.html#subsection-60" class="internal"><span class="codenumber">4.3.1</span> <span class="title">Bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-61" class="internal"><span class="codenumber">4.3.2</span> <span class="title">Coordinate systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-62" class="internal"><span class="codenumber">4.3.3</span> <span class="title">Examples of bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-63" class="internal"><span class="codenumber">4.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#exercises-14" class="internal"><span class="codenumber">4.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-jpeg.html" class="internal"><span class="codenumber">4.4</span> <span class="title">Image compression</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-64" class="internal"><span class="codenumber">4.4.1</span> <span class="title">Color models</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-65" class="internal"><span class="codenumber">4.4.2</span> <span class="title">The JPEG compression algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-66" class="internal"><span class="codenumber">4.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#exercises-15" class="internal"><span class="codenumber">4.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-determinants.html" class="internal"><span class="codenumber">4.5</span> <span class="title">Determinants</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsection-67" class="internal"><span class="codenumber">4.5.1</span> <span class="title">Determinants of <span class="process-math">\(2\by2\)</span> matrices</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-determinants.html#subsec-determinants-larger-matrices" class="internal"><span class="codenumber">4.5.2</span> <span class="title">Determinants of larger matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsubsection-4" class="internal"><span class="codenumber">4.5.2.1</span> <span class="title">Determinants of elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsubsec-rref-to-determinants" class="internal"><span class="codenumber">4.5.2.2</span> <span class="title">Using RREF to compute determinants</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsubsection-6" class="internal"><span class="codenumber">4.5.2.3</span> <span class="title">Cofactor expansions</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-69" class="internal"><span class="codenumber">4.5.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#exercises-16" class="internal"><span class="codenumber">4.5.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-subspaces.html" class="internal"><span class="codenumber">4.6</span> <span class="title">Subspaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-70" class="internal"><span class="codenumber">4.6.1</span> <span class="title">Subspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-71" class="internal"><span class="codenumber">4.6.2</span> <span class="title">The column space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-72" class="internal"><span class="codenumber">4.6.3</span> <span class="title">The null space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-73" class="internal"><span class="codenumber">4.6.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#exercises-17" class="internal"><span class="codenumber">4.6.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gaussian-revisited.html" class="internal"><span class="codenumber">4.7</span> <span class="title">Partial pivoting and LU factorizations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal"><span class="codenumber">4.7.1</span> <span class="title">Partial pivoting</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-75" class="internal"><span class="codenumber">4.7.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-76" class="internal"><span class="codenumber">4.7.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#exercises-18" class="internal"><span class="codenumber">4.7.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap5.html" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-eigen-intro.html" class="internal"><span class="codenumber">5.1</span> <span class="title">An introduction to eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-77" class="internal"><span class="codenumber">5.1.1</span> <span class="title">A few examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsec-eigen-use" class="internal"><span class="codenumber">5.1.2</span> <span class="title">The usefulness of eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-79" class="internal"><span class="codenumber">5.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#exercises-19" class="internal"><span class="codenumber">5.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-find.html" class="internal"><span class="codenumber">5.2</span> <span class="title">Finding eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-80" class="internal"><span class="codenumber">5.2.1</span> <span class="title">The characteristic polynomial</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-81" class="internal"><span class="codenumber">5.2.2</span> <span class="title">Finding eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-82" class="internal"><span class="codenumber">5.2.3</span> <span class="title">The characteristic polynomial and the dimension of eigenspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-83" class="internal"><span class="codenumber">5.2.4</span> <span class="title">Using Python to find eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-84" class="internal"><span class="codenumber">5.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#exercises-20" class="internal"><span class="codenumber">5.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-diag.html" class="internal"><span class="codenumber">5.3</span> <span class="title">Diagonalization, similarity, and powers of a matrix</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-85" class="internal"><span class="codenumber">5.3.1</span> <span class="title">Diagonalization of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-86" class="internal"><span class="codenumber">5.3.2</span> <span class="title">Powers of a diagonalizable matrix</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-87" class="internal"><span class="codenumber">5.3.3</span> <span class="title">Similarity and complex eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-88" class="internal"><span class="codenumber">5.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#exercises-21" class="internal"><span class="codenumber">5.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-dynamical.html" class="internal"><span class="codenumber">5.4</span> <span class="title">Dynamical systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-89" class="internal"><span class="codenumber">5.4.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-90" class="internal"><span class="codenumber">5.4.2</span> <span class="title">Classifying dynamical systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-91" class="internal"><span class="codenumber">5.4.3</span> <span class="title">A <span class="process-math">\(3\by3\)</span> system</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-92" class="internal"><span class="codenumber">5.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#exercises-22" class="internal"><span class="codenumber">5.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-stochastic.html" class="internal"><span class="codenumber">5.5</span> <span class="title">Markov chains and Google‚Äôs PageRank algorithm</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-93" class="internal"><span class="codenumber">5.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-94" class="internal"><span class="codenumber">5.5.2</span> <span class="title">Markov chains</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsec-google" class="internal"><span class="codenumber">5.5.3</span> <span class="title">Google‚Äôs PageRank algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-96" class="internal"><span class="codenumber">5.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#exercises-23" class="internal"><span class="codenumber">5.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-power-method.html" class="internal"><span class="codenumber">5.6</span> <span class="title">Finding eigenvectors numerically</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-power-method.html#subsection-97" class="internal"><span class="codenumber">5.6.1</span> <span class="title">The power method</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-98" class="internal"><span class="codenumber">5.6.2</span> <span class="title">Finding other eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-99" class="internal"><span class="codenumber">5.6.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#exercises-24" class="internal"><span class="codenumber">5.6.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap6.html" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-dot-product.html" class="internal"><span class="codenumber">6.1</span> <span class="title">The dot product</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dot-product.html#sec-projections-and-dot-products" class="internal"><span class="codenumber">6.1.1</span> <span class="title">Projections and dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsec-computing-dot-products" class="internal"><span class="codenumber">6.1.2</span> <span class="title">Computing dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-102" class="internal"><span class="codenumber">6.1.3</span> <span class="title"><span class="process-math">\(k\)</span>-means clustering</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-103" class="internal"><span class="codenumber">6.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#exercises-25" class="internal"><span class="codenumber">6.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transpose.html" class="internal"><span class="codenumber">6.2</span> <span class="title">Orthogonal complements and the matrix transpose</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transpose.html#subsection-104" class="internal"><span class="codenumber">6.2.1</span> <span class="title">Orthogonal complements</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-105" class="internal"><span class="codenumber">6.2.2</span> <span class="title">The matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-106" class="internal"><span class="codenumber">6.2.3</span> <span class="title">Properties of the matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-107" class="internal"><span class="codenumber">6.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#exercises-26" class="internal"><span class="codenumber">6.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-orthogonal-bases.html" class="internal"><span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-108" class="internal"><span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-109" class="internal"><span class="codenumber">6.3.2</span> <span class="title">Orthogonal projections</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-110" class="internal"><span class="codenumber">6.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#exercises-27" class="internal"><span class="codenumber">6.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gram-schmidt.html" class="internal"><span class="codenumber">6.4</span> <span class="title">Finding orthogonal bases</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-111" class="internal"><span class="codenumber">6.4.1</span> <span class="title">Gram-Schmidt orthogonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-112" class="internal"><span class="codenumber">6.4.2</span> <span class="title"><span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-113" class="internal"><span class="codenumber">6.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#exercises-28" class="internal"><span class="codenumber">6.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-least-squares.html" class="internal"><span class="codenumber">6.5</span> <span class="title">Least squares methods</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-114" class="internal"><span class="codenumber">6.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-linear-model-framework" class="internal"><span class="codenumber">6.5.2</span> <span class="title">The linear model framework</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-116" class="internal"><span class="codenumber">6.5.3</span> <span class="title">Solving least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-117" class="internal"><span class="codenumber">6.5.4</span> <span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-118" class="internal"><span class="codenumber">6.5.5</span> <span class="title">Polynomial Regression</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-skl-lm" class="internal"><span class="codenumber">6.5.6</span> <span class="title">Fitting linear models with standard tools</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-120" class="internal"><span class="codenumber">6.5.7</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#exercises-29" class="internal"><span class="codenumber">6.5.8</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap7.html" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a></div>
<ul class="structural">
<li class="active">
<div class="toc-item"><a href="sec-variance-covariance.html" class="internal"><span class="codenumber">7.1</span> <span class="title">Sample statistics as linear algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-variance-covariance.html#subsec-sample-mean" class="internal"><span class="codenumber">7.1.1</span> <span class="title">Sample mean</span></a></div></li>
<li><div class="toc-item"><a href="sec-variance-covariance.html#subsec-variance-covariance" class="internal"><span class="codenumber">7.1.2</span> <span class="title">Sample variance and covariance</span></a></div></li>
<li><div class="toc-item"><a href="sec-variance-covariance.html#subsection-123" class="internal"><span class="codenumber">7.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-variance-covariance.html#exercises-30" class="internal"><span class="codenumber">7.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-symmetric-matrices.html" class="internal"><span class="codenumber">7.2</span> <span class="title">Symmetric matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-124" class="internal"><span class="codenumber">7.2.1</span> <span class="title">Symmetric matrices and orthogonal diagonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-125" class="internal"><span class="codenumber">7.2.2</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#exercises-31" class="internal"><span class="codenumber">7.2.3</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-quadratic-forms.html" class="internal"><span class="codenumber">7.3</span> <span class="title">Quadratic forms</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-126" class="internal"><span class="codenumber">7.3.1</span> <span class="title">Quadratic forms</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-127" class="internal"><span class="codenumber">7.3.2</span> <span class="title">Definite symmetric matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-128" class="internal"><span class="codenumber">7.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#exercises-32" class="internal"><span class="codenumber">7.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pca.html" class="internal"><span class="codenumber">7.4</span> <span class="title">Principal Component Analysis</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pca.html#subsection-129" class="internal"><span class="codenumber">7.4.1</span> <span class="title">Themes of Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-130" class="internal"><span class="codenumber">7.4.2</span> <span class="title">Using Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-131" class="internal"><span class="codenumber">7.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#exercises-33" class="internal"><span class="codenumber">7.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-intro.html" class="internal"><span class="codenumber">7.5</span> <span class="title">Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-132" class="internal"><span class="codenumber">7.5.1</span> <span class="title">Finding singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-133" class="internal"><span class="codenumber">7.5.2</span> <span class="title">The structure of singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-134" class="internal"><span class="codenumber">7.5.3</span> <span class="title">Reduced singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-135" class="internal"><span class="codenumber">7.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#exercises-34" class="internal"><span class="codenumber">7.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-uses.html" class="internal"><span class="codenumber">7.6</span> <span class="title">Using Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-136" class="internal"><span class="codenumber">7.6.1</span> <span class="title">Least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-137" class="internal"><span class="codenumber">7.6.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-138" class="internal"><span class="codenumber">7.6.3</span> <span class="title">Principal component analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-139" class="internal"><span class="codenumber">7.6.4</span> <span class="title">Image compressing and denoising</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-140" class="internal"><span class="codenumber">7.6.5</span> <span class="title">Analyzing Supreme Court cases</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-141" class="internal"><span class="codenumber">7.6.6</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#exercises-35" class="internal"><span class="codenumber">7.6.7</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="backmatter.html" class="internal"><span class="title">Back Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="app-notation.html" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></div></li>
<li>
<div class="toc-item"><a href="app-python-reference.html" class="internal"><span class="codenumber">B</span> <span class="title">Python Reference</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsection-142.html" class="internal"><span class="codenumber">B.1</span> <span class="title">Accessing Python</span></a></div></li>
<li><div class="toc-item"><a href="subsection-143.html" class="internal"><span class="codenumber">B.2</span> <span class="title">Packages and libraries for data science</span></a></div></li>
<li><div class="toc-item"><a href="subsec-frequently-used-python.html" class="internal"><span class="codenumber">B.3</span> <span class="title">Frequently used Python commands</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="index-1.html" class="internal"><span class="title">Index</span></a></div></li>
<li><div class="toc-item"><a href="colophon-2.html" class="internal"><span class="title">Colophon</span></a></div></li>
</ul>
</li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-variance-covariance"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">7.1</span><span class="space"> </span><span class="title">Sample statistics as linear algebra</span>
</h2>
<section class="introduction" id="introduction-41"><div class="para logical" id="p-7351">
<div class="para">If you have worked with data before, you have likely encountered some of the more common sample statistics:</div>
<ul class="disc">
<li id="li-4932">
<span class="heading"><span class="title">Sample mean.</span></span><div class="para logical" id="p-7352"><div class="displaymath process-math">
\begin{equation*}
\overline{x} = \frac{1}{n} \sum x_i\text{.}
\end{equation*}
</div></div>
</li>
<li id="li-4933">
<span class="heading"><span class="title">Sample variance.</span></span><div class="para logical" id="p-7353"><div class="displaymath process-math">
\begin{equation*}
s_x^2 = \frac{1}{n-1} \sum (x_i - \overline{x})^2\text{.}
\end{equation*}
</div></div>
</li>
<li id="li-4934">
<span class="heading"><span class="title">Sample covariance.</span></span><div class="para logical" id="p-7354"><div class="displaymath process-math">
\begin{equation*}
s_{xy} = \frac{1}{n-1} \sum (x_i - \overline{x})(y_i - \overline{y})\text{.}
\end{equation*}
</div></div>
</li>
</ul>
<div class="para">Each of these can be interpreted as vector and matrix operations, and doing so can help us gain some additional insights.</div>
</div> <div class="para" id="p-7355">Throughout this section we will use the typical convention of letting <span class="process-math">\(n\)</span> be the number of cases or subjects in a data set and representing a data set with <span class="process-math">\(n\)</span> cases or subjects and <span class="process-math">\(p\)</span> variables as an <span class="process-math">\(n \by p\)</span> matrix.  This arrangment is sometimes called <dfn class="terminology">column-variate form</dfn> because the variables are in columns. This is by far the more common way to represent data as a matrix in data science, but you may also encounter matrices in <dfn class="terminology">row-variate form</dfn>. A row-variate matrix is just the transpose of a column-variate matrix: Cases are in the columns, variables in the rows.</div></section><section class="subsection" id="subsec-sample-mean"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.1.1</span><span class="space"> </span><span class="title">Sample mean</span>
</h3>
<div class="para logical" id="p-7356">
<div class="para">If we re-express the sample mean using s a sum of products</div>
<div class="displaymath process-math">
\begin{equation*}
\overline{x} = \frac{1}{n} \sum x_i = \frac{1}{n} \sum 1 x_i\text{,}
\end{equation*}
</div>
<div class="para">then we can intrepret the sum of products as a dot product, and equivalently as a matrix product</div>
<div class="displaymath process-math">
\begin{equation*}
\overline{x} 
= \frac{1}{n} \sum 1 x_i 
= \frac{1}{n} \onevec \cdot \xvec 
= \frac{1}{n} \onevec^{\transpose} \xvec \text{.}
\end{equation*}
</div>
<div class="para">Here we are expressing the data variable as a vector</div>
<div class="displaymath process-math">
\begin{equation*}
\xvec = \fourvec{x_1}{x_2}{\vdots}{x_n}
\end{equation*}
</div>
<div class="para">and using <span class="process-math">\(\onevec\)</span> to represent the <span class="process-math">\(n\)</span>-dimensional vector consisting entirely of 1‚Äôs:</div>
<div class="displaymath process-math">
\begin{equation*}
\onevec = \fourvec11{\vdots}1\text{.}
\end{equation*}
</div>
</div>
<div class="para logical" id="p-7357">
<div class="para">It is often convenient to work with <dfn class="terminology">demeaned</dfn> data, also called <dfn class="terminology">deviation scores</dfn>.  We obtained a demeaned vector by subtracting the mean from each entry in the vector.</div>
<div class="displaymath process-math">
\begin{equation*}
\xtilde = \xvec - \overline{x} \onevec = \xvec - \xmean\text{.}
\end{equation*}
</div>
<div class="para">Take note of the distinction between the scalar <span class="process-math">\(\overline{x}\)</span> and the vector</div>
<div class="displaymath process-math">
\begin{equation*}
\xmean = \overline{x} \onevec = \fourvec{\overline{x}}{\overline{x}}{\vdots}{\overline{x}}\text{.}
\end{equation*}
</div>
</div>
<article class="activity project-like" id="activity-90"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.1</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7358">
<div class="para">A little matrix algebra shows that the vector of demaned values is a linear transformation of the original data <span class="process-math">\(\xvec\text{:}\)</span>
</div>
<div class="displaymath process-math" id="md-40">
\begin{align*}
\xtilde \amp = \xvec - \onevec \xbar \\
\amp = \xvec - \onevec \frac{\onevec^{\transpose} \xvec}{n}\\
\amp = \xvec - \frac{\onevec \onevec^{\transpose}}{n} \xvec\\
\amp = \left(I - \frac{\onevec \onevec^{\transpose}}{n} \right) \xvec\\
\amp = \left(I - P \right) \xvec\\
\amp = Q \xvec
\end{align*}
</div>
<div class="para">where</div>
<div class="displaymath process-math" id="md-41">
\begin{align*}
P \amp = \frac{\onevec \onevec^{\transpose}}{n}\\
Q \amp = I - P \text{.}
\end{align*}
</div>
</div> <div class="para logical" id="p-7359"><ol class="decimal">
<li id="li-4935"><div class="para" id="p-7360">What is the shape of the matrix <span class="process-math">\(P\)</span> and what the entries in <span class="process-math">\(P\text{?}\)</span>
</div></li>
<li id="li-4936"><div class="para" id="p-7361">What are the entries in the matrix <span class="process-math">\(Q\text{?}\)</span> Is <span class="process-math">\(Q\)</span> symmetric?</div></li>
<li id="li-4937"><div class="para" id="p-7362"> Show that <span class="process-math">\(P^2 = P\text{.}\)</span>  Such a matrix is called <dfn class="terminology">idempotent</dfn>. Why must an idempotent matrix be square?</div></li>
<li id="li-4938"><div class="para" id="p-7363">Show that if <span class="process-math">\(A\)</span> is idempotent, then <span class="process-math">\(I - A\)</span> is also idempotent. This implies that <span class="process-math">\(Q\)</span> is also idempotent. What does that fact that <span class="process-math">\(Q\)</span> is idempotent tell you about the demeaning operation?</div></li>
<li id="li-4939"><div class="para" id="p-7364">Show that <span class="process-math">\(Q \onevec = \zerovec\text{.}\)</span>
</div></li>
<li id="li-4940"><div class="para" id="p-7365">Are the columns of  <span class="process-math">\(Q\)</span> orthogonal?</div></li>
<li id="li-4941"><div class="para" id="p-7366">Is <span class="process-math">\(Q\)</span> an orthogonal matrix?</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-363" id="solution-363"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-363"><div class="solution solution-like"><div class="para logical" id="p-7367"><ol class="decimal">
<li id="li-4942"><div class="para" id="p-7368">
<span class="process-math">\(P\)</span> is an <span class="process-math">\(n \by n\)</span> matrix and every entry is a <span class="process-math">\(\frac{1}{n}\text{.}\)</span>
</div></li>
<li id="li-4943"><div class="para" id="p-7369">The diagonal entries are <span class="process-math">\(1 - \frac{1/n} = \frac{n-1}{n}\text{.}\)</span> The off-diagonal entries are <span class="process-math">\(-\frac{1}{n}\text{.}\)</span> So <span class="process-math">\(Q\)</span> is symmetric.</div></li>
<li id="li-4944"><div class="para logical" id="p-7370"><div class="displaymath process-math">
\begin{equation*}
P^2 
= \frac{\onevec \onevec^{\transpose} \onevec \onevec^{\transpose}}{n^2}
= \frac{\onevec (\onevec^{\transpose} \onevec) \onevec^{\transpose}}{n^2}
= \frac{\onevec n \onevec^{\transpose}}{n^2}
= \frac{\onevec \onevec^{\transpose}}{n}
= P\text{.}
\end{equation*}
</div></div></li>
<li id="li-4945"><div class="para logical" id="p-7371">
<div class="para">If <span class="process-math">\(A\)</span> is idempotent, then</div>
<div class="displaymath process-math">
\begin{equation*}
(I - A)(I-A) 
= I^2 - IA - AI + A^2 
= I - A - A + A 
= I - A\text{.}
\end{equation*}
</div>
<div class="para">Since <span class="process-math">\(Q = I - P\text{,}\)</span> <span class="process-math">\(Q\)</span> is idempontent.  This means that <span class="process-math">\(\widetilde{\left( \xtilde \right)} = \xtilde\text{.}\)</span> That is, demeaning an alredy demeaned vector doesn‚Äôt change it.</div>
</div></li>
<li id="li-4946"><div class="para" id="p-7372">
<span class="process-math">\(P \onevec = 
1 \threevec{1/n}{\vdots}{1/n} +  
1 \threevec{1/n}{\vdots}{1/n} +  
\cdots
1 \threevec{1/n}{\vdots}{1/n} 
=
\threevec{n/n}{\vdots}{n/n}
= \onevec
\text{,}\)</span> So <span class="process-math">\(Q \onevec = (I - P)\onvec = \onvec - P\onvec = \onevec - \onevec = \zerovec\text{.}\)</span>
</div></li>
<li id="li-4947"><div class="para" id="p-7373">If <span class="process-math">\(i \neq j\text{,}\)</span> then <span class="process-math">\(Q_{i \cdot} \cdot Q_{j \cdot} 
=  2 \frac{n-1}{n} \frac{-1}{n} + (n-2) \frac{-1}{n} \frac{-1}{n} 
= \frac{2 (1-n)}{n^2} + \frac{n-2}{n^2} 
= \frac{2 - 2n + n - 2}{n^2} = \frac{-1}{n} \neq 0
\text{,}\)</span> so the columns of <span class="process-math">\(Q\)</span> are not quite orthogonal. (They are pretty close to orthogonal when <span class="process-math">\(n\)</span> is large.)</div></li>
<li id="li-4948"><div class="para" id="p-7374">No. The columns are not orthogonal.  The columns are also not unit vectors. So this is an instance where we are using the letter <span class="process-math">\(Q\)</span> but the matrix is <em class="emphasis">not</em> orthogonal.</div></li>
</ol></div></div></div>
</div></article><article class="example example-like" id="example-82"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">7.1.1</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7375">
<div class="para">If <span class="process-math">\(n = 4\text{,}\)</span> then matrix <span class="process-math">\(Q = \begin{bmatrix}
3/4 \amp -1/4 \amp -1/4 \amp -1/4 \\
-1/4 \amp  3/4 \amp -1/4 \amp -1/4 \\
-1/4 \amp -1/4 \amp  3/4 \amp -1/4 \\
-1/4 \amp -1/4 \amp -1/4 \amp  3/4 \\
\end{bmatrix}\)</span> perfoms the demeaning operation.  For example, let‚Äôs have Python compute</div>
<div class="displaymath process-math">
\begin{equation*}
Q \fourvec2105
\end{equation*}
</div>
</div> <pre class="ptx-sagecell sagecell-python-autoeval" id="sage-251"><script type="text/x-sage">import numpy as np
</script></pre> <pre class="ptx-sagecell sagecell-python" id="sage-252"><script type="text/x-sage">from pprint import pprint
Q = np.eye(4) - (np.ones((4,4)) / 4)
x = nd.array([2,1,0,5])

pprint("Q", "\n", Q)
pprint("Q @ x", "\n", Q @ x)
pprint("x - mean of x", "\n", x - x.mean())
</script></pre></article><article class="note remark-like" id="note-13"><h4 class="heading">
<span class="type">Note</span><span class="space"> </span><span class="codenumber">7.1.2</span><span class="period">.</span>
</h4>
<div class="para" id="p-7376">While expressing <span class="process-math">\(\xvec - \xbar\)</span> as <span class="process-math">\(Q \xvec\)</span> is useful to helping us understand the demaining operation, this would not be a good way to compute <span class="process-math">\(\xvec - \xbar\text{.}\)</span>  The matrix <span class="process-math">\(Q\)</span> could be very large and only contains two different values, so there are more effiicent ways to perform this calculation on large data.</div></article><div class="para logical" id="p-7377">
<div class="para">Not only is <span class="process-math">\(Q \xvec = \xtilde\)</span> for any vector <span class="process-math">\(\xvec\text{,}\)</span> we can apply <span class="process-math">\(Q\)</span> to an entire data matrix (in column-variate form) to get</div>
<div class="displaymath process-math">
\begin{equation*}
\Xtilde = Q X = \begin{bmatrix} Q X_{\cdot 1} \amp Q X_{\cdot 2} \amp \cdots \amp Q X_{\cdot n} \end{bmatrix} \text{.}
\end{equation*}
</div>
<div class="para"> Here we are using the handy notation <span class="process-math">\(X_{\cdot j}\)</span> to represent the <span class="process-math">\(i\)</span>th column of <span class="process-math">\(X\text{.}\)</span> We can use <span class="process-math">\(X_{i \cdot}\)</span> to represent the <span class="process-math">\(i\)</span>th row of <span class="process-math">\(X\text{.}\)</span>
</div>
</div></section><section class="subsection" id="subsec-variance-covariance"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.1.2</span><span class="space"> </span><span class="title">Sample variance and covariance</span>
</h3>
<div class="para logical" id="p-7378">
<div class="para">Now let‚Äôs turn our attention to the sample variance (and then to covariance). Once again, we can write a sum of products as a dot product and re-express the dot product as as matrix multiplication.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/subsec-sample-mean.html" id="md-42">
\begin{align*}
s_x^2 \amp = \frac{1}{n-1} \sum (x_i - \overline{x})^2\\
\amp = \frac{1}{n-1} \sum (\xvec - \xbar) \cdot (\xvec - \xbar)\\
\amp = \frac{1}{n-1} \sum \xtilde \cdot \xtilde\\
\amp = \frac{1}{n-1} \sum \xtilde^{\transpose} \xtilde\\
\amp = \frac{1}{n-1} \sum (Q \xvec)^{\transpose} Q\xvec  \\
\amp = \frac{1}{n-1} \sum \xvec^{\transpose} Q^{\transpose} Q\xvec  \\
\amp = \frac{1}{n-1} \sum \xvec^{\transpose} Q\xvec  \text{,}
\end{align*}
</div>
<div class="para">where <span class="process-math">\(Q\)</span> is the demaining matrix as defined in <a href="sec-variance-covariance.html#subsec-sample-mean" class="internal" title="Subsection 7.1.1: Sample mean">Subsection¬†7.1.1</a>. The last simplification follows because <span class="process-math">\(Q\)</span> is both symmetric and idempotent.</div>
</div>
<article class="activity project-like" id="activity-91"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.2</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7379">
<div class="para">Show that we get a similar result for covariance:</div>
<div class="displaymath process-math" id="md-43">
\begin{align*}
s_{xy} \amp = \frac{1}{n-1} \sum (x_i - \overline{x})(y -
\overline{y})\\
\amp = \frac{1}{n-1} \sum \xvec^{\transpose} Q\yvec\text{.}
\end{align*}
</div>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-364" id="solution-364"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-364"><div class="solution solution-like"><div class="para logical" id="p-7380"><ol class="decimal"><li id="li-4949"><div class="para logical" id="p-7381">
<div class="para">The algebra is essentially identical to the case for variance.</div>
<div class="displaymath process-math" id="md-44">
\begin{align*}
s_{xy} \amp = \frac{1}{n-1} \sum (x_i - \overline{x})(y - \overline{y})\\
\amp = \frac{1}{n-1} \sum (\xvec - \xbar) \cdot (\yvec - \ybar)\\
\amp = \frac{1}{n-1} \sum \xtilde \cdot \ytilde\\
\amp = \frac{1}{n-1} \sum \xtilde^{\transpose} \ytilde\\
\amp = \frac{1}{n-1} \sum (Q \xvec)^{\transpose} Q\yvec  \\
\amp = \frac{1}{n-1} \sum \xvec^{\transpose} Q^{\transpose} Q\yvec  \\
\amp = \frac{1}{n-1} \sum \xvec^{\transpose} Q\yvec  \text{.}
\end{align*}
</div>
</div></li></ol></div></div></div>
</div></article><article class="note remark-like" id="note-14"><h4 class="heading">
<span class="type">Note</span><span class="space"> </span><span class="codenumber">7.1.3</span><span class="period">.</span>
</h4>
<div class="para" id="p-7382">
<span class="process-math">\(s_{x}^2 = s_{xx}\text{,}\)</span> so the variance of a vector is the covariance of that vector with itself.  That is, variance is a special case of covariance.</div></article><div class="para logical" id="p-7383">
<div class="para">Given a column-variate data matrix <span class="process-math">\(X\text{,}\)</span> we can compute all the covariances simultaneously as a matrix operation:</div>
<div class="displaymath process-math">
\begin{equation*}
S = S_{XX} 
= \frac{1}{n-1} \Xtilde^{\transpose} Q \Xtilde
= \frac{1}{n-1} X^{\transpose} Q Q X
= \frac{1}{n-1} X^{\transpose} Q X\text{.}
\end{equation*}
</div>
<div class="para"> The resulting matrix <span class="process-math">\(S\)</span> (sometimes denoted <span class="process-math">\(\hat \Sigma\)</span>) is called the <dfn class="terminology">variance-covariance matrix</dfn> or simply the <dfn class="terminology">covariance matrix</dfn> (since the variance is a covariance). <span class="process-math">\(S_{XY} = \frac{1}{n-1} X^{\transpose} Q Y\)</span> can be defined similarly to compute the covariances of each column of <span class="process-math">\(X\)</span> with each column of <span class="process-math">\(Y\text{,}\)</span> assuming both matrices have the same number of rows.</div>
</div>
<article class="activity project-like" id="activity-var-of-linear-combo"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.3</span><span class="period">.</span>
</h4>
<div class="para" id="p-7384">Supose we have an <span class="process-math">\(n \by p\)</span> column-variate data matrix <span class="process-math">\(X\)</span> and wish to compute a new column (variable) <span class="process-math">\(\yvec\)</span> that is a linear combination of the columns in <span class="process-math">\(X\text{.}\)</span>  That is <span class="process-math">\(\yvec = X \bvec\)</span> for some <span class="process-math">\(p\)</span>-dimensional vector <span class="process-math">\(b\text{.}\)</span>
</div> <div class="para logical" id="p-7385"><ol class="decimal">
<li id="li-4950"><div class="para" id="p-7386">Show that <span class="process-math">\(\ytilde = \Xtilde \bvec\text{.}\)</span>
</div></li>
<li id="li-4951"><div class="para" id="p-7387">Why is <span class="process-math">\(s_y^2 = s_{\tilde{y}}^2\text{?}\)</span>
</div></li>
<li id="li-4952"><div class="para" id="p-7388">Show that <span class="process-math">\(s_{y}^2 = \bvec^{\transpose} S_{XX} \bvec\text{.}\)</span>
</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-365" id="solution-365"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-365"><div class="solution solution-like"><div class="para logical" id="p-7389"><ol class="decimal">
<li id="li-4953"><div class="para" id="p-7390"><span class="process-math">\(\ytilde = Q \yvec = Q X \bvec = \Xtilde \bvec\text{.}\)</span></div></li>
<li id="li-4954"><div class="para" id="p-7391">We can again use the idempotency of <span class="process-math">\(Q\text{:}\)</span> <span class="process-math">\(s_y^2 
= \frac{1}{n-1} \ytilde^{\transpose} \ytilde 
= \frac{1}{n-1} \yvec^{\transpose} Q^{\transpose} Q \yvec 
= \frac{1}{n-1} \yvec^{\transpose} Q^{\transpose} Q^{\transpose} Q Q \yvec 
= \frac{1}{n-1} \ytilde^{\transpose} Q^{\transpose} Q \ytilde 
s_{\tilde{y}}^2 
\text{.}\)</span>
</div></li>
<li id="li-4955"><div class="para" id="p-7392"><span class="process-math">\(s_y^2 
= \frac{1}{n-1} \ytilde^{\transpose} \ytilde 
= \frac{1}{n-1} \bvec^{\transpose} \Xtilde^{\transpose} \Xtilde \bvec 
= \bvec^{\transpose} \left( \frac{1}{n-1} \Xtilde^{\transpose} \Xtilde \right) \bvec 
= \bvec^{\transpose} S_{XX} \bvec 
\text{.}\)</span></div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7393">The last statement of <a href="" class="xref" data-knowl="./knowl/activity-var-of-linear-combo.html" title="Activity 7.1.3">Activity¬†7.1.3</a> is important enough to isolate as a proposition.</div>
<article class="proposition theorem-like" id="prop-var-of-linear-combo"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">7.1.4</span><span class="period">.</span><span class="space"> </span><span class="title">Variance of a linear combination.</span>
</h4>
<div class="para logical" id="p-7394">
<div class="para">If <span class="process-math">\(\yvec = X \bvec\text{,}\)</span> then</div>
<div class="displaymath process-math">
\begin{equation*}
s_{y}^2 
= \bvec^{\transpose} S_{XX} \bvec
= \bvec \cdot (S_{XX} \bvec)\text{.}
\end{equation*}
</div>
</div></article><article class="activity project-like" id="activity-93"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.4</span><span class="period">.</span>
</h4>
<div class="para" id="p-7395">How do we know that <span class="process-math">\(S_{XX}\)</span> will be symmetric?</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-366" id="solution-366"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-366"><div class="solution solution-like"><div class="para" id="p-7396">
<span class="process-math">\(S_{XX} = (QX)^{\transpose} (QX) \text{,}\)</span> so <span class="process-math">\(S_{XX}\)</span> is symmtric by <a href="" class="xref" data-knowl="./knowl/proposition-properties-of-transpose.html" title="Proposition 6.2.12: Properties of the transpose">Proposition¬†6.2.12</a>.</div></div></div>
</div></article><div class="para" id="p-7397">
<a href="" class="xref" data-knowl="./knowl/prop-var-of-linear-combo.html" title="Proposition 7.1.4: Variance of a linear combination">Proposition¬†7.1.4</a> tells us that we can compute the variance of a linear combination of vectors directly from the covariance matrix for those vectors and the weights used in the linear combination.</div>
<article class="example example-like" id="example-83"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">7.1.5</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7398">
<div class="para">Suppose <span class="process-math">\(\zvec = \xvec - \yvec\)</span> and the covariance matrix for <span class="process-math">\(\xvec\)</span> and <span class="process-math">\(\yvec\)</span> is</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{bmatrix}
7 \amp 2 \\ 
2 \amp 5 \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<div class="para">What is the variance of <span class="process-math">\(\zvec\text{?}\)</span>
</div>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-367" id="solution-367"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-367"><div class="solution solution-like">
<div class="para logical" id="p-7399"><div class="displaymath process-math">
\begin{equation*}
s_{z} = 
\begin{bmatrix} 1 \amp -1\end{bmatrix} 
\begin{bmatrix} 7 \amp 2 \\ 2 \amp 5\end{bmatrix} 
\begin{bmatrix} 1 \\ -1\end{bmatrix} 
= 
\begin{bmatrix} 8 \end{bmatrix} 
\end{equation*}
</div></div> <pre class="ptx-sagecell hidden-sagecell-python" id="sage-253"><script type="text/x-sage">b = np.array([1, -1])
S = np.array([ [7,2], [2, 5] ])
print(np.transpose(b) @ S @ b)
</script></pre>
</div></div>
</div></article><div class="para" id="p-7400">So far, we have taken a very algebraic approach. And linear algebra does make it easy to work with means and (co)variances and to learn about them.  But it can be good to visualize these things as well.</div>
<article class="activity project-like" id="activity-visual-variance"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.5</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7401">
<div class="para">We‚Äôll begin with a very small, artificial data set. This data set has three subjects.  For each subject, two measurements are recorded.  We can gather all of these data into a <span class="process-math">\(3\by2\)</span> matrix</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/fig-variance-data.html ./knowl/fig-variance-data.html ./knowl/fig-variance-demeaned.html ./knowl/fig-variance-projection.html ./knowl/fig-variance-projection-2.html ./knowl/prop-var-of-linear-combo.html">
\begin{equation*}
X = \begin{bmatrix}\xvec \amp \yvec\end{bmatrix}
= 
\begin{bmatrix} 1 \amp 1 \\ 2 \amp 1 \\ 3 \amp 4 \\ \end{bmatrix}\text{.}
\end{equation*}
</div>
<div class="para">
<span class="process-math">\(X_{i \cdot}\text{,}\)</span> the <span class="process-math">\(i\)</span>th row of <span class="process-math">\(X\)</span> represents the <span class="process-math">\(i\)</span>th case or subject. <span class="process-math">\(X_{\cdot j}\text{,}\)</span> the <span class="process-math">\(j\)</span>th column of <span class="process-math">\(X\)</span> represents the <span class="process-math">\(j\)</span>th variable.</div>
<ol class="lower-alpha">
<li id="li-4956">
<div class="para" id="p-7402">It is typical to plot the <em class="emphasis">rows</em> of <span class="process-math">\(X\)</span> (columns of <span class="process-math">\(X^{\transpose}\)</span>) as a scatter plot in <dfn class="terminology">case space</dfn>. For each case -- i.e., for each row of <span class="process-math">\(X\)</span> -- plot the <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> values as a dot in <a href="" class="xref" data-knowl="./knowl/fig-variance-data.html" title="Figure 7.1.6">Figure¬†7.1.6</a>.</div>
<figure class="figure figure-like" id="fig-variance-data"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/empty-4.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.1.6<span class="period">.</span></span><span class="space"> </span>Plot the data and their centroid here.</figcaption></figure>
</li>
<li id="li-4957">
<div class="para logical" id="p-7403">
<div class="para">If we compute the mean of each column we get another row vector called the <dfn class="terminology">centroid</dfn> or mean.  We will denote it as <span class="process-math">\(\overline{X} = \begin{bmatrix} \overline{x} \amp \overline{y}\end{bmatrix}\text{.}\)</span> Note that we can write this as</div>
<div class="displaymath process-math">
\begin{equation*}
\overline{X} = \frac{1}{3}\sum_{i = 1}^3 X_{i \cdot} = \frac13 ([1 1] + [2 1] + [3 4])\text{.}
\end{equation*}
</div>
<div class="para">In Python, we would calculate this as <code class="code-inline tex2jax_ignore">X.mean(axis = 0)</code>.</div>
</div>
<div class="para" id="p-7404">Compute the centroid and add it as another dot in <a href="" class="xref" data-knowl="./knowl/fig-variance-data.html" title="Figure 7.1.6">Figure¬†7.1.6</a>.</div>
</li>
<li id="li-4958">
<div class="para logical" id="p-7405">
<div class="para">Notice that the centroid lies in the center of the data so we can measure the spread of the entire data set by measuring by how far away the points are from the centroid.  To simplify our calculations, find the demeaned data</div>
<div class="displaymath process-math">
\begin{equation*}
\tilde{X} = \begin{bmatrix} \xvec - \overline{\xvec} \amp \yvec - \overline{\yvec} \end{bmatrix}\text{.}
\end{equation*}
</div>
</div>
<div class="para" id="p-7406">Plot the demeaned data and their centroid in <a href="" class="xref" data-knowl="./knowl/fig-variance-demeaned.html" title="Figure 7.1.7">Figure¬†7.1.7</a>. Why is the centroid where it is?</div>
<figure class="figure figure-like" id="fig-variance-demeaned"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/empty-4.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.1.7<span class="period">.</span></span><span class="space"> </span>Plot the demeaned data and their centroid here.</figcaption></figure>
</li>
<li id="li-4959"><div class="para logical" id="p-7407">
<div class="para">Now that the data have been demeaned, we will define the total variance of <span class="process-math">\(X\)</span> as the average of the squares of the distances of the dots in our plot from the origin; that is, the total variance is</div>
<div class="displaymath process-math">
\begin{equation*}
V = \frac 1n \sum_{i=1}^n |\Xtilde_{i\cdot}^{\transpose}|^2\text{.}
\end{equation*}
</div>
<div class="para">Find the total variance <span class="process-math">\(V\)</span> for our set of three points.</div>
</div></li>
<li id="li-4960">
<div class="para logical" id="p-7408">
<div class="para">To cut down on notation a bit, let‚Äôs define <span class="process-math">\(\dtil_i = \Xtilde_{i \cdot}\text{,}\)</span> so</div>
<div class="displaymath process-math">
\begin{equation*}
V = \frac 1n \sum_{i=1}^n |\dtil|^2\text{.}
\end{equation*}
</div>
<div class="para">Notice that each <span class="process-math">\(\dtil_i\)</span> is a column vector, but it is associated with a <em class="emphasis">row</em> of <span class="process-math">\(X\text{.}\)</span> We will refer to these as the <em class="emphasis">demeaned case vectors</em>.</div>
</div>
<div class="para" id="p-7409">Write down <span class="process-math">\(\dtil_1, \dtil_2, \dtil_3\text{.}\)</span>
</div>
</li>
<li id="li-4961">
<div class="para" id="p-7410">Now plot the projections of the demeaned case vectors <span class="process-math">\(\dtil_1, \dtil_2, \dtil_3\)</span> onto the <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> axes using <a href="" class="xref" data-knowl="./knowl/fig-variance-projection.html" title="Figure 7.1.8">Figure¬†7.1.8</a> and find the variances <span class="process-math">\(V_x\)</span> and <span class="process-math">\(V_y\)</span> of the projected points by computing the squares of the three distances and . adding them together.</div>
<figure class="figure figure-like" id="fig-variance-projection"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/x-axis-4.svg" role="img" class="contained"></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/y-axis-4.svg" role="img" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.1.8<span class="period">.</span></span><span class="space"> </span>Plot the projections of the demeaned case vectors onto the <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> axes.</figcaption></figure>
</li>
<li id="li-4962"><div class="para" id="p-7411">Which of the variances, <span class="process-math">\(V_x\)</span> and <span class="process-math">\(V_y\text{,}\)</span> is larger and how does the plot of the projected points explain your response?</div></li>
<li id="li-4963"><div class="para" id="p-7412">What do you notice about the relationship between <span class="process-math">\(V\text{,}\)</span> <span class="process-math">\(V_x\text{,}\)</span> and <span class="process-math">\(V_y\text{?}\)</span>  How does the Pythagorean theorem explain this relationship?</div></li>
<li id="li-4964">
<div class="para" id="p-7413">Plot the projections of the demeaned case vectors onto the lines defined by vectors <span class="process-math">\(\vvec_1=\twovec11\)</span> and <span class="process-math">\(\vvec_2=\twovec{-1}1\)</span> using <a href="" class="xref" data-knowl="./knowl/fig-variance-projection-2.html" title="Figure 7.1.9">Figure¬†7.1.9</a> and find the variances <span class="process-math">\(V_{\vvec_1}\)</span> and <span class="process-math">\(V_{\vvec_2}\)</span> of these projected case vectors.</div>
<figure class="figure figure-like" id="fig-variance-projection-2"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/empty-4-diag.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.1.9<span class="period">.</span></span><span class="space"> </span>Plot the projections of the deameaned case vectors onto the lines defined by <span class="process-math">\(\vvec_1\)</span> and <span class="process-math">\(\vvec_2\text{.}\)</span></figcaption></figure>
</li>
<li id="li-4965"><div class="para" id="p-7414">What is the relationship between the total variance <span class="process-math">\(V\)</span> and <span class="process-math">\(V_{\vvec_1}\)</span> and <span class="process-math">\(V_{\vvec_2}\text{?}\)</span> How does the Pythagorean theorem explain your response?</div></li>
<li id="li-4966"><div class="para" id="p-7415">Verify that you get the same results if you compute <span class="process-math">\(V_x\text{,}\)</span> <span class="process-math">\(V_y\text{,}\)</span> <span class="process-math">\(V_\vvec_1\text{,}\)</span> and <span class="process-math">\(V_\vvec_2\)</span> using <a href="" class="xref" data-knowl="./knowl/prop-var-of-linear-combo.html" title="Proposition 7.1.4: Variance of a linear combination">Proposition¬†7.1.4</a>
</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-286" id="answer-286"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-286"><div class="answer solution-like"><div class="para logical" id="p-7416"><ol class="lower-alpha">
<li id="li-4967"><div class="para" id="p-7417"><span class="process-math">\(\overline{X} = \twovec22\text{.}\)</span></div></li>
<li id="li-4968"><div class="para logical" id="p-7418"><div class="displaymath process-math">
\begin{equation*}
\Xtilde = 
\begin{bmatrix}
-1 \amp -1 \\ 0 \amp -1 \\ 1 \amp 2 \\
\end{bmatrix}\text{.}
\end{equation*}
</div></div></li>
<li id="li-4969"><div class="para" id="p-7419"><span class="process-math">\(V=4\text{.}\)</span></div></li>
<li id="li-4970"><div class="para logical" id="p-7420"><div class="displaymath process-math">
\begin{equation*}
\dtil_1=\twovec{-1}{-1},\hspace{24pt}
\dtil_2=\twovec{0}{-1},\hspace{24pt}
\dtil_3=\twovec{1}{2}\text{.}
\end{equation*}
</div></div></li>
<li id="li-4971"><div class="para" id="p-7421">
<span class="process-math">\(V_x = 1\)</span> and <span class="process-math">\(V_y=3\)</span>
</div></li>
<li id="li-4972"><div class="para" id="p-7422"><span class="process-math">\(\displaystyle V=V_x+V_y\)</span></div></li>
<li id="li-4973">
<div class="para" id="p-7423"><span class="process-math">\(V_{\vvec_1} = 7/2\)</span></div>
<div class="para" id="p-7424"><span class="process-math">\(V_{\vvec_2} = 1/2\)</span></div>
</li>
<li id="li-4974"><div class="para" id="p-7425"><span class="process-math">\(\displaystyle V = V_{\vvec_1} + V_{\vvec_2}\)</span></div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-368" id="solution-368"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-368"><div class="solution solution-like"><div class="para logical" id="p-7426"><ol class="lower-alpha">
<li id="li-4975"><div class="para" id="p-7427">The centroid is <span class="process-math">\(\overline{X} = \twovec22\text{.}\)</span>
</div></li>
<li id="li-4976"><div class="para logical" id="p-7428">
<div class="para">The demeaned data are</div>
<div class="displaymath process-math">
\begin{equation*}
\Xtilde = 
\begin{bmatrix}
-1 \amp -1 \\ 0 \amp -1 \\ 1 \amp 2 \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
</div></li>
<li id="li-4977"><div class="para" id="p-7429">The total variance is <span class="process-math">\(V=8/2 = 4\text{.}\)</span>
</div></li>
<li id="li-4978"><div class="para logical" id="p-7430"><div class="displaymath process-math">
\begin{equation*}
\dtil_1=\twovec{-1}{-1},\hspace{24pt}
\dtil_2=\twovec{0}{-1},\hspace{24pt}
\dtil_3=\twovec{1}{2}\text{.}
\end{equation*}
</div></div></li>
<li id="li-4979"><div class="para" id="p-7431">We find <span class="process-math">\(V_x = 2/2 = 1\)</span> and <span class="process-math">\(V_y=3\text{.}\)</span>  Notice that <span class="process-math">\(V_y\)</span> is larger because the points are more spread out in the vertical direction.</div></li>
<li id="li-4980"><div class="para" id="p-7432">We have <span class="process-math">\(V=V_x+V_y\)</span> due to the Pythagorean theorem.</div></li>
<li id="li-4981">
<div class="para" id="p-7433">The points projected onto the line defined by <span class="process-math">\(\vvec_1\)</span> are <span class="process-math">\(\twovec{-1}{-1}\text{,}\)</span> <span class="process-math">\(\twovec{-1/2}{-1/2}\text{,}\)</span> and <span class="process-math">\(\twovec{3/2}{3/2}\text{.}\)</span>  This gives the variance <span class="process-math">\(V_{\vvec_1} = 7/2\text{.}\)</span>
</div>
<div class="para" id="p-7434">The points projected onto the line defined by <span class="process-math">\(\vvec_2\)</span> are <span class="process-math">\(\twovec{0}{0}\text{,}\)</span> <span class="process-math">\(\twovec{1/2}{-1/2}\text{,}\)</span> and <span class="process-math">\(\twovec{-1/2}{1/2}\text{.}\)</span>  This gives the variance <span class="process-math">\(V_{\vvec_2} = 1/2\text{.}\)</span>
</div>
</li>
<li id="li-4982"><div class="para" id="p-7435">Once again, <span class="process-math">\(V = V_{\vvec_1} + V_{\vvec_2}\)</span> because of the Pythagorean theorem.</div></li>
<li id="li-4983">
<div class="para" id="p-7436">
<span class="process-math">\(V_{\vvec_1} 
= \twovec{\frac{1}{\sqrt 2}}{\frac{1}{\sqrt 2}} \cdot (S\twovec{\frac{1}{\sqrt 2}}{\frac{1}{\sqrt 2}} )
= 7/2
\text{.}\)</span> The others can be checked similarly.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-254"><script type="text/x-sage">import numpy as np
X = np.column_stack(((1,2,3), (1,1,4)))
X = X - X.mean(axis = 0)
n = X.shape[0]; p = X.shape[1]
print(X); print()

print(np.cov(X, rowvar = False)); print()
S = 1/(n-1) * np.transpose(X) @ X
print(S); print()

v1 = np.array((1,1))
v2 = np.array((1, -1))
e1 = np.eye(p)[0,:]
e2 = np.eye(p)[1,:]
u1 = w1 / np.linalg.norm(w1)
u2 = w2 / np.linalg.norm(w2)
print(e1 @ S @ e1, e2 @ S @ e2, u1 @ S @ u1, u2 @ S @ u2)
</script></pre>
</li>
</ol></div></div></div>
</div></article><div class="para logical" id="p-7437">
<div class="para">As <a href="" class="xref" data-knowl="./knowl/activity-visual-variance.html" title="Activity 7.1.5">Activity¬†7.1.5</a> suggests, the variance enjoys an additivity property. Consider, for instance, the situation where we have <span class="process-math">\(p=2\)</span> variables measured for each of <span class="process-math">\(n\)</span> cases, and suppose that the demeaned data are <span class="process-math">\(\Xtilde =\begin{bmatrix}\xtilde \amp \ytilde \end{bmatrix}\text{.}\)</span> For the <span class="process-math">\(i\)</span> row of <span class="process-math">\(X\)</span> we get</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/activity-visual-variance.html">
\begin{equation*}
|\dtil{i}|^2 = \xtilde_{i}^2 + \ytilde_{i}^2 \text{.}
\end{equation*}
</div>
<div class="para">If we take the average over all the cases, we find that the total variance <span class="process-math">\(V\)</span> is the sum of the variances in the <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> directions:</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/activity-visual-variance.html" id="md-45">
\begin{align*}
\frac1{n-1} \sum_i |\dtil{i}|^2 \amp =
\frac1{n-1} \sum_i \xtilde^2 +
\frac1{n-1} \sum_i \ytilde^2 \\
V \amp = V_x + V_y.
\end{align*}
</div>
</div>
<div class="para logical" id="p-7438">
<div class="para">More generally, suppose that we have an orthonormal basis <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  If we project the demeaned points onto the line defined by <span class="process-math">\(\uvec_1\text{,}\)</span> we obtain the points <span class="process-math">\((\dtil_j\cdot\uvec_1)\uvec_1\)</span> so that</div>
<div class="displaymath process-math">
\begin{equation*}
V_{\uvec_1} = \frac1{n-1}\sum_j
~|(\dtil_j\cdot\uvec_1)~\uvec_1|^2 =
\frac1{n-1}~(\dtil_j\cdot\uvec_1)^2.
\end{equation*}
</div>
</div>
<div class="para logical" id="p-7439">
<div class="para">For each of our demeaned case vectors, the Projection Formula tells us that</div>
<div class="displaymath process-math">
\begin{equation*}
\dtil_j = (\dtil_j\cdot\uvec_1)~\uvec_1 + 
(\dtil_j\cdot\uvec_2)~\uvec_2.
\end{equation*}
</div>
<div class="para">We then have</div>
<div class="displaymath process-math">
\begin{equation*}
|\dtil_j|^2 = \dtil_j\cdot\dtil_j =
(\dtil_j\cdot\uvec_1)^2 + (\dtil_j\cdot\uvec_2)^2
\end{equation*}
</div>
<div class="para">since <span class="process-math">\(\uvec_1\cdot\uvec_2 = 0\text{.}\)</span>  When we average over all the demeaned case vectors , we find that the total variance <span class="process-math">\(V\)</span> is the sum of the variances in the <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> directions.</div>
</div>
<div class="para" id="p-7440">The restriction to two variables in this example was just for notational ease. The same reasoning works for any number of variables. This leads to the following proposition.</div>
<article class="proposition theorem-like" id="prop-variance-additivity"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">7.1.10</span><span class="period">.</span><span class="space"> </span><span class="title">Additivity of Variance.</span>
</h4>
<div class="para logical" id="p-7441">
<div class="para">If <span class="process-math">\(W\)</span> is a subspace with orthonormal basis <span class="process-math">\(\uvec_1,\uvec_2,\ldots, \uvec_n\text{,}\)</span> then the variance of the points projected onto <span class="process-math">\(W\)</span> is the sum of the variances in the <span class="process-math">\(\uvec_j\)</span> directions:</div>
<div class="displaymath process-math">
\begin{equation*}
V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots + V_{\uvec_n}.
\end{equation*}
</div>
</div></article><div class="para" id="p-7442">The next activity demonstrates how we can use <a href="" class="xref" data-knowl="./knowl/prop-var-of-linear-combo.html" title="Proposition 7.1.4: Variance of a linear combination">Proposition¬†7.1.4</a> to compute the variance <span class="process-math">\(V_{\uvec}\)</span> more eficiently.</div>
<article class="activity project-like" id="activity-95"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.6</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7443">
<div class="para">Let‚Äôs return to the demeaned dataset from the previous activity:</div>
<div class="displaymath process-math">
\begin{equation*}
\Xtilde = 
\begin{bmatrix}
-1 \amp -1 \\ 0 \amp -1 \\ 1 \amp 2 \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<div class="para">The demeaned case vectors are</div>
<div class="displaymath process-math">
\begin{equation*}
\dtil_1=\twovec{-1}{-1},\hspace{24pt}
\dtil_2=\twovec{0}{-1},\hspace{24pt}
\dtil_3=\twovec{1}{2},
\end{equation*}
</div>
<div class="para">and</div>
<div class="displaymath process-math">
\begin{equation*}
\Xtilde^{\transpose} = \begin{bmatrix} \dtil_1 \amp \dtil_2 \amp \dtil_3 \end{bmatrix}\text{.}
\end{equation*}
</div>
<div class="para">Our goal is to compute the variance <span class="process-math">\(V_{\uvec}\)</span> in the direction defined by a unit vector <span class="process-math">\(\uvec\text{.}\)</span>
</div>
</div> <div class="para logical" id="p-7444">
<div class="para">Suppose that <span class="process-math">\(\uvec\)</span> is a unit vector.</div>
<ol class="lower-alpha">
<li id="li-4984">
<div class="para" id="p-7445">Compute the matrix <span class="process-math">\(S = X^{\transpose} X\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-255"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4985"><div class="para" id="p-derived-li-4985">Write an expression for <span class="process-math">\(V_\uvec\)</span> using <a href="" class="xref" data-knowl="./knowl/prop-var-of-linear-combo.html" title="Proposition 7.1.4: Variance of a linear combination">Proposition¬†7.1.4</a>.</div></li>
<li id="li-4986"><div class="para" id="p-7446">Use the covariance matrix <span class="process-math">\(S\)</span> to find the variance <span class="process-math">\(V_{\uvec_1}\)</span> when <span class="process-math">\(\uvec_1=\twovec{1/\sqrt{5}}{2/\sqrt{5}}\text{.}\)</span>
</div></li>
<li id="li-4987"><div class="para" id="p-7447">Use the covariance matrix to find the variance <span class="process-math">\(V_{\uvec_2}\)</span> when <span class="process-math">\(\uvec_2=\twovec{-2/\sqrt{5}}{1/\sqrt{5}}\text{.}\)</span>
</div></li>
<li id="li-4988"><div class="para" id="p-7448">Since <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> are orthogonal, verify that the sum of <span class="process-math">\(V_{\uvec_1}\)</span> and <span class="process-math">\(V_{\uvec_2}\)</span> gives the total variance.</div></li>
<li id="li-4989"><div class="para" id="p-7449">Now recompute <span class="process-math">\(V_{\uvec_1}\)</span>  and <span class="process-math">\(V_{\uvec_2}\)</span> by computing the squared lengths of the three projections and summing.</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-287" id="answer-287"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-287"><div class="answer solution-like"><div class="para logical" id="p-7450"><ol class="lower-alpha">
<li id="li-4990"><div class="para" id="p-derived-li-4990"><span class="process-math">\(\displaystyle S=\begin{bmatrix}
2/3 \amp 1 \\
1 \amp 2
\end{bmatrix}\)</span></div></li>
<li id="li-4991"><div class="para" id="p-7451"><span class="process-math">\(\displaystyle V_{\uvec_1} = 38/10\)</span></div></li>
<li id="li-4992"><div class="para" id="p-7452"><span class="process-math">\(V_{\uvec_2} = 2/10\text{.}\)</span></div></li>
<li id="li-4993"><div class="para" id="p-7453"><span class="process-math">\(38/10 + 2/10 = 40/10 = 4\text{.}\)</span></div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-369" id="solution-369"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-369"><div class="solution solution-like"><div class="para logical" id="p-7454"><ol class="lower-alpha">
<li id="li-4994"><div class="para" id="p-derived-li-4994"><span class="process-math">\(\displaystyle S= \frac{1/2} \Xtilde^{\transpose} \Xtilde = \begin{bmatrix}
1 \amp 3/2 \\
3/2 \amp 3 
\end{bmatrix}\)</span></div></li>
<li id="li-4995"><div class="para" id="p-7455"><span class="process-math">\(\displaystyle V_{\uvec} = \uvec^{\transpose} S \uvec \)</span></div></li>
<li id="li-4996"><div class="para" id="p-7456"><span class="process-math">\(\displaystyle V_{\uvec_1} = 38/10\)</span></div></li>
<li id="li-4997"><div class="para" id="p-7457">
<span class="process-math">\(V_{\uvec_2} = 2/10\text{.}\)</span>  Then <span class="process-math">\(V_{\uvec_1}+V_{\uvec_2} = 40/10 = 4\text{,}\)</span> which is the total variance.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7458">Our goal in the future will be to find directions <span class="process-math">\(\uvec\)</span> where the variance is as large as possible and directions where it is as small as possible.  The next activity demonstrates why this is useful.</div>
<article class="activity project-like" id="activity-96"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.1.7</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7459"><ol class="lower-alpha">
<li id="li-4998">
<div class="para" id="p-7460">Evaluating the following Python cell loads a dataset consisting of 100 demeaned data points and provides a plot of them.  It also provides the demeaned data matrix <span class="process-math">\(A\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-256"><script type="text/x-sage">import pandas as pd
import seaborn.objects as so
some_data = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/variance-data.csv', header=None)
X = some_data.to_numpy()
print("columns means: ", X.mean(axis = 0))
print("shape: ", X.shape)
(
    so.Plot(x = X[:, 0], y = X[:, 1])
    .add(so.Dot())
    .show()
)
</script></pre>
<div class="para" id="p-7461">What is the shape of the covariance matrix <span class="process-math">\(S\text{?}\)</span>  Find <span class="process-math">\(S\)</span> and verify your response.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-257"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4999"><div class="para" id="p-7462">By visually inspecting the data, determine which is larger, <span class="process-math">\(V_x\)</span> or <span class="process-math">\(V_y\text{.}\)</span>  Then compute both of these quantities to verify your response.</div></li>
<li id="li-5000"><div class="para" id="p-7463">What is the total variance <span class="process-math">\(V\text{?}\)</span>
</div></li>
<li id="li-5001"><div class="para" id="p-7464">In approximately what direction is the variance greatest?  Choose a reasonable vector <span class="process-math">\(\uvec\)</span> that points in approximately that direction and find <span class="process-math">\(V_{\uvec}\text{.}\)</span>
</div></li>
<li id="li-5002"><div class="para" id="p-7465">In approximately what direction is the variance smallest?  Choose a reasonable vector <span class="process-math">\(\wvec\)</span> that points in approximately that direction and find <span class="process-math">\(V_{\wvec}\text{.}\)</span>
</div></li>
<li id="li-5003"><div class="para" id="p-7466">How are the directions <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\wvec\)</span> in the last two parts of this problem related to one another?  Why does this relationship hold?</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-288" id="answer-288"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-288"><div class="answer solution-like"><div class="para logical" id="p-7467"><ol class="lower-alpha">
<li id="li-5004"><div class="para" id="p-7468"><span class="process-math">\(\displaystyle S=\begin{bmatrix}
1.38 \amp 0.70 \\
0.70 \amp 0.37
\end{bmatrix}\)</span></div></li>
<li id="li-5005"><div class="para" id="p-7469">
<span class="process-math">\(V_x = 1.38\)</span> and <span class="process-math">\(V_y=0.37\)</span>
</div></li>
<li id="li-5006"><div class="para" id="p-7470"><span class="process-math">\(\displaystyle V=1.75\)</span></div></li>
<li id="li-5007"><div class="para" id="p-7471">If <span class="process-math">\(\uvec_1=\twovec{2/\sqrt{5}}{1/\sqrt{5}}\text{,}\)</span> then <span class="process-math">\(V_{\uvec_1} = 1.74\text{.}\)</span>
</div></li>
<li id="li-5008"><div class="para" id="p-7472">If <span class="process-math">\(\uvec_2=\twovec{-1/\sqrt{5}}{2/\sqrt{5}}\text{,}\)</span> then <span class="process-math">\(V_{\uvec_2} = 0.01\text{.}\)</span>
</div></li>
<li id="li-5009"><div class="para" id="p-7473">They are orthogonal to one another.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-370" id="solution-370"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-370"><div class="solution solution-like"><div class="para logical" id="p-7474"><ol class="lower-alpha">
<li id="li-5010"><div class="para" id="p-7475">
<span class="process-math">\(S\)</span> will be the <span class="process-math">\(2\by2\)</span> matrix <span class="process-math">\(S=\begin{bmatrix}
1.38 \amp 0.70 \\
0.70 \amp 0.37
\end{bmatrix}\)</span>
</div></li>
<li id="li-5011"><div class="para" id="p-7476">
<span class="process-math">\(V_x = 1.38\)</span> and <span class="process-math">\(V_y=0.37\text{,}\)</span> which agrees with the fact that the data is more spread out in the horizontal than vertical direction.</div></li>
<li id="li-5012"><div class="para" id="p-7477"><span class="process-math">\(\displaystyle V=V_x+V_y=1.75\)</span></div></li>
<li id="li-5013"><div class="para" id="p-7478">It looks like the direction <span class="process-math">\(\twovec21\)</span> defined by the unit vector <span class="process-math">\(\uvec_1=\twovec{2/\sqrt{5}}{1/\sqrt{5}}\text{.}\)</span>  We find that <span class="process-math">\(V_{\uvec_1} = 1.74\text{,}\)</span> which is almost all of the total variance.</div></li>
<li id="li-5014"><div class="para" id="p-7479">It looks like the direction <span class="process-math">\(\twovec{-1}{2}\)</span> defined by the unit vector <span class="process-math">\(\uvec_2=\twovec{-1/\sqrt{5}}{2/\sqrt{5}}\text{.}\)</span>  We find that <span class="process-math">\(V_{\uvec_2} = 0.01\text{.}\)</span>
</div></li>
<li id="li-5015"><div class="para" id="p-7480">They are orthogonal to one another.  Since the total variance <span class="process-math">\(V=V_{\uvec_1}+V_{\uvec_2}\)</span> when <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> are orthogonal, <span class="process-math">\(V_{\uvec_1}\)</span> will be as large as possible when <span class="process-math">\(V_{\uvec_2}\)</span> is as small as possible.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7481">This activity illustrates how variance can identify a line along which the data are concentrated.  When the data primarily lie along a line defined by a vector <span class="process-math">\(\uvec_1\text{,}\)</span> then the variance in that direction will be large while the variance in an orthogonal direction <span class="process-math">\(\uvec_2\)</span> will be small.</div>
<div class="para logical" id="p-7482">
<div class="para">Remember that variance is additive, according to <a href="" class="xref" data-knowl="./knowl/prop-variance-additivity.html" title="Proposition 7.1.10: Additivity of Variance">Proposition¬†7.1.10</a>, so that if <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> are orthogonal unit vectors, then the total variance is</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/prop-variance-additivity.html">
\begin{equation*}
V = V_{\uvec_1} + V_{\uvec_2}.
\end{equation*}
</div>
<div class="para">Therefore, if we choose <span class="process-math">\(\uvec_1\)</span> to be the direction where <span class="process-math">\(V_{\uvec_1}\)</span> is a maximum, then <span class="process-math">\(V_{\uvec_2}\)</span> will be a minimum.</div>
</div>
<div class="para" id="p-7483">In the next section, we will use an orthogonal diagonalization of the covariance matrix <span class="process-math">\(S\)</span> to find the directions having the greatest and smallest variances. In this way, we will be able to determine when data are concentrated along a line or subspace.</div></section><section class="subsection" id="subsection-123"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.1.3</span><span class="space"> </span><span class="title">Summary</span>
</h3>
<div class="para logical" id="p-7484">
<div class="para">This section explored and variance and covariance. Let <span class="process-math">\(X\)</span> be a column-variate data matrix and let <span class="process-math">\(\Xtilde\)</span> be the demeaned version of <span class="process-math">\(X\text{.}\)</span> Then</div>
<ul class="disc">
<li id="li-5016"><div class="para" id="p-7485">The variance of a vector is just a special case of covariance -- it is the covariance of a vector with itself.</div></li>
<li id="li-5017"><div class="para logical" id="p-7486"><div class="displaymath process-math" id="md-46">
\begin{align*}
\cov(\xvec, \yvec) = s_{xy} \amp = \frac{1}{n-1} (\xvec - \xbar) \cdot (\yvec - \ybar)\\
\var(\xvec) = 
s^2_{x} = s_{xx} \amp 
= \frac{1}{n-1} (\xvec - \xbar) \cdot (\xvec - \xbar)
= \frac{1}{n-1} \len{\xvec - \xbar}^2
\end{align*}
</div></div></li>
<li id="li-5018"><div class="para logical" id="p-7487">
<div class="para">We can compute all the variances and covariances of the columns of <span class="process-math">\(X\)</span> with</div>
<div class="displaymath process-math">
\begin{equation*}
S_{XX} = \frac{1}{n-1} \Xtilde^{\transpose} \Xtilde\text{.}
\end{equation*}
</div>
<div class="para">This <dfn class="terminology">covariance matrix</dfn> is always symmetric.</div>
</div></li>
<li id="li-5019"><div class="para logical" id="p-7488">
<div class="para">We can compute the variance of a linear combination of columns of <span class="process-math">\(X\)</span> with</div>
<div class="displaymath process-math">
\begin{equation*}
\var(X \bvec) = \bvec^{\transpose} S_{XX} \bvec\text{.}
\end{equation*}
</div>
</div></li>
<li id="li-5020"><div class="para logical" id="p-7489">
<div class="para">The total variance of a dataset can be computed using</div>
<div class="displaymath process-math">
\begin{equation*}
V = \frac 1n \sum_{i=1}^n |\Xtilde_{i\cdot}^{\transpose}|^2
= \frac 1n \sum_{i=1}^n |\dtil|^2
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(\dtil_i\)</span> is the <span class="process-math">\(i\)</span>th demeaned case vector.</div>
</div></li>
<li id="li-5021"><div class="para logical" id="p-7490">
<div class="para">Variance is additive so that if <span class="process-math">\(W\)</span> is a subspace with orthonormal basis <span class="process-math">\(\uvec_1,
\uvec_2,\ldots,\uvec_n\text{,}\)</span> then</div>
<div class="displaymath process-math">
\begin{equation*}
V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots + V_{\uvec_n}.
\end{equation*}
</div>
</div></li>
</ul>
</div></section><section class="exercises" id="exercises-30"><h3 class="heading hide-type">
<span class="type">Exercises</span><span class="space"> </span><span class="codenumber">7.1.4</span><span class="space"> </span><span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-256"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<ol class="lower-alpha">
<li id="li-5022"><div class="para" id="p-7491">Let <span class="process-math">\(\avec = \fourvec1326\)</span> and let <span class="process-math">\(\bvec = \fourvec524{-3}\text{.}\)</span> Compute <span class="process-math">\(\atilde \cdot \btilde\text{,}\)</span> <span class="process-math">\(\avec \cdot \btilde\text{,}\)</span> and <span class="process-math">\(\atilde \cdot \bvec\)</span> and show that you get the same result each time.</div></li>
<li id="li-5023"><div class="para" id="p-7492">Show that <span class="process-math">\(\avec \cdot \bvec\)</span> gives a different result. (So it is important to demean at least one of the vectors.)</div></li>
<li id="li-5024"><div class="para" id="p-7493">Show that for any vectors <span class="process-math">\(\xvec\)</span> and <span class="process-math">\(\yvec\text{,}\)</span> <span class="process-math">\(\xtilde \cdot \ytilde = \xvec \cdot \ytilde = \xtilde \cdot \yvec\text{.}\)</span> So to form the dot product of two demeaned vectors, it suffices to demean one of them and not the other.  (Hint: One way to do this begins by expressing things in terms of <span class="process-math">\(Q\)</span> where <span class="process-math">\(\xtilde = Q \xvec\text{.}\)</span>)</div></li>
</ol></article><article class="exercise exercise-like" id="exercise-257"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="para logical" id="p-7497">
<div class="para">Suppose that you have a column-variate data matrix</div>
<div class="displaymath process-math">
\begin{equation*}
X = \begin{bmatrix}
2 \amp 0 \\
2 \amp 3 \\ 
4 \amp 1 \\ 
3 \amp 2 \\
4 \amp 4 \\  
\end{bmatrix}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5025"><div class="para" id="p-7498">Find the demeaned case vectors.</div></li>
<li id="li-5026"><div class="para" id="p-7499">Find the total variance <span class="process-math">\(V\)</span> of the dataset.</div></li>
<li id="li-5027"><div class="para" id="p-7500">Find the variance in the direction <span class="process-math">\(\evec_1 =
\twovec10\)</span> and the variance in the direction <span class="process-math">\(\evec_2=\twovec01\text{.}\)</span>
</div></li>
<li id="li-5028"><div class="para" id="p-7501">Project the demeaned data points onto the line defined by <span class="process-math">\(\vvec_1=\twovec21\)</span> and find the variance of these projected points.</div></li>
<li id="li-5029"><div class="para" id="p-7502">Project the demeaned data points onto the line defined by <span class="process-math">\(\vvec_2=\twovec1{-2}\)</span> and find the variance of these projected points.</div></li>
<li id="li-5030"><div class="para" id="p-7503">How and why are the results of from the last two parts related to the total variance?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-258"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="para logical" id="p-7518">
<div class="para">Suppose you have the following columnn-variate data matrix</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/ex-demean-plot.html">
\begin{equation*}
X = \begin{bmatrix}
2 \amp 1 \\  
0 \amp 0 \\  
4 \amp 3 \\  
4 \amp 5 \\ 
5 \amp 4 \\
3 \amp 5 \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5043">
<div class="para" id="p-7519">Find the demeaned data matrix <span class="process-math">\(\Xtilde\)</span> and plot the demeaned case vectors as points in <a href="" class="xref" data-knowl="./knowl/ex-demean-plot.html" title="Figure 7.1.11">Figure¬†7.1.11</a>.</div>
<figure class="figure figure-like" id="ex-demean-plot"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/empty-4.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">7.1.11<span class="period">.</span></span><span class="space"> </span>A plot for the demeaned case vectors.</figcaption></figure>
</li>
<li id="li-5044"><div class="para" id="p-7520">Construct the covariance matrix <span class="process-math">\(S\text{.}\)</span>
</div></li>
<li id="li-5045"><div class="para" id="p-7521">Sketch the lines corresponding to the two eigenvectors on the plot above.</div></li>
<li id="li-5046"><div class="para" id="p-7522">Find the variances in the directions of the eigenvectors.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-259"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="para logical" id="p-7534">
<div class="para">Suppose that <span class="process-math">\(S\)</span> is the covariance matrix of a demeaned dataset.</div>
<ol class="lower-alpha">
<li id="li-5056"><div class="para" id="p-7535">Suppose that <span class="process-math">\(\uvec\)</span> is an eigenvector of <span class="process-math">\(S\)</span> with associated eigenvalue <span class="process-math">\(\lambda\)</span> and that <span class="process-math">\(\uvec\)</span> has unit length.  Explain why <span class="process-math">\(V_{\uvec} = \lambda\text{.}\)</span>
</div></li>
<li id="li-5057"><div class="para logical" id="p-7536">
<div class="para">Suppose that the covariance matrix of a demeaned dataset can be written as <span class="process-math">\(S=QDQ^{\transpose}\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
Q = \begin{bmatrix} \uvec_1 \amp \uvec_2 \end{bmatrix},
\hspace{24pt}
D = \begin{bmatrix}
10 \amp 0 \\
0 \amp 0 \\
\end{bmatrix}.
\end{equation*}
</div>
<div class="para">What is <span class="process-math">\(V_{\uvec_2}\text{?}\)</span>  What does this tell you about the demeaned data?</div>
</div></li>
<li id="li-5058"><div class="para" id="p-7537">Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-260"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="para logical" id="p-7546">
<div class="para">Let  <span class="process-math">\(S = \begin{bmatrix} 
1 \amp 2 \amp -1 \\
2 \amp 3 \amp  0 \\
-1 \amp 0 \amp  4 \\
\end{bmatrix}\)</span> be a covariance matrix for a column-variate data matrix <span class="process-math">\(X = \begin{bmatrix} \xvec_1 \amp \xvec_2 \amp \xvec_3 \end{bmatrix}\)</span> with 125 rows.</div>
<ol class="lower-alpha">
<li id="li-5065"><div class="para" id="p-7547">What is the variance of each of the columns of <span class="process-math">\(X\text{?}\)</span>
</div></li>
<li id="li-5066"><div class="para" id="p-7548">What is the variance of <span class="process-math">\(2 \xvec_1\text{?}\)</span>
</div></li>
<li id="li-5067"><div class="para" id="p-7549">What is the variance of <span class="process-math">\(2 \xvec_1 + \xvec_3\text{?}\)</span>
</div></li>
<li id="li-5068"><div class="para" id="p-7550">What is the variance of <span class="process-math">\(2 \xvec_1 - \xvec_3\text{?}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-261"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="para logical" id="p-7551">
<div class="para">In the expressions below <span class="process-math">\(a\)</span> and <span class="process-math">\(b\)</span> are scalars and <span class="process-math">\(\xvec_1\)</span> and <span class="process-math">\(\xvec_2\)</span> are vectors of the same dimension. Use <a href="" class="xref" data-knowl="./knowl/prop-var-of-linear-combo.html" title="Proposition 7.1.4: Variance of a linear combination">Proposition¬†7.1.4</a> to derive formulas for the following in terms of <span class="process-math">\(\var(\xvec_1)\text{,}\)</span> <span class="process-math">\(\var(\xvec_2)\text{,}\)</span> and <span class="process-math">\(\cov(\xvec_1, \xvec_2)\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-5069"><div class="para" id="p-7552"><span class="process-math">\(\var(a \xvec_1)\text{.}\)</span></div></li>
<li id="li-5070"><div class="para" id="p-7553"><span class="process-math">\(\var(a \xvec_1 + b \xvec_2)\text{.}\)</span></div></li>
<li id="li-5071"><div class="para" id="p-7554"><span class="process-math">\(\var(a \xvec_1 - b \xvec_2)\text{.}\)</span></div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-262"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="para logical" id="p-7555">
<div class="para">Identify each statement below as true or false and provide an explanation.</div>
<ol class="lower-alpha">
<li id="li-5072"><div class="para" id="p-7556">
<span class="process-math">\(\var(\xvec_1 - \xvec_2)\)</span> is always smaller than <span class="process-math">\(\var(\xvec_1 + \xvec_2)\text{.}\)</span>
</div></li>
<li id="li-5073"><div class="para" id="p-7557">
<span class="process-math">\(\var(\xvec_1 + \xvec_2)\)</span> is always larger than <span class="process-math">\(\var(\xvec_1)\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-263"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="para" id="p-7558">Show that the eigenvalues of a symmetric real-valued matrix are always real (i.e., they cannot be complex).</div> <div class="para" id="p-7559">Hint: Suppose <span class="process-math">\(\lambda,  \vvec\)</span> is an eigenpair. Simplify <span class="process-math">\((A \vvec)^{\transpose} A \vvec\)</span> and solve for <span class="process-math">\(\lambda\text{.}\)</span>
</div></article></section></section></div>
<div class="ptx-content-footer">
<a class="previous-button button" href="chap7.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon">^</span><span class="name">Top</span></a><a class="next-button button" href="sec-symmetric-matrices.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
