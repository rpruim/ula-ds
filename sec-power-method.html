<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Finding eigenvectors numerically</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" Randall Pruim ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math",
    "renderActions": {
      "findScript": [
        10,
        function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        },
        ""
      ]
    }
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-sage",
  "linked": true,
  "linkKey": "linked-sage",
  "autoeval": false,
  "languages": [
    "sage"
  ],
  "evalButtonText": "Evaluate (Sage)"
});
</script><script>// Make *any* pre with class 'sagecell-python' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-python",
  "linked": true,
  "linkKey": "linked-python",
  "autoeval": false,
  "languages": [
    "python"
  ],
  "evalButtonText": "Evaluate (Python)"
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.3/pretext_search.js"></script><link href="https://pretextbook.org/css/0.7/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.3</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.3/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.3/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.3/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link href="https://pretextbook.org/css/0.7/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra:</span> <span class="subtitle">Data Science Edition</span></a></h1>
<p class="byline">Randall Pruim</p>
</div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" class="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms" class="searchterms"></span>
</h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" aria-label="Show or hide table of contents"><span class="icon">‚ò∞</span><span class="name">Contents</span></button><a class="index-button button" href="index-1.html" title="Index"><span class="name">Index</span></a><button id="user-preferences-button" class="user-preferences-button button" title="Modify user preferences"><span id="avatarbutton" class="avatarbutton name">You!</span><div id="preferences_menu_holder" class="preferences_menu_holder hidden"><ol id="preferences_menu" class="preferences_menu" style="font-family: 'Roboto Serif', serif;">
<li data-env="avatar" tabindex="-1">Choose avatar<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden avatar">
<li data-val="You!" tabindex="-1">
<span id="theYou!" class="avatarcheck">‚úîÔ∏è</span>You!</li>
<li data-val="üò∫" tabindex="-1">
<span id="theüò∫" class="avatarcheck"></span>üò∫</li>
<li data-val="üë§" tabindex="-1">
<span id="theüë§" class="avatarcheck"></span>üë§</li>
<li data-val="üëΩ" tabindex="-1">
<span id="theüëΩ" class="avatarcheck"></span>üëΩ</li>
<li data-val="üê∂" tabindex="-1">
<span id="theüê∂" class="avatarcheck"></span>üê∂</li>
<li data-val="üêº" tabindex="-1">
<span id="theüêº" class="avatarcheck"></span>üêº</li>
<li data-val="üåà" tabindex="-1">
<span id="theüåà" class="avatarcheck"></span>üåà</li>
</ol>
</li>
<li data-env="fontfamily" tabindex="-1">Font family<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fontfamily">
<li data-val="face" data-change="OS" tabindex="-1" style="font-family: 'Open Sans'">
<span id="theOS" class="ffcheck">‚úîÔ∏è</span><span class="name">Open Sans</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
<li data-val="face" data-change="RS" tabindex="-1" style="font-family: 'Roboto Serif'">
<span id="theRS" class="ffcheck"></span><span class="name">Roboto Serif</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
</ol>
</li>
<li data-env="font" tabindex="-1">Adjust font<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fonts">
<li>Size</li>
<li><span id="thesize">12</span></li>
<li data-val="size" data-change="-1" tabindex="-1" style="font-size: 80%">Smaller</li>
<li data-val="size" data-change="1" tabindex="-1" style="font-size: 110%">Larger</li>
<li>Width</li>
<li><span id="thewdth">100</span></li>
<li data-val="wdth" data-change="-5" tabindex="-1" style="font-variation-settings: 'wdth' 60">narrower</li>
<li data-val="wdth" data-change="5" tabindex="-1" style="font-variation-settings: 'wdth' 150">wider</li>
<li>Weight</li>
<li><span id="thewght">400</span></li>
<li data-val="wght" data-change="-50" tabindex="-1" style="font-weight: 200">thinner</li>
<li data-val="wght" data-change="50" tabindex="-1" style="font-weight: 700">heavier</li>
<li>Letter spacing</li>
<li>
<span id="thelspace">0</span><span class="byunits">/200</span>
</li>
<li data-val="lspace" data-change="-1" tabindex="-1">closer</li>
<li data-val="lspace" data-change="1" tabindex="-1">f a r t h e r</li>
<li>Word spacing</li>
<li>
<span id="thewspace">0</span><span class="byunits">/50</span>
</li>
<li data-val="wspace" data-change="-1" tabindex="-1">smaller‚ÄÖgap‚ÄÉ</li>
<li data-val="wspace" data-change="1" tabindex="-1">larger‚ÄÉgap</li>
<li>Line Spacing</li>
<li>
<span id="theheight">135</span><span class="byunits">/100</span>
</li>
<li data-val="height" data-change="-5" tabindex="-1" style="line-height: 1">closer<br>together</li>
<li data-val="height" data-change="5" tabindex="-1" style="line-height: 1.75">further<br>apart</li>
</ol>
</li>
<li data-env="atmosphere" tabindex="-1">Light/dark mode<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden atmosphere">
<li data-val="default" tabindex="-1">
<span id="thedefault" class="atmospherecheck">‚úîÔ∏è</span>default</li>
<li data-val="pastel" tabindex="-1">
<span id="thepastel" class="atmospherecheck"></span>pastel</li>
<li data-val="darktwilight" tabindex="-1">
<span id="thedarktwilight" class="atmospherecheck"></span>twilight</li>
<li data-val="dark" tabindex="-1">
<span id="thedark" class="atmospherecheck"></span>dark</li>
<li data-val="darkmidnight" tabindex="-1">
<span id="thedarkmidnight" class="atmospherecheck"></span>midnight</li>
</ol>
</li>
<li data-env="ruler" tabindex="-1">Reading ruler<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden ruler">
<li data-val="none" tabindex="-1">
<span id="thenone" class="rulercheck">‚úîÔ∏è</span>none</li>
<li data-val="underline" tabindex="-1">
<span id="theunderline" class="rulercheck"></span>underline</li>
<li data-val="lunderline" tabindex="-1">
<span id="thelunderline" class="rulercheck"></span>L-underline</li>
<li data-val="greybar" tabindex="-1">
<span id="thegreybar" class="rulercheck"></span>grey bar</li>
<li data-val="lightbox" tabindex="-1">
<span id="thelightbox" class="rulercheck"></span>light box</li>
<li data-val="sunrise" tabindex="-1">
<span id="thesunrise" class="rulercheck"></span>sunrise</li>
<li data-val="sunriseunderline" tabindex="-1">
<span id="thesunriseunderline" class="rulercheck"></span>sunrise underline</li>
<li class="moveQ">Motion by:</li>
<li data-val="mouse" tabindex="-1">
<span id="themouse" class="motioncheck">‚úîÔ∏è</span>follow the mouse</li>
<li data-val="arrow" tabindex="-1">
<span id="thearrow" class="motioncheck"></span>up/down arrows - not yet</li>
<li data-val="eye" tabindex="-1">
<span id="theeye" class="motioncheck"></span>eye tracking - not yet</li>
</ol>
</li>
</ol></div></button><span class="treebuttons"><a class="previous-button button" href="sec-stochastic.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="up-button button" href="chap5.html" title="Up"><span class="icon">^</span><span class="name">Up</span></a><a class="next-button button" href="chap6.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a></span><div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" class="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\boldsymbol a}}
\newcommand{\bvec}{{\boldsymbol b}}
\newcommand{\cvec}{{\boldsymbol c}}
\newcommand{\dvec}{{\boldsymbol d}}
\newcommand{\dtil}{\widetilde{\boldsymbol d}}
\newcommand{\evec}{{\boldsymbol e}}
\newcommand{\fvec}{{\boldsymbol f}}
\newcommand{\mvec}{{\boldsymbol m}}
\newcommand{\nvec}{{\boldsymbol n}}
\newcommand{\pvec}{{\boldsymbol p}}
\newcommand{\qvec}{{\boldsymbol q}}
\newcommand{\rvec}{{\boldsymbol r}}
\newcommand{\svec}{{\boldsymbol s}}
\newcommand{\tvec}{{\boldsymbol t}}
\newcommand{\uvec}{{\boldsymbol u}}
\newcommand{\vvec}{{\boldsymbol v}}
\newcommand{\wvec}{{\boldsymbol w}}
\newcommand{\xvec}{{\boldsymbol x}}
\newcommand{\yvec}{{\boldsymbol y}}
\newcommand{\zvec}{{\boldsymbol z}}
\newcommand{\betavec}{{\boldsymbol \beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\zerovec}{{\boldsymbol 0}}
\newcommand{\onevec}{{\boldsymbol 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\xmean}{\overline{\xvec}}
\newcommand{\yhat}{\widehat{\yvec}}
\newcommand{\ymean}{\overline{\yvec}}
\newcommand{\betahat}{\widehat{\betavec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\by}{\times}
\newcommand{\transpose}{\top}
\newcommand{\proj}[2]{\operatorname{proj}\left(#1 \to #2\right)}
\newcommand{\projsub}[2]{\operatorname{proj}_{#2}(#1)}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural">
<li>
<div class="toc-item"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="colophon-1.html" class="internal"><span class="title">Colophon</span></a></div></li>
<li><div class="toc-item"><a href="preface-1.html" class="internal"><span class="title">Our goals -- Preface to David Austin‚Äôs original edition</span></a></div></li>
<li><div class="toc-item"><a href="preface-2.html" class="internal"><span class="title">What‚Äôs different in the data science edition?</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap1.html" class="internal"><span class="codenumber">1</span> <span class="title">Scalars, Vectors and Matrices</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-vectors.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsection-1" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Three ways to think about vectors</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-vectors.html#subsection-2" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Vector operations: scalar multiplication and vector addition.</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-scalar-multiplication" class="internal"><span class="codenumber">1.1.2.1</span> <span class="title">Scalar Multiplication</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-addition" class="internal"><span class="codenumber">1.1.2.2</span> <span class="title">Vector addition</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-properties" class="internal"><span class="codenumber">1.1.2.3</span> <span class="title">Mathematical properties of vector operations</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-3" class="internal"><span class="codenumber">1.1.3</span> <span class="title">The (Euclidean) length of a vector</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-4" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Summary</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-vectors-in-python.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Vectors in Python</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-intro-to-python" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Introduction to Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-numpy-vectors" class="internal"><span class="codenumber">1.2.2</span> <span class="title"><code class="code-inline tex2jax_ignore">numpy</code> vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-vector-length-numpy" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Vector length</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsection-8" class="internal"><span class="codenumber">1.2.4</span> <span class="title">Plotting vectors</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-combos-of-vectors.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Linear combinations of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#subsection-9" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#exercises-1" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-matrices.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrices.html#subsec-matrices-and-their-uses" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Matrices and their uses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-11" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Scalar multiplication and addition of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-12" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Matrix-vector multiplication and linear combinations</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-13" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Matrix-vector multiplication and linear systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#sec-matrices-in-python" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Matrices in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-15" class="internal"><span class="codenumber">1.4.6</span> <span class="title">Matrix-matrix products</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsec-special-matrices" class="internal"><span class="codenumber">1.4.7</span> <span class="title">Some special types of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-17" class="internal"><span class="codenumber">1.4.8</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#exercises-2" class="internal"><span class="codenumber">1.4.9</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-tensors.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Tensors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-tensors.html#subsec-numpy-tensors" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Tensors in NumPy</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-axes" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Aggregation and Axes</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-ndarray-append" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expanding an array</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-broadcasting" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Broadcasting</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#exercises-1-5" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap2.html" class="internal"><span class="codenumber">2</span> <span class="title">Systems of equations: Solving <span class="process-math">\(A \xvec = \bvec\)</span></span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-expect.html" class="internal"><span class="codenumber">2.1</span> <span class="title">What can we expect</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-expect.html#subsection-22" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Some simple examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-23" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Systems of linear equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-24" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#exercises-4" class="internal"><span class="codenumber">2.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-finding-solutions.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Finding solutions to linear systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-25" class="internal"><span class="codenumber">2.2.1</span> <span class="title">Gaussian elimination</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-26" class="internal"><span class="codenumber">2.2.2</span> <span class="title">Augmented matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-27" class="internal"><span class="codenumber">2.2.3</span> <span class="title">Reduced row echelon form</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsec-solving-matrix-equations" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Solving matrix equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-29" class="internal"><span class="codenumber">2.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#exercises-5" class="internal"><span class="codenumber">2.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-python-introduction.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Computational Linear Algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-30" class="internal"><span class="codenumber">2.3.1</span> <span class="title">Reduced row echelon form in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-linalg-solve" class="internal"><span class="codenumber">2.3.2</span> <span class="title">np.linalg.solve()</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-compute-effort" class="internal"><span class="codenumber">2.3.3</span> <span class="title">Computational effort</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-33" class="internal"><span class="codenumber">2.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-6" class="internal"><span class="codenumber">2.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pivots.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Pivots and their relationship to solution spaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pivots.html#subsection-34" class="internal"><span class="codenumber">2.4.1</span> <span class="title">The existence of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-35" class="internal"><span class="codenumber">2.4.2</span> <span class="title">The uniqueness of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-36" class="internal"><span class="codenumber">2.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#exercises-7" class="internal"><span class="codenumber">2.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap3.html" class="internal"><span class="codenumber">3</span> <span class="title">Linear combinations and transformations</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-span.html" class="internal"><span class="codenumber">3.1</span> <span class="title">The span of a set of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-span.html#subsection-37" class="internal"><span class="codenumber">3.1.1</span> <span class="title">The span of a set of vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-38" class="internal"><span class="codenumber">3.1.2</span> <span class="title">Pivot positions and span</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsec-span-and-linear-models" class="internal"><span class="codenumber">3.1.3</span> <span class="title">Span and linear models</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-40" class="internal"><span class="codenumber">3.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#exercises-8" class="internal"><span class="codenumber">3.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-dep.html" class="internal"><span class="codenumber">3.2</span> <span class="title">Linear independence</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-41" class="internal"><span class="codenumber">3.2.1</span> <span class="title">Linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-42" class="internal"><span class="codenumber">3.2.2</span> <span class="title">How to recognize linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-43" class="internal"><span class="codenumber">3.2.3</span> <span class="title">Homogeneous equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-44" class="internal"><span class="codenumber">3.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#exercises-9" class="internal"><span class="codenumber">3.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-trans.html" class="internal"><span class="codenumber">3.3</span> <span class="title">Matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-matrix-trans" class="internal"><span class="codenumber">3.3.1</span> <span class="title">Matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-linear-trans" class="internal"><span class="codenumber">3.3.2</span> <span class="title">Linear transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-47" class="internal"><span class="codenumber">3.3.3</span> <span class="title">Composing matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-dynamical-systems" class="internal"><span class="codenumber">3.3.4</span> <span class="title">Discrete Dynamical Systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-49" class="internal"><span class="codenumber">3.3.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#exercises-10" class="internal"><span class="codenumber">3.3.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transforms-geom.html" class="internal"><span class="codenumber">3.4</span> <span class="title">The geometry of matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-50" class="internal"><span class="codenumber">3.4.1</span> <span class="title">The geometry of <span class="process-math">\(2\by2\)</span> matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-51" class="internal"><span class="codenumber">3.4.2</span> <span class="title">Matrix transformations and computer animation</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-52" class="internal"><span class="codenumber">3.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#exercises-11" class="internal"><span class="codenumber">3.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap4.html" class="internal"><span class="codenumber">4</span> <span class="title">Invertibility, bases, and coordinate systems</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-matrix-inverse.html" class="internal"><span class="codenumber">4.1</span> <span class="title">Invertibility</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-53" class="internal"><span class="codenumber">4.1.1</span> <span class="title">Invertible matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-54" class="internal"><span class="codenumber">4.1.2</span> <span class="title">Solving equations with an inverse</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsec-finding-inverses" class="internal"><span class="codenumber">4.1.3</span> <span class="title">Finding inverses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-56" class="internal"><span class="codenumber">4.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#exercises-12" class="internal"><span class="codenumber">4.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="subsec-triangular-invertible.html" class="internal"><span class="codenumber">4.2</span> <span class="title">Triangular matrices and Gaussian elimination</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-triangular-matrices" class="internal"><span class="codenumber">4.2.1</span> <span class="title">Triangular matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-elementary-matrices" class="internal"><span class="codenumber">4.2.2</span> <span class="title">Elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsection-59" class="internal"><span class="codenumber">4.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#exercises-13" class="internal"><span class="codenumber">4.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-bases.html" class="internal"><span class="codenumber">4.3</span> <span class="title">Bases and coordinate systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-bases.html#subsection-60" class="internal"><span class="codenumber">4.3.1</span> <span class="title">Bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-61" class="internal"><span class="codenumber">4.3.2</span> <span class="title">Coordinate systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-62" class="internal"><span class="codenumber">4.3.3</span> <span class="title">Examples of bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-63" class="internal"><span class="codenumber">4.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#exercises-14" class="internal"><span class="codenumber">4.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-jpeg.html" class="internal"><span class="codenumber">4.4</span> <span class="title">Image compression</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-64" class="internal"><span class="codenumber">4.4.1</span> <span class="title">Color models</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-65" class="internal"><span class="codenumber">4.4.2</span> <span class="title">The JPEG compression algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-66" class="internal"><span class="codenumber">4.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#exercises-15" class="internal"><span class="codenumber">4.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-determinants.html" class="internal"><span class="codenumber">4.5</span> <span class="title">Determinants</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsection-67" class="internal"><span class="codenumber">4.5.1</span> <span class="title">Determinants of <span class="process-math">\(2\by2\)</span> matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsec-determinants-larger-matrices" class="internal"><span class="codenumber">4.5.2</span> <span class="title">Determinants of larger matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-69" class="internal"><span class="codenumber">4.5.3</span> <span class="title">Determinants of elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-70" class="internal"><span class="codenumber">4.5.4</span> <span class="title">Cofactor expansions</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-71" class="internal"><span class="codenumber">4.5.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#exercises-16" class="internal"><span class="codenumber">4.5.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-subspaces.html" class="internal"><span class="codenumber">4.6</span> <span class="title">Subspaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-72" class="internal"><span class="codenumber">4.6.1</span> <span class="title">Subspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-73" class="internal"><span class="codenumber">4.6.2</span> <span class="title">The column space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-74" class="internal"><span class="codenumber">4.6.3</span> <span class="title">The null space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-75" class="internal"><span class="codenumber">4.6.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#exercises-17" class="internal"><span class="codenumber">4.6.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gaussian-revisited.html" class="internal"><span class="codenumber">4.7</span> <span class="title">Partial pivoting and LU factorizations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal"><span class="codenumber">4.7.1</span> <span class="title">Partial pivoting</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-77" class="internal"><span class="codenumber">4.7.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-78" class="internal"><span class="codenumber">4.7.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#exercises-18" class="internal"><span class="codenumber">4.7.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap5.html" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-eigen-intro.html" class="internal"><span class="codenumber">5.1</span> <span class="title">An introduction to eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-79" class="internal"><span class="codenumber">5.1.1</span> <span class="title">A few examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsec-eigen-use" class="internal"><span class="codenumber">5.1.2</span> <span class="title">The usefulness of eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-81" class="internal"><span class="codenumber">5.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#exercises-19" class="internal"><span class="codenumber">5.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-find.html" class="internal"><span class="codenumber">5.2</span> <span class="title">Finding eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-82" class="internal"><span class="codenumber">5.2.1</span> <span class="title">The characteristic polynomial</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-83" class="internal"><span class="codenumber">5.2.2</span> <span class="title">Finding eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-84" class="internal"><span class="codenumber">5.2.3</span> <span class="title">The characteristic polynomial and the dimension of eigenspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-85" class="internal"><span class="codenumber">5.2.4</span> <span class="title">Using Python to find eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-86" class="internal"><span class="codenumber">5.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#exercises-20" class="internal"><span class="codenumber">5.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-diag.html" class="internal"><span class="codenumber">5.3</span> <span class="title">Diagonalization, similarity, and powers of a matrix</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-87" class="internal"><span class="codenumber">5.3.1</span> <span class="title">Diagonalization of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-88" class="internal"><span class="codenumber">5.3.2</span> <span class="title">Powers of a diagonalizable matrix</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-89" class="internal"><span class="codenumber">5.3.3</span> <span class="title">Similarity and complex eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-90" class="internal"><span class="codenumber">5.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#exercises-21" class="internal"><span class="codenumber">5.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-dynamical.html" class="internal"><span class="codenumber">5.4</span> <span class="title">Dynamical systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-91" class="internal"><span class="codenumber">5.4.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-92" class="internal"><span class="codenumber">5.4.2</span> <span class="title">Classifying dynamical systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-93" class="internal"><span class="codenumber">5.4.3</span> <span class="title">A <span class="process-math">\(3\by3\)</span> system</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-94" class="internal"><span class="codenumber">5.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#exercises-22" class="internal"><span class="codenumber">5.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-stochastic.html" class="internal"><span class="codenumber">5.5</span> <span class="title">Markov chains and Google‚Äôs PageRank algorithm</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-95" class="internal"><span class="codenumber">5.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-96" class="internal"><span class="codenumber">5.5.2</span> <span class="title">Markov chains</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsec-google" class="internal"><span class="codenumber">5.5.3</span> <span class="title">Google‚Äôs PageRank algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-98" class="internal"><span class="codenumber">5.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#exercises-23" class="internal"><span class="codenumber">5.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li class="active">
<div class="toc-item"><a href="sec-power-method.html" class="internal"><span class="codenumber">5.6</span> <span class="title">Finding eigenvectors numerically</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-power-method.html#subsection-99" class="internal"><span class="codenumber">5.6.1</span> <span class="title">The power method</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-100" class="internal"><span class="codenumber">5.6.2</span> <span class="title">Finding other eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-101" class="internal"><span class="codenumber">5.6.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#exercises-24" class="internal"><span class="codenumber">5.6.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap6.html" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-dot-product.html" class="internal"><span class="codenumber">6.1</span> <span class="title">The dot product</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dot-product.html#sec-projections-and-dot-products" class="internal"><span class="codenumber">6.1.1</span> <span class="title">Projections and dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsec-computing-dot-products" class="internal"><span class="codenumber">6.1.2</span> <span class="title">Computing dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-104" class="internal"><span class="codenumber">6.1.3</span> <span class="title"><span class="process-math">\(k\)</span>-means clustering</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-105" class="internal"><span class="codenumber">6.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#exercises-25" class="internal"><span class="codenumber">6.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transpose.html" class="internal"><span class="codenumber">6.2</span> <span class="title">Orthogonal complements and the matrix transpose</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transpose.html#subsection-106" class="internal"><span class="codenumber">6.2.1</span> <span class="title">Orthogonal complements</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-107" class="internal"><span class="codenumber">6.2.2</span> <span class="title">The matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-108" class="internal"><span class="codenumber">6.2.3</span> <span class="title">Properties of the matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-109" class="internal"><span class="codenumber">6.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#exercises-26" class="internal"><span class="codenumber">6.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-orthogonal-bases.html" class="internal"><span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-110" class="internal"><span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-111" class="internal"><span class="codenumber">6.3.2</span> <span class="title">Orthogonal projections</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-112" class="internal"><span class="codenumber">6.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#exercises-27" class="internal"><span class="codenumber">6.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gram-schmidt.html" class="internal"><span class="codenumber">6.4</span> <span class="title">Finding orthogonal bases</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-113" class="internal"><span class="codenumber">6.4.1</span> <span class="title">Gram-Schmidt orthogonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-114" class="internal"><span class="codenumber">6.4.2</span> <span class="title"><span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-115" class="internal"><span class="codenumber">6.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#exercises-28" class="internal"><span class="codenumber">6.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-least-squares.html" class="internal"><span class="codenumber">6.5</span> <span class="title">Least squares methods</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-116" class="internal"><span class="codenumber">6.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-linear-model-framework" class="internal"><span class="codenumber">6.5.2</span> <span class="title">The linear model framework</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-118" class="internal"><span class="codenumber">6.5.3</span> <span class="title">Solving least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-119" class="internal"><span class="codenumber">6.5.4</span> <span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-120" class="internal"><span class="codenumber">6.5.5</span> <span class="title">Polynomial Regression</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-skl-lm" class="internal"><span class="codenumber">6.5.6</span> <span class="title">Fitting linear models with standard tools</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-122" class="internal"><span class="codenumber">6.5.7</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#exercises-29" class="internal"><span class="codenumber">6.5.8</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap7.html" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-symmetric-matrices.html" class="internal"><span class="codenumber">7.1</span> <span class="title">Symmetric matrices and variance</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-123" class="internal"><span class="codenumber">7.1.1</span> <span class="title">Symmetric matrices and orthogonal diagonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-124" class="internal"><span class="codenumber">7.1.2</span> <span class="title">Variance</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-125" class="internal"><span class="codenumber">7.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#exercises-30" class="internal"><span class="codenumber">7.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-quadratic-forms.html" class="internal"><span class="codenumber">7.2</span> <span class="title">Quadratic forms</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-126" class="internal"><span class="codenumber">7.2.1</span> <span class="title">Quadratic forms</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-127" class="internal"><span class="codenumber">7.2.2</span> <span class="title">Definite symmetric matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-128" class="internal"><span class="codenumber">7.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#exercises-31" class="internal"><span class="codenumber">7.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pca.html" class="internal"><span class="codenumber">7.3</span> <span class="title">Principal Component Analysis</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pca.html#subsection-129" class="internal"><span class="codenumber">7.3.1</span> <span class="title">Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-130" class="internal"><span class="codenumber">7.3.2</span> <span class="title">Using Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-131" class="internal"><span class="codenumber">7.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#exercises-32" class="internal"><span class="codenumber">7.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-intro.html" class="internal"><span class="codenumber">7.4</span> <span class="title">Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-132" class="internal"><span class="codenumber">7.4.1</span> <span class="title">Finding singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-133" class="internal"><span class="codenumber">7.4.2</span> <span class="title">The structure of singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-134" class="internal"><span class="codenumber">7.4.3</span> <span class="title">Reduced singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-135" class="internal"><span class="codenumber">7.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#exercises-33" class="internal"><span class="codenumber">7.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-uses.html" class="internal"><span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-136" class="internal"><span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-137" class="internal"><span class="codenumber">7.5.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-138" class="internal"><span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-139" class="internal"><span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-140" class="internal"><span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-141" class="internal"><span class="codenumber">7.5.6</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#exercises-34" class="internal"><span class="codenumber">7.5.7</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="backmatter.html" class="internal"><span class="title">Back Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="app-notation.html" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></div></li>
<li>
<div class="toc-item"><a href="app-python-reference.html" class="internal"><span class="codenumber">B</span> <span class="title">Python Reference</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsection-142.html" class="internal"><span class="codenumber">B.1</span> <span class="title">Accessing Python</span></a></div></li>
<li><div class="toc-item"><a href="subsection-143.html" class="internal"><span class="codenumber">B.2</span> <span class="title">Packages and libraries for data science</span></a></div></li>
<li><div class="toc-item"><a href="subsec-frequently-used-python.html" class="internal"><span class="codenumber">B.3</span> <span class="title">Frequently used Python commands</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="index-1.html" class="internal"><span class="title">Index</span></a></div></li>
<li><div class="toc-item"><a href="colophon-2.html" class="internal"><span class="title">Colophon</span></a></div></li>
</ul>
</li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-power-method"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">5.6</span><span class="space"> </span><span class="title">Finding eigenvectors numerically</span>
</h2>
<section class="introduction" id="introduction-29"><div class="para" id="p-5528">We have typically found eigenvalues of a square matrix <span class="process-math">\(A\)</span> as the roots of the characteristic polynomial <span class="process-math">\(\det(A-\lambda I) = 0\)</span> and the associated eigenvectors as the null space <span class="process-math">\(\nul(A-\lambda I)\text{.}\)</span>  Unfortunately, this approach is not practical when we are working with large matrices. First, finding the charactertic polynomial of a large matrix requires considerable computation, as does finding the roots of that polynomial.  Second, finding the null space of a singular matrix is plagued by numerical problems, as we will see in the preview activity.</div> <div class="para" id="p-5529">For this reason, we will explore a technique called the <dfn class="terminology">power method</dfn> that finds numerical approximations to the eigenvalues and eigenvectors of a square matrix.</div> <article class="exploration project-like" id="exploration-20"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">5.6.1</span><span class="period">.</span>
</h3>
<div class="para logical" id="p-5530">
<div class="para">Let‚Äôs recall some earlier observations about eigenvalues and eigenvectors.</div>
<ol class="lower-alpha">
<li id="li-3678"><div class="para" id="p-5531">How are the eigenvalues and associated eigenvectors of <span class="process-math">\(A\)</span> related to those of <span class="process-math">\(A^{-1}\text{?}\)</span>
</div></li>
<li id="li-3679"><div class="para" id="p-5532">How are the eigenvalues and associated eigenvectors of <span class="process-math">\(A\)</span> related to those of <span class="process-math">\(A-3I\text{?}\)</span>
</div></li>
<li id="li-3680"><div class="para" id="p-5533">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> what can we say about the pivot positions of <span class="process-math">\(A-\lambda
I\text{?}\)</span>
</div></li>
<li id="li-3681">
<div class="para" id="p-5534">Suppose that <span class="process-math">\(A = \left[\begin{array}{rr}
0.8 \amp 0.4 \\
0.2 \amp 0.6 \\
\end{array}\right]
\text{.}\)</span>  Explain how we know that <span class="process-math">\(1\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> and then explain why the following computation is incorrect.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-184"><script type="text/x-sage">import numpy as np
import sympy as s   
A = s.Matrix([[0.8, 0.4], [0.2, 0.6]])
I = s.eye(2) 
print((A-I).rref())
</script></pre>
</li>
<li id="li-3682"><div class="para" id="p-5535">Suppose that <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{,}\)</span> and we define a sequence <span class="process-math">\(\xvec_{k+1} = A\xvec_k\text{;}\)</span>  in other words, <span class="process-math">\(\xvec_{k} = A^k\xvec_0\text{.}\)</span>  What happens to <span class="process-math">\(\xvec_k\)</span> as <span class="process-math">\(k\)</span> grows increasingly large?</div></li>
<li id="li-3683"><div class="para" id="p-5536">Explain how the eigenvalues of <span class="process-math">\(A\)</span> are responsible for the behavior noted in the previous question.</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-273" id="solution-273"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-273"><div class="solution solution-like"><div class="para logical" id="p-5537"><ol class="lower-alpha">
<li id="li-3684"><div class="para" id="p-5538">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(A\vvec=\lambda\vvec\)</span> for an associated eigenvector <span class="process-math">\(\vvec\text{.}\)</span>  Multiplying by <span class="process-math">\(A^{-1}\)</span> and <span class="process-math">\(\lambda^{-1}\text{,}\)</span> we obtain <span class="process-math">\(\lambda^{-1}\vvec =
A^{-1}\vvec\text{,}\)</span> which shows that <span class="process-math">\(\lambda^{-1}\)</span> is an eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3685"><div class="para" id="p-5539">In the same way, if <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(A\vvec=\lambda\vvec\)</span> for an associated eigenvector <span class="process-math">\(\vvec\text{.}\)</span>  This means that <span class="process-math">\((A-3I)\vvec = (\lambda-3)\vvec\)</span> so that <span class="process-math">\(\lambda-3\)</span> is an eigenvalue of <span class="process-math">\(A-3I\text{.}\)</span>
</div></li>
<li id="li-3686"><div class="para" id="p-5540">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(A-\lambda I\)</span> is not invertible and so has a row without a pivot position.</div></li>
<li id="li-3687"><div class="para" id="p-5541">Since <span class="process-math">\(A\)</span> is a positive stochastic matrix, we know that <span class="process-math">\(\lambda=1\)</span> is an eigenvalue and hence that <span class="process-math">\(A-I\)</span> is not invertible.  Sympy, however, tells us that <span class="process-math">\(A-I\sim I\text{,}\)</span> which cannot be true since <span class="process-math">\(A-I\)</span> is not invertible.</div></li>
<li id="li-3688"><div class="para" id="p-5542">The vectors <span class="process-math">\(\xvec_k\)</span> form a Markov chain, which must converge to the steady-state vector <span class="process-math">\(\qvec=\twovec{\frac23}{\frac13}\text{.}\)</span>
</div></li>
<li id="li-3689"><div class="para" id="p-5543">We have eigenvalues <span class="process-math">\(\lambda_1=1\)</span> and <span class="process-math">\(\lambda_2=0.4\text{.}\)</span>  If we begin with <span class="process-math">\(\xvec_0=c_1\vvec_1+c_2\vvec_2\)</span> and successively multiply by <span class="process-math">\(A\text{,}\)</span> we have <span class="process-math">\(\xvec_k=c_1\vvec_1 + c_2(0.4)^k\vvec_2\text{.}\)</span>  When <span class="process-math">\(k\)</span> becomes large, the coefficient of <span class="process-math">\(\vvec_2\)</span> becomes insignificantly small so we are left with an eigenvector in <span class="process-math">\(E_1\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsection-99"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">5.6.1</span><span class="space"> </span><span class="title">The power method</span>
</h3>
<div class="para" id="p-5544">Our goal is to find a technique that produces numerical approximations to the eigenvalues and associated eigenvectors of a matrix <span class="process-math">\(A\text{.}\)</span>  We begin by searching for the eigenvalue having the largest absolute value, which is called the <dfn class="terminology">dominant eignevalue</dfn>.  The next two examples demonstrate this technique.</div>
<article class="example example-like" id="example-60"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.6.1</span><span class="period">.</span>
</h4> <div class="para" id="p-5545">Let‚Äôs begin with the positive stochastic matrix <span class="process-math">\(A=\left[\begin{array}{rr}
0.7 \amp 0.6 \\
0.3 \amp 0.4 \\
\end{array}\right]
\text{.}\)</span> We spent quite a bit of time studying this type of matrix in <a href="sec-stochastic.html" class="internal" title="Section 5.5: Markov chains and Google‚Äôs PageRank algorithm">Section¬†5.5</a>; in particular, we saw that any Markov chain will converge to the unique steady state vector.  Let‚Äôs rephrase this statement in terms of the eigenvectors of <span class="process-math">\(A\text{.}\)</span>
</div> <div class="para logical" id="p-5546">
<div class="para">This matrix has eigenvalues <span class="process-math">\(\lambda_1 = 1\)</span> and <span class="process-math">\(\lambda_2 =0.1\)</span> so the dominant eigenvalue is <span class="process-math">\(\lambda_1 = 1\text{.}\)</span> The associated eigenvectors are <span class="process-math">\(\vvec_1 =
\twovec{2}{1}\)</span> and <span class="process-math">\(\vvec_2 = \twovec{-1}{1}\text{.}\)</span> Suppose we begin with the vector</div>
<div class="displaymath process-math">
\begin{equation*}
\xvec_0 = \twovec{1}{0} = \frac13 \vvec_1 - \frac13 \vvec_2
\end{equation*}
</div>
<div class="para">and find</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
\xvec_1 \amp {}={} A\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)
\vvec_2 \\
\xvec_2 \amp {}={} A^2\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^2
\vvec_2 \\
\xvec_3 \amp {}={} A^3\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^3
\vvec_2 \\
\amp \vdots \\
\xvec_k \amp {}={} A^k\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^k
\vvec_2 \\
\end{aligned}
\end{equation*}
</div>
<div class="para">and so forth.  Notice that the powers <span class="process-math">\(0.1^k\)</span> become increasingly small as <span class="process-math">\(k\)</span> grows so that <span class="process-math">\(\xvec_k\approx
\frac13\vvec_1\)</span> when <span class="process-math">\(k\)</span> is large.  Therefore, the vectors <span class="process-math">\(\xvec_k\)</span> become increasingly close to a vector in the eigenspace <span class="process-math">\(E_1\text{,}\)</span> the eigenspace associated to the dominant eigenvalue.  If we did not know the eigenvector <span class="process-math">\(\vvec_1\text{,}\)</span> we could use a Markov chain in this way to find a basis vector for <span class="process-math">\(E_1\text{,}\)</span> which is essentially how the Google PageRank algorithm works.</div>
</div></article><article class="example example-like" id="example-61"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.6.2</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-5547">
<div class="para">Let‚Äôs now look at the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
2 \amp 1 \\ 1 \amp 2 \\ \end{array}\right] \text{,}\)</span> which has eigenvalues <span class="process-math">\(\lambda_1=3\)</span> and <span class="process-math">\(\lambda_2 = 1\text{.}\)</span>  The dominant eigenvalue is <span class="process-math">\(\lambda_1=3\text{,}\)</span> and the associated eigenvectors are <span class="process-math">\(\vvec_1 = \twovec{1}{1}\)</span> and <span class="process-math">\(\vvec_{2} =
\twovec{-1}{1}\text{.}\)</span>  Once again, begin with the vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}=\frac12 \vvec_1 - \frac12 \vvec_2\)</span> so that</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
\xvec_1 \amp {}={} A\xvec_0 = 3\frac12 \vvec_1 - \frac12
\vvec_2 \\
\xvec_2 \amp {}={} A^2\xvec_0 = 3^2\frac13 \vvec_1 - \frac12
\vvec_2 \\
\xvec_3 \amp {}={} A^3\xvec_0 = 3^3\frac13 \vvec_1 - \frac12
\vvec_2 \\
\amp \vdots \\
\xvec_k \amp {}={} A^k\xvec_0 = 3^k\frac13 \vvec_1 - \frac12
\vvec_2\text{.} \\
\end{aligned}
\end{equation*}
</div>
</div> <div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:60%;"><div class="para" id="p-5548">As the figure shows, the vectors <span class="process-math">\(\xvec_k\)</span> are stretched by a factor of <span class="process-math">\(3\)</span> in the <span class="process-math">\(\vvec_1\)</span> direction and not at all in the <span class="process-math">\(\vvec_2\)</span> direction. Consequently, the vectors <span class="process-math">\(\xvec_k\)</span> become increasingly long, but their direction becomes closer to the direction of the eigenvector <span class="process-math">\(\vvec_1=\twovec{1}{1}\)</span> associated to the dominant eigenvalue.</div></div>
<div class="sbspanel top" style="width:40%;"><img src="external/images/numerical-power.svg" role="img" class="contained"></div>
</div></div> <div class="para" id="p-5549">To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors <span class="process-math">\(\xvec_k\)</span> from growing arbitrarily large by multiplying by an appropriate scaling constant.  Here is one way to do this. Given the vector <span class="process-math">\(\xvec_k\text{,}\)</span> we identify its component having the largest absolute value and call it <span class="process-math">\(m_k\text{.}\)</span>  We then define <span class="process-math">\(\overline{\xvec}_k = \frac{1}{m_k} \xvec_k\text{,}\)</span> which means that the component of <span class="process-math">\(\overline{\xvec}_k\)</span> having the largest absolute value is <span class="process-math">\(1\text{.}\)</span>
</div> <div class="para" id="p-5550">For example, beginning with <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{,}\)</span> we find <span class="process-math">\(\xvec_1 = A\xvec_{0} =
\twovec{2}{1}\text{.}\)</span>  The component of <span class="process-math">\(\xvec_1\)</span> having the largest absolute value is <span class="process-math">\(m_1=2\)</span> so we multiply by <span class="process-math">\(\frac{1}{m_1} = \frac12\)</span> to obtain <span class="process-math">\(\overline{\xvec}_1 =
\twovec{1}{\frac12}\text{.}\)</span>  Then <span class="process-math">\(\xvec_2 = A\overline{\xvec}_1 =
\twovec{\frac52}{2}\text{.}\)</span>  Now the component having the largest absolute value is <span class="process-math">\(m_2=\frac52\)</span> so we multiply by <span class="process-math">\(\frac25\)</span> to obtain <span class="process-math">\(\overline{\xvec}_2 = \twovec{1}{\frac45}\text{.}\)</span>
</div> <div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:60%;"><div class="para" id="p-5551">The resulting sequence of vectors <span class="process-math">\(\overline{\xvec}_k\)</span> is shown in the figure.  Notice how the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> now approach the eigenvector <span class="process-math">\(\vvec_1\text{,}\)</span> which gives us a way to find the eigenvector <span class="process-math">\(\vvec=\twovec{1}{1}\text{.}\)</span> This is the <dfn class="terminology">power method</dfn> for finding an eigenvector associated to the dominant eigenvalue of a matrix. </div></div>
<div class="sbspanel top" style="width:40%;"><img src="external/images/numerical-power-norm.svg" role="img" class="contained"></div>
</div></div></article><article class="activity project-like" id="activity-66"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.6.2</span><span class="period">.</span>
</h4>
<div class="para" id="p-5552">Let‚Äôs begin by considering the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
0.5 \amp 0.2 \\
0.4 \amp 0.7 \\
\end{array}\right]\)</span> and the initial vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{.}\)</span>
</div> <pre class="ptx-sagecell sagecell-python" id="sage-185"><script type="text/x-sage">
</script></pre> <div class="para logical" id="p-5553"><ol class="lower-alpha">
<li id="li-3690"><div class="para" id="p-5554">Compute the vector <span class="process-math">\(\xvec_1 =
A\xvec_0\text{.}\)</span>
</div></li>
<li id="li-3691"><div class="para" id="p-5555">Find <span class="process-math">\(m_1\text{,}\)</span> the component of <span class="process-math">\(\xvec_1\)</span> that has the largest absolute value.  Then form <span class="process-math">\(\overline{\xvec}_1 =
\frac 1{m_1} \xvec_1\text{.}\)</span>  Notice that the component having the largest absolute value of <span class="process-math">\(\overline{\xvec}_1\)</span> is <span class="process-math">\(1\text{.}\)</span>
</div></li>
<li id="li-3692"><div class="para" id="p-5556">Find the vector <span class="process-math">\(\xvec_2 = A\overline{\xvec}_1\text{.}\)</span> Identify the component <span class="process-math">\(m_2\)</span> of <span class="process-math">\(\xvec_2\)</span> having the largest absolute value.  Then form <span class="process-math">\(\overline{\xvec}_2 =
\frac1{m_2}\overline{\xvec}_1\)</span> to obtain a vector in which the component with the largest absolute value is <span class="process-math">\(1\text{.}\)</span>
</div></li>
<li id="li-3693">
<div class="para" id="p-5557">The Python cell below defines a function that implements the power method.  Define the matrix <span class="process-math">\(A\)</span> and initial vector <span class="process-math">\(\xvec_0\)</span> below.  The command <code class="code-inline tex2jax_ignore">power(A, x0,	N)</code> will print out the multiplier <span class="process-math">\(m\)</span> and the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> for <span class="process-math">\(N\)</span> steps of the power method.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-186"><script type="text/x-sage">import sympy as s
def power(A, x, N):
    for i in range(N):
        x = A@x
        m = max([comp for comp in x], key = abs).evalf(digits = 14)
        x = 1 / float(m) * x
        print (m, x)

### Define the matrix A and initial vector x0 below
A =
x0 =
power(A, x0, 20)
</script></pre>
<div class="para" id="p-5558">How does this computation identify an eigenvector of the matrix <span class="process-math">\(A\text{?}\)</span>
</div>
</li>
<li id="li-3694"><div class="para" id="p-5559">What is the corresponding eigenvalue of this eigenvector?</div></li>
<li id="li-3695"><div class="para" id="p-5560">How do the values of the multipliers <span class="process-math">\(m_k\)</span> tell us the eigenvalue associated to the eigenvector we have found?</div></li>
<li id="li-3696"><div class="para" id="p-5561">Consider now the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]
\text{.}\)</span> Use the power method to find the dominant eigenvalue of <span class="process-math">\(A\)</span> and an associated eigenvector.</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-274" id="solution-274"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-274"><div class="solution solution-like"><div class="para logical" id="p-5562"><ol class="lower-alpha">
<li id="li-3697"><div class="para" id="p-5563">We find <span class="process-math">\(\xvec_1=A\xvec_0 =
\twovec{0.5}{0.4}\text{.}\)</span>
</div></li>
<li id="li-3698"><div class="para" id="p-5564">The first component has the largest absolute value so <span class="process-math">\(m_1=0.5\text{.}\)</span>  Therefore, <span class="process-math">\(\overline{\xvec}_1=\frac1{0.5}\xvec_0 =
\twovec1{0.8}\text{.}\)</span>
</div></li>
<li id="li-3699"><div class="para" id="p-5565">In the same way, we obtain <span class="process-math">\(\xvec_2=A\overline{\xvec}_1 = \twovec{0.66}{0.96}\text{.}\)</span> We see that <span class="process-math">\(m_2=0.96\)</span> so we have <span class="process-math">\(\overline{\xvec}_2= \twovec{0.688}{1}\text{.}\)</span>
</div></li>
<li id="li-3700"><div class="para" id="p-5566">We see that the vectors <span class="process-math">\(\xvec_k\)</span> are getting closer and closer to <span class="process-math">\(\twovec{0.5}1\text{,}\)</span> which we therefore identify as an eigenvector associated to the dominant eigenvalue.</div></li>
<li id="li-3701"><div class="para" id="p-5567">We see that <span class="process-math">\(A\twovec{0.5}{1} =
\twovec{0.45}{0.9}=0.9 \twovec{0.5}1\text{.}\)</span>  Therefore, the dominant eigenvalue is <span class="process-math">\(\lambda_1=0.9\text{.}\)</span>
</div></li>
<li id="li-3702"><div class="para" id="p-5568">More generally, we see that the multiplier <span class="process-math">\(m_k\)</span> will converge to the dominant eigenvalue.</div></li>
<li id="li-3703"><div class="para" id="p-5569">The power method constructs a sequence of vectors <span class="process-math">\(\overline{\xvec}_k\)</span> converging to an eigenvector <span class="process-math">\(\vvec_1=\twovec{1}{\frac23}\text{.}\)</span>  The multipliers <span class="process-math">\(m_k\)</span> converge to <span class="process-math">\(\lambda_1=1.3\text{,}\)</span> the dominant eigenvalue.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-5570">Notice that the power method gives us not only an eigenvector <span class="process-math">\(\vvec\)</span> but also its associated eigenvalue.  As in the activity, consider the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]
\text{,}\)</span> which has eigenvector <span class="process-math">\(\vvec=\twovec{3}{2}\text{.}\)</span>  The first component has the largest absolute value so we multiply by <span class="process-math">\(\frac13\)</span> to obtain <span class="process-math">\(\overline{\vvec}=\twovec{1}{\frac23}\text{.}\)</span>  When we multiply by <span class="process-math">\(A\text{,}\)</span> we have <span class="process-math">\(A\overline{\vvec} = \twovec{-1.30}{-0.86}\text{.}\)</span> Notice that the first component still has the largest absolute value so that the multiplier <span class="process-math">\(m=-1.3\)</span> is the eigenvalue <span class="process-math">\(\lambda\)</span> corresponding to the eigenvector.  This demonstrates the fact that the multipliers <span class="process-math">\(m_k\)</span> approach the eigenvalue <span class="process-math">\(\lambda\)</span> having the largest absolute value.</div>
<div class="para" id="p-5571">Notice that the power method requires us to choose an initial vector <span class="process-math">\(\xvec_0\text{.}\)</span>  For most choices, this method will find the eigenvalue having the largest absolute value.  However, an unfortunate choice of <span class="process-math">\(\xvec_0\)</span> may not.  For instance, if we had chosen <span class="process-math">\(\xvec_0 = \vvec_2\)</span> in our example above, the vectors in the sequence <span class="process-math">\(\xvec_k =
A^k\xvec_0=\lambda_2^k\vvec_2\)</span> will not detect the eigenvector <span class="process-math">\(\vvec_1\text{.}\)</span>  However, it usually happens that our initial guess <span class="process-math">\(\xvec_0\)</span> has some contribution from <span class="process-math">\(\vvec_1\)</span> that enables us to find it.</div>
<div class="para" id="p-5572">The power method, as presented here, will fail for certain unlucky matrices.  This is examined in <a href="" class="xref" data-knowl="./knowl/exercise-power-method.html" title="Exercise 5.6.4.5">Exercise¬†5.6.4.5</a> along with a means to improve the power method to work for all matrices.</div></section><section class="subsection" id="subsection-100"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">5.6.2</span><span class="space"> </span><span class="title">Finding other eigenvalues</span>
</h3>
<div class="para" id="p-5573">The power method gives a technique for finding the dominant eigenvalue of a matrix.  We can modify the method to find the other eigenvalues as well.</div>
<article class="activity project-like" id="activity-67"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.6.3</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-5574">
<div class="para">The key to finding the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value is to note that the eigenvectors of <span class="process-math">\(A\)</span> are the same as those of <span class="process-math">\(A^{-1}\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-3704"><div class="para" id="p-5575">If <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A\)</span> with associated eigenvector <span class="process-math">\(\lambda\text{,}\)</span> explain why <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A^{-1}\)</span> with associated eigenvalue <span class="process-math">\(\lambda^{-1}\text{.}\)</span>
</div></li>
<li id="li-3705"><div class="para" id="p-5576">Explain why the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value is the reciprocal of the dominant eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3706"><div class="para" id="p-5577">Explain how to use the power method applied to <span class="process-math">\(A^{-1}\)</span> to find the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value.</div></li>
<li id="li-3707"><div class="para" id="p-5578">If we apply the power method to <span class="process-math">\(A^{-1}\text{,}\)</span> we begin with an intial vector <span class="process-math">\(\xvec_0\)</span> and generate the sequence <span class="process-math">\(\xvec_{k+1} = A^{-1}\xvec_k\text{.}\)</span>  It is not computationally efficient to compute <span class="process-math">\(A^{-1}\text{,}\)</span> however, so instead we solve the equation <span class="process-math">\(A\xvec_{k+1} =
\xvec_k\text{.}\)</span>  Explain why an <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\)</span> is useful for implementing the power method applied to <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3708">
<div class="para" id="p-5579">The following Python cell defines a command called <code class="code-inline tex2jax_ignore">inverse_power</code> that applies the power method to <span class="process-math">\(A^{-1}\text{.}\)</span>  That is, <code class="code-inline tex2jax_ignore">inverse_power(A, x0, N)</code> prints the vectors <span class="process-math">\(\xvec_k\text{,}\)</span> where <span class="process-math">\(\xvec_{k+1} =
A^{-1}\xvec_k\text{,}\)</span> and multipliers <span class="process-math">\(\frac{1}{m_k}\text{,}\)</span> which approximate the eigenvalue of <span class="process-math">\(A\text{.}\)</span>  Use it to find the eigenvalue of <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]\)</span> having the smallest absolute value.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-187"><script type="text/x-sage">import numpy as np
from scipy import linalg
def inverse_power(A, x, N):
	result = []
    for i in range(N):
        x = linalg.solve(A, x)
        m = max([comp for comp in x], key=abs) # .numerical_approx(digits=14)
        x = 1/m * x  # 1/float(m)*x
        result.append((1/m, x))
### define the matrix A and vector x0
A =
x0 =
print(inverse_power(A, x0, 20))
</script></pre>
</li>
<li id="li-3709"><div class="para" id="p-5580">The inverse power method only works if <span class="process-math">\(A\)</span> is invertible.  If <span class="process-math">\(A\)</span> is not invertible, what is its eigenvalue having the smallest absolute value?</div></li>
<li id="li-3710"><div class="para" id="p-5581">Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
-0.23 \amp -2.33 \\
-1.16 \amp 1.08 \\
\end{array}\right]
\text{.}\)</span>
</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-275" id="solution-275"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-275"><div class="solution solution-like"><div class="para logical" id="p-5582"><ol class="lower-alpha">
<li id="li-3711"><div class="para" id="p-5583">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(A\vvec=\lambda\vvec\)</span> for an associated eigenvector <span class="process-math">\(\vvec\text{.}\)</span>  Multiplying by <span class="process-math">\(A^{-1}\)</span> and <span class="process-math">\(\lambda^{-1}\text{,}\)</span> we obtain <span class="process-math">\(\lambda^{-1}\vvec =
A^{-1}\vvec\text{,}\)</span> which shows that <span class="process-math">\(\lambda^{-1}\)</span> is an eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3712"><div class="para" id="p-5584">If <span class="process-math">\(|\lambda_2| \lt |\lambda_1|\text{,}\)</span> then <span class="process-math">\(|\lambda_2^{-1}| \gt |\lambda_1^{-1}|\text{.}\)</span>  Therefore, the reciprocal of the smallest eigenvalue of <span class="process-math">\(A\)</span> is the dominant eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3713"><div class="para" id="p-5585">If we apply the power method to the matrix <span class="process-math">\(A^{-1}\text{,}\)</span> we will find the dominant eigenvalue <span class="process-math">\(\lambda\)</span> and an associated eigenvector <span class="process-math">\(\vvec\)</span> of <span class="process-math">\(A^{-1}\text{.}\)</span>  We know, however, that <span class="process-math">\(\lambda^{-1}\)</span> will be the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value and <span class="process-math">\(\vvec\)</span> will be an associated eigenvector.</div></li>
<li id="li-3714"><div class="para" id="p-5586">We would like to solve equations of the form <span class="process-math">\(A\xvec = \bvec\)</span> for many different vectors <span class="process-math">\(\bvec\text{.}\)</span>  Using an <span class="process-math">\(LU\)</span> factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.</div></li>
<li id="li-3715"><div class="para" id="p-5587">We obtain the eigenvector <span class="process-math">\(\vvec_2=\twovec11\)</span> and associated eigenvalue <span class="process-math">\(\lambda_2=0.6\text{.}\)</span>
</div></li>
<li id="li-3716"><div class="para" id="p-5588">If <span class="process-math">\(A\)</span> is not invertible, then <span class="process-math">\(\lambda=0\)</span> is the eigenvalue having the smallest absolute value.</div></li>
<li id="li-3717"><div class="para" id="p-5589">We find the dominant eigenvalue to be <span class="process-math">\(\lambda_1=2.195\)</span> with associated eigenvector <span class="process-math">\(\vvec_1=\twovec{-0.961}1\text{.}\)</span>  The smallest eigenvalue is <span class="process-math">\(\lambda_2=-1.345\)</span> with associated eigenvector <span class="process-math">\(\vvec_2=\twovec1{0.478}\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-5590">With the power method and the inverse power method, we can now find the eigenvalues of a matrix <span class="process-math">\(A\)</span> having the largest and smallest absolute values.  With one more modification, we can find all the eigenvalues of <span class="process-math">\(A\text{.}\)</span>
</div>
<article class="activity project-like" id="activity-68"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.6.4</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-5591">
<div class="para">Remember that the absolute value of a number tells us how far that number is from <span class="process-math">\(0\)</span> on the real number line.  We may therefore think of the inverse power method as telling us the eigenvalue closest to <span class="process-math">\(0\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-3718"><div class="para" id="p-5592">If <span class="process-math">\(\vvec\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> with associated eigenvalue <span class="process-math">\(\lambda\text{,}\)</span> explain why <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A - sI\)</span> where <span class="process-math">\(s\)</span> is some scalar.</div></li>
<li id="li-3719"><div class="para" id="p-5593">What is the eigenvalue of <span class="process-math">\(A-sI\)</span> associated to the eigenvector <span class="process-math">\(\vvec\text{?}\)</span>
</div></li>
<li id="li-3720"><div class="para" id="p-5594">Explain why the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\)</span> is the eigenvalue of <span class="process-math">\(A-sI\)</span> closest to <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-3721"><div class="para" id="p-5595">Explain why applying the inverse power method to <span class="process-math">\(A-sI\)</span> gives the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\text{.}\)</span>
</div></li>
<li id="li-3722">
<div class="para" id="p-5596">Consider the matrix <span class="process-math">\(A = \left[\begin{array}{rrrr}
3.6 \amp 1.6 \amp 4.0 \amp 7.6 \\
1.6 \amp 2.2 \amp 4.4 \amp 4.1 \\
3.9 \amp 4.3 \amp 9.0 \amp 0.6 \\
7.6 \amp 4.1 \amp 0.6 \amp 5.0 \\
\end{array}\right]
\text{.}\)</span> If we use the power method and inverse power method, we find two eigenvalues, <span class="process-math">\(\lambda_1=16.35\)</span> and <span class="process-math">\(\lambda_2=0.75\text{.}\)</span>  Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between <span class="process-math">\(-\lambda_1\)</span> and <span class="process-math">\(\lambda_1\text{,}\)</span> as shaded in <a href="" class="xref" data-knowl="./knowl/fig-numerical-power-line.html" title="Figure 5.6.3">Figure¬†5.6.3</a>.</div>
<figure class="figure figure-like" id="fig-numerical-power-line"><div class="sidebyside"><div class="sbsrow" style="margin-left:12.5%;margin-right:12.5%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/numerical-power-line.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">5.6.3<span class="period">.</span></span><span class="space"> </span>The range of eigenvalues of <span class="process-math">\(A\text{.}\)</span></figcaption></figure><div class="para" id="p-5597">The Python cell below has a function <code class="code-inline tex2jax_ignore">find_closest_eigenvalue(A, s, x, N)</code> that implements <span class="process-math">\(N\)</span> steps of the inverse power method using the matrix <span class="process-math">\(A-sI\)</span> and an initial vector <span class="process-math">\(x\text{.}\)</span>  This function prints approximations to the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\)</span> and its associated eigenvector.  By trying different values of <span class="process-math">\(s\)</span> in the shaded regions of the number line shown in <a href="" class="xref" data-knowl="./knowl/fig-numerical-power-line.html" title="Figure 5.6.3">Figure¬†5.6.3</a>, find the other two eigenvalues of <span class="process-math">\(A\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-188"><script type="text/x-sage">from scipy import linalg 
def find_closest_eigenvalue(A, s, x, N):
    B = A - s @ linalg.eye(A.shape[0])
	result = []
    for i in range(N):
        x = B.solve(x)
        m = max([comp for comp in x], key=abs)
        x = 1/m * x
		result.append((1/m + s, x))
	return result

### define the matrix A and vector x0
A =
x0 =
print(find_closest_eigenvalue(A, 2, x0, 20))
</script></pre>
</li>
<li id="li-3723"><div class="para" id="p-5598">Write a list of the four eigenvalues of <span class="process-math">\(A\)</span> in increasing order.</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-276" id="solution-276"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-276"><div class="solution solution-like"><div class="para logical" id="p-5599"><ol class="lower-alpha">
<li id="li-3724"><div class="para" id="p-5600">If <span class="process-math">\(A\vvec=\lambda\vvec\text{,}\)</span> then <span class="process-math">\((A-sI)\vvec =
(\lambda-s)\vvec\text{,}\)</span> which shows that <span class="process-math">\(\vvec\)</span> is also an eigenvector of <span class="process-math">\(A-sI\text{.}\)</span>
</div></li>
<li id="li-3725"><div class="para" id="p-5601">From the previous part, we see that the associated eigenvalue is <span class="process-math">\(\lambda-s\text{.}\)</span>
</div></li>
<li id="li-3726"><div class="para" id="p-5602">If <span class="process-math">\(\lambda\)</span> is the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\text{,}\)</span> then <span class="process-math">\(\lambda-s\)</span> is an eigenvalue of <span class="process-math">\(A-sI\)</span> that must be the closest to <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-3727"><div class="para" id="p-5603">The inverse power method applied to <span class="process-math">\(A-sI\)</span> tells us the eigenvalue of <span class="process-math">\(A-sI\)</span> having the smallest absolute value and an associated eigenvector <span class="process-math">\(\vvec\text{.}\)</span> Therefore, <span class="process-math">\(\lambda+s\)</span> is the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\)</span> and <span class="process-math">\(\vvec\)</span> is an associated eigenvector.</div></li>
<li id="li-3728"><div class="para" id="p-5604">We begin by trying to find the closest eigenvalue to, say, <span class="process-math">\(-10\text{.}\)</span>  The power method tells us that this eigenvalue is <span class="process-math">\(\lambda_3=-4.823\text{.}\)</span> If we then try to find the eigenvalue closest to <span class="process-math">\(10\text{,}\)</span> we find the fourth eigenvalue <span class="process-math">\(\lambda_4=7.526\text{.}\)</span>  It may require some experimentation to find all of the eigenvalues.</div></li>
<li id="li-3729"><div class="para" id="p-5605">The eigenvalues are <span class="process-math">\(-4.823, 0.746, 7.526,
16.351\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-5606">There are some restrictions on the matrices to which this technique applies as we have assumed that the eigenvalues of <span class="process-math">\(A\)</span> are real and distinct.  If <span class="process-math">\(A\)</span> has repeated or complex eigenvalues, this technique will need to be modified, as explored in some of the exercises.</div></section><section class="subsection" id="subsection-101"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">5.6.3</span><span class="space"> </span><span class="title">Summary</span>
</h3>
<div class="para logical" id="p-5607">
<div class="para">We have explored the power method as a tool for numerically approximating the eigenvalues and eigenvectors of a matrix.</div>
<ul class="disc">
<li id="li-3730"><div class="para" id="p-5608">After choosing an initial vector <span class="process-math">\(\xvec_0\text{,}\)</span> we define the sequence <span class="process-math">\(\xvec_{k+1}=A\xvec_k\text{.}\)</span>  As <span class="process-math">\(k\)</span> grows larger, the direction of the vectors <span class="process-math">\(\xvec_k\)</span> closely approximates the direction of the eigenspace corresponding to the eigenvalue <span class="process-math">\(\lambda_1\)</span> having the largest absolute value.</div></li>
<li id="li-3731"><div class="para" id="p-5609">We normalize the vectors <span class="process-math">\(\xvec_k\)</span> by multiplying by <span class="process-math">\(\frac{1}{m_k}\text{,}\)</span> where <span class="process-math">\(m_k\)</span> is the component having the largest absolute value.  In this way, the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> approach an eigenvector associated to <span class="process-math">\(\lambda_1\text{,}\)</span> and the multipliers <span class="process-math">\(m_k\)</span> approach the eigenvalue <span class="process-math">\(\lambda_1\text{.}\)</span>
</div></li>
<li id="li-3732"><div class="para" id="p-5610">To find the eigenvalue having the smallest absolute value, we apply the power method using the matrix <span class="process-math">\(A^{-1}\text{.}\)</span>
</div></li>
<li id="li-3733"><div class="para" id="p-5611">To find the eigenvalue closest to some number <span class="process-math">\(s\text{,}\)</span> we apply the power method using the matrix <span class="process-math">\((A-sI)^{-1}\text{.}\)</span>
</div></li>
</ul>
</div></section><section class="exercises" id="exercises-24"><h3 class="heading hide-type">
<span class="type">Exercises</span><span class="space"> </span><span class="codenumber">5.6.4</span><span class="space"> </span><span class="title">Exercises</span>
</h3>
<div class="para" id="p-5612">This Sage cell has the commands <code class="code-inline tex2jax_ignore">power</code>, <code class="code-inline tex2jax_ignore">inverse_power</code>, and <code class="code-inline tex2jax_ignore">find_closest_eigenvalue</code> that we have developed in this section.  After evaluating this cell, these commands will be available in any other cell on this page. <pre class="ptx-sagecell sagecell-sage" id="sage-189"><script type="text/x-sage">def power(A, x, N):
    for i in range(N):
        x = A*x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (m, x)
def find_closest_eigenvalue(A, s, x, N):
    B = A-s*identity_matrix(A.nrows())
    for i in range(N):
        x = B \ x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (1/float(m)+s, x)
def inverse_power(A, x, N):
    find_closest_eigenvalue(A, 0, x, N)
</script></pre>
</div>
<article class="exercise exercise-like" id="exercise-195"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="para logical" id="p-5613">
<div class="para">Suppose that <span class="process-math">\(A\)</span> is a matrix having eigenvalues <span class="process-math">\(-3\text{,}\)</span> <span class="process-math">\(-0.2\text{,}\)</span> <span class="process-math">\(1\text{,}\)</span> and <span class="process-math">\(4\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-3734"><div class="para" id="p-5614">What are the eigenvalues of <span class="process-math">\(A^{-1}\text{?}\)</span>
</div></li>
<li id="li-3735"><div class="para" id="p-5615">What are the eigenvalues of <span class="process-math">\(A+7I\text{?}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-196"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="para logical" id="p-5622">
<div class="para">Use the commands <code class="code-inline tex2jax_ignore">power</code>, <code class="code-inline tex2jax_ignore">inverse_power</code>, and <code class="code-inline tex2jax_ignore">find_closest_eigenvalue</code> to approximate the eigenvalues and associated eigenvectors of the following matrices. <pre class="ptx-sagecell sagecell-sage" id="sage-190"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-3740"><div class="para" id="p-5623"><span class="process-math">\(A= \left[\begin{array}{rr}
-2 \amp -2 \\
-8 \amp -2 \\
\end{array}\right]
\text{.}\)</span></div></li>
<li id="li-3741"><div class="para" id="p-5624"><span class="process-math">\(A= \left[\begin{array}{rr}
0.6 \amp 0.7 \\
0.5 \amp 0.2 \\
\end{array}\right]
\text{.}\)</span></div></li>
<li id="li-3742"><div class="para" id="p-5625"><span class="process-math">\(A= \left[\begin{array}{rrrr}
1.9  \amp -16.0 \amp  -13.0 \amp 27.0 \\
-2.4 \amp  20.3 \amp  4.6 \amp -17.7 \\
-0.51 \amp -11.7 \amp -1.4 \amp  13.1  \\
-2.1 \amp  15.3 \amp  6.9 \amp -20.5 \\
\end{array}\right]
\text{.}\)</span></div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-197"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="para logical" id="p-5635">
<div class="para">Use the techniques we have seen in this section to find the eigenvalues of the matrix</div>
<div class="displaymath process-math">
\begin{equation*}
A= \left[\begin{array}{rrrrr}
-14.6 \amp 9.0 \amp -14.1 \amp 5.8 \amp  13.0 \\
27.8 \amp -4.2 \amp  16.0 \amp 0.9 \amp -21.3 \\
-5.5 \amp 3.4 \amp  3.4 \amp  3.3 \amp  1.1 \\
-25.4 \amp 11.3 \amp -15.4 \amp 4.7 \amp  20.3 \\
-33.7 \amp 14.8 \amp -22.5 \amp 9.7 \amp  26.6 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<div class="para"><pre class="ptx-sagecell sagecell-sage" id="sage-191"><script type="text/x-sage">A = matrix(5,5, [-14.6,  9.0, -14.1, 5.8,  13.0,
                  27.8, -4.2,  16.0, 0.9, -21.3,
                  -5.5,  3.4,   3.4, 3.3,   1.1,
                 -25.4, 11.3, -15.4, 4.7,  20.3,
                 -33.7, 14.8, -22.5, 9.7,  26.6])
</script></pre></div>
</div></article><article class="exercise exercise-like" id="exercise-198"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="para logical" id="p-5638">
<div class="para">Consider the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
0 \amp -1 \\
-4 \amp 0 \\
\end{array}\right]
\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-192"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-3749"><div class="para" id="p-5639">Describe what happens if we apply the power method and the inverse power method using the initial vector <span class="process-math">\(\xvec_0 =
\twovec{1}{0}\text{.}\)</span>
</div></li>
<li id="li-3750"><div class="para" id="p-5640">Find the eigenvalues of this matrix and explain this observed behavior.</div></li>
<li id="li-3751"><div class="para" id="p-5641">How can we apply the techniques of this section to find the eigenvalues of <span class="process-math">\(A\text{?}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-power-method"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="para logical" id="p-5650">
<div class="para">We have seen that the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
1 \amp 2 \\
2 \amp 1 \\
\end{array}\right]\)</span> has eigenvalues <span class="process-math">\(\lambda_1 = 3\)</span> and <span class="process-math">\(\lambda_2=-1\)</span> and associated eigenvectors <span class="process-math">\(\vvec_1 =
\twovec{1}{1}\)</span> and <span class="process-math">\(\vvec_2=\twovec{-1}{1}\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-3758"><div class="para" id="p-5651">Describe what happens when we apply the inverse power method using the initial vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{.}\)</span>
</div></li>
<li id="li-3759"><div class="para" id="p-5652">Explain why this is happening and provide a contrast with how the power method usually works.</div></li>
<li id="li-3760"><div class="para" id="p-5653">How can we modify the power method to give the dominant eigenvalue in this case?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-200"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="para" id="p-5662">Suppose that <span class="process-math">\(A\)</span> is a <span class="process-math">\(2\by2\)</span> matrix with eigenvalues <span class="process-math">\(4\)</span> and <span class="process-math">\(-3\)</span> and that <span class="process-math">\(B\)</span> is a <span class="process-math">\(2\by2\)</span> matrix with eigenvalues <span class="process-math">\(4\)</span> and <span class="process-math">\(1\text{.}\)</span> If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm?  Explain your response.</div></article><article class="exercise exercise-like" id="exercise-201"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="para" id="p-5665">Suppose that we apply the power method to the matrix <span class="process-math">\(A\)</span> with an initial vector <span class="process-math">\(\xvec_0\)</span> and find the eigenvalue <span class="process-math">\(\lambda=3\)</span> and eigenvector <span class="process-math">\(\vvec\text{.}\)</span> Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue <span class="process-math">\(\lambda=3\)</span> but a different eigenvector <span class="process-math">\(\wvec\text{.}\)</span>  What can we conclude about the matrix <span class="process-math">\(A\)</span> in this case?</div></article><article class="exercise exercise-like" id="exercise-202"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="para" id="p-5668">The power method we have developed only works if the matrix has real eigenvalues. Suppose that <span class="process-math">\(A\)</span> is a <span class="process-math">\(2\by2\)</span> matrix that has a complex eigenvalue <span class="process-math">\(\lambda = 2+3i\text{.}\)</span>  What would happen if we apply the power method to <span class="process-math">\(A\text{?}\)</span>
</div></article><article class="exercise exercise-like" id="exercise-203"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<div class="para logical" id="p-5671">
<div class="para">Consider the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
1 \amp 1 \\
0 \amp 1 \\
\end{array}\right]
\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-3767"><div class="para" id="p-5672">Find the eigenvalues and associated eigenvectors of <span class="process-math">\(A\text{.}\)</span>
</div></li>
<li id="li-3768"><div class="para" id="p-5673">Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of <span class="process-math">\(A\text{.}\)</span>
</div></li>
<li id="li-3769"><div class="para" id="p-5674">Verify your prediction using Sage. <pre class="ptx-sagecell sagecell-sage" id="sage-193"><script type="text/x-sage">
</script></pre>
</div></li>
</ol>
</div></article></section></section></div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-stochastic.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon">^</span><span class="name">Top</span></a><a class="next-button button" href="chap6.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
