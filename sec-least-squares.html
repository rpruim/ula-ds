<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Least squares methods</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" Randall Pruim ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math",
    "renderActions": {
      "findScript": [
        10,
        function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        },
        ""
      ]
    }
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-sage",
  "linked": true,
  "linkKey": "linked-sage",
  "autoeval": false,
  "languages": [
    "sage"
  ],
  "evalButtonText": "Evaluate (Sage)"
});
</script><script>// Make *any* pre with class 'sagecell-python' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-python",
  "linked": true,
  "linkKey": "linked-python",
  "autoeval": false,
  "languages": [
    "python"
  ],
  "evalButtonText": "Evaluate (Python)"
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.3/pretext_search.js"></script><link href="https://pretextbook.org/css/0.7/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.3</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.3/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.3/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.3/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link href="https://pretextbook.org/css/0.7/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra:</span> <span class="subtitle">Data Science Edition</span></a></h1>
<p class="byline">Randall Pruim</p>
</div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" class="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms" class="searchterms"></span>
</h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" aria-label="Show or hide table of contents"><span class="icon">‚ò∞</span><span class="name">Contents</span></button><a class="index-button button" href="index-1.html" title="Index"><span class="name">Index</span></a><button id="user-preferences-button" class="user-preferences-button button" title="Modify user preferences"><span id="avatarbutton" class="avatarbutton name">You!</span><div id="preferences_menu_holder" class="preferences_menu_holder hidden"><ol id="preferences_menu" class="preferences_menu" style="font-family: 'Roboto Serif', serif;">
<li data-env="avatar" tabindex="-1">Choose avatar<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden avatar">
<li data-val="You!" tabindex="-1">
<span id="theYou!" class="avatarcheck">‚úîÔ∏è</span>You!</li>
<li data-val="üò∫" tabindex="-1">
<span id="theüò∫" class="avatarcheck"></span>üò∫</li>
<li data-val="üë§" tabindex="-1">
<span id="theüë§" class="avatarcheck"></span>üë§</li>
<li data-val="üëΩ" tabindex="-1">
<span id="theüëΩ" class="avatarcheck"></span>üëΩ</li>
<li data-val="üê∂" tabindex="-1">
<span id="theüê∂" class="avatarcheck"></span>üê∂</li>
<li data-val="üêº" tabindex="-1">
<span id="theüêº" class="avatarcheck"></span>üêº</li>
<li data-val="üåà" tabindex="-1">
<span id="theüåà" class="avatarcheck"></span>üåà</li>
</ol>
</li>
<li data-env="fontfamily" tabindex="-1">Font family<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fontfamily">
<li data-val="face" data-change="OS" tabindex="-1" style="font-family: 'Open Sans'">
<span id="theOS" class="ffcheck">‚úîÔ∏è</span><span class="name">Open Sans</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
<li data-val="face" data-change="RS" tabindex="-1" style="font-family: 'Roboto Serif'">
<span id="theRS" class="ffcheck"></span><span class="name">Roboto Serif</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
</ol>
</li>
<li data-env="font" tabindex="-1">Adjust font<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fonts">
<li>Size</li>
<li><span id="thesize">12</span></li>
<li data-val="size" data-change="-1" tabindex="-1" style="font-size: 80%">Smaller</li>
<li data-val="size" data-change="1" tabindex="-1" style="font-size: 110%">Larger</li>
<li>Width</li>
<li><span id="thewdth">100</span></li>
<li data-val="wdth" data-change="-5" tabindex="-1" style="font-variation-settings: 'wdth' 60">narrower</li>
<li data-val="wdth" data-change="5" tabindex="-1" style="font-variation-settings: 'wdth' 150">wider</li>
<li>Weight</li>
<li><span id="thewght">400</span></li>
<li data-val="wght" data-change="-50" tabindex="-1" style="font-weight: 200">thinner</li>
<li data-val="wght" data-change="50" tabindex="-1" style="font-weight: 700">heavier</li>
<li>Letter spacing</li>
<li>
<span id="thelspace">0</span><span class="byunits">/200</span>
</li>
<li data-val="lspace" data-change="-1" tabindex="-1">closer</li>
<li data-val="lspace" data-change="1" tabindex="-1">f a r t h e r</li>
<li>Word spacing</li>
<li>
<span id="thewspace">0</span><span class="byunits">/50</span>
</li>
<li data-val="wspace" data-change="-1" tabindex="-1">smaller‚ÄÖgap‚ÄÉ</li>
<li data-val="wspace" data-change="1" tabindex="-1">larger‚ÄÉgap</li>
<li>Line Spacing</li>
<li>
<span id="theheight">135</span><span class="byunits">/100</span>
</li>
<li data-val="height" data-change="-5" tabindex="-1" style="line-height: 1">closer<br>together</li>
<li data-val="height" data-change="5" tabindex="-1" style="line-height: 1.75">further<br>apart</li>
</ol>
</li>
<li data-env="atmosphere" tabindex="-1">Light/dark mode<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden atmosphere">
<li data-val="default" tabindex="-1">
<span id="thedefault" class="atmospherecheck">‚úîÔ∏è</span>default</li>
<li data-val="pastel" tabindex="-1">
<span id="thepastel" class="atmospherecheck"></span>pastel</li>
<li data-val="darktwilight" tabindex="-1">
<span id="thedarktwilight" class="atmospherecheck"></span>twilight</li>
<li data-val="dark" tabindex="-1">
<span id="thedark" class="atmospherecheck"></span>dark</li>
<li data-val="darkmidnight" tabindex="-1">
<span id="thedarkmidnight" class="atmospherecheck"></span>midnight</li>
</ol>
</li>
<li data-env="ruler" tabindex="-1">Reading ruler<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden ruler">
<li data-val="none" tabindex="-1">
<span id="thenone" class="rulercheck">‚úîÔ∏è</span>none</li>
<li data-val="underline" tabindex="-1">
<span id="theunderline" class="rulercheck"></span>underline</li>
<li data-val="lunderline" tabindex="-1">
<span id="thelunderline" class="rulercheck"></span>L-underline</li>
<li data-val="greybar" tabindex="-1">
<span id="thegreybar" class="rulercheck"></span>grey bar</li>
<li data-val="lightbox" tabindex="-1">
<span id="thelightbox" class="rulercheck"></span>light box</li>
<li data-val="sunrise" tabindex="-1">
<span id="thesunrise" class="rulercheck"></span>sunrise</li>
<li data-val="sunriseunderline" tabindex="-1">
<span id="thesunriseunderline" class="rulercheck"></span>sunrise underline</li>
<li class="moveQ">Motion by:</li>
<li data-val="mouse" tabindex="-1">
<span id="themouse" class="motioncheck">‚úîÔ∏è</span>follow the mouse</li>
<li data-val="arrow" tabindex="-1">
<span id="thearrow" class="motioncheck"></span>up/down arrows - not yet</li>
<li data-val="eye" tabindex="-1">
<span id="theeye" class="motioncheck"></span>eye tracking - not yet</li>
</ol>
</li>
</ol></div></button><span class="treebuttons"><a class="previous-button button" href="sec-gram-schmidt.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="up-button button" href="chap6.html" title="Up"><span class="icon">^</span><span class="name">Up</span></a><a class="next-button button" href="chap7.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a></span><div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" class="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\boldsymbol a}}
\newcommand{\bvec}{{\boldsymbol b}}
\newcommand{\cvec}{{\boldsymbol c}}
\newcommand{\dvec}{{\boldsymbol d}}
\newcommand{\dtil}{\widetilde{\boldsymbol d}}
\newcommand{\evec}{{\boldsymbol e}}
\newcommand{\fvec}{{\boldsymbol f}}
\newcommand{\mvec}{{\boldsymbol m}}
\newcommand{\nvec}{{\boldsymbol n}}
\newcommand{\pvec}{{\boldsymbol p}}
\newcommand{\qvec}{{\boldsymbol q}}
\newcommand{\rvec}{{\boldsymbol r}}
\newcommand{\svec}{{\boldsymbol s}}
\newcommand{\tvec}{{\boldsymbol t}}
\newcommand{\uvec}{{\boldsymbol u}}
\newcommand{\vvec}{{\boldsymbol v}}
\newcommand{\wvec}{{\boldsymbol w}}
\newcommand{\xvec}{{\boldsymbol x}}
\newcommand{\yvec}{{\boldsymbol y}}
\newcommand{\zvec}{{\boldsymbol z}}
\newcommand{\betavec}{{\boldsymbol \beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\zerovec}{{\boldsymbol 0}}
\newcommand{\onevec}{{\boldsymbol 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\xmean}{\overline{\xvec}}
\newcommand{\yhat}{\widehat{\yvec}}
\newcommand{\ymean}{\overline{\yvec}}
\newcommand{\betahat}{\widehat{\betavec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\by}{\times}
\newcommand{\transpose}{\top}
\newcommand{\proj}[2]{\operatorname{proj}\left(#1 \to #2\right)}
\newcommand{\projsub}[2]{\operatorname{proj}_{#2}(#1)}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural">
<li>
<div class="toc-item"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="colophon-1.html" class="internal"><span class="title">Colophon</span></a></div></li>
<li><div class="toc-item"><a href="preface-1.html" class="internal"><span class="title">Our goals -- Preface to David Austin‚Äôs original edition</span></a></div></li>
<li><div class="toc-item"><a href="preface-2.html" class="internal"><span class="title">What‚Äôs different in the data science edition?</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap1.html" class="internal"><span class="codenumber">1</span> <span class="title">Scalars, Vectors and Matrices</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-vectors.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsection-1" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Three ways to think about vectors</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-vectors.html#subsection-2" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Vector operations: scalar multiplication and vector addition.</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-scalar-multiplication" class="internal"><span class="codenumber">1.1.2.1</span> <span class="title">Scalar Multiplication</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-addition" class="internal"><span class="codenumber">1.1.2.2</span> <span class="title">Vector addition</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-properties" class="internal"><span class="codenumber">1.1.2.3</span> <span class="title">Mathematical properties of vector operations</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-3" class="internal"><span class="codenumber">1.1.3</span> <span class="title">The (Euclidean) length of a vector</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-4" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Summary</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-vectors-in-python.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Vectors in Python</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-intro-to-python" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Introduction to Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-numpy-vectors" class="internal"><span class="codenumber">1.2.2</span> <span class="title"><code class="code-inline tex2jax_ignore">numpy</code> vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-vector-length-numpy" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Vector length</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsection-8" class="internal"><span class="codenumber">1.2.4</span> <span class="title">Plotting vectors</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-combos-of-vectors.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Linear combinations of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#subsection-9" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#exercises-1" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-matrices.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrices.html#subsec-matrices-and-their-uses" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Matrices and their uses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-11" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Scalar multiplication and addition of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-12" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Matrix-vector multiplication and linear combinations</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-13" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Matrix-vector multiplication and linear systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#sec-matrices-in-python" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Matrices in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-15" class="internal"><span class="codenumber">1.4.6</span> <span class="title">Matrix-matrix products</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsec-special-matrices" class="internal"><span class="codenumber">1.4.7</span> <span class="title">Some special types of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-17" class="internal"><span class="codenumber">1.4.8</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#exercises-2" class="internal"><span class="codenumber">1.4.9</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-tensors.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Tensors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-tensors.html#subsec-numpy-tensors" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Tensors in NumPy</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-axes" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Aggregation and Axes</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-ndarray-append" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expanding an array</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-broadcasting" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Broadcasting</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#exercises-1-5" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap2.html" class="internal"><span class="codenumber">2</span> <span class="title">Systems of equations: Solving <span class="process-math">\(A \xvec = \bvec\)</span></span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-expect.html" class="internal"><span class="codenumber">2.1</span> <span class="title">What can we expect</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-expect.html#subsection-22" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Some simple examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-23" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Systems of linear equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-24" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#exercises-4" class="internal"><span class="codenumber">2.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-finding-solutions.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Finding solutions to linear systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-25" class="internal"><span class="codenumber">2.2.1</span> <span class="title">Gaussian elimination</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-26" class="internal"><span class="codenumber">2.2.2</span> <span class="title">Augmented matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-27" class="internal"><span class="codenumber">2.2.3</span> <span class="title">Reduced row echelon form</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsec-solving-matrix-equations" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Solving matrix equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-29" class="internal"><span class="codenumber">2.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#exercises-5" class="internal"><span class="codenumber">2.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-python-introduction.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Computational Linear Algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-30" class="internal"><span class="codenumber">2.3.1</span> <span class="title">Reduced row echelon form in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-linalg-solve" class="internal"><span class="codenumber">2.3.2</span> <span class="title">np.linalg.solve()</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-compute-effort" class="internal"><span class="codenumber">2.3.3</span> <span class="title">Computational effort</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-33" class="internal"><span class="codenumber">2.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-6" class="internal"><span class="codenumber">2.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pivots.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Pivots and their relationship to solution spaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pivots.html#subsection-34" class="internal"><span class="codenumber">2.4.1</span> <span class="title">The existence of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-35" class="internal"><span class="codenumber">2.4.2</span> <span class="title">The uniqueness of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-36" class="internal"><span class="codenumber">2.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#exercises-7" class="internal"><span class="codenumber">2.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap3.html" class="internal"><span class="codenumber">3</span> <span class="title">Linear combinations and transformations</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-span.html" class="internal"><span class="codenumber">3.1</span> <span class="title">The span of a set of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-span.html#subsection-37" class="internal"><span class="codenumber">3.1.1</span> <span class="title">The span of a set of vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-38" class="internal"><span class="codenumber">3.1.2</span> <span class="title">Pivot positions and span</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsec-span-and-linear-models" class="internal"><span class="codenumber">3.1.3</span> <span class="title">Span and linear models</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-40" class="internal"><span class="codenumber">3.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#exercises-8" class="internal"><span class="codenumber">3.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-dep.html" class="internal"><span class="codenumber">3.2</span> <span class="title">Linear independence</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-41" class="internal"><span class="codenumber">3.2.1</span> <span class="title">Linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-42" class="internal"><span class="codenumber">3.2.2</span> <span class="title">How to recognize linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-43" class="internal"><span class="codenumber">3.2.3</span> <span class="title">Homogeneous equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-44" class="internal"><span class="codenumber">3.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#exercises-9" class="internal"><span class="codenumber">3.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-trans.html" class="internal"><span class="codenumber">3.3</span> <span class="title">Matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-matrix-trans" class="internal"><span class="codenumber">3.3.1</span> <span class="title">Matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-linear-trans" class="internal"><span class="codenumber">3.3.2</span> <span class="title">Linear transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-47" class="internal"><span class="codenumber">3.3.3</span> <span class="title">Composing matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-dynamical-systems" class="internal"><span class="codenumber">3.3.4</span> <span class="title">Discrete Dynamical Systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-49" class="internal"><span class="codenumber">3.3.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#exercises-10" class="internal"><span class="codenumber">3.3.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transforms-geom.html" class="internal"><span class="codenumber">3.4</span> <span class="title">The geometry of matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-50" class="internal"><span class="codenumber">3.4.1</span> <span class="title">The geometry of <span class="process-math">\(2\by2\)</span> matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-51" class="internal"><span class="codenumber">3.4.2</span> <span class="title">Matrix transformations and computer animation</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-52" class="internal"><span class="codenumber">3.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#exercises-11" class="internal"><span class="codenumber">3.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap4.html" class="internal"><span class="codenumber">4</span> <span class="title">Invertibility, bases, and coordinate systems</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-matrix-inverse.html" class="internal"><span class="codenumber">4.1</span> <span class="title">Invertibility</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-53" class="internal"><span class="codenumber">4.1.1</span> <span class="title">Invertible matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-54" class="internal"><span class="codenumber">4.1.2</span> <span class="title">Solving equations with an inverse</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsec-finding-inverses" class="internal"><span class="codenumber">4.1.3</span> <span class="title">Finding inverses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-56" class="internal"><span class="codenumber">4.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#exercises-12" class="internal"><span class="codenumber">4.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="subsec-triangular-invertible.html" class="internal"><span class="codenumber">4.2</span> <span class="title">Triangular matrices and Gaussian elimination</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-triangular-matrices" class="internal"><span class="codenumber">4.2.1</span> <span class="title">Triangular matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-elementary-matrices" class="internal"><span class="codenumber">4.2.2</span> <span class="title">Elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsection-59" class="internal"><span class="codenumber">4.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#exercises-13" class="internal"><span class="codenumber">4.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-bases.html" class="internal"><span class="codenumber">4.3</span> <span class="title">Bases and coordinate systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-bases.html#subsection-60" class="internal"><span class="codenumber">4.3.1</span> <span class="title">Bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-61" class="internal"><span class="codenumber">4.3.2</span> <span class="title">Coordinate systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-62" class="internal"><span class="codenumber">4.3.3</span> <span class="title">Examples of bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-63" class="internal"><span class="codenumber">4.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#exercises-14" class="internal"><span class="codenumber">4.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-jpeg.html" class="internal"><span class="codenumber">4.4</span> <span class="title">Image compression</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-64" class="internal"><span class="codenumber">4.4.1</span> <span class="title">Color models</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-65" class="internal"><span class="codenumber">4.4.2</span> <span class="title">The JPEG compression algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-66" class="internal"><span class="codenumber">4.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#exercises-15" class="internal"><span class="codenumber">4.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-determinants.html" class="internal"><span class="codenumber">4.5</span> <span class="title">Determinants</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsection-67" class="internal"><span class="codenumber">4.5.1</span> <span class="title">Determinants of <span class="process-math">\(2\by2\)</span> matrices</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-determinants.html#subsec-determinants-larger-matrices" class="internal"><span class="codenumber">4.5.2</span> <span class="title">Determinants of larger matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsubsection-4" class="internal"><span class="codenumber">4.5.2.1</span> <span class="title">Determinants of elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsubsec-rref-to-determinants" class="internal"><span class="codenumber">4.5.2.2</span> <span class="title">Using RREF to compute determinants</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsubsection-6" class="internal"><span class="codenumber">4.5.2.3</span> <span class="title">Cofactor expansions</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-69" class="internal"><span class="codenumber">4.5.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#exercises-16" class="internal"><span class="codenumber">4.5.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-subspaces.html" class="internal"><span class="codenumber">4.6</span> <span class="title">Subspaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-70" class="internal"><span class="codenumber">4.6.1</span> <span class="title">Subspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-71" class="internal"><span class="codenumber">4.6.2</span> <span class="title">The column space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-72" class="internal"><span class="codenumber">4.6.3</span> <span class="title">The null space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-73" class="internal"><span class="codenumber">4.6.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#exercises-17" class="internal"><span class="codenumber">4.6.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gaussian-revisited.html" class="internal"><span class="codenumber">4.7</span> <span class="title">Partial pivoting and LU factorizations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal"><span class="codenumber">4.7.1</span> <span class="title">Partial pivoting</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-75" class="internal"><span class="codenumber">4.7.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-76" class="internal"><span class="codenumber">4.7.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#exercises-18" class="internal"><span class="codenumber">4.7.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap5.html" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-eigen-intro.html" class="internal"><span class="codenumber">5.1</span> <span class="title">An introduction to eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-77" class="internal"><span class="codenumber">5.1.1</span> <span class="title">A few examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsec-eigen-use" class="internal"><span class="codenumber">5.1.2</span> <span class="title">The usefulness of eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-79" class="internal"><span class="codenumber">5.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#exercises-19" class="internal"><span class="codenumber">5.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-find.html" class="internal"><span class="codenumber">5.2</span> <span class="title">Finding eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-80" class="internal"><span class="codenumber">5.2.1</span> <span class="title">The characteristic polynomial</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-81" class="internal"><span class="codenumber">5.2.2</span> <span class="title">Finding eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-82" class="internal"><span class="codenumber">5.2.3</span> <span class="title">The characteristic polynomial and the dimension of eigenspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-83" class="internal"><span class="codenumber">5.2.4</span> <span class="title">Using Python to find eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-84" class="internal"><span class="codenumber">5.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#exercises-20" class="internal"><span class="codenumber">5.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-diag.html" class="internal"><span class="codenumber">5.3</span> <span class="title">Diagonalization, similarity, and powers of a matrix</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-85" class="internal"><span class="codenumber">5.3.1</span> <span class="title">Diagonalization of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-86" class="internal"><span class="codenumber">5.3.2</span> <span class="title">Powers of a diagonalizable matrix</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-87" class="internal"><span class="codenumber">5.3.3</span> <span class="title">Similarity and complex eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-88" class="internal"><span class="codenumber">5.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#exercises-21" class="internal"><span class="codenumber">5.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-dynamical.html" class="internal"><span class="codenumber">5.4</span> <span class="title">Dynamical systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-89" class="internal"><span class="codenumber">5.4.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-90" class="internal"><span class="codenumber">5.4.2</span> <span class="title">Classifying dynamical systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-91" class="internal"><span class="codenumber">5.4.3</span> <span class="title">A <span class="process-math">\(3\by3\)</span> system</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-92" class="internal"><span class="codenumber">5.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#exercises-22" class="internal"><span class="codenumber">5.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-stochastic.html" class="internal"><span class="codenumber">5.5</span> <span class="title">Markov chains and Google‚Äôs PageRank algorithm</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-93" class="internal"><span class="codenumber">5.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-94" class="internal"><span class="codenumber">5.5.2</span> <span class="title">Markov chains</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsec-google" class="internal"><span class="codenumber">5.5.3</span> <span class="title">Google‚Äôs PageRank algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-96" class="internal"><span class="codenumber">5.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#exercises-23" class="internal"><span class="codenumber">5.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-power-method.html" class="internal"><span class="codenumber">5.6</span> <span class="title">Finding eigenvectors numerically</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-power-method.html#subsection-97" class="internal"><span class="codenumber">5.6.1</span> <span class="title">The power method</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-98" class="internal"><span class="codenumber">5.6.2</span> <span class="title">Finding other eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-99" class="internal"><span class="codenumber">5.6.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#exercises-24" class="internal"><span class="codenumber">5.6.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap6.html" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-dot-product.html" class="internal"><span class="codenumber">6.1</span> <span class="title">The dot product</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dot-product.html#sec-projections-and-dot-products" class="internal"><span class="codenumber">6.1.1</span> <span class="title">Projections and dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsec-computing-dot-products" class="internal"><span class="codenumber">6.1.2</span> <span class="title">Computing dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-102" class="internal"><span class="codenumber">6.1.3</span> <span class="title"><span class="process-math">\(k\)</span>-means clustering</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-103" class="internal"><span class="codenumber">6.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#exercises-25" class="internal"><span class="codenumber">6.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transpose.html" class="internal"><span class="codenumber">6.2</span> <span class="title">Orthogonal complements and the matrix transpose</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transpose.html#subsection-104" class="internal"><span class="codenumber">6.2.1</span> <span class="title">Orthogonal complements</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-105" class="internal"><span class="codenumber">6.2.2</span> <span class="title">The matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-106" class="internal"><span class="codenumber">6.2.3</span> <span class="title">Properties of the matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-107" class="internal"><span class="codenumber">6.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#exercises-26" class="internal"><span class="codenumber">6.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-orthogonal-bases.html" class="internal"><span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-108" class="internal"><span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-109" class="internal"><span class="codenumber">6.3.2</span> <span class="title">Orthogonal projections</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-110" class="internal"><span class="codenumber">6.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#exercises-27" class="internal"><span class="codenumber">6.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gram-schmidt.html" class="internal"><span class="codenumber">6.4</span> <span class="title">Finding orthogonal bases</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-111" class="internal"><span class="codenumber">6.4.1</span> <span class="title">Gram-Schmidt orthogonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-112" class="internal"><span class="codenumber">6.4.2</span> <span class="title"><span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-113" class="internal"><span class="codenumber">6.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#exercises-28" class="internal"><span class="codenumber">6.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li class="active">
<div class="toc-item"><a href="sec-least-squares.html" class="internal"><span class="codenumber">6.5</span> <span class="title">Least squares methods</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-114" class="internal"><span class="codenumber">6.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-linear-model-framework" class="internal"><span class="codenumber">6.5.2</span> <span class="title">The linear model framework</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-116" class="internal"><span class="codenumber">6.5.3</span> <span class="title">Solving least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-117" class="internal"><span class="codenumber">6.5.4</span> <span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-118" class="internal"><span class="codenumber">6.5.5</span> <span class="title">Polynomial Regression</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-skl-lm" class="internal"><span class="codenumber">6.5.6</span> <span class="title">Fitting linear models with standard tools</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-120" class="internal"><span class="codenumber">6.5.7</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#exercises-29" class="internal"><span class="codenumber">6.5.8</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap7.html" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-symmetric-matrices.html" class="internal"><span class="codenumber">7.1</span> <span class="title">Symmetric matrices and variance</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-121" class="internal"><span class="codenumber">7.1.1</span> <span class="title">Symmetric matrices and orthogonal diagonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-122" class="internal"><span class="codenumber">7.1.2</span> <span class="title">Variance</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-123" class="internal"><span class="codenumber">7.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#exercises-30" class="internal"><span class="codenumber">7.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-quadratic-forms.html" class="internal"><span class="codenumber">7.2</span> <span class="title">Quadratic forms</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-124" class="internal"><span class="codenumber">7.2.1</span> <span class="title">Quadratic forms</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-125" class="internal"><span class="codenumber">7.2.2</span> <span class="title">Definite symmetric matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-126" class="internal"><span class="codenumber">7.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#exercises-31" class="internal"><span class="codenumber">7.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pca.html" class="internal"><span class="codenumber">7.3</span> <span class="title">Principal Component Analysis</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pca.html#subsection-127" class="internal"><span class="codenumber">7.3.1</span> <span class="title">Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-128" class="internal"><span class="codenumber">7.3.2</span> <span class="title">Using Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-129" class="internal"><span class="codenumber">7.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#exercises-32" class="internal"><span class="codenumber">7.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-intro.html" class="internal"><span class="codenumber">7.4</span> <span class="title">Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-130" class="internal"><span class="codenumber">7.4.1</span> <span class="title">Finding singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-131" class="internal"><span class="codenumber">7.4.2</span> <span class="title">The structure of singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-132" class="internal"><span class="codenumber">7.4.3</span> <span class="title">Reduced singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-133" class="internal"><span class="codenumber">7.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#exercises-33" class="internal"><span class="codenumber">7.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-uses.html" class="internal"><span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-134" class="internal"><span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-135" class="internal"><span class="codenumber">7.5.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-136" class="internal"><span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-137" class="internal"><span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-138" class="internal"><span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-139" class="internal"><span class="codenumber">7.5.6</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#exercises-34" class="internal"><span class="codenumber">7.5.7</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="backmatter.html" class="internal"><span class="title">Back Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="app-notation.html" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></div></li>
<li>
<div class="toc-item"><a href="app-python-reference.html" class="internal"><span class="codenumber">B</span> <span class="title">Python Reference</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsection-140.html" class="internal"><span class="codenumber">B.1</span> <span class="title">Accessing Python</span></a></div></li>
<li><div class="toc-item"><a href="subsection-141.html" class="internal"><span class="codenumber">B.2</span> <span class="title">Packages and libraries for data science</span></a></div></li>
<li><div class="toc-item"><a href="subsec-frequently-used-python.html" class="internal"><span class="codenumber">B.3</span> <span class="title">Frequently used Python commands</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="index-1.html" class="internal"><span class="title">Index</span></a></div></li>
<li><div class="toc-item"><a href="colophon-2.html" class="internal"><span class="title">Colophon</span></a></div></li>
</ul>
</li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content">
<section class="section" id="sec-least-squares"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">6.5</span><span class="space"> </span><span class="title">Least squares methods</span>
</h2>
<section class="introduction" id="introduction-36"><div class="para" id="p-6887">Suppose we collect some data when performing an experiment and plot it as shown on the left of <a href="" class="xref" data-knowl="./knowl/lst-squares-intro.html" title="Figure 6.5.1">Figure¬†6.5.1</a>.  Notice that there is no line on which all the points lie; in fact, it would be surprising if there were since we can expect some uncertainty in the measurements recorded.  There does, however, appear to be a line, as shown on the right, on which the points <em class="emphasis">almost</em> lie.</div> <figure class="figure figure-like" id="lst-squares-intro"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/lst-squares-1.svg" role="img" class="contained"></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/lst-squares-2.svg" role="img" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.1<span class="period">.</span></span><span class="space"> </span>A collection of points and a line approximating the linear relationship implied by them.</figcaption></figure> <div class="para" id="p-6888">In this section, we‚Äôll explore how the techniques developed in this chapter enable us to find the line that best approximates the data.</div> <div class="para" id="p-6889">More generally, that whenever <span class="process-math">\(A\xvec=\bvec\)</span> is inconsistent, we can instead seek an approximate solution -- a solution to <span class="process-math">\(A\xvec=\bhat\)</span> where <span class="process-math">\(\bhat\)</span> is a close as possible to <span class="process-math">\(\bvec\text{.}\)</span> Orthogonal projection gives us just the right tool for doing this.</div> <article class="exploration project-like" id="exploration-24"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">6.5.1</span><span class="period">.</span>
</h3>
<div class="para logical" id="p-6890"><ol class="lower-alpha">
<li id="li-4629">
<div class="para logical" id="p-6891">
<div class="para">Is there a solution to the equation <span class="process-math">\(A\xvec=\bvec\)</span> where <span class="process-math">\(A\)</span> and <span class="process-math">\(\bvec\)</span> are such that</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{bmatrix}
1 \amp 2 \\
2 \amp 5 \\
-1 \amp 0 \\
\end{bmatrix}
\xvec = \threevec5{-3}{-1}\text{.}
\end{equation*}
</div>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-221"><script type="text/x-sage">import numpy as np
A = np.array([1, 2, 2, 5, -1, 0]).reshape(3,2)
b = np.array([5, -3, -1)]
</script></pre>
</li>
<li id="li-4630"><div class="para" id="p-6892">We know that <span class="process-math">\(\threevec12{-1}\)</span> and <span class="process-math">\(\threevec250\)</span> form a basis for <span class="process-math">\(\col(A)\text{.}\)</span>  Find an orthogonal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-4631"><div class="para" id="p-6893">Find the orthogonal projection <span class="process-math">\(\widehat\bvec\)</span> of <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-4632"><div class="para" id="p-6894">Explain why the equation <span class="process-math">\(A\xvec=\widehat\bvec\)</span> must be consistent and then find its solution.</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-344" id="solution-344"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-344"><div class="solution solution-like"><div class="para logical" id="p-6895"><ol class="lower-alpha">
<li id="li-4633"><div class="para logical" id="p-6896">
<div class="para">The reduced row echelon form</div>
<div class="displaymath process-math">
\begin{equation*}
\left[
\begin{array}{cc|c}
1 \amp 2 \amp 5 \\
2 \amp 5 \amp -3 \\
-1 \amp 0 \amp -1\\
\end{array}
\right]
\sim
\left[
\begin{array}{cc|c}
1 \amp 0 \amp 0 \\
0 \amp 1 \amp 0 \\
0 \amp 0 \amp 1 \\
\end{array}
\right]
\end{equation*}
</div>
<div class="para">shows that there is no solution.</div>
</div></li>
<li id="li-4634"><div class="para" id="p-6897">Applying Gram-Schmidt, we find an orthogonal basis consisting of <span class="process-math">\(\wvec_1=\cthreevec12{-1}\)</span> and <span class="process-math">\(\wvec_2=\threevec012\text{.}\)</span>
</div></li>
<li id="li-4635"><div class="para" id="p-6898">The projection formula gives <span class="process-math">\(\bhat =
\cthreevec0{-1}{-2}\text{.}\)</span>
</div></li>
<li id="li-4636"><div class="para" id="p-6899">The equation is consistent because <span class="process-math">\(\bhat\)</span> is in <span class="process-math">\(\col(A)\text{.}\)</span>  We find the solution <span class="process-math">\(\xvec=\twovec2{-1}\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsection-114"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.1</span><span class="space"> </span><span class="title">A first example</span>
</h3>
<div class="para" id="p-6900">When we‚Äôve encountered inconsistent systems in the past, we‚Äôve simply said there is no solution and moved on.  The preview activity, however, shows how we can find approximate solutions to an inconsistent system: if there are no solutions to <span class="process-math">\(A\xvec = \bvec\text{,}\)</span> we instead solve the consistent system <span class="process-math">\(A\xvec = \bhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{.}\)</span>  As we‚Äôll see, this solution is, in a specific sense, the best possible.</div>
<article class="activity project-like" id="activity-85"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.5.2</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-6901">
<div class="para">Suppose we have three data points <span class="process-math">\((1,1)\text{,}\)</span> <span class="process-math">\((2,1)\text{,}\)</span> and <span class="process-math">\((3,3)\)</span> and that we would like to find a line passing through them.</div>
<ol class="lower-alpha">
<li id="li-4637">
<div class="para" id="p-6902">Plot these three points in <a href="" class="xref" data-knowl="./knowl/fig-ls-empty.html" title="Figure 6.5.2">Figure¬†6.5.2</a>.  Are you able to draw a line that passes through all three points?</div>
<figure class="figure figure-like" id="fig-ls-empty"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/empty-ls.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.2<span class="period">.</span></span><span class="space"> </span>Plot the three data points here.</figcaption></figure>
</li>
<li id="li-4638">
<div class="para logical" id="p-6903">
<div class="para">Remember that the equation of a line can be written as</div>
<div class="displaymath process-math">
\begin{equation*}
y = mx + b
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(m\)</span> is the slope and <span class="process-math">\(b\)</span> is the <span class="process-math">\(y\)</span>-intercept. Statisticans prefer the notation</div>
<div class="displaymath process-math">
\begin{equation*}
y = \beta_0 + \beta_1 x\text{,}
\end{equation*}
</div>
<div class="para">and we‚Äôre going to adopt statistical preferences for most of the remainder of this chapter since least squares is such an important method in statistics.</div>
</div>
<div class="para" id="p-6904">To begin, we will try to find <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\)</span> so that the three points lie on the line with equation  <span class="process-math">\(y = \beta_0 + \beta_1 x\text{.}\)</span> The first data point <span class="process-math">\((1,1)\)</span> gives an equation for <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\text{.}\)</span> In particular, we know that when <span class="process-math">\(x=1\text{,}\)</span> then <span class="process-math">\(y=1\)</span> so we have <span class="process-math">\(\beta_0 + \beta_1(1) = 1\)</span> or <span class="process-math">\(\beta_0 + \beta_1 = 1\text{.}\)</span>
</div>
<div class="para" id="p-6905">Use the other two data points to create a linear system describing <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\text{.}\)</span>
</div>
</li>
<li id="li-4639">
<div class="para" id="p-6906">We have obtained a linear system having three equations, one from each data point, for the two unknowns <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\text{.}\)</span> Identify a matrix <span class="process-math">\(X\)</span> and vector <span class="process-math">\(\betavec\)</span> so that the system has the form <span class="process-math">\(\yvec = X \betavec\text{,}\)</span> where <span class="process-math">\(\betavec=\ctwovec{\beta_0}{\beta_1}\text{.}\)</span>
</div>
<div class="para" id="p-6907">Notice that the unknown vector <span class="process-math">\(\betavec=\ctwovec {\beta_0}{\beta_1}\)</span> specifies the intercept and slope of the line that we seek.</div>
</li>
<li id="li-4640">
<div class="para" id="p-6908">Is there a solution to this linear system?  How does this question relate to your attempt to draw a line through the three points above?</div>
<pre class="ptx-sagecell sagecell-python" id="sage-222"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4641"><div class="para" id="p-6909">Since this system is inconsistent, we know that <span class="process-math">\(\yvec\)</span> is not in the column space <span class="process-math">\(\col(X)\text{.}\)</span> Find an orthogonal basis for <span class="process-math">\(\col(X)\)</span> and use it to find the orthogonal projection <span class="process-math">\(\yhat = \proj{\yvec}{\col(X)}\text{.}\)</span>
</div></li>
<li id="li-4642"><div class="para" id="p-6910">Since <span class="process-math">\(\yhat\)</span> is in <span class="process-math">\(\col(X)\text{,}\)</span> the equation <span class="process-math">\(X\betavec = \yhat\)</span> is consistent.  Find its solution, which we will denote <span class="process-math">\(\betahat = \ctwovec{\hat\beta_0}{\hat\beta_1}\text{,}\)</span> and sketch the line <span class="process-math">\(y=\hat{\beta}_0 + \hat{\beta}_1 x\)</span> in <a href="" class="xref" data-knowl="./knowl/fig-ls-empty.html" title="Figure 6.5.2">Figure¬†6.5.2</a>. This line is called the <dfn class="terminology">least squares regression line</dfn>.  That "hat" on <span class="process-math">\(\betahat\)</span> indicates that these coefficients (most likely) do not fit the data exactly, but come as close as we can to doing so (in the sense of minimizing the distance between <span class="process-math">\(\yhat = X\betahat\)</span> and <span class="process-math">\(\yvec\)</span>).</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original has-image" data-refid="hk-answer-270" id="answer-270"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-270"><div class="answer solution-like"><div class="para logical" id="p-6911"><ol class="lower-alpha">
<li id="li-4643"><div class="para" id="p-6912">It‚Äôs not possible to draw a line through all three points.</div></li>
<li id="li-4644"><div class="para logical" id="p-6913">
<div class="para">We have the equations</div>
<div class="displaymath process-math" id="md-26">
\begin{align*}
\beta_0 + \beta_1 \amp {}={} 1\\
\beta_0 + 2\beta_1 \amp {}={} 1\\
\beta_0 + 3\beta_1 \amp {}={} 3
\end{align*}
</div>
</div></li>
<li id="li-4645"><div class="para" id="p-6914">We have <span class="process-math">\(X = \begin{bmatrix}
1 \amp 1 \\
1 \amp 2 \\
1 \amp 3 \\
\end{bmatrix}\)</span> and <span class="process-math">\(\yvec=\threevec113\text{.}\)</span>
</div></li>
<li id="li-4646"><div class="para" id="p-6915">This linear system is inconsistent.</div></li>
<li id="li-4647"><div class="para" id="p-6916"><span class="process-math">\(\yhat=\threevec{2/3}{5/3}{8/3}\text{.}\)</span></div></li>
<li id="li-4648">
<div class="para" id="p-6917">
<span class="process-math">\(\betahat=\twovec{-1/3}1\text{.}\)</span> This line is shown in <a href="" class="xref" data-knowl="./knowl/fig-best-fit-line-ans.html" title="Figure 6.5.3">Figure¬†6.5.3</a>.</div>
<figure class="figure figure-like" id="fig-best-fit-line-ans"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/line-regress-2.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.3<span class="period">.</span></span><span class="space"> </span>The line that best approximates the three data points.</figcaption></figure>
</li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original has-image" data-refid="hk-solution-345" id="solution-345"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-345"><div class="solution solution-like"><div class="para logical" id="p-6918"><ol class="lower-alpha">
<li id="li-4649"><div class="para" id="p-6919">After plotting the points, we see that it‚Äôs not possible to draw a line through all three points.</div></li>
<li id="li-4650"><div class="para logical" id="p-6920">
<div class="para">We have the equations</div>
<div class="displaymath process-math" id="md-27">
\begin{align*}
\beta_0 + \beta_1 \amp {}={} 1\\
\beta_0 + 2\beta_1 \amp {}={} 1\\
\beta_0 + 3\beta_1 \amp {}={} 3
\end{align*}
</div>
</div></li>
<li id="li-4651"><div class="para" id="p-6921">We have <span class="process-math">\(X = \begin{bmatrix}
1 \amp 1 \\
1 \amp 2 \\
1 \amp 3 \\
\end{bmatrix}\)</span> and <span class="process-math">\(\yvec=\threevec113\text{.}\)</span>
</div></li>
<li id="li-4652"><div class="para" id="p-6922">Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system.  Since a solution would describe a line passing through the three points, we should expect this.</div></li>
<li id="li-4653"><div class="para" id="p-6923">Applying Gram-Schmidt gives us the orthogonal basis <span class="process-math">\(\wvec_1=\threevec111\)</span> and <span class="process-math">\(\wvec_2 =
\threevec{-1}01\text{.}\)</span>  Projecting <span class="process-math">\(\yvec\)</span> onto <span class="process-math">\(\col(X)\)</span> gives <span class="process-math">\(\yhat=\threevec{2/3}{5/3}{8/3}\text{.}\)</span>
</div></li>
<li id="li-4654">
<div class="para" id="p-6924">Solving the equation <span class="process-math">\(X\betavec = \yhat\)</span> gives <span class="process-math">\(\betahat=\ctwovec{-1/3}1\text{,}\)</span> which describes a line having vertical intercept <span class="process-math">\(b=-1/3\)</span> and the slope <span class="process-math">\(=1\text{.}\)</span>  This line is shown in <a href="" class="xref" data-knowl="./knowl/fig-best-fit-line.html" title="Figure 6.5.4">Figure¬†6.5.4</a>.</div>
<figure class="figure figure-like" id="fig-best-fit-line"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/line-regress-2.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.4<span class="period">.</span></span><span class="space"> </span>The line that best approximates the three data points.</figcaption></figure>
</li>
</ol></div></div></div>
</div></article><div class="para logical" id="p-6925">
<div class="para">This activity illustrates the idea behind a technique known as  <dfn class="terminology">least squares regression</dfn>, which we have been working toward throughout this chapter.  If the data points are denoted as <span class="process-math">\((x_i, y_i)\text{,}\)</span> we construct the matrix <span class="process-math">\(X\)</span> and vector <span class="process-math">\(\yvec\)</span> as</div>
<div class="displaymath process-math">
\begin{equation*}
A =
\begin{bmatrix}
1 \amp x_1 \\
1 \amp x_2 \\
1 \amp x_3 \\
\end{bmatrix},\hspace{24pt}
\yvec = \threevec{y_1}{y_2}{y_3}\text{.}
\end{equation*}
</div>
<div class="para">With the vector <span class="process-math">\(\betavec=\ctwovec{\beta_0}{\beta_1}\)</span> representing the line with equation <span class="process-math">\(y = \beta_0 + \beta_1 x\text{,}\)</span> we see that the equation <span class="process-math">\(\yvec = X\betavec\)</span> describes a line passing through all the data points.  In our activity, it is visually apparent that there is no such line, which agrees with the fact that the equation <span class="process-math">\(\yvec = X\betavec\)</span> is inconsistent.</div>
</div>
<div class="para" id="p-6926">Remember that <span class="process-math">\(\yhat = \proj{\yvec}{\col(X)}\)</span> is the closest vector in <span class="process-math">\(\col(X)\)</span> to <span class="process-math">\(\yvec\text{.}\)</span> Therefore, when we solve the equation <span class="process-math">\(X\betavec=\yhat\text{,}\)</span> we are finding the vector <span class="process-math">\(\betahat\)</span> so that <span class="process-math">\(\yhat = X\betahat = \threevec{\hat{\beta}_0+ \hat{\beta_1} x_1}{\hat{\beta}_0+ \hat{\beta_1} x_2}{\hat{\beta}_0+ \hat{\beta_1} x_3}\)</span> is as close to <span class="process-math">\(\yvec=\threevec{y_1}{y_2}{y_3}\)</span> as possible.  Let‚Äôs think about what this means within the context of this problem.</div>
<div class="para logical" id="p-6927">
<div class="para">The difference</div>
<div class="displaymath process-math">
\begin{equation*}
\yvec-\yhat =
\threevec{y_1-(\hat{\beta_0} +\hat{\beta_1} x_1)}{y_2-(\hat{\beta_0} + 
\hat{\beta_1} x_2)}{y_3-(\hat{\beta_0} +\hat{\beta_1} x_3)}
\end{equation*}
</div>
<div class="para">so that the square of the distance between <span class="process-math">\(\yhat = X\betahat\)</span> and <span class="process-math">\(\yvec\)</span> is</div>
<div class="displaymath process-math" id="md-28">
\begin{align*}
\len{\yvec - \yhat}^2 \amp =\\
\amp 
\left(y_1-(\hat{\beta}_0 +\hat{\beta}_1 x_1)\right)^2 + 
\left(y_2-(\hat{\beta}_0 +\hat{\beta}_1 x_2)\right)^2 +
\left(y_3-(\hat{\beta}_0 +\hat{\beta}_1 x_3)\right)^2\text{.}
\end{align*}
</div>
<div class="para"> Our approach finds the values for <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\)</span> that make this sum of squares as small as possible, which is why we call this a <dfn class="terminology">least squares</dfn> problem.</div>
</div>
<div class="para logical" id="p-6928">
<div class="para">Usually the least squares regression line does not pass through all of the data points. Statisticians call <span class="process-math">\(y_i - \hat{y}_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i)\)</span> the <dfn class="terminology">residual</dfn> for observation <span class="process-math">\(i\text{.}\)</span> As shown in <a href="" class="xref" data-knowl="./knowl/fig-least-squares-def.html" title="Figure 6.5.5">Figure¬†6.5.5</a>, residuals measure the vertical distance between the observed response value <span class="process-math">\(y_i\)</span> and the predicted response value <span class="process-math">\(\hat y_i\text{:}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/fig-least-squares-def.html">
\begin{equation*}
\mbox{residual} = \mbox{observed} - \mbox{expected}\text{.}
\end{equation*}
</div>
<div class="para">Seen in this way, the square of the distance <span class="process-math">\(\len{\yvec-\hat y}^2\)</span> is a measure of how much the line defined by the vector <span class="process-math">\(\betahat\)</span> misses the data points.  The solution to the least squares problem is the line that misses the data points by the smallest amount possible (when measured in this way).</div>
</div>
<figure class="figure figure-like" id="fig-least-squares-def"><div class="sidebyside"><div class="sbsrow" style="margin-left:25%;margin-right:25%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/line-regress-1.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.5<span class="period">.</span></span><span class="space"> </span>The solution of the least squares problem and the vertical distances between the line and the data points.</figcaption></figure></section><section class="subsection" id="subsec-linear-model-framework"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.2</span><span class="space"> </span><span class="title">The linear model framework</span>
</h3>
<div class="para" id="p-6929">The previous example is an example of <dfn class="terminology">simple linear regression</dfn>. In simple linear regression there is a single quantitative predictor and the model proposes a linear relationship between the explanatory variable and the response.  Least squares regression can be used with multiple explanatory variables just as easily as with one -- at least if we are willing to let the computer take care of the tedious arithmetic involved. In this section we describe a general framework called <dfn class="terminology">linear models</dfn>. Linear models and their generalizations are arguably the most important and commonly used method of data analysis.</div>
<div class="para logical" id="p-6930">
<div class="para">Suppose we are looking for a relationship of the form</div>
<div class="displaymath process-math">
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p\text{.}
\end{equation*}
</div>
<div class="para">We mean for this to hold for every subject in the data, each of whom has (likely different) values of <span class="process-math">\(y\)</span> and the <span class="process-math">\(x_i\)</span>‚Äôs.  But we want the values of the <span class="process-math">\(\beta's \)</span> to be the same for everyone.  Let‚Äôs rewrite our equation to emphasize that we are really dealing with vectors here.</div>
<div class="displaymath process-math">
\begin{equation*}
\yvec = \beta_0 \onevec + \beta_1 \xvec_1 + \beta_2 \xvec_2 + \cdots + \beta_p \xvec_p\text{.}
\end{equation*}
</div>
<div class="para">Notice that we snuck in a vector of 1‚Äôs to make the "constant" term look like all the others.</div>
</div>
<div class="para logical" id="p-6931">
<div class="para">Now let‚Äôs go one more step and express this model using matrices.  The vector of ones and the <span class="process-math">\(\xvec_i\)</span> vectors form the columns of the <span class="process-math">\(n \by (p+1)\)</span> matrix <span class="process-math">\(X\text{.}\)</span> This matrix is usually refered to as the <dfn class="terminology">model matrix</dfn>.  The <dfn class="terminology">coefficients</dfn> <span class="process-math">\(\beta_i\)</span> are arranged into a column vector. Then the entire relationship is expressed as a simple equation of the form <span class="process-math">\(A\xvec = \bvec\text{,}\)</span> but with different letters. And typically statisticians prefer to swap the left and right sides of the equation as well. So the model is exprssed like this:</div>
<div class="displaymath process-math" id="md-29">
\begin{align*}
X \amp = \begin{bmatrix} \onevec \; \xvec_1 \; \xvec_2 \; \cdots \; \xvec_p \end{bmatrix} \; , \quad
\betavec = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix}\\
\yvec \amp = X \betavec\text{.}
\end{align*}
</div>
<div class="para">This equation will almost never have an exact solution because in typical applications <span class="process-math">\(X\)</span> will have many more rows than columns.</div>
</div>
<div class="para logical" id="p-6932">
<div class="para">Since there is typically no exact solution, we seek an approximate solution, a solution to</div>
<div class="displaymath process-math">
\begin{equation*}
\yhat = X \betahat\; \mbox{where} \; \yhat = \proj{\yvec}{\col(X)}\text{.}
\end{equation*}
</div>
<div class="para"> In this context, <span class="process-math">\(\col(X)\)</span> is called the <dfn class="terminology">model space</dfn>. It will be important below  to note that this means that</div>
<div class="displaymath process-math">
\begin{equation*}
\yvec - \yhat \mbox{\ is orthogonal to (every vector in) the model space}\text{.}
\end{equation*}
</div>
</div>
<div class="para" id="p-6933">Whether we express our equation as <span class="process-math">\(A \xvec = \bvec\text{,}\)</span> as we have mostly done to this point, or as <span class="process-math">\(\yvec = X \betavec\text{,}\)</span> as we will typically do in statistical applications, or using some other letters, the linear algebra is the same (and familiar): orthogonal projection, solving sytems of linear equations, etc.</div>
<article class="example example-like" id="example-80"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.6</span><span class="period">.</span>
</h4>
<div class="para" id="p-6934">Add example of setting up a multiple regression problem. Include at least a binary categorical predictor.</div></article><div class="para" id="p-6935">We conclude this section with a note about the intercept term.</div>
<article class="note remark-like" id="note-8"><h4 class="heading">
<span class="type">Note</span><span class="space"> </span><span class="codenumber">6.5.7</span><span class="period">.</span><span class="space"> </span><span class="title">Models without an intercept.</span>
</h4> <div class="para" id="p-6936">It is possible to fit models without an intecept term.  In this case the column of 1‚Äôs will be omitted from the model matrix <span class="process-math">\(X\text{.}\)</span> Algebraically, a few things, like the defnition and interpretation of <span class="process-math">\(R^2\)</span> below do not work out as well in that case. And statistically, omitting the intercept makes a strong assumption about the nature of the relationship.  In most statistical software, the default it to always include an intercept, but there are options to fit models without the intercept term if so desired.</div></article></section><section class="subsection" id="subsection-116"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.3</span><span class="space"> </span><span class="title">Solving least squares problems</span>
</h3>
<div class="para" id="p-6937">Now that we‚Äôve discussed least squares approximate solutions to <span class="process-math">\(A \xvec = \bvec\)</span> and seen an important application of this method in linear models, usually expressed as <span class="process-math">\(\yvec = X \betavec\text{,}\)</span> it is time to turn our attention to some of the details involved in solving least squares problems. We‚Äôll continue with the statistical notation for this.</div>
<div class="para logical" id="p-6938">
<div class="para">We already know one way to solve a least squares problem <span class="process-math">\(\yvec = X \betavec\text{,}\)</span> namely</div>
<ol class="decimal">
<li id="li-4655">
<span class="heading"><span class="title">Project <span class="process-math">\(y\)</span> into the model space.</span></span><div class="para" id="p-6939">Compute <span class="process-math">\(\yhat = \proj{\yvec}{\col(X)}\text{.}\)</span>
</div>
</li>
<li id="li-4656">
<span class="heading"><span class="title">Solve the new equation <span class="process-math">\(\yhat = X \betavec\)</span> for <span class="process-math">\(\betavec\)</span>.</span></span><div class="para" id="p-6940">Because <span class="process-math">\(\yhat \in \col(X)\text{,}\)</span> we know this equation is consistent. If the columns of <span class="process-math">\(X\)</span> are linearly independent (as will usually be the case in linear model applications), then there is exactly one solution. We denote the solution to this  as <span class="process-math">\(\betahat\text{.}\)</span> The entries in <span class="process-math">\(\betahat\)</span> are called the (estiamted) coeffients of the model.</div>
<div class="para" id="p-6941">Because we can measure how close <span class="process-math">\(\yhat\)</span> is to <span class="process-math">\(\yvec\)</span> using an expression that involves a sum of squares, and <span class="process-math">\(\betahat\)</span> makes this expression as small as possible, <span class="process-math">\(\betahat\)</span> is called a <dfn class="terminology">least squares approximate solution</dfn> to the original equation <span class="process-math">\(\yvec = X\betavec\text{.}\)</span>
</div>
</li>
</ol>
</div>
<div class="para" id="p-6942">That is the method we have outlined above. But there are other methods that are usually used in practice, because they are more efficient and more stable numerically.</div>
<div class="para logical" id="p-6943">
<div class="para">We begin by describing a method for finding <span class="process-math">\(\betahat\)</span> that does not involve first finding the orthogonal projection <span class="process-math">\(\yhat\text{.}\)</span> Remember that</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/prop-col-orthog.html">
\begin{equation*}
\yhat = \proj{\yvec}{\col(X)}\text{,}
\end{equation*}
</div>
<div class="para">so <span class="process-math">\(\yvec - \yhat\)</span> is orthogonal to <span class="process-math">\(\col(A)\text{.}\)</span> In other words, <span class="process-math">\(\yvec - \yhat\)</span> is in the orthogonal complement <span class="process-math">\(\col(A)^\perp\text{,}\)</span> which <a href="" class="xref" data-knowl="./knowl/prop-col-orthog.html" title="Proposition 6.2.10">Proposition¬†6.2.10</a> tells us is the same as <span class="process-math">\(\nul(X^{\transpose})\text{.}\)</span>  Since <span class="process-math">\(\yvec - \yhat\)</span> is in <span class="process-math">\(\nul(X^{\transpose})\text{,}\)</span> it follows that</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/prop-col-orthog.html">
\begin{equation*}
X^{\transpose}(\yvec -\yhat) = \zerovec\text{.}
\end{equation*}
</div>
<div class="para">This is just another way of writing down that <span class="process-math">\(\yvec - \yhat\)</span> is orthogonal to each column of <span class="process-math">\(X\text{.}\)</span>
</div>
</div>
<div class="para logical" id="p-6944">
<div class="para">Because the least squares approximate solution is the vector <span class="process-math">\(\betahat\)</span> such that <span class="process-math">\(X\betahat = \yhat\text{,}\)</span> we can rearrange this equation to see that</div>
<div class="displaymath process-math" id="md-30">
\begin{align*}
X^{\transpose}(X\betahat - \yvec) \amp = \zerovec\\
X^{\transpose}X\betahat - X^{\transpose}\yvec \amp = \zerovec\\
X^{\transpose}X\betahat \amp = X^{\transpose}\yvec\text{.}
\end{align*}
</div>
<div class="para"> This equation is called the <dfn class="terminology">normal equation</dfn>, and we have the following proposition.</div>
</div>
<article class="proposition theorem-like" id="proposition-58"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.5.8</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-6945">
<div class="para">If the columns of <span class="process-math">\(X\)</span> are linearly independent, then there is a unique least squares approximate solution <span class="process-math">\(\betahat\)</span> to the equation <span class="process-math">\(X\betahat=\yvec\)</span> given by the normal equation</div>
<div class="displaymath process-math">
\begin{equation*}
X^{\transpose}X\betahat = X^{\transpose}\yvec\text{.}
\end{equation*}
</div>
</div></article><div class="para" id="p-6946">The next example demonstrates how we can use the normal equation to find the least squares approximate solution.</div>
<article class="example example-like" id="example-toy-normal-equation"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.5.9</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-6947">
<div class="para">Consider the equation</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{array}{cccc}
\threevec{16}{-1}7 \amp = \amp
\begin{bmatrix}
2 \amp 1 \\
2 \amp 0 \\
-1 \amp 3 \\
\end{bmatrix}
\amp
\betavec
\\
\yvec \amp = \amp X \amp \betavec  
\end{array}\text{.}
\end{equation*}
</div>
<div class="para">Since this equation is inconsistent, we will find the least squares approximate solution <span class="process-math">\(\betahat\)</span> by solving the normal equation <span class="process-math">\(X^{\transpose}X\betahat = X^{\transpose}\yvec\text{,}\)</span> which has the form</div>
<div class="displaymath process-math" id="md-31">
\begin{align*}
X^{\transpose}X\betahat \amp =  X^{\transpose}\yvec \\
\begin{bmatrix} 9 \amp -1 \\ -1 \amp 10 \\ \end{bmatrix} \betahat \amp = \twovec{23}{37} \text{.}
\end{align*}
</div>
<div class="para">Solving this yields <span class="process-math">\(\betahat=\twovec34\text{.}\)</span>
</div>
</div></article><div class="para" id="p-6948">You may wonder why the approach in <a href="" class="xref" data-knowl="./knowl/example-toy-normal-equation.html" title="Example 6.5.9">Example¬†6.5.9</a> is better than the original appraoch.  Here‚Äôs one reason why.  Suppose we have a larger example and <span class="process-math">\(X\)</span> is <span class="process-math">\(n\by m\)</span> with <span class="process-math">\(n\)</span> much larger than <span class="process-math">\(p\text{,}\)</span> as is often the case. Then <span class="process-math">\(X^{\transpose} X\)</span> is <span class="process-math">\(p\by p\text{,}\)</span> which is small. But <span class="process-math">\(X  X^\transpose\)</span> is <span class="process-math">\(n \by n\text{,}\)</span> which is large -- much larger than <span class="process-math">\(X\text{.}\)</span>  So computing <span class="process-math">\(\yhat = \proj{\yvec}{\col(X)} = X X^{\transpose} \yvec\)</span> is expensive. Working with <span class="process-math">\(X^\transpose \yvec\)</span> is comparatively much less computationally intensive.</div>
<article class="activity project-like" id="activity-crickets"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.5.3</span><span class="period">.</span>
</h4>
<div class="para" id="p-6949">The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we‚Äôll study in this activity.  The chirp rate <span class="process-math">\(C\)</span> is expressed in chirps per second while the temperature <span class="process-math">\(T\)</span> is in degrees Fahrenheit.  Evaluate the following cell to load the data:</div> <pre class="ptx-sagecell sagecell-sage" id="sage-223"><script type="text/x-sage">import pandas as pd 
import numpy as np 
import seaborn.objects as so 
url = 'https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/crickets.csv'
Crickets = pd.read_csv(url)
print(Crickets)
so.Plot(data = Crickets, x = "Chirps", "Temperature").add(so.Dot()).show()
</script></pre> <div class="para logical" id="p-6950">
<div class="para">We would like to represent this relationship by a linear function</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 + \beta_1 C = T\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-4657"><div class="para" id="p-6951">Use the first data point <span class="process-math">\((C_1,T_1)=(20.0,88.6)\)</span> to write an equation involving <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\text{.}\)</span>
</div></li>
<li id="li-4658">
<div class="para" id="p-6952">Suppose that we represent the coefficients using a vector <span class="process-math">\(\betavec = \twovec{\beta_0}{\beta_1}\text{.}\)</span> Use the 15 data points to create the matrix <span class="process-math">\(X\)</span> and vector <span class="process-math">\(\yvec\)</span> so that the linear system <span class="process-math">\(\yvec = X \betavec\)</span> describes the desired relationship.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-224"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4659"><div class="para logical" id="p-6953">
<div class="para">Write the normal equations</div>
<div class="displaymath process-math">
\begin{equation*}
X^{\transpose}X\betahat = X^{\transpose}\yvec\text{;}
\end{equation*}
</div>
<div class="para">that is, find the matrix <span class="process-math">\(X^{\transpose}X\)</span> and the vector <span class="process-math">\(X^{\transpose}\yvec\text{.}\)</span>
</div>
</div></li>
<li id="li-4660">
<div class="para" id="p-6954">Solve the normal equations to find <span class="process-math">\(\betahat\text{,}\)</span> the least squares approximate solution to the equation <span class="process-math">\(\yvec = X \betavec\text{.}\)</span>  Call your solution <span class="process-math">\(\betahat\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-225"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-6955">What are the values of <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\)</span> that you found?</div>
</li>
<li id="li-4661">
<div class="para" id="p-6956">If the chirp rate is 22 chirps per second, what is your prediction for the temperature?</div>
<div class="para" id="p-6957">Plot the data and your least squares regression line.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-226"><script type="text/x-sage">
</script></pre>
</li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-271" id="answer-271"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-271"><div class="answer solution-like"><div class="para logical" id="p-6958"><ol class="lower-alpha">
<li id="li-4662"><div class="para" id="p-6959"><span class="process-math">\(\beta_0 + 20.0\beta_1 = 88.6\text{.}\)</span></div></li>
<li id="li-4663"><div class="para" id="p-6960">
<span class="process-math">\(X\)</span> is the <span class="process-math">\(15\by2\)</span> matrix whose first column consists only of 1‚Äôs and whose second column is the vector of chirp rates.  The vector <span class="process-math">\(\yvec\)</span> is the vector of temperatures.</div></li>
<li id="li-4664"><div class="para" id="p-6961">
<span class="process-math">\(X^{\transpose}X=\begin{bmatrix}
15.0 \amp 248.5 \\
248.5 \amp 4157.9 \\
\end{bmatrix}\)</span> and <span class="process-math">\(X^{\transpose}\yvec =
\twovec{1190.2}{19857.7}\)</span>
</div></li>
<li id="li-4665"><div class="para" id="p-6962"><span class="process-math">\(\xhat = \twovec{\beta_0}{\beta_1} =
\twovec{22.8}{3.4}\text{.}\)</span></div></li>
<li id="li-4666"><div class="para" id="p-6963">
<span class="process-math">\(97.9\)</span> degrees.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-346" id="solution-346"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-346"><div class="solution solution-like"><div class="para logical" id="p-6964"><ol class="lower-alpha">
<li id="li-4667"><div class="para" id="p-6965">We have the equation <span class="process-math">\(\beta_0 + 20.0\beta_1 =
88.6\text{.}\)</span>
</div></li>
<li id="li-4668"><div class="para" id="p-6966">
<span class="process-math">\(X\)</span> is the <span class="process-math">\(15\by2\)</span> matrix whose first column consists only of 1‚Äôs and whose second column is the vector of chirp rates.  The vector <span class="process-math">\(\yvec\)</span> is the vector of temperatures.</div></li>
<li id="li-4669"><div class="para" id="p-6967">
<span class="process-math">\(X^{\transpose}X=\begin{bmatrix}
15.0 \amp 248.5 \\
248.5 \amp 4157.9 \\
\end{bmatrix}\)</span> and <span class="process-math">\(X^{\transpose}\yvec =
\twovec{1190.2}{19857.7}\)</span>
</div></li>
<li id="li-4670"><div class="para" id="p-6968"><span class="process-math">\(\betahat = \twovec{\beta_0}{\beta_1} =
\twovec{22.8}{3.4}\text{.}\)</span></div></li>
<li id="li-4671"><div class="para" id="p-6969">The predicted temperature is <span class="process-math">\(\beta_0 + 22\beta_1 = 97.9\)</span> degrees.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-6970"> Once we have the linear function that best fits the data, we can make predictions about situations that we haven‚Äôt encountered in the data. If we‚Äôre going to use our function to make predictions, it‚Äôs natural to ask how much confidence we have in these predictions.  This is a statistical question that leads to a rich and well-developed theory, which we won‚Äôt explore in much detail here.  However, there is one simple measure of how well our linear function fits the data that is known as the coefficient of determination and denoted by <span class="process-math">\(R^2\text{.}\)</span>
</div>
<div class="para" id="p-6971">We have seen that the predicted values from our model are given by <span class="process-math">\(\yhat = X \betahat\)</span> and that the square of the distance <span class="process-math">\(\len{\yvec-\yhat}^2\)</span> measures the amount by which the line fails to pass through the data points.  When the line is close to the data points, we expect this number to be small.  However, the size of this measure depends on the scale of the data.  For instance, the two lines shown in <a href="" class="xref" data-knowl="./knowl/fig-regression-scale.html" title="Figure 6.5.10">Figure¬†6.5.10</a> seem to fit the data equally well, but <span class="process-math">\(|\yvec-\yhat|^2\)</span> is 100 times larger on the right.</div>
<figure class="figure figure-like" id="fig-regression-scale"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/line-regress-1.svg" role="img" class="contained"></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="external/images/line-regress-10.svg" role="img" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.5.10<span class="period">.</span></span><span class="space"> </span>The lines appear to fit equally well in spite of the fact that <span class="process-math">\(\len{\yvec-\yhat}^2\)</span> differs by a factor of 100.</figcaption></figure><div class="para logical" id="p-6972">
<div class="para">We can create a measure of fit that is indpendent of scale if we consider the relationship among three important vectors:</div>
<ul class="disc">
<li id="li-4672"><div class="para" id="p-6973">
<span class="process-math">\(\yvec - \ymean\)</span> holds the differences between the observed response values and their mean value. The (square of the) length of this vector is a measure of the total variability in the response variable.</div></li>
<li id="li-4673"><div class="para" id="p-6974">
<span class="process-math">\(\yvec - \yhat\)</span> holds the differences between the observed response values and model fitted values. These differences are called <dfn class="terminology">residuals</dfn> This vector is orthogonal to the <dfn class="terminology">model space</dfn>, <span class="process-math">\(\col(X)\text{.}\)</span>
</div></li>
<li id="li-4674">
<div class="para logical" id="p-6975">
<div class="para">
<span class="process-math">\(\yhat - \ymean\)</span> holds the differences between the fitted values and the mean response. Importantly, this vector <em class="emphasis">is in the model space</em>.  We can see this as follows. The vector <span class="process-math">\(\yhat\)</span> is in the model space by definition. If our model includes an intercept (so the first column of <span class="process-math">\(X\)</span> is <span class="process-math">\(\onevec\)</span>), then</div>
<div class="displaymath process-math">
\begin{equation*}
\ymean = \overline{y} \onevec = \overline{y}\onevec + 0 \xvec_1 + 0 \xvec_2 + \cdots + 0 \xvec_p
= X \begin{bmatrix}\overline{y} \\0\\\vdots\\0\end{bmatrix}\text{,}
\end{equation*}
</div>
<div class="para">so <span class="process-math">\(\ymean\)</span> is also in the model space.  This implies that <span class="process-math">\(\yhat - \ymean\)</span> is in the model space.</div>
</div>
<div class="para" id="p-6976">Because <span class="process-math">\(\yhat - \ymean\)</span> is in the model space and <span class="process-math">\(y - \yhat\)</span> is orthogonal to the model space, we know that these vectors are orthogonal.</div>
</li>
</ul>
<div class="para">Putting this together we see that</div>
<div class="displaymath process-math" id="md-32">
\begin{align*}
\yvec - \ymean \amp = (\yvec - \yhat ) + (\yhat - \ymean) \\
\len{\yvec - \ymean}^2 \amp = \len{\yvec - \yhat}^2 + \len{\yhat - \ymean}^2 \text{.}
\end{align*}
</div>
<div class="para">The second equation is the just the Pythagorean Theorem applied to the triangle formed by our three vectors.</div>
</div>
<div class="para" id="p-6977">This provides a natural way to measure how well our model fits the data:</div>
<article class="definition definition-like" id="def-rsquared"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.5.11</span><span class="period">.</span><span class="space"> </span><span class="title">Coefficient of determination.</span>
</h4>  <div class="para logical" id="p-6978">
<div class="para">For any model that determines predictions <span class="process-math">\(\yhat\)</span> for a response variable <span class="process-math">\(\yvec\text{,}\)</span> we can define the <dfn class="terminology">coefficient of determination</dfn> as</div>
<div class="displaymath process-math" id="md-33">
\begin{align*}
R^2 \amp = \frac{ \len{\yhat - \ymean}^2}{\len{\yvec - \ymean}^2}\\
\amp = \frac{ \len{\yhat - \ymean}^2}{\len{\yvec - \yhat}^2 + \len{\yhat - \ymean}^2 }\text{.}
\end{align*}
</div>
<div class="para"> For a linear model,</div>
<div class="displaymath process-math">
\begin{equation*}
\yhat = X \betahat\text{,}
\end{equation*}
</div>
<div class="para">but the definition can be applied to other models as well.</div>
</div></article><div class="para logical" id="p-6979">
<div class="para">It is clear from the definition that for linear models with an intercept,</div>
<div class="displaymath process-math">
\begin{equation*}
0 \le R^2 \le 1\text{.}
\end{equation*}
</div>
<div class="para">So one way to interpret <span class="process-math">\(R^2\)</span> is as the proportion of the variation in <span class="process-math">\(\yvec\)</span> that is explained by the model (i.e., by <span class="process-math">\(\yhat\)</span>). If <span class="process-math">\(\yhat = \yvec\text{,}\)</span> the fit is perfect and <span class="process-math">\(R^2 = 1\text{.}\)</span> At the other extreme, if our model predicts <span class="process-math">\(\yhat = \ymean\)</span> (everyone is average), then <span class="process-math">\(R^2 = 0\text{.}\)</span>
</div>
</div>
<div class="para" id="p-6980">A more complete explanation of this definition relies on the concept of variance, which we explore in <a href="" class="xref" data-knowl="./knowl/ex-r2-meaning.html" title="Exercise 6.5.8.12">Exercise¬†6.5.8.12</a> and the next chapter.  For the time being, it‚Äôs enough to know that <span class="process-math">\(0\leq R^2 \leq 1\)</span> and that the closer <span class="process-math">\(R^2\)</span> is to 1, the better the line fits the data. In our original example, illustrated in <a href="" class="xref" data-knowl="./knowl/fig-regression-scale.html" title="Figure 6.5.10">Figure¬†6.5.10</a>, we find that <span class="process-math">\(R^2 = 0.75\text{,}\)</span> and in our study of cricket chirp rates, we have <span class="process-math">\(R^2=0.69\text{.}\)</span>  However, assessing the confidence we have in predictions made by solving a least squares problem can require considerable thought, and it would be naive to rely only on the value of <span class="process-math">\(R^2\text{.}\)</span>
</div>
<div class="para logical" id="p-6981">
<div class="para">There is also a connection between the correlation coefficient and the coefficient of of determination.  For a simple linear model <span class="process-math">\(\yvec = \beta_0 + \beta_1 \xvec\text{,}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
\corr(\xvec, \yvec) = R^2\text{.}
\end{equation*}
</div>
<div class="para">For this model, an orthogonal basis for the model space is</div>
<div class="displaymath process-math" id="md-34">
\begin{align*}
\wvec_1 \amp = \onevec \\
\wvec_2 \amp = \xvec - \proj{\xvec}{\onevec} = \xvec - \xmean \text{,}
\end{align*}
</div>
<div class="para">and</div>
<div class="displaymath process-math">
\begin{equation*}
\yhat = \proj{\yvec}{\onevec} + \proj{\yvec}{\xvec - \xmean} = ???
\end{equation*}
</div>
</div></section><section class="subsection" id="subsection-117"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.4</span><span class="space"> </span><span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span>
</h3>
<div class="para" id="p-6982">As we‚Äôve seen, the least squares approximate solution <span class="process-math">\(\xhat\)</span> to <span class="process-math">\(A\xvec=\bvec\)</span> may be found by solving the normal equation <span class="process-math">\(A^{\transpose}A\xhat = A^{\transpose}\bvec\text{,}\)</span> and this can be a practical strategy for some problems.  However, this approach can be problematic as small rounding errors can accumulate and lead to inaccurate final results.</div>
<div class="para" id="p-6983">As the next activity demonstrates, there is an third method for finding the least squares approximate solution <span class="process-math">\(\xhat\)</span> using a <span class="process-math">\(QR\)</span> factorization of the matrix <span class="process-math">\(A\text{,}\)</span> and this method is preferable as it is both computatinoally efficient and numerically more reliable. This is the method implemented in most statistical software packages.</div>
<article class="activity project-like" id="activity-BFI"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.5.4</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-6984"><ol class="lower-alpha">
<li id="li-4675"><div class="para logical" id="p-6985">
<div class="para">Suppose we are interested in finding the least squares approximate solution to the equation <span class="process-math">\(A\xvec =
\bvec\)</span> and that we have the <span class="process-math">\(QR\)</span> factorization <span class="process-math">\(A=QR\text{.}\)</span>  Explain why the least squares approximation solution is given by solving</div>
<div class="displaymath process-math" id="md-35">
\begin{align*}
A\xhat \amp = QQ^{\transpose}\bvec \\\\
QR\xhat \amp = QQ^{\transpose}\bvec \\
\end{align*}
</div>
</div></li>
<li id="li-4676">
<div class="para logical" id="p-6986">
<div class="para">Multiply both sides of the second expression by <span class="process-math">\(Q^{\transpose}\)</span> and explain why</div>
<div class="displaymath process-math">
\begin{equation*}
R\xhat = Q^{\transpose}\bvec.
\end{equation*}
</div>
</div>
<div class="para logical" id="p-6987">
<div class="para">Since <span class="process-math">\(R\)</span> is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in <a href="sec-gaussian-revisited.html" class="internal" title="Section 4.7: Partial pivoting and LU factorizations">Section¬†4.7</a>.  We will therefore write the least squares approximate solution as</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/sec-gaussian-revisited.html">
\begin{equation*}
\xhat = R^{-1}Q^{\transpose}\bvec,
\end{equation*}
</div>
<div class="para">and put this to use in the following context.</div>
</div>
</li>
<li id="li-4677">
<div class="para logical" id="p-6988">
<div class="para">Brozak‚Äôs formula, which is used to calculate a person‚Äôs body fat index <span class="process-math">\(BFI\text{,}\)</span> is</div>
<div class="displaymath process-math">
\begin{equation*}
BFI = 100 \left(\frac{4.57}{\rho} - 4.142\right)
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(\rho\)</span> denotes a person‚Äôs body density in grams per cubic centimeter.  Obtaining an accurate measure of <span class="process-math">\(\rho\)</span> is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced.  Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict <span class="process-math">\(BFI\text{.}\)</span>
</div>
</div>
<div class="para" id="p-6989">For instance, suppose we take 10 patients and measure their weight <span class="process-math">\(w\)</span> in pounds, height <span class="process-math">\(h\)</span> in inches, abdomen <span class="process-math">\(a\)</span> in centimeters, wrist circumference <span class="process-math">\(r\)</span> in centimeters, neck circumference <span class="process-math">\(n\)</span> in centimeters, and <span class="process-math">\(BFI\text{.}\)</span>  Evaluating the following cell loads and displays the data.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-227"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/bfi.csv')
weight = vector(df['Weight'])
height = vector(df['Height'])
abdomen = vector(df['Abdomen'])
wrist = vector(df['Wrist'])
neck = vector(df['Neck'])
BFI = vector(df['BFI'])
print(df)
</script></pre>
<div class="para logical" id="p-6990">
<div class="para">In addition, that cell provides:</div>
<ol class="lower-alpha">
<li id="li-4678"><div class="para" id="p-6991">vectors <code class="code-inline tex2jax_ignore">weight</code>, <code class="code-inline tex2jax_ignore">height</code>, <code class="code-inline tex2jax_ignore">abdomen</code>, <code class="code-inline tex2jax_ignore">wrist</code>, <code class="code-inline tex2jax_ignore">neck</code>, and <code class="code-inline tex2jax_ignore">BFI</code> formed from the columns of the dataset.</div></li>
<li id="li-4679"><div class="para" id="p-6992">the command <code class="code-inline tex2jax_ignore">onesvec(n)</code>, which returns an <span class="process-math">\(n\)</span>-dimensional vector whose entries are all one.</div></li>
<li id="li-4680"><div class="para" id="p-6993">the command <code class="code-inline tex2jax_ignore">QR(A)</code> that returns the <span class="process-math">\(QR\)</span> factorization of <span class="process-math">\(A\)</span> as <code class="code-inline tex2jax_ignore">Q, R = QR(A)</code>.</div></li>
<li id="li-4681"><div class="para" id="p-6994">the command <code class="code-inline tex2jax_ignore">demean(v)</code>, which returns the demeaned vector <span class="process-math">\(\widetilde{\vvec}\text{.}\)</span>
</div></li>
</ol>
</div>
<div class="para logical" id="p-6995">
<div class="para">We would like to find the linear function</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 + \beta_1w + \beta_2h + \beta_3a + \beta_4r +
\beta_5n = BFI
\end{equation*}
</div>
<div class="para">that best fits the data.</div>
</div>
<div class="para" id="p-6996">Use the first data point to write an equation for the parameters <span class="process-math">\(\beta_0,\beta_1,\ldots,\beta_5\text{.}\)</span>
</div>
</li>
<li id="li-4682"><div class="para" id="p-6997">Describe the linear system <span class="process-math">\(A\xvec = \bvec\)</span> for these parameters.  More specifically, describe how the matrix <span class="process-math">\(A\)</span> and the vector <span class="process-math">\(\bvec\)</span> are formed.</div></li>
<li id="li-4683">
<div class="para" id="p-6998">Construct the matrix <span class="process-math">\(A\)</span> and find its <span class="process-math">\(QR\)</span> factorization in the cell below.</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-228"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4684"><div class="para" id="p-6999">Find the least squares approximate solution <span class="process-math">\(\xhat\)</span> by solving the equation <span class="process-math">\(R\xhat =
Q^{\transpose}\bvec\text{.}\)</span>  You may want to use <code class="code-inline tex2jax_ignore">N(xhat)</code> to display a decimal approximation of the vector. What are the parameters <span class="process-math">\(\beta_0,\beta_1,\ldots,\beta_5\)</span> that best fit the data?</div></li>
<li id="li-4685">
<div class="para" id="p-7000">Find the coefficient of determination <span class="process-math">\(R^2\)</span> for your parameters.  What does this imply about the quality of the fit?</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-229"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4686"><div class="para" id="p-7001">Suppose a person‚Äôs measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35.  Estimate this person‚Äôs <span class="process-math">\(BFI\text{.}\)</span>
</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-272" id="answer-272"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-272"><div class="answer solution-like"><div class="para logical" id="p-7002"><ol class="lower-alpha">
<li id="li-4687"><div class="para" id="p-7003">Use the fact that <span class="process-math">\(\bhat=QQ^{\transpose}\bvec\text{.}\)</span>
</div></li>
<li id="li-4688"><div class="para" id="p-7004">Use the fact that <span class="process-math">\(Q^{\transpose}Q=I\text{.}\)</span>
</div></li>
<li id="li-4689"><div class="para" id="p-7005"><span class="process-math">\(\displaystyle \beta_0 + 154\beta_1 + 68\beta_2 + 85\beta_3 +
17\beta_4 + 36\beta_5 = 13\)</span></div></li>
<li id="li-4690"><div class="para" id="p-7006">
<span class="process-math">\(A\)</span> is the <span class="process-math">\(10\by6\)</span> matrix whose columns are a vector of all 1‚Äôs followed by the vectors of weights, heights, abdominal, wrist, and neck measurements.  The vector <span class="process-math">\(\bvec\)</span> is the vector of BFI readings.</div></li>
<li id="li-4691"><div class="para" id="p-7007">
<span class="process-math">\(Q\)</span> is a <span class="process-math">\(10\by6\)</span> matrix and <span class="process-math">\(R\)</span> is a <span class="process-math">\(6\by6\)</span> upper triangular matrix.</div></li>
<li id="li-4692"><div class="para logical" id="p-7008">
<div class="para">We find that</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 = 54.08, \beta_1 = 0.19, \beta_2 =
-2.62,
\beta_3 = 0.92, \beta_4 = 2.70, \beta_5 =
-0.41
\end{equation*}
</div>
</div></li>
<li id="li-4693"><div class="para" id="p-7009"><span class="process-math">\(\displaystyle R^2=0.95\)</span></div></li>
<li id="li-4694"><div class="para" id="p-7010">Evaluating <span class="process-math">\(\beta_0 + 190\beta_1 + 70\beta_2 +
90\beta_3 + 18\beta_4 + 35\beta_5 = 22.9\)</span>
</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-347" id="solution-347"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-347"><div class="solution solution-like"><div class="para logical" id="p-7011"><ol class="lower-alpha">
<li id="li-4695"><div class="para" id="p-7012">The columns of <span class="process-math">\(Q\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\)</span> so that <span class="process-math">\(\bhat=QQ^{\transpose}\bvec\text{.}\)</span>  The equation <span class="process-math">\(A\xhat=\bhat\)</span> then becomes <span class="process-math">\(QR\xhat=QQ^{\transpose}\bvec\text{.}\)</span>
</div></li>
<li id="li-4696"><div class="para" id="p-7013">Since <span class="process-math">\(Q^{\transpose}Q=I\text{,}\)</span> we have <span class="process-math">\(Q^{\transpose}QR\xhat =
Q^{\transpose}QQ^{\transpose}\bvec\text{,}\)</span> which gives <span class="process-math">\(R\xhat =
Q^{\transpose}\bvec\text{.}\)</span>
</div></li>
<li id="li-4697"><div class="para" id="p-7014"><span class="process-math">\(\displaystyle \beta_0 + 154\beta_1 + 68\beta_2 + 85\beta_3 +
17\beta_4 + 36\beta_5 = 13\)</span></div></li>
<li id="li-4698"><div class="para" id="p-7015">
<span class="process-math">\(A\)</span> is the <span class="process-math">\(10\by6\)</span> matrix whose columns are a vector of all 1‚Äôs followed by the vectors of weights, heights, abdominal, wrist, and neck measurements.  The vector <span class="process-math">\(\bvec\)</span> is the vector of BFI readings.</div></li>
<li id="li-4699"><div class="para" id="p-7016">
<span class="process-math">\(Q\)</span> is a <span class="process-math">\(10\by6\)</span> matrix and <span class="process-math">\(R\)</span> is a <span class="process-math">\(6\by6\)</span> upper triangular matrix.</div></li>
<li id="li-4700"><div class="para logical" id="p-7017">
<div class="para">We find that</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 = 54.08, \beta_1 = 0.19, \beta_2 =
-2.62,
\beta_3 = 0.92, \beta_4 = 2.70, \beta_5 =
-0.41
\end{equation*}
</div>
</div></li>
<li id="li-4701"><div class="para" id="p-7018"><span class="process-math">\(\displaystyle R^2=0.95\)</span></div></li>
<li id="li-4702"><div class="para" id="p-7019">Evaluating <span class="process-math">\(\beta_0 + 190\beta_1 + 70\beta_2 +
90\beta_3 + 18\beta_4 + 35\beta_5 = 22.9\)</span>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-7020">To summarize, we have seen that</div>
<article class="proposition theorem-like" id="proposition-59"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.5.12</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7021">
<div class="para">If the columns of <span class="process-math">\(A\)</span> are linearly independent and we have the <span class="process-math">\(QR\)</span> factorization <span class="process-math">\(A=QR\text{,}\)</span> then the least squares approximate solution <span class="process-math">\(\xhat\)</span> to the equation <span class="process-math">\(A\xvec=\bvec\)</span> is given by</div>
<div class="displaymath process-math">
\begin{equation*}
\xhat = R^{-1}Q^{\transpose}\bvec\text{.}
\end{equation*}
</div>
</div></article></section><section class="subsection" id="subsection-118"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.5</span><span class="space"> </span><span class="title">Polynomial Regression</span>
</h3>
<div class="para" id="p-7022">In the examples we‚Äôve seen so far, we have fit a linear function to a dataset.  Sometimes, however, a polynomial, such as a quadratic function, may be more appropriate.  It turns out that the techniques we‚Äôve developed in this section are still useful as the next activity demonstrates.</div>
<article class="activity project-like" id="activity-88"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.5.5</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-7023"><ol class="lower-alpha">
<li id="li-4703">
<div class="para" id="p-7024">Suppose that we have a small dataset containing the points <span class="process-math">\((0,2)\text{,}\)</span> <span class="process-math">\((1,1)\text{,}\)</span> <span class="process-math">\((2,3)\text{,}\)</span> and <span class="process-math">\((3,3)\text{,}\)</span> such as appear when the following cell is evaluated.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-230"><script type="text/x-sage">import numpy as np
import seaborn.objects as so 
small_data = np.array([[0, 2], [1, 1], [2, 3], [3, 3]])
so.Plot(x=small_data[:,0], y = small_data[:, 1]).add(so.Dot())
list_plot(data, color='blue', size=40)
</script></pre>
<div class="para logical" id="p-7025">
<div class="para">Let‚Äôs fit a quadratic function of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\yvec = \beta_0 + \beta_1 \xvec + \beta_2 \xvec^2 
\end{equation*}
</div>
<div class="para">to this dataset.</div>
</div>
<div class="para" id="p-7026">Write four equations, one for each data point, that describe the coefficients <span class="process-math">\(\beta_0\text{,}\)</span> <span class="process-math">\(\beta_1\text{,}\)</span> and <span class="process-math">\(\beta_2\text{.}\)</span>
</div>
</li>
<li id="li-4704">
<div class="para" id="p-7027">Express these four equations as a linear system <span class="process-math">\(\yvec = X \betavec \)</span> where <span class="process-math">\(\betavec = \threevec{\beta_0}{\beta_1}{\beta_2}\text{.}\)</span>
</div>
<div class="para" id="p-7028">Find the <span class="process-math">\(QR\)</span> factorization of <span class="process-math">\(X\)</span> and use it to find the least squares approximate solution <span class="process-math">\(\betahat\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-231"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4705">
<div class="para" id="p-7029">Use the parameters <span class="process-math">\(\beta_0\text{,}\)</span> <span class="process-math">\(\beta_1\text{,}\)</span> and <span class="process-math">\(\beta_2\)</span> that you found to write the quadratic function that fits the data. Creat a plot that incluedes the raw data and the quadratic fit.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-232"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4706"><div class="para" id="p-7030">What is your predicted <span class="process-math">\(\hat{y}\)</span> value when <span class="process-math">\(x=1.5\text{.}\)</span>
</div></li>
<li id="li-4707"><div class="para" id="p-7031">Find the coefficient of determination <span class="process-math">\(R^2\)</span> for the quadratic function.  What does this say about the quality of the fit?</div></li>
<li id="li-4708">
<div class="para logical" id="p-7032">
<div class="para">Now fit a cubic polynomial of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\yvec = \beta_0 \onevec + \beta_1 \xvec  + \beta_2 \xvec^2 + \beta_3 \xvec ^3
\end{equation*}
</div>
<div class="para">to this dataset.</div>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-233"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4709"><div class="para" id="p-7033">Find the coefficient of determination <span class="process-math">\(R^2\)</span> for the cubic function.  What does this say about the quality of the fit?</div></li>
<li id="li-4710">
<div class="para" id="p-7034">What do you notice when you plot the cubic function along with the data?  How does this reflect the value of <span class="process-math">\(R^2\)</span> that you found?</div>
<pre class="ptx-sagecell sagecell-python" id="sage-234"><script type="text/x-sage">
</script></pre>
</li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-273" id="answer-273"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-273"><div class="answer solution-like"><div class="para logical" id="p-7035"><ol class="lower-alpha">
<li id="li-4711"><div class="para logical" id="p-7036">
<div class="para">We have the equations</div>
<div class="displaymath process-math" id="md-36">
\begin{align*}
\beta_0 + 0\beta_1 + 0^2\beta_2 \amp {}={} 2\\
\beta_0 + 1\beta_1 + 1^2\beta_2 \amp {}={} 1\\
\beta_0 + 2\beta_1 + 2^2\beta_2 \amp {}={} 3\\
\beta_0 + 3\beta_1 + 3^2\beta_2 \amp {}={} 3
\end{align*}
</div>
</div></li>
<li id="li-4712"><div class="para" id="p-7037"><span class="process-math">\(\displaystyle \betahat = \threevec{7/4}{-1/4}{1/4}\)</span></div></li>
<li id="li-4713"><div class="para" id="p-7038"><span class="process-math">\(\frac74 - \frac14 x + \frac14x^2 = y\text{.}\)</span></div></li>
<li id="li-4714"><div class="para" id="p-7039"><span class="process-math">\(1.9375\text{.}\)</span></div></li>
<li id="li-4715"><div class="para" id="p-7040"><span class="process-math">\(\displaystyle 0.54\)</span></div></li>
<li id="li-4716"><div class="para" id="p-7041"><span class="process-math">\(\yhat = 2-\frac{25}{6} \xvec +4 \xvec ^2-\frac56 \xvec^3 \text{.}\)</span></div></li>
<li id="li-4717"><div class="para" id="p-7042"><span class="process-math">\(\displaystyle R^2=1\)</span></div></li>
<li id="li-4718"><div class="para" id="p-7043">The graph of the cubic function passes through each data point.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-348" id="solution-348"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-348"><div class="solution solution-like"><div class="para logical" id="p-7044"><ol class="lower-alpha">
<li id="li-4719"><div class="para logical" id="p-7045">
<div class="para">We have the equations</div>
<div class="displaymath process-math" id="md-37">
\begin{align*}
\beta_0 + 0\beta_1 + 0^2\beta_2 \amp {}={} 2\\
\beta_0 + 1\beta_1 + 1^2\beta_2 \amp {}={} 1\\
\beta_0 + 2\beta_1 + 2^2\beta_2 \amp {}={} 3\\
\beta_0 + 3\beta_1 + 3^2\beta_2 \amp {}={} 3
\end{align*}
</div>
</div></li>
<li id="li-4720"><div class="para" id="p-7046">With <span class="process-math">\(X=\begin{bmatrix}
1 \amp 0 \amp 0 \\
1 \amp 1 \amp 1 \\
1 \amp 2 \amp 4 \\
1 \amp 3 \amp 9 \\
\end{bmatrix}\)</span> and <span class="process-math">\(\yvec=\fourvec2133\text{,}\)</span> we find <span class="process-math">\(\betahat = \threevec{7/4}{-1/4}{1/4}\)</span>
</div></li>
<li id="li-4721"><div class="para" id="p-7047">The quadratic function is <span class="process-math">\(\frac74 - \frac14 x +
\frac14x^2 = y\text{.}\)</span>
</div></li>
<li id="li-4722"><div class="para" id="p-7048">The predicted value is <span class="process-math">\(y=\frac74 -
\frac14(1.5)+\frac14(1.5)^2 = 1.9375\text{.}\)</span>
</div></li>
<li id="li-4723"><div class="para" id="p-7049"><span class="process-math">\(\displaystyle R^2 = 0.54\)</span></div></li>
<li id="li-4724"><div class="para" id="p-7050">We find <span class="process-math">\(\yhat = 2-\frac{25}{6} \xvec +4 \xvec ^2-\frac56 \xvec^3 \text{.}\)</span>
</div></li>
<li id="li-4725"><div class="para" id="p-7051">
<span class="process-math">\(R^2=1\text{,}\)</span> which means that we have a perfect fit.</div></li>
<li id="li-4726"><div class="para" id="p-7052">The graph of the cubic function passes through each data point.</div></li>
</ol></div></div></div>
</div></article><div class="para logical" id="p-7053">
<div class="para">The matrices <span class="process-math">\(X\)</span> that you created in the last activity when fitting a quadratic and cubic function to a dataset have a special form.  In particular, if the data points are labeled <span class="process-math">\((x_i, y_i)\)</span> and we seek a degree <span class="process-math">\(k\)</span> polynomial, then</div>
<div class="displaymath process-math">
\begin{equation*}
X =
\begin{bmatrix}
1 \amp x_1 \amp x_1^2 \amp \ldots \amp x_1^k \\
1 \amp x_2 \amp x_2^2 \amp \ldots \amp x_2^k \\
\vdots \amp \vdots \amp \vdots \amp \ddots \amp \vdots \\
1 \amp x_m \amp x_m^2 \amp \ldots \amp x_m^k \\
\end{bmatrix}.
\end{equation*}
</div>
<div class="para"> This is called a <dfn class="terminology">Vandermonde matrix</dfn> of degree <span class="process-math">\(k\text{.}\)</span> You can use <code class="code-inline tex2jax_ignore">numpy.polynomial.polynomial.polyvander()</code> to create  these matrices for a specified vector <span class="process-math">\(\xvec\)</span> and degree.</div>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-235"><script type="text/x-sage">import numpy as np 
from numpy.polynomial.polynomial import polyvander
x = np.arange(5)
print(polyvander(x, 5))
</script></pre>
<div class="para" id="p-7054">Notice that <span class="process-math">\(0^0\)</span> is treated as <span class="process-math">\(1\)</span> for the purposes of this matrix.</div>
<article class="activity project-like" id="activity-89"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.5.6</span><span class="period">.</span>
</h4>
<div class="para" id="p-7055">This activity explores a dataset describing Arctic sea ice and that comes from <a class="external" href="http://sustainabilitymath.org/" target="_blank">Sustainability Math.</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-5" id="fn-5"><sup>‚Äâ1‚Äâ</sup></a>
</div> <div class="para" id="p-7056">Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 1980, 2012, and 2017. We will focus primarily on 2012.</div> <pre class="ptx-sagecell sagecell-python" id="sage-236"><script type="text/x-sage">import pandas as pd
import numpy as np
import seaborn.objects as so
ice = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/sea_ice.csv')
print(ice)
so.Plot(data = ice, x = "Month", y = "2012").add(so.Dot())
</script></pre> <div class="para logical" id="p-7057"><ol class="lower-alpha">
<li id="li-4727">
<div class="para" id="p-7058">Find the vector <span class="process-math">\(\betahat\text{,}\)</span> the least squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-237"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4728">
<div class="para" id="p-7059">Plot the data along with the fitted polynomial model.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-238"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4729"><div class="para" id="p-7060">Find the coefficient of determination <span class="process-math">\(R^2\)</span> for this polynomial fit.</div></li>
<li id="li-4730">
<div class="para" id="p-7061">Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find <span class="process-math">\(R^2\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-239"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-4731">
<div class="para" id="p-7062">Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding <span class="process-math">\(R^2\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-sage" id="sage-240"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-7063">It‚Äôs certainly true that higher degree polynomials fit the data better, as seen by the increasing values of <span class="process-math">\(R^2\text{,}\)</span> but that‚Äôs not always a good thing. For instance, when <span class="process-math">\(k=11\text{,}\)</span> you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it‚Äôs obtained from measurements.  The error built in to the data is called <dfn class="terminology">noise</dfn>, and its presence means that we shouldn‚Äôt expect our polynomial to fit the data perfectly.  When we choose a polynomial whose degree is too high, we give the noise too much influence over the fit of the model, which leads to some undesirable behavior, like the wiggles in the graph.</div>
<div class="para" id="p-7064">Fitting the data with a function that is too flexible and fits the training data better than it can be expected to fit new data high is called <dfn class="terminology">overfitting</dfn>, a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose <span class="process-math">\(k\)</span> large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model.  What we really need is a method for selecting a good value for <span class="process-math">\(k\)</span> and a better way to measure how well we should expect the model to fit <em class="emphasis">new</em> data, not the data used to train the model. That discussion would take us too far afield for the moment, but it is an important discussion.</div>
</li>
<li id="li-4732"><div class="para" id="p-7065">Choosing a reasonable value of <span class="process-math">\(k\text{,}\)</span> estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-274" id="answer-274"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-274"><div class="answer solution-like"><div class="para logical" id="p-7066"><ol class="lower-alpha">
<li id="li-4733"><div class="para" id="p-7067"><span class="process-math">\(\displaystyle \xhat = \begin{bmatrix}
17.4\\-7.1\\4.5\\-1.1\\0.1\\-0.003
\end{bmatrix}\)</span></div></li>
<li id="li-4734"><div class="para" id="p-7068">The fifth degree polynomial fits the data fairly well.</div></li>
<li id="li-4735"><div class="para" id="p-7069"><span class="process-math">\(\displaystyle R^2=0.99\)</span></div></li>
<li id="li-4736"><div class="para" id="p-7070"><span class="process-math">\(R^2=0.9997\text{.}\)</span></div></li>
<li id="li-4737"><div class="para" id="p-7071"><span class="process-math">\(\displaystyle R^2=1\)</span></div></li>
<li id="li-4738"><div class="para" id="p-7072">
<span class="process-math">\(8.7\)</span> million square kilometers of sea ice.</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-349" id="solution-349"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-349"><div class="solution solution-like"><div class="para logical" id="p-7073"><ol class="lower-alpha">
<li id="li-4739"><div class="para" id="p-7074"><span class="process-math">\(\displaystyle \xhat = \begin{bmatrix}
17.4\\-7.1\\4.5\\-1.1\\0.1\\-0.003
\end{bmatrix}\)</span></div></li>
<li id="li-4740"><div class="para" id="p-7075">The fifth degree polynomial fits the data fairly well.</div></li>
<li id="li-4741"><div class="para" id="p-7076"><span class="process-math">\(\displaystyle R^2=0.99\)</span></div></li>
<li id="li-4742"><div class="para" id="p-7077"><span class="process-math">\(R^2=0.9997\text{.}\)</span></div></li>
<li id="li-4743"><div class="para" id="p-7078"><span class="process-math">\(\displaystyle R^2=1\)</span></div></li>
<li id="li-4744"><div class="para" id="p-7079">
<span class="process-math">\(k=5\)</span> seems like a good choice, and this gives the prediction of <span class="process-math">\(8.7\)</span> million square kilometers of sea ice.</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsec-skl-lm"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.6</span><span class="space"> </span><span class="title">Fitting linear models with standard tools</span>
</h3>
<div class="para" id="p-7080">Coming soon.</div></section><section class="subsection" id="subsection-120"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">6.5.7</span><span class="space"> </span><span class="title">Summary</span>
</h3>
<div class="para logical" id="p-7081">
<div class="para">This section introduced some types of least squares problems and a framework for working with them.</div>
<ul class="disc">
<li id="li-4745"><div class="para" id="p-7082">Given an inconsistent system <span class="process-math">\(A\xvec=\bvec\text{,}\)</span> we find <span class="process-math">\(\xhat\text{,}\)</span> the <dfn class="terminology">least squares approximate solution</dfn> by requiring that <span class="process-math">\(A\xhat\)</span> be as close to <span class="process-math">\(\bvec\)</span> as possible.  In other words, we solve <span class="process-math">\(A\xhat = \bhat\)</span> where <span class="process-math">\(\bhat = \proj{\bvec}{\col(A)}\)</span>
</div></li>
<li id="li-4746"><div class="para logical" id="p-7083">
<div class="para">One important application of this is fitting <dfn class="terminology">linear models</dfn> to data. In that context, we typically use different letters. Instead of <span class="process-math">\(A \xvec = \bvec\text{,}\)</span> you are more likely to see <span class="process-math">\(\yvec = X \betavec\text{.}\)</span> Here</div>
<ol class="decimal">
<li id="li-4747">
<span class="heading"><span class="title"><span class="process-math">\(\yvec\)</span> represents the <dfn class="terminology">response variable</dfn>..</span></span><div class="para" id="p-7084">the variable we are trying to predict or estimate from other available data.</div>
</li>
<li id="li-4748">
<span class="heading"><span class="title"><span class="process-math">\(X\)</span> represents the <dfn class="terminology">data matrix</dfn>..</span></span><div class="para" id="p-7085">Each row of <span class="process-math">\(X\)</span> represents an observation unit. Each column represents a data variable. Often we include a column of 1‚Äôs in <span class="process-math">\(X\text{.}\)</span> This  allows us to model an <dfn class="terminology">intercept</dfn> which represents a baseline amount that is  part of every prediction. <span class="process-math">\(X\)</span> may include the results of applying a function to some of the "raw data", after all, that‚Äôs just another variable.</div>
</li>
<li id="li-4749">
<span class="heading"><span class="title"><span class="process-math">\(\betavec = \begin{bmatrix}\beta_0\\\beta_1\\ \vdots \\ \beta_p \\\end{bmatrix}\)</span> represents the coefficients of the model..</span></span><div class="para" id="p-7086"></div>
</li>
</ol>
</div></li>
<li id="li-4750">
<div class="para logical" id="p-7087">
<div class="para">One way to find <span class="process-math">\(\xhat\)</span> with <span class="process-math">\(A \xhat = \bhat\)</span> is by solving the normal equations</div>
<div class="displaymath process-math">
\begin{equation*}
A^{\transpose}A\xhat = A^{\transpose}\bvec\text{.}
\end{equation*}
</div>
<div class="para">This is not our preferred method since numerical problems can arise.</div>
</div>
<div class="para logical" id="p-7088">
<div class="para">The statistical version of the normal equation is</div>
<div class="displaymath process-math">
\begin{equation*}
X^{\transpose}X\betahat = A^{\transpose}\yvec\text{.}
\end{equation*}
</div>
</div>
</li>
<li id="li-4751">
<div class="para" id="p-7089">A second way to find <span class="process-math">\(\xhat\)</span> with <span class="process-math">\(A \xhat = \bhat\)</span> uses a <span class="process-math">\(QR\)</span> factorization of <span class="process-math">\(A\text{.}\)</span>  If <span class="process-math">\(A=QR\text{,}\)</span> then <span class="process-math">\(\xhat = R^{-1}Q^{\transpose}\bvec\)</span> and finding <span class="process-math">\(R^{-1}\)</span> is computationally feasible since <span class="process-math">\(R\)</span> is upper triangular.  Alternatively, we can use backsubstitution to solve <span class="process-math">\(R \xhat = Q^{\transpose} \bvec\text{.}\)</span>
</div>
<div class="para" id="p-7090">The statistical version of this is <span class="process-math">\(X = QR\)</span> and <span class="process-math">\(R \betahat = Q^{\transpose} \yvec\text{.}\)</span>
</div>
</li>
<li id="li-4752"><div class="para" id="p-7091">This technique may be applied widely and is useful for modeling data. We saw examples in this section where linear functions of several input variables and polynomials provided effective models for different datasets.</div></li>
<li id="li-4753"><div class="para" id="p-7092">A simple measure of the quality of the fit is the coefficient of determination <span class="process-math">\(R^2\)</span> though some care must be used in interpreting this number in context.  In particular, as models become more complex, <span class="process-math">\(R^2\)</span> generally increases because more flexible models can fit the data better. But they may be prone to overfitting. Our goal is generally not to fit the data at hand but to learn something of value about other data.</div></li>
</ul>
</div></section><section class="exercises" id="exercises-29"><h3 class="heading hide-type">
<span class="type">Exercises</span><span class="space"> </span><span class="codenumber">6.5.8</span><span class="space"> </span><span class="title">Exercises</span>
</h3>
<div class="para logical" id="p-7093">
<div class="para">Evaluating the following cell loads in some commands that will be helpful in the following exercises.  In particular, there are commands</div>
<ul class="disc">
<li id="li-4754"><div class="para" id="p-7094">
<code class="code-inline tex2jax_ignore">QR(A)</code> that returns the <span class="process-math">\(QR\)</span> factorization of <code class="code-inline tex2jax_ignore">A</code> as <code class="code-inline tex2jax_ignore">Q, R = QR(A)</code>,</div></li>
<li id="li-4755"><div class="para" id="p-7095">
<code class="code-inline tex2jax_ignore">onesvec(n)</code> that returns the <span class="process-math">\(n\)</span>-dimensional vector whose entries are all 1,</div></li>
<li id="li-4756"><div class="para" id="p-7096">
<code class="code-inline tex2jax_ignore">demean(v)</code> that demeans the vector <code class="code-inline tex2jax_ignore">v</code>,</div></li>
<li id="li-4757"><div class="para" id="p-7097">
<code class="code-inline tex2jax_ignore">vandermonde(x, k)</code> that returns the Vandermonde matrix of degree <span class="process-math">\(k\)</span> formed from the components of the vector <code class="code-inline tex2jax_ignore">x</code>, and</div></li>
<li id="li-4758"><div class="para" id="p-7098">
<code class="code-inline tex2jax_ignore">plot_model(xhat, data)</code> that plots the <code class="code-inline tex2jax_ignore">data</code> and the model <code class="code-inline tex2jax_ignore">xhat</code>.</div></li>
</ul>
<div class="para"><pre class="ptx-sagecell sagecell-sage" id="sage-241"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
</script></pre></div>
</div>
<article class="exercise exercise-like" id="exercise-243"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="para logical" id="p-7099">
<div class="para">Suppose we write the linear system</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{bmatrix}
1 \amp -1 \\
2 \amp -1 \\
-1 \amp 3 
\end{bmatrix}
\xvec = \threevec{-8}5{-10}
\end{equation*}
</div>
<div class="para">as <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-4759"><div class="para" id="p-7100">Find an orthogonal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-4760"><div class="para" id="p-7101">Find <span class="process-math">\(\bhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-4761"><div class="para" id="p-7102">Find a solution to the linear system <span class="process-math">\(A\xvec =
\bhat\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="ex-lst-squares-line"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="para logical" id="p-7111">
<div class="para">Consider the data in <a href="" class="xref" data-knowl="./knowl/table-lst-squares-line.html" title="Table 6.5.13: A data set with four points.">Table¬†6.5.13</a>. <figure class="table table-like" id="table-lst-squares-line"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">6.5.13<span class="period">.</span></span><span class="space"> </span>A data set with four points.</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(x\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(y\)</span></td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">1</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">1</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">3</td>
<td class="c m b0 r0 l0 t0 lines">1</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">4</td>
<td class="c m b0 r0 l0 t0 lines">2</td>
</tr>
</table></div></figure> <pre class="ptx-sagecell sagecell-sage" id="sage-242"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4768"><div class="para" id="p-7112">Set up the linear system <span class="process-math">\(A\xvec=\bvec\)</span> that describes the line <span class="process-math">\(b + mx = y\)</span> passing through these points.</div></li>
<li id="li-4769"><div class="para" id="p-7113">Write the normal equations that describe the least squares approximate solution to <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>
</div></li>
<li id="li-4770"><div class="para" id="p-7114">Find the least squares approximate solution <span class="process-math">\(\xhat\)</span> and plot the data and the resulting line.</div></li>
<li id="li-4771"><div class="para" id="p-7115">What is your predicted <span class="process-math">\(y\)</span>-value when <span class="process-math">\(x=3.5\text{?}\)</span>
</div></li>
<li id="li-4772"><div class="para" id="p-7116">Find the coefficient of determination <span class="process-math">\(R^2\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-245"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="para logical" id="p-7129">
<div class="para">Consider the four points in <a href="" class="xref" data-knowl="./knowl/table-lst-squares-line.html" title="Table 6.5.13: A data set with four points.">Table¬†6.5.13</a>. <pre class="ptx-sagecell sagecell-sage" id="sage-243"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4783"><div class="para logical" id="p-7130">
<div class="para">Set up a linear system <span class="process-math">\(A\xvec = \bvec\)</span> that describes a quadratic function</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0+\beta_1x+\beta_2x^2 = y
\end{equation*}
</div>
<div class="para">passing through the points.</div>
</div></li>
<li id="li-4784"><div class="para" id="p-7131">Use a <span class="process-math">\(QR\)</span> factorization to find the least squares approximate solution <span class="process-math">\(\xhat\)</span> and plot the data and the graph of the resulting quadratic function.</div></li>
<li id="li-4785"><div class="para" id="p-7132">What is your predicted <span class="process-math">\(y\)</span>-value when <span class="process-math">\(x=3.5\text{?}\)</span>
</div></li>
<li id="li-4786"><div class="para" id="p-7133">Find the coefficient of determination <span class="process-math">\(R^2\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-246"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="para logical" id="p-7139">
<div class="para">Consider the data in <a href="" class="xref" data-knowl="./knowl/table-lst-squares-multi.html" title="Table 6.5.14: A simple data set">Table¬†6.5.14</a>. <figure class="table table-like" id="table-lst-squares-multi"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">6.5.14<span class="period">.</span></span><span class="space"> </span>A simple data set</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(x_1\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(x_2\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(y\)</span></td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">4.2</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">3.3</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">5.9</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">5.1</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">3</td>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">7.5</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">3</td>
<td class="c m b0 r0 l0 t0 lines">3</td>
<td class="c m b0 r0 l0 t0 lines">6.3</td>
</tr>
</table></div></figure> <pre class="ptx-sagecell sagecell-sage" id="sage-244"><script type="text/x-sage">
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4791"><div class="para logical" id="p-7140">
<div class="para">Set up a linear system <span class="process-math">\(A\xvec = \bvec\)</span> that describes the relationship</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 + \beta_1 x_1 + \beta_2 x_2 = y.
\end{equation*}
</div>
</div></li>
<li id="li-4792"><div class="para" id="p-7141">Find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>
</div></li>
<li id="li-4793"><div class="para" id="p-7142">What is your predicted <span class="process-math">\(y\)</span>-value when <span class="process-math">\(x_1 =
2.4\)</span> and <span class="process-math">\(x_2=2.9\text{?}\)</span>
</div></li>
<li id="li-4794"><div class="para" id="p-7143">Find the coefficient of determination <span class="process-math">\(R^2\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-247"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="para logical" id="p-7154">
<div class="para">Determine whether the following statements are true or false and explain your thinking.</div>
<ol class="lower-alpha">
<li id="li-4803"><div class="para" id="p-7155">If <span class="process-math">\(A\xvec=\bvec\)</span> is consistent, then <span class="process-math">\(\xhat\)</span> is a solution to <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>
</div></li>
<li id="li-4804"><div class="para" id="p-7156">If <span class="process-math">\(R^2=1\text{,}\)</span> then the least squares approximate solution <span class="process-math">\(\xhat\)</span> is also a solution to the original equation <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>
</div></li>
<li id="li-4805"><div class="para" id="p-7157">Given the <span class="process-math">\(QR\)</span> factorization <span class="process-math">\(A=QR\text{,}\)</span> we have <span class="process-math">\(A\xhat=Q^{\transpose}Q\bvec\text{.}\)</span>
</div></li>
<li id="li-4806"><div class="para" id="p-7158">A <span class="process-math">\(QR\)</span> factorization provides a method for finding the approximate least squares solution to <span class="process-math">\(A\xvec=\bvec\)</span> that is more reliable than solving the normal equations.</div></li>
<li id="li-4807"><div class="para" id="p-7159">A solution to <span class="process-math">\(AA^{\transpose}\xvec = A\bvec\)</span> is the least squares approximate solution to <span class="process-math">\(A\xvec = \bvec\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-248"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="para logical" id="p-7172">
<div class="para">Explain your response to the following questions.</div>
<ol class="lower-alpha">
<li id="li-4818"><div class="para" id="p-7173">If <span class="process-math">\(\xhat=\zerovec\text{,}\)</span> what does this say about the vector <span class="process-math">\(\bvec\text{?}\)</span>
</div></li>
<li id="li-4819"><div class="para" id="p-7174">If the columns of <span class="process-math">\(A\)</span> are orthonormal, how can you easily find the least squares approximate solution to <span class="process-math">\(A\xvec=\bvec\text{?}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-249"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="para logical" id="p-7181">
<div class="para">The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors <code class="code-inline tex2jax_ignore">year</code>, which records the years in the data set, and <code class="code-inline tex2jax_ignore">people</code>, which records the number of people. <pre class="ptx-sagecell sagecell-sage" id="sage-245"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/bangladesh.csv')
data = [vector(row) for row in df.values]
year = vector(df['Year'])
people = vector(df['People'])
print(df)
list_plot(data, size=40, color='blue')
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4824"><div class="para logical" id="p-7182">
<div class="para">Suppose we want to write</div>
<div class="displaymath process-math">
\begin{equation*}
N = \beta_0 + \beta_1 t
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(t\)</span> is the year and <span class="process-math">\(N\)</span> is the number of people. Construct the matrix <span class="process-math">\(A\)</span> and vector <span class="process-math">\(\bvec\)</span> so that the linear system <span class="process-math">\(A\xvec=\bvec\)</span> describes the vector <span class="process-math">\(\xvec=\twovec{\beta_0}{\beta_1}\text{.}\)</span>
</div>
</div></li>
<li id="li-4825"><div class="para" id="p-7183">Using a <span class="process-math">\(QR\)</span> factorization of <span class="process-math">\(A\text{,}\)</span> find the values of <span class="process-math">\(\beta_0\)</span> and <span class="process-math">\(\beta_1\)</span> in the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>
</div></li>
<li id="li-4826"><div class="para" id="p-7184">What is the coefficient of determination <span class="process-math">\(R^2\)</span> and what does this tell us about the quality of the approximation?</div></li>
<li id="li-4827"><div class="para" id="p-7185">What is your prediction for the number of people living without electricity in 1985?</div></li>
<li id="li-4828"><div class="para" id="p-7186">Estimate the year in which there will be no people living without electricity.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-250"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="para" id="p-7199">This problem concerns a data set describing planets in our Solar system. For each planet, we have the length <span class="process-math">\(L\)</span> of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period <span class="process-math">\(P\text{,}\)</span> the length of time in years required to complete one orbit around the Sun.</div> <div class="para logical" id="p-7200">
<div class="para">We would like to model this data using the function <span class="process-math">\(P = CL^r\)</span> where <span class="process-math">\(C\)</span> and <span class="process-math">\(r\)</span> are parameters we need to determine.  Since this isn‚Äôt a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain</div>
<div class="displaymath process-math">
\begin{equation*}
\ln(P) = \ln(C) + r\ln(L).
\end{equation*}
</div>
</div> <div class="para logical" id="p-7201">
<div class="para">Evaluating the following cell loads the data set and defines two vectors <code class="code-inline tex2jax_ignore">logaxis</code>, whose components are <span class="process-math">\(\ln(L)\text{,}\)</span> and <code class="code-inline tex2jax_ignore">logperiod</code>, whose components are <span class="process-math">\(\ln(P)\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-246"><script type="text/x-sage">import numpy as np	    
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/planets.csv',index_col=0)
logaxis = vector(np.log(df['Semi-major axis']))
logperiod = vector(np.log(df['Period']))
print(df)
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4839"><div class="para" id="p-7202">Construct the matrix <span class="process-math">\(A\)</span> and vector <span class="process-math">\(\bvec\)</span> so that the solution to <span class="process-math">\(A\xvec=\bvec\)</span> is the vector <span class="process-math">\(\xvec=\ctwovec{\ln(C)}r\text{.}\)</span>
</div></li>
<li id="li-4840"><div class="para" id="p-7203">Find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>  What does this give for the values of <span class="process-math">\(C\)</span> and <span class="process-math">\(r\text{?}\)</span>
</div></li>
<li id="li-4841"><div class="para" id="p-7204">Find the coefficient of determination <span class="process-math">\(R^2\text{.}\)</span>  What does this tell us about the quality of the approximation?</div></li>
<li id="li-4842"><div class="para" id="p-derived-li-4842">Suppose that the orbit of an asteroid has a semi-major axis whose length is <span class="process-math">\(L=4.0\)</span> AU.  Estimate the period <span class="process-math">\(P\)</span> of the asteroid‚Äôs orbit.</div></li>
<li id="li-4843"><div class="para" id="p-7205">Halley‚Äôs Comet has a period of <span class="process-math">\(P=75\)</span> years. Estimate the length of its semi-major axis.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-251"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<div class="para logical" id="p-7218">
<div class="para">Evaluating the following cell loads a data set describing the temperature in the Earth‚Äôs atmosphere at various altitudes. There are also two vectors <code class="code-inline tex2jax_ignore">altitude</code>, expressed in kilometers, and <code class="code-inline tex2jax_ignore">temperature</code>, in degrees Celsius. <pre class="ptx-sagecell sagecell-sage" id="sage-247"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/altitude-temps.csv')
data = [vector(row) for row in df.values]
altitude = vector(df['Altitude'])
temperature = vector(df['Temperature'])
print(df)
list_plot(data, size=40, color='blue')
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-4854"><div class="para" id="p-7219">Describe how to form the matrix <span class="process-math">\(A\)</span> and vector <span class="process-math">\(\bvec\)</span> so that the linear system <span class="process-math">\(A\xvec=\bvec\)</span> describes a degree <span class="process-math">\(k\)</span> polynomial fitting the data.</div></li>
<li id="li-4855"><div class="para" id="p-7220">After choosing a value of <span class="process-math">\(k\text{,}\)</span> construct the matrix <span class="process-math">\(A\)</span> and vector <span class="process-math">\(\bvec\text{,}\)</span> and find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>
</div></li>
<li id="li-4856"><div class="para" id="p-7221">Plot the polynomial and data using <code class="code-inline tex2jax_ignore">plot_model(xhat, data)</code>.</div></li>
<li id="li-4857"><div class="para" id="p-7222">Now examine what happens as you vary the degree of the polynomial <span class="process-math">\(k\text{.}\)</span>  Choose an appropriate value of <span class="process-math">\(k\)</span> that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.</div></li>
<li id="li-4858"><div class="para" id="p-7223">Use your value of <span class="process-math">\(k\)</span> to estimate the temperature at an altitude of 55 kilometers.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-252"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<div class="para logical" id="p-7236">
<div class="para">The following cell loads some data describing 1057 houses in a particular real estate market.  For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars.  The cell also defines variables <code class="code-inline tex2jax_ignore">area</code>, <code class="code-inline tex2jax_ignore">size</code>, <code class="code-inline tex2jax_ignore">age</code>, and <code class="code-inline tex2jax_ignore">price</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-248"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv',index_col=0)
df = df.fillna(df.mean())
area = vector(df['Living.Area'])
size = vector(df['Lot.Size'])
age = vector(df['Age'])
price = vector(df['Price'])
df
</script></pre> We will use linear regression to predict the price of a house given its living area, lot size, and age:</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 + \beta_1~\text{Living Area} +
\beta_2~\text{Lot Size} + \beta_3~\text{Age} = \text{Price}.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-4869"><div class="para" id="p-7237">Use a <span class="process-math">\(QR\)</span> factorization to find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>
</div></li>
<li id="li-4870"><div class="para" id="p-7238">Discuss the significance of the signs of <span class="process-math">\(\beta_1\text{,}\)</span> <span class="process-math">\(\beta_2\text{,}\)</span> and <span class="process-math">\(\beta_3\text{.}\)</span>
</div></li>
<li id="li-4871"><div class="para" id="p-7239">If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?</div></li>
<li id="li-4872"><div class="para" id="p-7240">Find the coefficient of determination <span class="process-math">\(R^2\text{.}\)</span>  What does this say about the quality of the fit?</div></li>
<li id="li-4873"><div class="para" id="p-7241">Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-253"><h4 class="heading"><span class="codenumber">11<span class="period">.</span></span></h4>
<div class="para" id="p-7254">We observed that if the columns of <span class="process-math">\(A\)</span> are linearly independent, then there is a unique least squares approximate solution to the equation <span class="process-math">\(A\xvec=\bvec\)</span> because the equation <span class="process-math">\(A\xhat=\bhat\)</span> has a unique solution.  We also said that <span class="process-math">\(\xhat\)</span> is the unique solution to the normal equation <span class="process-math">\(A^{\transpose}A\xhat = A^{\transpose}\bvec\)</span> without explaining why this equation has a unique solution.  This exercise offers an explanation.</div> <div class="para logical" id="p-7255">
<div class="para">Assuming that the columns of <span class="process-math">\(A\)</span> are linearly independent, we would like to conclude that the equation <span class="process-math">\(A^{\transpose}A\xhat=A^{\transpose}\bvec\)</span> has a unique solution.</div>
<ol class="lower-alpha">
<li id="li-4884"><div class="para logical" id="p-7256">
<div class="para">Suppose that <span class="process-math">\(\xvec\)</span> is a vector for which <span class="process-math">\(A^{\transpose}A\xvec = \zerovec\text{.}\)</span>  Explain why the following argument is valid and allows us to conclude that <span class="process-math">\(A\xvec = \zerovec\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
A^{\transpose}A\xvec \amp = \zerovec \\
\xvec\cdot A^{\transpose}A\xvec \amp = \xvec\cdot\zerovec = 0 \\
(A\xvec)\cdot(A\xvec) \amp = 0 \\
\len{A\xvec}^2 \amp = 0. \\
\end{aligned}
\end{equation*}
</div>
<div class="para">In other words, if <span class="process-math">\(A^{\transpose}A\xvec = \zerovec\text{,}\)</span> we know that <span class="process-math">\(A\xvec = \zerovec\text{.}\)</span>
</div>
</div></li>
<li id="li-4885"><div class="para" id="p-7257">If the columns of <span class="process-math">\(A\)</span> are linearly independent and <span class="process-math">\(A\xvec = \zerovec\text{,}\)</span> what do we know about the vector <span class="process-math">\(\xvec\text{?}\)</span>
</div></li>
<li id="li-4886"><div class="para" id="p-7258">Explain why <span class="process-math">\(A^{\transpose}A\xvec = \zerovec\)</span> can only happen when <span class="process-math">\(\xvec = \zerovec\text{.}\)</span>
</div></li>
<li id="li-4887"><div class="para" id="p-7259">Assuming that the columns of <span class="process-math">\(A\)</span> are linearly independent, explain why <span class="process-math">\(A^{\transpose}A\xhat=A^{\transpose}\bvec\)</span> has a unique solution.</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="ex-r2-meaning"><h4 class="heading"><span class="codenumber">12<span class="period">.</span></span></h4>
<div class="para" id="p-7270">This problem is about the meaning of the coefficient of determination <span class="process-math">\(R^2\)</span> and its connection to variance, a topic that appears in the next section.  Throughout this problem, we consider the linear system <span class="process-math">\(A\xvec=\bvec\)</span> and the approximate least squares solution <span class="process-math">\(\xhat\text{,}\)</span> where <span class="process-math">\(A\xhat=\bhat\text{.}\)</span>  We suppose that <span class="process-math">\(A\)</span> is an <span class="process-math">\(m\by n\)</span> matrix, and we will denote the <span class="process-math">\(m\)</span>-dimensional vector <span class="process-math">\(\onevec =
\fourvec11{\vdots}1\text{.}\)</span>
</div> <div class="para logical" id="p-7271"><ol class="lower-alpha">
<li id="li-4896"><div class="para logical" id="p-7272">
<div class="para">Explain why <span class="process-math">\(\bbar\text{,}\)</span> the mean of the components of <span class="process-math">\(\bvec\text{,}\)</span> can be found as the dot product</div>
<div class="displaymath process-math">
\begin{equation*}
\bbar = \frac 1m \bvec\cdot\onevec.
\end{equation*}
</div>
</div></li>
<li id="li-4897"><div class="para" id="p-7273">In the examples we have seen in this section, explain why <span class="process-math">\(\onevec\)</span> is in <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-4898"><div class="para logical" id="p-7274">
<div class="para">If we write <span class="process-math">\(\bvec = \bhat + \bvec^\perp\text{,}\)</span> explain why</div>
<div class="displaymath process-math">
\begin{equation*}
\bvec^\perp\cdot\onevec = 0
\end{equation*}
</div>
<div class="para">and hence why the mean of the components of <span class="process-math">\(\bvec^\perp\)</span> is zero.</div>
</div></li>
<li id="li-4899">
<div class="para" id="p-7275">The variance of an <span class="process-math">\(m\)</span>-dimensional vector <span class="process-math">\(\vvec\)</span> is <span class="process-math">\(\var(\vvec) = \frac1m \len{\widetilde{\vvec}}^2\text{,}\)</span> where <span class="process-math">\(\widetilde{\vvec}\)</span> is the vector obtained by demeaning <span class="process-math">\(\vvec\text{.}\)</span>
</div>
<div class="para logical" id="p-7276">
<div class="para">Explain why</div>
<div class="displaymath process-math">
\begin{equation*}
\var(\bvec) = \var(\bhat) + \var(\bvec^\perp).
\end{equation*}
</div>
</div>
</li>
<li id="li-4900">
<div class="para logical" id="p-7277">
<div class="para">Explain why</div>
<div class="displaymath process-math">
\begin{equation*}
\frac{\len{\bvec - A\xhat}^2}{\len{\widetilde{\bvec}}^2}
= \frac{\var(\bvec^\perp)}{\var(\bvec)}
\end{equation*}
</div>
<div class="para">and hence</div>
<div class="displaymath process-math">
\begin{equation*}
R^2 = \frac{\var(\bhat)}{\var(\bvec)} =
\frac{\var(A\xhat)}{\var(\bvec)}.
\end{equation*}
</div>
</div>
<div class="para" id="p-7278">These expressions indicate why it is sometimes said that <span class="process-math">\(R^2\)</span> measures the ‚Äúfraction of variance explained‚Äù by the function we are using to fit the data.  As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.</div>
</li>
<li id="li-4901"><div class="para" id="p-7279">Explain why <span class="process-math">\(0\leq R^2 \leq 1\text{.}\)</span>
</div></li>
</ol></div></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-5"><div class="fn"><code class="code-inline tex2jax_ignore">sustainabilitymath.org</code></div></div>
</div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-gram-schmidt.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon">^</span><span class="name">Top</span></a><a class="next-button button" href="chap7.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
