<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-python-introduction"
	 xmlns:xi="http://www.w3.org/2001/XInclude">

  <title>
    Computational Linear Algebra 
  </title>

  <introduction>
    <p>
      Linear algebra owes its prominence as a powerful scientific tool
      to the ever-growing power of computers.  Carl Cowen, a former
      president of the Mathematical Association of America, has said,
      <q> No serious application of linear algebra happens without a
      computer.</q> Indeed, Cowen notes that, in the 1950s, working
      with a system of 100 equations in 100 variables was difficult.
      Today, scientists (including data scientists) routinely work on problems
      that are vastly larger.  This is only possible because of today's
      computing power.
    </p>

    <p>
		For learning the principles of linear algebra, small examples
		in 2 or 3 dimensions are useful because (a) we can visualize  
		things geometrically, and (b) we can see all the numbers and 
		computations involved.  But as we build our understanding and intuition 
		with these small examples, we also want to be learning how to 
		do linear algebra computationally so that we can work with much 
		larger examples.
    </p>

	<p>
	There are many computational tools that could be used.  One of them, Matlab
	(and its open source analog, Octave), was specifically designed (and named) with 
	matrix algebra as its primary use case.  Linear algebra is important for statistics 
	and machine learning, but so are other things, like data wrangling and visualing data 
	and models.  For this reason, langauges like R and Python are much more important in 
	the data science community.  
	Although linear algebra is used 
	extensively in both languages, end users of Python are more likely to manipulate data 
	in vector and matrix format than are users of R, where this is often hidden behind 
	an additional layer of abstraction.
	For this reason, we'll focus primarily on Python in this text, with some 
	references to how things differ in R thrown in along the way. 
	</p>
  </introduction>
    

  <subsection>
    <title> Reduced row echelon form in Python </title>

    <p>
      When we encounter a matrix, <xref
      ref="thm-rref-is-unique" /> tells us that there 
      is exactly one reduced row echelon matrix that is row equivalent
      to it.
    </p>
    
    <p>
      In fact, the uniqueness of this reduced row echelon matrix is
      what motivates us to define this particular form.  When solving
      a system of linear equations using Gaussian elimination, there
      are other row equivalent matrices that reveal the structure of
      the solution space.  The reduced row echelon matrix is simply a
      convenience as it is an agreement we make with one another to
      seek the same matrix.
    </p>

    <p>
      An added benefit is that we can ask a computer 
      to find reduced row echelon matrices for us.  We will
      learn how to do this now that we have a little familiarity with
      Python.
    </p>


	<note>
	  <title>Augmented matrices in numpy</title>
    <p>
	Notice that in <c>numpy</c> there is no way of
    specifying whether a matrix is an augmented matrix or a coefficient matrix,
	so it will be up to us to interpret our results appropriately.
    </p>
	</note>

    <assemblage>
      <title> Python syntax </title>
      <p> Some common mistakes when entering a matrix in Python include 
      <ul>
	<li> <p>
          forgetting the square brackets around the list of entries,
	</p></li>
	<li> <p>
	  forgetting an entry from the list or to add an extra one,
	</p></li>
	<li> <p>
	  forgetting to separate the rows, and entries within a row with commas, 
	  and
	</p></li>
	<li> <p>
	  forgetting the closing parenthesis (because you are so worried about getting the 
	  correct number of square brackets, perhaps). 
	</p></li>
      </ul>
      If you see an error message, carefully proofread your input and
      try again.
      </p>
    </assemblage>

    
    <activity>
      <title> Using Python to find row reduced echelon matrices </title>

      <statement>
	<p>
	  <ol marker="a.">
	    <li xml:id = "li-example-rref">
	      <p>
		Enter the following matrix into Python.
		<me>
		  \left[
		  \begin{array}{rrrr}
		  -1 \amp -2 \amp 2 \amp -1 \\
		  2 \amp 4 \amp -1 \amp 5 \\
		  1 \amp 2 \amp 0 \amp 3
		  \end{array}
		  \right]
		</me>
		<sage language="python">
		  <input> 
		  </input>
		</sage>
		
	      </p>
	    </li>
	    
	    <li>
	      <p> Give the matrix the name <m>A</m> by entering
	      <cd>
A = np.array([ ... ])
	      </cd>
		  </p>
			
		  <p>
		  <c>numpy</c> does not have a built-in method for computing the 
		  reduced row echelon form of a matrix, so we will use another package,
		  <c>sympy</c>.  
	      We may then find its reduced row echelon form by
	      entering 
	      <cd>
import sympy as sp
A = np.array([ ... ])
sp.Matrix(A).rref()
	      </cd>
		  Note that we must first convert our <c>numpy</c> array into a 
		  <c>sympy</c> matrix.
	      A common mistake is to forget the parentheses after
	      <c>rref</c>.
	      </p>
	      
	      <p> Use Python to find the reduced row echelon form of the
	      matrix from <xref ref="li-example-rref" /> of this activity.
	      <sage language="python">
		<input>
		</input>
	      </sage>
	      </p>
	    </li>

		<li>
			<p>
				Notice that <c>rref()</c> returns a tuple with two elements.
				The first element is the reduced row echelon form of our matrix.
				See if you can figure out what the second element is.
			</p>
		</li>
	    
	    <li xml:id = "li-rref-solve">
	      <p> Use Python to describe the
	      solution space of the system 
	      of linear equations
	      <me>
		\begin{alignedat}{5}
		-x_1 \amp  \amp  \amp  \amp  \amp  {}+{}  \amp 2x_4 \amp
		{}={}  \amp 4 \\ 
		\amp  \amp 3x_2 \amp  {}+{}  \amp x_3 \amp  {}+{}  \amp 2x_4
		\amp  {}={}  \amp 3 \\ 
		4x_1 \amp  {}-{}  \amp 3x_2 \amp  \amp  \amp  {}+{}  \amp
		x_4 \amp  {}={}  \amp 14 \\ 
		\amp  \amp 2x_2 \amp  {}+{}  \amp 2x_3 \amp  {}+{}  \amp x_4
		\amp  {}={}  \amp 1 \\ 
		\end{alignedat}
	      </me>
	      <sage language="python">
		<input>
		</input>
	      </sage>
	      </p>
	    </li>
	    <li xml:id = "li-augmentation-principle">
	      <p> Consider the two matrices:
	      <me>
		\begin{array}{rcl}
		A \amp = \amp \left[
		\begin{array}{rrrr}
		1 \amp -2 \amp 1 \amp -3 \\
		-2 \amp 4 \amp 1 \amp 1 \\
		-4 \amp 8 \amp -1 \amp 7 \\
		\end{array}\right] \\
		B \amp = \amp \left[
		\begin{array}{rrrrrr}
		1 \amp -2 \amp 1 \amp -3 \amp 0 \amp 3 \\
		-2 \amp 4 \amp 1 \amp 1 \amp 1 \amp -1 \\
		-4 \amp 8 \amp -1 \amp 7 \amp 3 \amp 2 \\
		\end{array}\right] \\
		\end{array}
	      </me>
	      We say that <m>B</m> is an <term>augmentation</term> of <m>A</m>
	      because it is obtained from <m>A</m> by adding some more
	      columns.  
	      </p>
	      <p> Using Python, define the matrices and compare their reduced
	      row echelon forms.  What do you notice about the relationship
	      between the two reduced row echelon forms?
	      </p>
	      <p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>
	    
	    <li>
	      <p> Using the system of equations in <xref
	      ref="li-rref-solve" />, write the
	      augmented matrix corresponding to the system of equations.
	      What did you find for the
	      reduced row echelon form of the augmented matrix?
	      </p>
	      
	      <p> Now write the coefficient matrix of this system of
	      equations.  What does <xref ref="li-augmentation-principle" />
	      of this activity tell you about its reduced row echelon form?
	    </p> </li>
	  </ol>
	</p>
      </statement>
      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<cd>
np.array([[-1,-2, 2,-1],
          [ 2, 4,-1, 5],
          [ 1, 2, 0, 3]])
		</cd>
	      </p>
	    </li>
	    <li>
	      <p>
		The reduced row echelon form of the matrix is
		<me>
		  \begin{bmatrix}
		  1 \amp 2 \amp 0 \amp 3 \\
		  0 \amp 0 \amp 1 \amp 1 \\
		  0 \amp 0 \amp 0 \amp 0
		  \end{bmatrix}.
		</me>
	      </p>
	    </li>

	    <li>
	      <p>
		Python tells us that the reduced row echelon form of the
		corresponding augmented matrix is
		<me>
		  \begin{bmatrix}
		  1 \amp 0 \amp 0 \amp 0 \amp -2 \\
		  0 \amp 1 \amp 0 \amp 0 \amp -1 \\
		  0 \amp 0 \amp 1 \amp 0 \amp 0 \\
		  0 \amp 0 \amp 0 \amp 1 \amp 3 \\
		  \end{bmatrix}
		</me>
		so there is a unique solution <m>(x_1,x_2,x_3,x_4) =
		(-2,-1,0,3)</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The first four columns of the reduced row echelon form
		of <m>B</m> form the reduced row echelon form of
		<m>A</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The reduced row echelon form of the coefficient matrix
		is 
		<me>
		  \begin{bmatrix}
		  1 \amp 0 \amp 0 \amp 0 \\
		  0 \amp 1 \amp 0 \amp 0 \\
		  0 \amp 0 \amp 1 \amp 0 \\
		  0 \amp 0 \amp 0 \amp 1 \\
		  \end{bmatrix}.
		</me>
		This can be computed with 
		<sage language="python">
			<input>
import numpy as np
import sympy as sp
A = np.array([[-1, -2, 2, -1], [2, 4, -1, 5], [1, 2, 0, 3]])
print(A)
print(sp.Matrix(A).rref())
			</input>
			<output>
				(for static output)
			</output>
		</sage>
		
	      </p>
	    </li>
		<li>
			<p>
				<c>(0,2)</c> tells us that the leading 1 in the row 0 is in column 0 and that 
				the leading 1 in row 1 is in column 2.  This uses 0-based indexing.  We might 
				more naturally say that the leading 1 in the first row is in the first column 
				and the leading 1 in the second row is in the third column.
			</p>
		</li>
	  </ol>
	</p>
      </solution>
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<cd>
np.array([[-1,-2, 2,-1],
          [ 2, 4,-1, 5],
          [ 1, 2, 0, 3]])
		</cd>
	      </p>
	    </li>
	    <li>
	      <p>
		The reduced row echelon form of the matrix is
		<me>
		  \begin{bmatrix}
		  1 \amp 2 \amp 0 \amp 3 \\
		  0 \amp 0 \amp 1 \amp 1 \\
		  0 \amp 0 \amp 0 \amp 0
		  \end{bmatrix}.
		</me>
	      </p>
	    </li>

	    <li>
	      <p>
		There is a unique solution <m>(x_1,x_2,x_3,x_4) =
		(-2,-1,0,3)</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The first four columns of the reduced row echelon form
		of <m>B</m> form the reduced row echelon form of
		<m>A</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The reduced row echelon form of the coefficient matrix
		is 
		<me>
		  \begin{bmatrix}
		  1 \amp 0 \amp 0 \amp 0 \\
		  0 \amp 1 \amp 0 \amp 0 \\
		  0 \amp 0 \amp 1 \amp 0 \\
		  0 \amp 0 \amp 0 \amp 1 \\
		  \end{bmatrix}.
		</me>
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>


    <p> The last part of the previous activity, <xref
    ref="li-augmentation-principle" />, demonstrates something that
    will be helpful for us in the future.  In that activity, we
    started with a matrix <m>A</m>, which we augmented by adding some
    columns to obtain a matrix <m>B</m>.  We then noticed that the
    reduced row echelon form of <m>B</m> is itself an augmentation of
    the reduced row echelon form of <m>A</m>.
    </p>

    <p>
      To illustrate, we can consider the reduced row echelon form of
      the augmented matrix:
      <me>
	\left[
	\begin{array}{ccc|c}
	-2 \amp 3 \amp 0 \amp 2 \\
	-1 \amp 4 \amp 1 \amp 3 \\
	3 \amp 0 \amp 2 \amp 2 \\
	1 \amp 5 \amp 3 \amp 7 \\
	\end{array}
	\right]
	\sim
	\left[
	\begin{array}{ccc|c}
	1 \amp 0 \amp 0 \amp -4 \\
	0 \amp 1 \amp 0 \amp -2 \\
	0 \amp 0 \amp 1 \amp 7 \\
	0 \amp 0 \amp 0 \amp 0 \\
	\end{array}
	\right]
      </me>
    </p>

    <p>
      We can then determine the reduced row echelon form of the
      coefficient matrix by looking inside the augmented matrix.
      <me>
	\left[
	\begin{array}{ccc}
	-2 \amp 3 \amp 0 \\
	-1 \amp 4 \amp 1 \\
	3 \amp 0 \amp 2 \\
	1 \amp 5 \amp 3 \\
	\end{array}
	\right]
	\sim
	\left[
	\begin{array}{ccc}
	1 \amp 0 \amp 0 \\
	0 \amp 1 \amp 0 \\
	0 \amp 0 \amp 1 \\
	0 \amp 0 \amp 0 \\
	\end{array}
	\right]
      </me>
    </p>

    <p> If we trace through the steps in the Gaussian elimination
    algorithm carefully, we see that this is a general principle, which
    we now state.
    </p>

    <proposition xml:id="principle-augmentation-principle">
      <title> Augmentation Principle </title>

      <p> If matrix <m>B</m> is an augmentation of matrix <m>A</m>,
      then the reduced row echelon form of <m>B</m> is an augmentation
      of the reduced row echelon form of <m>A</m>.
      </p>
    </proposition>

  </subsection>

  <subsection xml:id="subsec-compute-effort">
    <title> Computational effort </title>
    <p>
      At the beginning of this section, we indicated that linear
      algebra has become more prominent as computers have grown
      more powerful.  Computers, however, still have limits.  Let's
      consider how much effort is expended when we ask to find the
      reduced row echelon form of a matrix.  We will measure, very
      roughly, the effort by the number of times the algorithm
      requires us to multiply or add two numbers.
    </p>

    <p>
      We will assume that our matrix has the same number of rows
      as columns, which we call <m>n</m>.  We are mainly interested in
      the case when <m>n</m> is very large, which is when we need to
      worry about how
      much effort is required.
    </p>

    <p> Let's first consider the effort required for each of our row 
    operations. 
      <ul>
	<li> <p> Scaling a row multiplies each of the <m>n</m> entries
	in a row by some number, which requires <m>n</m>
	operations. </p></li>
	<li> <p> Interchanging two rows requires no multiplications or
	additions so we won't worry about the effort required by an
	interchange.  </p></li>
	<li> <p> A replacement requires us to multiply
	each entry in a row by some number, which takes <m>n</m>
	operations, and then add the resulting entries to another row,
	which requires another <m>n</m> operations.  The total number
	of operations is <m>2n</m>.</p></li>
      </ul>
    </p>

    <p>
      Our goal is to transform a matrix to its reduced row echelon
      form, which looks something like this:
      <me>
	\left[
	\begin{array}{cccc}
	1 \amp 0 \amp \ldots \amp 0 \\
	0 \amp 1 \amp \ldots \amp 0 \\
	\vdots \amp \vdots \amp \ddots \amp 0 \\
	0 \amp 0 \amp \ldots \amp 1 \\
	\end{array}
	\right]
	</me>.
	We roughly perform one replacement operation for every 0 entry
	in the reduced row echelon matrix.  When <m>n</m> is very
	large, most of the <m>n^2</m> entries in the reduced row
	echelon form are 0 so we need roughly <m>n^2</m>
	replacements.  Since each replacement operation
	requires <m>2n</m> operations, the number of operations
	resulting from the needed replacements is roughly <m>n^2(2n) =
	2n^3</m>.
    </p>
    
    <p> Each row is scaled roughly one time so there are roughly
    <m>n</m> scaling operations, each of which requires <m>n</m>
    operations.  The number of operations due to scaling is roughly
    <m>n^2</m>.
    </p>

    <p>
      Therefore, the total number of operations is roughly
      <me> 2n^3 + n^2</me>.  When <m>n</m> is very large, the
      <m>n^2</m> term is much smaller than the <m>n^3</m> term.  We
      therefore state that
    </p>

    <observation>
      <statement>
	<p>
	The number of operations required to find the reduced row
	echelon form of an <m>n\times n</m> matrix is roughly
	proportional to <m>n^3</m>.
	</p>
      </statement>
    </observation>

    <p>
      This is a very rough measure of the effort required to find the
      reduced row echelon form;  a more careful accounting shows that
      the number of arithmetic operations is roughly <m>\frac23
      n^3</m>.  As we have seen, some matrices 
      require more effort than others, but the upshot of this
      observation is that the effort is proportional to <m>n^3</m>.
      We can think of this in the following way: If the size of the
      matrix grows by a factor of 10, then the effort required
      grows by a factor of <m>10^3 = 1000</m>.
    </p>

    <p>
      While today's computers are powerful, they cannot handle every
      problem we might ask of them.  Eventually, we would like to be
      able to consider matrices that have <m>n=10^{12}</m> (a
      trillion) rows and columns.  In very broad terms, the effort
      required to find the reduced row echelon matrix will require
      roughly <m>(10^{12})^3 = 10^{36}</m> operations.
    </p>

    <p>
      To put this into context, imagine we need to solve a linear
      system with a trillion equations and a trillion variables and
      that we have a computer that can perform a trillion,
      <m>10^{12}</m>, operations every second.  Finding the reduced
      row echelon form would take about <m>10^{16}</m> years.  At this
      time, the universe is estimated to be approximately
      <m>10^{10}</m> years old.  If we had started the calculation 
      <m>10^{10}</m> years ago, then we'd be about one-millionth of
      the way through.  
    </p>

    <p> This may seem like an absurd situation, but
      we'll see in <xref ref="subsec-google" /> how we use the results
      of such a computation every day.
      Clearly, we will need some better tools to deal with
      <em>really</em> big problems like this one.
    </p>
	
  </subsection>

  <subsection>
    <title> Summary </title>

    <p> We learned some basic more features of Python with an emphasis on
    finding the reduced row echelon form of a matrix.
    <ul>
      <!-- <li> <p> Python can perform basic arithmetic using standard
      operators.  Python can also save results from one command to be
      reused in a later command.
      </p></li> -->

      <li> <p> Our preference is to use <c>numpy.array()</c> to create matrices.
      </p></li>
	  <p>
	  We can find the reduced row echelon form using the <c>rref()</c> method
	  <strong><em>after converting to a <c>sympy.Matrix</c></em></strong>.  The
	  output is a tuple of length 2.  The first element is the reduced row
	  echelon form of the matrix and the second is a tuple identifying the
	  location of the pivots -- the topic of the next section.
      </p>
      
      <li> <p> We saw an example of the
      <xref ref="principle-augmentation-principle" text="custom">
	Augmentation Principle
      </xref>, which we then stated as a general principle. 
      </p></li>

      <li> <p> We saw that the computational effort required to find
      the reduced row echelon form of an <m>n\times n</m> matrix is
      proportional to <m>n^3</m>.
      </p></li>
    </ul>
    <xref ref="app-python-reference" text="custom"> Appendix A</xref>
    contains a reference outlining the Python commands that we have
    encountered. 
    </p>
  </subsection>

  <xi:include href="exercises/exercises1-3.ptx" />

  <xi:include href="exercises/exercises2-2.ptx" />
  
</section>
