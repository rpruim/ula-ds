<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-matrix-inverse"
	 xmlns:xi="http://www.w3.org/2001/XInclude">

  <title>
    Invertibility
  </title>

  <introduction>
    <p> Up to this point, we have used the Gaussian elimination
    algorithm to find solutions to linear systems, which is equivalent 
	to solving matrix equations of the form <m>A \xvec = \bvec</m>, where  
	<m>\bvec</m> is a vector of constants, <m>A</m> a matrix of coefficients, 
	and <m>\xvec</m> a vector of variables, or unknowns. 
	We now investigate another way to find solutions to the 
    equation <m>A\xvec=\bvec</m> when the matrix <m>A</m> is square.
	To get started, let's look at some familiar examples.
    </p>
    
    <exploration>
      <statement>
      <p>
	<ol marker="a.">
	  <li><p> Explain how you would solve the equation <m>3x =
	  5</m> using multiplication rather than
	  division. </p></li>

	  <li><p> Find the <m>2\by2</m> matrix <m>A</m> that
	  rotates vectors counterclockwise by <m>90^\circ</m>. 
	  <!-- That is, for any vector <m>\xvec</m>,
	  <m>A \xvec</m> is the vector the results from rotating 
	  <m>\xvec</m> <m>90^\circ</m> counterclockwise.  
	  For example, 
	  <m>A \twovec 10 = \twovec{0}{1}</m>,
	  <m>A \twovec 01 = \twovec{-1}{0}</m> and 
	  <m>A \twovec 11 = \twovec{-1}{1}</m>. -->
	  </p></li>

	  <li><p> Find the <m>2\by2</m> matrix <m>B</m> that
	  rotates vectors <em>clockwise</em> by
	  <m>90^\circ</m>. </p></li>

	  <li><p> What do you expect the product <m>AB</m> to be?  
	  Explain the reasoning behind your expectation and then
	  compute <m>AB</m> to verify it. </p></li>

	  <li><p> Solve the equation <m>A\xvec = \twovec{3}{-2}</m>
	  using Gaussian elimination.
	  </p>
	  <sage language="python">
	    <input>
	    </input>
	  </sage>
	  </li>

	  <li><p> Explain why your solution may also be found by
	  computing <m>\xvec = B\twovec{3}{-2}</m>. </p></li>
	</ol>
      </p>

      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p> Dividing by <m>3</m> is the same as
	  multiplying by <m>\frac13</m>, the multiplicative inverse
	  <m>\frac13</m> of <m>3</m>.  We have
	    <me>
	      \begin{array}{rl}
	      \frac13(3x) \amp = \dfrac13 5  \\
	      \left(\frac13 3\right)x\amp = \dfrac53   \\
	      1x \amp = \dfrac 53 \\
	      x \amp = \dfrac53 \\
	      \end{array}
	    </me>.
	  </p></li>

	  <li><p>
	    As we have seen a few times, the matrix is
	    <m>A=\left[\begin{array}{rr}
	    0 \amp -1 \\
	    1 \amp 0 \\
	    \end{array}\right]
	    </m>.
	  </p></li>

	  <li><p>
	    Here, the matrix is
	    <m>B=\left[\begin{array}{rr}
	    0 \amp 1 \\
	    -1 \amp 0 \\
	    \end{array}\right]
	    </m>.
	  </p></li>

	  <li><p> We should expect that <m>AB=I</m> since the effect
	  of rotating by <m>90^\circ</m> clockwise followed by
	  rotating <m>90^\circ</m> counterclockwise is to leave a
	  vector unchanged.  We can verify this by performing the
	  matrix multiplication. 
	  </p></li>

	  <li><p> We have
	  <me>
	    \left[\begin{array}{rr|r}
	    0 \amp -1 \amp 3 \\
	    1 \amp 0 \amp -2 \\
	    \end{array}\right]
	    \sim
	    \left[\begin{array}{rr|r}
	    1 \amp 0 \amp -2 \\
	    0 \amp 1 \amp -3 \\
	    \end{array}\right]
	  </me>
	  so the solution is <m>\xvec=\twovec{-2}{-3}</m>.
	  </p></li>

	  <li><p>
	    The equation <m>A\xvec=\twovec{3}{-2}</m> is asking us to
	    find the vector that becomes <m>\twovec{3}{-2}</m> after
	    being rotated by <m>90^\circ</m>.  If we rotate
	    <m>\twovec{3}{-2}</m> by <m>90^\circ</m> in the opposite
	    direction, it will 
	    have this property.  That is,
	    if <m>\xvec = B\twovec{-2}{-3}</m>, then
	    <me>
	      A\xvec = A(B\xvec) 
	      = (AB)\twovec{3}{-2} 
	      = I\twovec{3}{-2} 
	      = \twovec{3}{-2}.
	    </me>
	  </p></li>
	</ol></p>
      </solution>
      
    </exploration>
    
  </introduction>

  <subsection>
    <title> Invertible matrices </title>

    <p> The preview activity began with a familiar type of 
    equation, <m>3x = 5</m>, and asked for a strategy to solve it.
    One possible response is to divide both sides by 3.
    Instead, let's rephrase this as multiplying by <m>3^{-1} = \frac
    13</m>, the multiplicative inverse of 3.  
    </p>

    <p> Now that we are interested in solving equations of the form
    <m>A\xvec = \bvec</m>, we might try to find a similar approach.
    Is there a matrix <m>A^{-1}</m> that plays the role of the
    multiplicative inverse of <m>A</m>?  Of course, the real number
    <m>0</m> does not have a multiplicative inverse so we probably
    shouldn't expect every matrix to have a multiplicative inverse.
    We will see, however, that many do.
    </p>

	<definition> 
		<idx><h>left inverse</h><see>inverse, left</see></idx>
		<idx><h>right inverse</h><see>inverse, right</see></idx>
		<idx><h>inverse</h><h>left</h></idx>
		<idx><h>inverse</h><h>right</h></idx>
		<statement>
			<p>
				Let <m>A</m> be an <m>m \by n</m> matrix. 
				If <m>LA = I_n</m>, then we call <m>L</m> a <term>left inverse</term> of <m>A</m>.  
				If <m>AR = I_m</m>, then we call <m>R</m> a <term>right inverse</term> of <m>A</m>.  
			</p>
		</statement>
	</definition>

	<activity>
		<statement>
			<p>
				<ol>
					<li>
						<p>
							If <m>A</m> is <m>m \by n</m> and has a left inverse <m>L</m>, what shape must <m>L</m> be? 
						</p>
					</li>
					<li>
						<p>
							If <m>A</m> is <m>m \by n</m> and has a right inverse <m>R</m>, what shape must <m>R</m> be? 
						</p>
					</li>
				</ol>
			</p>
		</statement>

		<answer>
			<p>
				In both cases, <m>n \by m</m>, the reverse of the shape of <m>A</m>.
			</p>
		</answer>
		<solution>
			<p>
				In both cases, <m>n \by m</m>, the reverse of the shape of <m>A</m>. For example, a left inverse <m>L</m>
				must have <m>m</m> columns because <m>A</m> has <m>m</m> rows.  And the product <m>LA</m> will have  
				<m>n</m> columns because <m>A</m> has <m>n</m> coloumns.  Since an idenity matrix is square, the product 
				also has <m>n</m> rows.  This implies the <m>L</m> has <m>n</m> rows.
			</p>
			<p>A similar argument shows that <m>R</m> must be <m>n \by m</m>.</p>
		</solution>
	</activity>

	<activity xml:id="activity-left-right-match">
		<statement>
		<p>
			<ol>
			<li>
				<p>
					Show that if a matrix <m>A</m> has left inverse <m>L</m> and 
					right inverse <m> R </m>, then <m>L = R </m>.  Hint: Consider the product <m>LAR</m>.
				</p>
			</li>
			<li>
				<p>
					Show that if an <m>m \by n</m> matrix <m>A</m> has right inverse <m>R</m>, then 
					<m> n \ge m</m>
				</p>
			</li>
			<li>
				<p>
					Show that if an <m>m \by n</m> matrix <m>A</m> has left inverse <m>L</m>, then 
					<m> m \ge n</m>. Hint: What does <m>LA\xvec = L\bvec</m> tell you about the 
					equation <m>A \xvec = \bvec</m>?
				</p>
			</li>
			<li>
				<p>
					Show that if an <m>m \by n</m> matrix <m>A</m> has both a left and a right inverse,
					then <m> m = n</m>, so <m>A</m> is a square matrix.
				</p>
			</li>
		</ol>
		</p>
	</statement>

	<solution>
		<p>
		<ol>
			<li>
				<p>
		If <m>LA = I</m> and <m>AR = I</m>, then 
			<md>
			<mrow>
			LAR \amp = (LA)R = I R = R
			</mrow>
			<mrow>
			LAR \amp = L(AR) = L I = L
			</mrow>
			</md>
			so <m>L = R</m>
				</p>
			</li>
			<li>
				<p>
					If <m>AR = I</m>, then for any <m>\bvec \in \real^m</m>, <m>AR\bvec = I \bvec = \bvec</m>.
					This means that <m>A \xvec = \bvec</m> is consistent for every <m>\bvec \in \real^m</m>. 
					That means every row of the RREF of <m>A</m> has a pivot position. So there are <m>m</m> pivots.
					There can be at most one pivot in each column, so there must be at least as many columns as 
					pivots: <m>n \ge m</m>.
				</p>
			</li>
			<li>
				<p>
					If <m>A</m> has a left inverse <m>L</m>, then <m>L</m> has a right inverse (namely <m>A</m>). 
					So <m>L</m> must have at least as many columns as rows, which means <m>A</m> must have 
					at least as many rows as columns: <m> m \ge n</m>
				</p>
			</li>
			<li>
				<p>
					Combine the two previous parts.
				</p>
			</li>
		</ol>
	</p>
	</solution>
	</activity> 

	<p>
		<xref ref="activity-left-right-match"/> tells us two important pieces of information about 
		matrices that have both a left and a right inverse.
	</p>

	<proposition xml:id="prop-left-right-match">
		<statement>
			<p>
			If a matrix <m>A</m> has left inverse <m>L</m> and 
			right inverse <m> R </m>, then 
			<ul>
				<li>
					<p>
						<m>L = R</m>.
					</p>
				</li>
				<li>
					<p>
						<m>A</m> is square.
					</p>
				</li>
			</ul>
			</p>
		</statement>
	</proposition>

	<p>
		This leads to the following definition
	</p>


    <definition>
	<idx> <h>invertible matrix</h> </idx>
	<idx> <h>matrix</h><h>inverse</h> </idx>
      <statement>
	<p> An <m>n \by n</m> square matrix <m>A</m> is called <term>invertible</term> 
		if there is a matrix <m>B</m> that is both a left and a right inverse for <m>A</m>.
	That is <m>BA = I_n</m> and <m> AB = I_n</m>.
	The matrix <m>B</m> is called the <term>inverse</term> of <m>A</m> and denoted <m>A^{-1}</m>.
	</p>
      </statement>
    </definition>

	<aside>
		<p>
			It makes sense to refer to <em>the</em> inverse of an invertible matrix because
			<xref ref="prop-left-right-match"/> implies that there can only be one 
				matrix that is both a left and a right inverse.
		</p>
	</aside>

    <proposition xml:id="prop-inverse-inverse">
      <statement>
	<p>
	  If <m>A</m> is a <m>n\by n</m> invertible matrix with
	  inverse <m>B</m>, then <m>B</m> is also invertbile and 
	  the inverse of <m>B</m> is <m>A</m>.
	  In other words,
	  <me>
	    (A^{-1})^{-1} = A.
	  </me>
	</p>
      </statement>
	  <proof>
		<p>
			If <m>B</m> is the inverse of <m>A</m>, then <m>AB = I</m> and <m>BA = I</m>.
			But this means that <m>A</m> is the inverse of <m>B</m> as well.
		</p>
	  </proof>
    </proposition>


	<p>
		We have seen that if a matrix has a left and a right inverse, then these must be  
		the same matrix.
		In fact, more is true. 
		For square matrices, any right inverse is also a left inverse, 
		and any left inverse is also a right inverse.
	</p>

	<p>
		It is important to remember that the product of two matrices
		depends on the order in which they are multiplied.  That is, if
		<m>C</m> and <m>D</m> are matrices, then it sometimes happens
		that <m>CD \neq DC</m>, even if the matrices are square.  
		So it is not immediate that <m>RA = I</m> just becuase <m>AR = I</m>.  
		For matrices that are not square, 
		<m>AR</m> and <m>RA</m> won't even be the same shape, and at most one of the 
		two products will be an identity matrix.
		So <xref ref="prop-left-inverse-implies-right-inverse"/> establishes something 
		special about one-sided inverses of square matrices.
	</p>

	<proposition xml:id="prop-left-inverse-implies-right-inverse">
		<statement>
			<p>
				If <m>A</m> is an <m>n \by n</m> square matrix and <m>AR = I_n</m>,
				then <m>RA = I_n</m>.  Similarly if <m>LA = I_n</m> then <m>AL = I_n</m>.
			</p>
		</statement>

		<proof>
			<p>
				Suppose <m>A</m> is <m>n \by n</m> and has a right inverse <m>R</m>. 
				Then for any <m>\bvec \in \real^n</m>, <m>AR\bvec = \bvec</m>. So <m>A \xvec = \bvec</m>
				is consistent for every <m>\bvec</m>.  This implies that the RREF of <m>A</m> has a pivot in 
				each row, and therefore also in each column.  In other words, <m>A \sim I</m>.
			</p>

			<p>
				Notice that if <m>R\xvec = R \yvec</m>, then <m>\xvec = AR \xvec = AR \yvec = \yvec</m>.
				This means the matrix transformation associated with <m>R</m> is one-to-one, so the RREF  
				for <m>R</m> has a pivot in every column, and therefore in every row.  In other words,
				<m>R \sim I</m>.
			</p>

			<p>
				Now consider <m>RA \xvec</m> for any <m>\xvec \in \real^n</m>.  Because <m>R \sim I</m>, there 
				must be a <m>\yvec</m> with <m>R \yvec = \xvec</m>. So
				<me>
					RA \xvec = RAR \yvec = R \yvec = \xvec
				</me>
				But this means that <m>RA</m> is the identity matrix since the transformation associated with 
				it is the identity transformation.  In other words, <m>RA = I</m> and <m>R</m> is also a left 
				inverse of <m>A</m>
			</p>

			<p>Now suppose that <m>L</m> is a left inverse for <m>A</m>. That means <m>A</m> is a right inverse  
				for <m>L</m>, and we just show that this means <m>A</m> is also a left inverse for <m>L</m>. 
				So <m> AL = I</m> and <m>L</m> is a right inverse for <m>A</m>.
			</p>
		</proof>

	</proposition>

	<p>
		<xref ref="prop-left-inverse-implies-right-inverse"/> means that to show
		that matrix is invertible, it suffices to find a one-sided inverse -- it
		will always also be a full inverse.
	</p>

	
		<!-- However, something fortunate happens for square matrices:
		If <m>A</m> and <m>R</m> are <m>n\by n</m> matrices and <m>AR=I</m>, 
		then it is also true that <m>RA=I</m>.  Similarly, if <m>A</m> has a 
		left inverse, it will also be a right inverse. -->

	<example>
		<statement>
			<p>Consider the following two matrices  
				<me>
				A = \begin{bmatrix} 1 \amp 0 \amp 2\\ 2 \amp 2 \amp 1\\ \end{bmatrix} 
				\quad
				B = \begin{bmatrix} 1 \amp 0\\ -1 \amp 0.5 \\ 0 \amp 0\\ \end{bmatrix}
				</me>.
				Then 
				<md>
					<mrow> 
					AB  \amp =
					\begin{bmatrix}
					1 \amp 0\\
					0 \amp 1\\ 
				  \end{bmatrix}
					= I_2
					</mrow>
					<mrow> 
					BA  \amp =
				  \begin{bmatrix}
					1 \amp 0 \amp 2\\
					0 \amp 1 \amp -1.5\\
					0 \amp 0 \amp 0\\
				  \end{bmatrix} \neq I_3
					</mrow>
				</md>
				So <m>B</m> is a right inverse of <m>A</m>, but <m>B</m> is not a left inverse of <m>A</m>. 
			</p>
			<p>
				Now consider 
				<me>B_2 = \begin{bmatrix}
					-3 \amp -4 \\
					2 \amp 3.5 \\ 
					2 \amp 2 \\
					\end{bmatrix}
				</me>. 
				Multiplication shows that 
				<me>
					A B_2 = I_2
				</me>,
				So <m>A</m> has more than one right inverse.  In fact, there are infinitely many and none of them 
				can be a left inverse, since if <m>L</m> had both a left and a right inverse, they would have to
				be the same by <xref ref="prop-left-right-match" />. But that means a left inverse would 
				have to be the same as both <m>B</m> and <m>B_2</m>, which is not posssible.
			</p>
		</statement>
	</example>



	<example>
		<statement>
	  <p>
		Suppose that <m>A</m> is the matrix that rotates
		two-dimensional vectors counterclockwise by <m>90^\circ</m>
		and that <m>B</m> rotates vectors
		by <m>-90^\circ</m>.  We have
		<me>
		  A=\left[\begin{array}{rr}
		  0 \amp -1 \\
		  1 \amp 0 \\
		  \end{array}\right],~~~
		  B=\left[\begin{array}{rr}
		  0 \amp 1 \\
		  -1 \amp 0 \\
		  \end{array}\right].
		</me>
		We can check that
		<me>
		  AB = \begin{bmatrix}
		  0 \amp -1 \\
		  1 \amp 0 \\
		  \end{bmatrix}
		  \begin{bmatrix}
		  0 \amp 1 \\
		  -1 \amp 0 \\
		  \end{bmatrix}
		  = \begin{bmatrix}
		  1 \amp 0 \\
		  0 \amp 1 \\
		  \end{bmatrix}
		  = I 
		</me>
		which shows that <m>B</m> is a right inverse of <m>A</m>.
	  </p>
	  <p>
		If we multiply the matrices in the opposite
		order, we find that 
		<me>
		  BA = 
		  \begin{bmatrix}
		  0 \amp 1 \\
		  -1 \amp 0 \\
		  \end{bmatrix}
		  \begin{bmatrix}
		  0 \amp -1 \\
		  1 \amp 0 \\
		  \end{bmatrix}
		  = \begin{bmatrix}
		  1 \amp 0 \\
		  0 \amp 1 \\
		  \end{bmatrix}
		  = I 
		</me>,
		which says that (a) <m>B</m> is also the left inverse of <m>A</m>, 
		and (b) <m>A</m> is the inverse of <m>B</m> as well.
		Inverses always come in pairs like this.  If <m>A</m> is invertible with 
		inverse <m>B</m>, then <m>B</m> is also invertible and its inverse is <m>A</m>.
		In other words, <m>A</m> and <m>B</m> are inverses of each other.  
	  </p>

	  <p>
		If we think about the matrix transformations associated with <m>A</m> and <m>B</m>, this makes  
		geometric sense.  Rotating clockwise "undoes" a counterclockwise rotation.  Rotating counterclockwise 
		"undoes" a clockwise rotation.
	  </p>
		</statement>
	  </example>


    <!-- <p>
      It is important to remember that the product of two matrices
      depends on the order in which they are multiplied.  That is, if
      <m>C</m> and <m>D</m> are matrices, then it sometimes happens
      that <m>CD \neq DC</m>.  However, something fortunate happens
      when we consider invertibility.  It turns that if <m>A</m> is an
      <m>n\by n</m> matrix and that <m>AB=I</m>, then it is also
      true that <m>BA=I</m>.  We have verified this in a few examples
      so far, and <xref ref="ex-right-inverse" /> explains why it 
      always happens.  This leads to the following proposition.
    </p> -->

  </subsection>

  <subsection>
    <title> Solving equations with an inverse </title>

	<p>
		If <m>A</m> is an invertible matrix, then for any <m>\bvec \in \real^n</m>,  
		<me>A (A^{-1} \bvec) = (A A^{-1}) \bvec  = \bvec</me>.
		This implies that <m>A \xvec = \bvec</m> has a unique solution for every <m>\bvec</m>, 
		namely <m>\xvec = A^{-1} \bvec</m>.
	</p>

	<p>
		This result is import enough to capture in a proposition.
	</p>

    <proposition xml:id="prop-inverse-solve">
      <statement>
	<p>
	  If <m>A</m> is an invertible <m>n \by n</m> matrix with inverse <m>A^{-1}</m>,
	  and <m>\bvec \in \real^n</m>,
	  then the equation <m>A\xvec = \bvec</m> is consistent and
	  <m>\xvec = A^{-1} \bvec</m> is the unique solution.  
	  <!-- In other words, the solution to <m>A\xvec = \bvec</m> is <m>\xvec = A^{-1}\bvec</m>.  -->
	</p>
      </statement>
    </proposition>

    <p>
      Notice that this is similar to saying that the solution to
      <m>3x=5</m> is <m>x = \frac13\cdot 5</m>, as we saw in the
      preview activity.
    </p>

    <p>
      You may have noticed that <xref
				ref="prop-inverse-solve" /> says that <em>the</em>
      solution to the equation <m>A\xvec = \bvec</m> is <m>\xvec =
      A^{-1}\bvec</m>.  Indeed, we know that this equation has a
      unique solution because <m>A</m> has a pivot position in every
      <em>column</em>.
    </p>
      
    <p>
      <xref ref="prop-inverse-solve" />
      shows us how to use <m>A^{-1}</m> to solve equations of the form 
	  <m>A\xvec = \bvec</m>.  In <xref ref="subsec-finding-inverses"/> we will 
	  learn how to find an inverse for any square matrix that has an inverse. 
	  But before we get to that, let's see some examples of how to use an inverse  
	  once we have it.
    </p>

    <activity>
      <statement>
	<p>
	  We'll begin by considering the square matrix
	  <me>
	    A = \begin{bmatrix}
	    1 \amp 0 \amp 2 \\
	    2 \amp 2 \amp 1 \\
	    1 \amp 1 \amp 1 \\
	    \end{bmatrix}.
	  </me>
	  
	  <ol marker="a.">
	    <li>
	      <p>
		Describe the solution space to the equation <m>A\xvec
		= \threevec343</m> by augmenting <m>A</m> and finding
		the reduced row echelon form.
		</p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>
	    <li>
	      <p>
			Show that 
			<me>
				A^{-1} = \begin{bmatrix}
				1 \amp 2 \amp -4\\
				-1 \amp -1 \amp 3\\
				0 \amp -1 \amp 2\\
			  \end{bmatrix}
			</me>
	      </p>
	    </li>
	    <li>
	      <p>
		Now use <m>A^{-1}</m> to solve the equation 
		<m>A\xvec = \threevec343</m> and verify that your result agrees
		with what you found in part a.
	      </p>
	    </li>
		
	    <li>
	      <p>
			We'll learn a way to compute the inverse of a matrix shortly.
			NumPy knows how to do this, of course.
		If you have defined a matrix <c>B</c> in Python, you can
		find it's inverse as <c>np.linalg.inv(B)</c>. 
		Use Python to find the inverse of the matrix
		<me>
		  B = \left[\begin{array}{rrr}
		  1 \amp -2 \amp -1 \\
		  -1 \amp 5 \amp 6 \\
		  5 \amp -4 \amp 6 \\
		  \end{array}\right]
		</me>
		and use the inverse to solve the equation <m>B\xvec = \threevec83{36}</m>. 
		</p>
		  <sage language="python">
		    <input>
		    </input>
		  </sage>
	    </li>
	    
	    <li>
	      <p>
		If <m>A</m> and <m>B</m> are the two matrices defined
		in this activity, compute their product <m>AB</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Compute the products <m>A^{-1}B^{-1}</m> and
		<m>B^{-1}A^{-1}</m>.  One of these is <m>(AB)^{-1}</m>.  Which one?
	      </p>
	    </li>
	    <li>
	      <p>
		Explain your finding by considering the product
		<me>
		  (AB)(B^{-1}A^{-1})
		</me>
		and using associativity to regroup the products so
		that the middle two terms are multiplied first.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p>
		We can demonstrate that the matrix is an inverse by multiplication.
	      <me>
		\begin{bmatrix}
		1 \amp 2 \amp -4 \\
		-1 \amp -1 \amp 3 \\
		0 \amp -1 \amp 2 \\
		\end{bmatrix}
		\begin{bmatrix}
	    1 \amp 0 \amp 2 \\
	    2 \amp 2 \amp 1 \\
	    1 \amp 1 \amp 1 \\
	    \end{bmatrix}
		= I_3
	      </me>.
		It suffices to show we have a left inverse, but you can multiply the other way around to see that it also a right inverse.
	    </p>
	  </li>
	  <li>
	    <p>
	      We see that <m>A^{-1}\threevec343 = \threevec{-1}22</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Python tells us that <m>B^{-1}\threevec{8}3{36} =
	      \threevec4{-1}2</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Python helps us see that <m>(AB)\sim I</m>, which tells us
	      that <m>AB</m> is invertible.
	    </p>
	  </li>

	  <li><p>
	    We find that <m>(AB)^{-1} = B^{-1}A^{-1}</m>.
	  </p></li>

	  <li>
	    <p>
	      We see that
	      <me>
		(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} =
		AA^{-1} = I.
	      </me>
	    </p>
	  </li>
	</ol></p>
      </solution>

    </activity>

</subsection>

<subsection xml:id="subsec-finding-inverses">
	<title>Finding inverses</title>

	<p>
		Now that we have seen one use of inverses -- solving equations of the
		form <m>A \xvec = \bvec</m> -- we turn our attention to the task of
		computing the inverse of a matrix, or demonstrating that no inverse
		exists.
	</p>

    <activity>
      <statement>
	<p>
	  This activity demonstrates a procedure for finding the (right)
	  inverse of a matrix <m>A</m>.

	  <ol marker="a.">
	    <li>
	      <p>
		Suppose that <m>A = \begin{bmatrix}
		3 \amp -2 \\
		1 \amp -1 \\
		\end{bmatrix}
		</m>.  To find a right inverse <m>B</m>, we write its columns
		as <m>B = \begin{bmatrix}\bvec_1 \amp \bvec_2
		\end{bmatrix}</m> and require that
		<me>
		  \begin{aligned}
		  AB \amp = I \\
		  \begin{bmatrix}
		  A\bvec_1 \amp A\bvec_2
		  \end{bmatrix}
		  \amp = 
		  \begin{bmatrix}
		  1 \amp 0 \\
		  0 \amp 1 \\
		  \end{bmatrix}.
		  \end{aligned}
		</me>
		In other words, we can find the columns of <m>B</m> by
		solving the equations
		<me>
		  A\bvec_1 = \twovec10, \quad
		  A\bvec_2 = \twovec01.
		</me>
		Solve these equations to find <m>\bvec_1</m> and
		<m>\bvec_2</m>.  Then write the matrix <m>B</m> and
		verify that <m>AB=I</m>.  
	</p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>
	    <li>
	      <p>
		By <xref ref="prop-left-inverse-implies-right-inverse"/>
		this is enough for us to
		conclude that <m>B</m> is the inverse of <m>A</m>.
		But le'ts compute the product <m>BA</m> just to confirm.
		  </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>
	    <li>
	      <p>
		What happens when you try to find the inverse of
		<m>C = \begin{bmatrix}
		-2 \amp 1 \\
		4 \amp -2 \\
		\end{bmatrix}</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		We now develop a condition that must be satisfied by
		an invertible matrix.
		Suppose that <m>A</m> is an invertible
		<m>n\by n</m> matrix with inverse <m>B</m> and 
		suppose that <m>\bvec</m> is any <m>n</m>-dimensional
		vector. Since <m>AB=I</m>, we have
		<me>
		  A(B\bvec) = (AB)\bvec = I\bvec = \bvec.
		</me>
		This says that the equation <m>A\xvec = \bvec</m> is
		consistent and that <m>\xvec=B\bvec</m> is a solution.
	      </p>

	      <p>
		Since we know that <m>A\xvec = \bvec</m> is consistent
		for any vector <m>\bvec</m>, what does this say about
		the span of the columns of <m>A</m>?
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>A</m> is a square matrix,
		what does this say
		about the pivot positions of <m>A</m>? What is the
		reduced row echelon form of <m>A</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		In this activity, we have studied the matrices
		<me>
		  A = \begin{bmatrix}
		  3 \amp -2 \\
		  1 \amp -1 \\
		  \end{bmatrix}, \qquad
		  C = \begin{bmatrix}
		  -2 \amp 1 \\
		  4 \amp -2 \\
		  \end{bmatrix}.
		</me>
		Find the reduced row echelon form of each and explain
		how those forms enable us to conclude that one matrix
		is invertible and the other is not.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p>
	    Solving the two equations for <m>\bvec_1</m> and
	    <m>\bvec_2</m> gives
	    <m>B = \begin{bmatrix}
	    1 \amp -2 \\
	    1 \amp -3 \\
	    \end{bmatrix}</m>.  We can verify that, as we
	    expect, <m>AB=I</m>.
	  </p></li>

	  <li><p>
	    We find that <m>BA=I</m>, which is the condition that
	    tells us that <m>B</m> is invertible.
	  </p></li>

	  <li><p>
	    Seeking the first column of <m>C^{-1}</m>, we see that
	    the equation <m>C\xvec = \twovec10</m> is not consistent.
	    This means that <m>C</m> is not invertible.
	  </p></li>

	  <li><p>
	    Since the equation <m>A\xvec = \bvec</m> is consistent for
	    every <m>\bvec</m>, we know that the span of the columns
	    of <m>A</m> is <m>\real^n</m>.
	  </p></li>

	  <li><p>
	    Because the span of the columns of <m>A</m> is
	    <m>\real^n</m>, there is a pivot position in every row.
	    Since <m>A</m> is square, there is also a pivot position
	    in every column.  This means that the reduced row echelon
	    form of <m>A</m> must be the identity matrix <m>I_n</m>.
	  </p></li>

	  <li><p>
	    We see that
	    <me>
	      A\sim \begin{bmatrix}
	      1 \amp 0 \\
	      0 \amp 1 \\
	      \end{bmatrix}, \quad
	      C\sim
	      \begin{bmatrix}
	      1 \amp -\frac12 \\
	      0 \amp 0 \\
	      \end{bmatrix},
	    </me>
	    which shows that <m>A</m> is invertible and <m>C</m> is
	    not. 
	  </p></li>
	</ol></p>
      </solution>

    </activity>

    <example xml:id="example-inverse-augment-I">
      <statement>
	<p>
	  We can reformulate this procedure for finding the inverse of a
	  matrix.  For the sake of convenience, suppose that <m>A</m> is a
	  <m>2\by2</m> invertible matrix with inverse
	  <m>B=\begin{bmatrix} \bvec_1 \amp \bvec_2 \end{bmatrix}</m>.  
	  Rather than solving the equations
	  <me>
	    A\bvec_1 = \twovec10, \quad
	    A\bvec_2 = \twovec01
	  </me>
	  separately, we can solve them at the same time by augmenting
	  <m>A</m> by both vectors <m>\twovec10</m> and <m>\twovec01</m>
	  and finding the reduced row echelon form.
	</p>
	
	<p>
	  For example, if
	  <m>A =
	  \begin{bmatrix}
	  1 \amp 2 \\
	  1 \amp 1 \\
	  \end{bmatrix}</m>,
	  we form
	  <me>
	    \left[
	    \begin{array}{rr|rr}
	    1 \amp 2 \amp 1 \amp 0 \\
	    1 \amp 1 \amp 0 \amp 1 \\
	    \end{array}
	    \right]
	    \sim
	    \left[
	    \begin{array}{rr|rr}
	    1 \amp 0 \amp -1 \amp 2 \\
	    0 \amp 1 \amp 1 \amp -1 \\
	    \end{array}
	    \right].
	  </me>
	  This shows that the matrix
	  <m> B =
	  \begin{bmatrix}
	  -1 \amp 2 \\
	  1 \amp 1 \\
	  \end{bmatrix}
	  </m> is the inverse of <m>A</m>.
	</p>
	<p>
	  In other words, beginning with <m>A</m>, we augment by the
	  identify and find the reduced row echelon form to determine
	  <m>A^{-1}</m>:
	  <me>
	    \left[
	    \begin{array}{r|r}
	    A \amp I \\
	    \end{array}
	    \right]
	    \sim
	    \left[
	    \begin{array}{r|r}
	    I \amp A^{-1} \\
	    \end{array}
	    \right].
	  </me>
	</p>
      </statement>
    </example>

    <p>
      This reformulation will always work. 
	</p>

	<!-- <p>
      Now since <m>A\xvec=\bvec</m> is consistent for every vector
      <m>\bvec</m>, the columns of <m>A</m>
      must span <m>\real^n</m> so there is a pivot position in every
      row.  Since <m>A</m> is also square, this means that the reduced
      row echelon form of <m>A</m> is the identity matrix.  
    </p> -->

    <proposition xml:id="prop-invertible-rref">
      <statement>
	<p>
	  The matrix <m>A</m> is invertible if and only if
	  the reduced row echelon form of <m>A</m> is the identity
	  matrix:  <m>A\sim I</m>.
	  In addition, we can find the inverse by augmenting <m>A</m>
	  by the identity and finding the reduced row echelon form:
	  <me>
	    \left[
	    \begin{array}{r|r}
	    A \amp I \\
	    \end{array}
	    \right]
	    \sim
	    \left[
	    \begin{array}{r|r}
	    I \amp A^{-1} \\
	    \end{array}
	    \right].
	  </me>
	</p>
      </statement>
    </proposition>


    <p>
      The next proposition summarizes much of what we have found about
      invertible matrices.
    </p>
    
    <proposition xml:id="prop-invertible-properties">
      <title> Properties of invertible matrices </title>
      <statement>
	<p> 
	  <ul>
	    <li><p> An <m>n\by n</m> matrix <m>A</m> is invertible if
	    and only if <m>A\sim I</m>. </p></li>

	    <li><p> If <m>A</m> is invertible, then the solution to the
	    equation <m>A\xvec = \bvec</m> is given by <m>\xvec =
	    A^{-1}\bvec</m>. </p></li>

	    <li><p> We can find <m>A^{-1}</m> by finding the reduced row
	    echelon form of <m>\left[\begin{array}{r|r} A \amp I
	    \end{array}\right]</m>; namely,
	    <me>
	      \left[\begin{array}{r|r} A \amp I \end{array}\right]
	      \sim
	      \left[\begin{array}{r|r} I \amp A^{-1} \end{array}\right]
	    </me>.</p></li>

	    <li><p> If <m>A</m> and <m>B</m> are two invertible <m>n\by
	    n</m> matrices, then their product <m>AB</m> is also
	    invertible and <m>(AB)^{-1} = B^{-1}A^{-1}</m>. </p></li>
	  </ul>
	</p>
      </statement>
    </proposition>

   <remark>
	<title>Formulas for inverse matrices</title>
    <p> There is a simple formula for finding the inverse of a
    <m>2\by2</m> matrix:
    <me>
      \left[\begin{array}{rr}
      a \amp b \\
      c \amp d \\
      \end{array}\right]^{-1}
      =
      \frac{1}{ad-bc}
      \left[\begin{array}{rr}
      d \amp -b \\
      -c \amp a \\
      \end{array}\right]
    </me>,
    which can be easily checked.  The condition that <m>A</m> be
    invertible is, in this case, reduced to the condition that
    <m>ad-bc\neq 0</m>.  We will understand this condition better once
    we have explored determinants in
    <xref ref="sec-determinants" />.
    There is a similar formula for the inverse of a <m>3\by
    3</m> matrix, but there is not a good reason to write it here.
    </p>
   </remark> 

  </subsection>

  <subsection>
    <title> Summary </title>

    <p> In this section, we found conditions guaranteeing that a
    matrix has an inverse.  When these conditions hold, we also found
    an algorithm for finding the inverse.
    <ul>
      <li><p> A square matrix is invertible if there is a matrix
      <m>B</m>, known as the inverse of <m>A</m>, such that <m>AB =
      I</m>.  We usually write <m>A^{-1} = B</m>.  </p></li>

      <li><p> The <m>n\by n</m> matrix <m>A</m> is
      invertible if and only if it is row equivalent to <m>I_n</m>,
      the <m>n\by n</m> identity matrix. </p></li>

      <li><p> If a matrix <m>A</m> is invertible, we can use Gaussian 
      elimination to find its inverse:
      <me>
	\left[\begin{array}{r|r} A \amp I \end{array}\right] \sim 
	\left[\begin{array}{r|r} I \amp A^{-1} \end{array}\right]
      </me>. </p></li>

      <li><p> If a matrix <m>A</m> is invertible, then the solution to
      the equation <m>A\xvec = \bvec</m> is <m>\xvec =
      A^{-1}\bvec</m>. </p></li> 

    </ul>
    </p>

  </subsection>

	<xi:include href="exercises/exercises4-1.ptx" />
</section>



