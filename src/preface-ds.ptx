<?xml version="1.0" encoding="UTF-8"?>

  <preface>
    <title>What's different in the data science edition?</title>

    <p>
    David Austin generously made the source of his <em>Understanding Linear Algebra</em> publicly
    available and licensed so that others could make modifications.  The <em>Data Science Edition</em> 
    is one example. You may be wondering how the two compare and which is the right version for you.
    Here is a summary of differences between the two editions.

      <ul>
        <li>
          <title> Python instead of Sage</title>
          <p>
            This is probably the biggest and most noticeable change. 
            This edition uses Python and important Python libraries for data science, like  
            <c>numpy</c>, <c>scipy</c>, and <c>pandas</c>. Data scientists are much more likely to 
            encounter these tools than they are to use Sage.
          </p>
        </li>
        <li>
            <title>A different starting point</title>
          <p>
            The original version motivates linear algebra through an attempt to
            solve linear systems of equations.  That is an important
            application, but why do we want to solve linear systems in the first
            place? And how is that related to data science?
          </p>
          <p>
            This edition begins by exploring how vectors and matrices can be
            used to store data and emphasizing three complementary ways to think
            about vectors, which we might call the data science perspective, the
            geometry perspective, and the mathematical perspective. Right from
            the start, we want to develop skill in moving among these three ways
            of thinking.
          </p>
        </li>
        <li>
            <title>Some additional data science applications</title>
          <p>
            Linear models make their first appearance much earlier -- as an
            example of linear combinations and matrix-vector multiplication,
            even though least squares methods don't come until later.  Tensors
            (multi-dimensional arrays) are introduced immediately after
            matrices, in part because they help demystify how <c>numpy</c>
            approaches things like  aggregation with matrices, and in part
            because many data science applications make use of
            higher-dimensional arrays.
          </p>
          <p>
            Over time, I hope to add additional data science examples.  
          </p>
        </li>
        <li>
          <title>A different approach to the dot product</title>
          <p>
            The traditional approach begins with a computational form and links
            this to geometry using the Law of Cosines, a result that is
            unfamiliar to many students.  The result is that the computational
            and geometric forms can seem unrelated, and projections can feel a
            bit mysterious.
          </p>
          <p>
            Here we motivate dot products from a desire to compute projections
            and establish the connection to the computational formala without
            citing the Law of Cosines. Our hope is that this will make  these
            topics feel more natural and intuitive.  In the end, we will end in
            the same place, relying on the important interplay between three
            ways of thinking: data-centric, geometric, and mathematical.
          </p>
        </li>
      </ul>
      
    </p>
   
    
  </preface>