<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="subsec-triangular-invertible"
	 xmlns:xi="http://www.w3.org/2001/XInclude">

    <title> Triangular matrices and Gaussian elimination </title>

  <introduction>

    <p>
      With some of the ideas we've developed, we can recast the
      Gaussian elimination algorithm in terms of matrix multiplication
      and invertibility.  These ideas will be especially helpful later when
      we consider
      <xref ref="sec-determinants" text="custom">
	determinants
      </xref>
      and 
      <xref ref="sec-gaussian-revisited" text="custom">
	LU factorizations.
      </xref>
      Triangular matrices will play an important role.
    </p>

	</introduction>

	<subsection xml:id="subsec-triangular-matrices">
		<title>Triangular matrices</title>
		
    <definition>
      <statement>
	<idx> lower triangular matrix </idx>
	<idx> upper triangular matrix </idx>
	<idx> diagonal matrix </idx>
	<p> We say that a matrix <m>A</m> is <term>lower triangular</term>
	if all its entries above the diagonal are zero.  Similarly,
	<m>A</m> is <term>upper triangular</term> if all the entries below
	the diagonal are zero.  A matrix that is both upper and lower triangular 
	is called <term>diagonal</term> because all of its non-zero entries are 
	along the diagonal of the matrix.
	</p>
      </statement>
    </definition>

    <p> For example, the matrix <m>L</m> below is a lower triangular
    matrix while <m>U</m> is upper triangular and <m>D</m> is diagonal.
    <me>
      L = \left[\begin{array}{rrrr}
      * \amp 0 \amp 0 \amp 0 \\
      * \amp * \amp 0 \amp 0 \\
      * \amp * \amp * \amp 0 \\
      * \amp * \amp * \amp * \\
      \end{array}\right],
      \hspace{20pt}
      U = 
      \left[\begin{array}{rrrr}
      * \amp * \amp * \amp * \\
      0 \amp * \amp * \amp * \\
      0 \amp 0 \amp * \amp * \\
      0 \amp 0 \amp 0 \amp * \\
      \end{array}\right],
      \hspace{20pt}
      D = 
      \left[\begin{array}{rrrr}
      * \amp 0 \amp 0 \amp 0 \\
      0 \amp * \amp 0 \amp 0 \\
      0 \amp 0 \amp * \amp 0 \\
      0 \amp 0 \amp 0 \amp * \\
      \end{array}\right]
    </me>.
    </p>

    <p> We can develop a simple test to determine whether an
    <m>n\times n</m> lower triangular matrix is invertible.  Let's
    use Gaussian elimination to find the reduced row echelon form of
    the lower triangular matrix
    <me>
      \begin{aligned}
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      2 \amp -2 \amp 0 \\
      -3 \amp 4 \amp -4 \\
      \end{array}\right]
      \amp {}\sim{}
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      0 \amp -2 \amp 0 \\
      0 \amp 4 \amp -4 \\
      \end{array}\right]
      \\
      \amp {}\sim{}
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      0 \amp -2 \amp 0 \\
      0 \amp 0 \amp -4 \\
      \end{array}\right]      
      \sim
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      0 \amp 1 \amp 0 \\
      0 \amp 0 \amp 1 \\
      \end{array}\right]\text{.}
      \end{aligned}
    </me>
      Because the entries on the diagonal are nonzero, we find a pivot
      position in every row, which tells us that the matrix is
      invertible.
    </p>
    <p>
      If, however, there is a zero entry on the diagonal,
      the matrix cannot be invertible.  Considering the matrix below,
      we see that having a zero on the diagonal leads to a row without
      a pivot position.
    <me>
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      2 \amp 0 \amp 0 \\
      -3 \amp 4 \amp -4 \\
      \end{array}\right]
      \sim
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      0 \amp 0 \amp 0 \\
      0 \amp 4 \amp -4 \\
      \end{array}\right]
      \sim
      \left[\begin{array}{rrr}
      1 \amp 0 \amp 0 \\
      0 \amp 1 \amp -1 \\
      0 \amp 0 \amp 0 \\
      \end{array}\right]\text{.}
    </me>
    </p>

    <proposition xml:id="proposition-triangular-invertibility">
      <statement>
	<p> An <m>n\times n</m> triangular matrix is invertible if and
	only if the entries on the diagonal are all nonzero.
	</p>
      </statement>
    </proposition>

	</subsection>

<subsection xml:id="subsec-elementary-matrices">
	<title>Elementary matrices</title>
	
    <activity>
      <title> Gaussian elimination and matrix multiplication </title>
      <statement>
	<p>
	  This activity explores how the row operations of scaling,
	  interchange, and replacement can be performed using matrix
	  multiplication.
	</p>
	<p>
	  As an example, we consider the matrix
	  <me>
	    A = \left[\begin{array}{rrr}
	    1 \amp 2 \amp 1 \\
	    2 \amp 0 \amp -2 \\
	    -1 \amp 2 \amp -1 \\
	    \end{array}\right]
	  </me>
	  and apply a replacement operation that multiplies the first
	  row by <m>-2</m> and adds it to the second row.  Rather than
	  performing this operation in the usual way, we construct a
	  new matrix by applying the desired replacement operation to
	  the identity matrix.  To illustrate, we begin with the
	  identity matrix
	  <me>
	    I = \begin{bmatrix}
	    1 \amp 0 \amp 0 \\
	    0 \amp 1 \amp 0 \\
	    0 \amp 0 \amp 1 \\
	    \end{bmatrix}
	  </me>
	  and form a new matrix by multiplying the first row by
	  <m>-2</m> and adding it to the second row to obtain
	  <me>
	    R = \begin{bmatrix}
	    1 \amp 0 \amp 0 \\
	    -2 \amp 1 \amp 0 \\
	    0 \amp 0 \amp 1 \\
	    \end{bmatrix}.
	  </me>
	  <ol marker="a.">
	    <li><p>
	      Show that the product <m>RA</m> is the result of
	      applying the replacement operation to <m>A</m>.
	      <sage language="python">
		<input>
		</input>
	      </sage>
	    </p></li>

	    <li><p> Explain why <m>R</m> is invertible and find its
	    inverse <m>R^{-1}</m>.
	    </p></li>
	    
	    <li><p> Describe the relationship between <m>R</m> and
	    <m>R^{-1}</m> and use the connection 
	    to replacement operations to explain why it holds.
	    </p></li>

	    <li><p> Other row operations can be performed using a
	    similar procedure.  For instance, suppose we want to scale
	    the second row of <m>A</m> by <m>4</m>.  Find a matrix
	    <m>S</m> so that <m>SA</m> is the same as that obtained
	    from the scaling operation.  Why is <m>S</m> invertible
	    and what is <m>S^{-1}</m>?
	    <sage language="python">
	      <input>
	      </input>
	    </sage>
	    </p></li>

	    <li><p> Finally, suppose we want to interchange the first
	    and third rows of <m>A</m>.  Find a matrix <m>P</m>,
	    usually called a <term>permutation matrix</term> that performs
	    this operation.  What is <m>P^{-1}</m>?
	    </p></li>
	      
	    <li><p> The original matrix <m>A</m> is seen to be row
	    equivalent to the upper triangular matrix <m>U</m> by
	    performing three replacement operations on <m>A</m>:
	    <me>
	      A = \left[\begin{array}{rrr}
	      1 \amp 2 \amp 1 \\
	      2 \amp 0 \amp -2 \\
	      -1 \amp 2 \amp -1 \\
	      \end{array}\right]
	      \sim
	      \left[\begin{array}{rrr}
	      1 \amp 2 \amp 1 \\
	      0 \amp -4 \amp -4 \\
	      0 \amp 0 \amp -4 \\
	      \end{array}\right] = U.
	      </me>
	      Find the matrices <m>L_1</m>, <m>L_2</m>, and <m>L_3</m>
	      that perform these row replacement operations so that
	      <m>L_3L_2L_1 A = U</m>.  </p></li>
	    
	      <li><p> Explain why the matrix product <m>L_3L_2L_1</m> is
	      invertible and use this fact to write <m>A = LU</m>.  What
	      is the matrix <m>L</m> that you find?  Why do you think we
	      denote it by <m>L</m>?
	      <sage language="python">
		<input>
		</input>
	      </sage>
	      
	      </p></li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p>
	    Performing the matrix multiplication, we find that
	    <me>
	      RA = 
	      \left[\begin{array}{rrr}
	      1 \amp 0 \amp 0 \\
	      -2 \amp 1 \amp 0 \\
	      0 \amp 0 \amp 1 \\
	      \end{array}\right]
	      \left[\begin{array}{rrr}
	      1 \amp 2 \amp 1 \\
	      2 \amp 0 \amp -2 \\
	      -1 \amp 2 \amp -1 \\
	      \end{array}\right]
	      =
	      \left[\begin{array}{rrr}
	      1 \amp 2 \amp 1 \\
	      0 \amp -4 \amp -4 \\
	      -1 \amp 2 \amp -1 \\
	      \end{array}\right]\text{.}
	    </me>
	  </p></li>

	  <li><p> We know that <m>R</m> is invertible because
	  it is a lower triangular matrix whose diagonal entries are
	  all 1.  We find that
	  <m>
	    R^{-1}
	    = \left[\begin{array}{rrr}
	    1 \amp 0 \amp 0 \\
	    2 \amp 1 \amp 0 \\
	    0 \amp 0 \amp 1 \\
	    \end{array}\right]
	    </m>, which can be verified.
	  </p></li>

	  <li><p>
	    But we can see this in another way as well.
	    The replacement operation is reversible;  that is, 
	    multiplying the first row by <m>-2</m> and adding it to the
	    second row can be undone by multiplying the first row by
	    <m>2</m> and adding it to the second row.  
	  </p></li>

	  <li><p> We find that
	  <me>
	    S = \begin{bmatrix}
	    1 \amp 0 \amp 0 \\
	    0 \amp 4 \amp 0 \\
	    0 \amp 0 \amp 1 \\
	    \end{bmatrix},~~~
	    S^{-1} = \begin{bmatrix}
	    1 \amp 0 \amp 0 \\
	    0 \amp \frac14 \amp 0 \\
	    0 \amp 0 \amp 1 \\
	    \end{bmatrix}.
	  </me>
	  This makes sense because scaling a row by <m>4</m> can be
	  undone by scaling the same row by <m>\frac14</m>.
	  </p></li>

	  <li><p> We find that
	  <me>
	    P = \begin{bmatrix}
	    0 \amp 0 \amp 1 \\
	    0 \amp 1 \amp 0 \\
	    1 \amp 0 \amp 0 \\
	    \end{bmatrix}.
	  </me>
	  Moreover, <m>P=P^{-1}</m> because we can undo the
	  interchange operation by repeating it.
	  </p></li>

	  <li><p>
	    Continuing with the Gaussian elimination algorithm, we
	    have <m>L_1 = R</m>, as above, 
	    <me>
	      L_2 = \left[\begin{array}{rrr}
	      1 \amp 0 \amp 0 \\
	      0 \amp 1 \amp 0 \\
	      1 \amp 0 \amp 1 \\
	      \end{array}\right],~~~
	      L_3 = \left[\begin{array}{rrr}
	      1 \amp 0 \amp 0 \\
	      0 \amp 1 \amp 0 \\
	      0 \amp 1 \amp 1 \\
	      \end{array}\right]\text{.}
	    </me>
	    we then have
	    <m>L_3L_2L_1A = U</m>.
	  </p></li>

	  <li><p>
	    Each of the matrices <m>L_1</m>, <m>L_2</m>, and
	    <m>L_3</m> is invertible so their product will be as
	    well.  Since <m>(L_3L_2L_1)A = U</m>, we have <m>A =
	    (L_3L_2L_1)^{-1}U</m>.  
	    Moreover,
	    <m>L = (L_3L_2L_1)^{-1} = L_1^{-1}L_2^{-1}L_3^{-1}</m>
	    gives
	    <m>L=\left[\begin{array}{rrr}
	    1 \amp 0 \amp 0 \\
	    2 \amp 1 \amp 0 \\
	    -1 \amp -1 \amp 1 \\
	    \end{array}\right]</m>.  Notice that this matrix is lower
	    triangular so we call it <m>L</m>.
	  </p></li>

	</ol></p>
      </solution>
	      
    </activity>

    <p>
      <idx> matrix, elementary </idx>
      The following are examples of matrices, known as 
      <term>elementary matrices</term>,
      that perform the row operations on a matrix having three rows.
      <dl>
	<li><title> Replacement </title>
	<p> Multiplying the second row by 3 and adding it to the third
	row is performed by
	<me>
	  L = \begin{bmatrix}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  0 \amp 3 \amp 1 \\
	  \end{bmatrix}.
	</me>  We often use <m>L</m> to describe these matrices because
	they are lower triangular.
	</p></li>
	<li><title> Scaling </title>
	<p> Multiplying the third row by 2 is performed by
	<me>
	  S = \begin{bmatrix}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 2 \\
	  \end{bmatrix}.
	</me>
	</p></li>

	<li><title> Interchange </title>
	<p> Interchanging the first two rows is performed by
	<me>
	  P = \begin{bmatrix}
	  0 \amp 1 \amp 0 \\
	  1 \amp 0 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  \end{bmatrix}.
	</me>
	</p></li>
      </dl>
    </p>
    
    <example>
      <statement>
	<p> Suppose we have 
	<me>
	  A = \left[\begin{array}{rrr}
	  1 \amp 3 \amp -2 \\
	  -3 \amp -6 \amp 3 \\
	  2 \amp 0 \amp -2 \\
	  \end{array}\right].
	</me>
	For the forward substitution phase of Gaussian elimination, we
	perform a sequence of three replacement operations.
	The first replacement operation multiplies the first row
	by <m>3</m> and adds the result to the second row.  We can perform
	this operation by multiplying <m>A</m> by the lower triangular
	matrix <m>L_1</m> where 
	<me>
	  L_1A =
	  \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  3 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  \end{array}\right]
	  \left[\begin{array}{rrr}
	  1 \amp 3 \amp -2 \\
	  -3 \amp -6 \amp 3 \\
	  2 \amp 0 \amp -2 \\
	  \end{array}\right]
	  = 
	  \left[\begin{array}{rrr}
	  1 \amp 3 \amp -2 \\
	  0 \amp 3 \amp -3 \\
	  2 \amp 0 \amp -1 \\
	  \end{array}\right].
	  </me>
	</p>

	<p> The next two replacement operations are performed by the
	matrices
	<me>
	  L_2 = \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  -2 \amp 0 \amp 1 \\
	  \end{array}\right],
	  \hspace{24pt}
	  L_3 = \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  0 \amp 1 \amp 0 \\
	  0 \amp 2 \amp 1 \\
	  \end{array}\right]
	</me>
	so that
	<me>L_3L_2L_1A = U = \begin{bmatrix}
	1 \amp 3 \amp -2 \\
	0 \amp 3 \amp -3 \\
	0 \amp 0 \amp -3 \\
	\end{bmatrix}.
	</me>
	</p>
	
	<p> Notice that the inverse of <m>L_1</m> has the simple form:
	<me>
	  L_1 = \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  3 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  \end{array}\right],
	  \hspace{24pt}
	  L_1^{-1} = \left[\begin{array}{rrr}
	  1 \amp 0 \amp 0 \\
	  -3 \amp 1 \amp 0 \\
	  0 \amp 0 \amp 1 \\
	  \end{array}\right]
	  </me>.
	  This says that if we want to undo the operation of multiplying
	  the first row by <m>3</m> and adding to the second row, we should
	  multiply the first row by <m>-3</m> and add it to the second row.
	  That is the effect of <m>L_1^{-1}</m>.
	</p>
	
	<p> Notice that we now have <m>L_3L_2L_1A = U</m>, which gives
	<me>
	  \begin{aligned}
	  (L_3L_2L_1)A \amp = U \\
	  (L_3L_2L_1)^{-1}(L_3L_2L_1)A \amp = 
	  (L_3L_2L_1)^{-1}U \\
	  A \amp = (L_3L_2L_1)^{-1}U = LU\\
	  \end{aligned}
	</me>
	where <m>L</m> is the lower triangular matrix
	<me> L = (L_3L_2L_1)^{-1}=\begin{bmatrix}
	1 \amp 0 \amp 0 \\
	-3 \amp 1 \amp 0 \\
	2 \amp -2 \amp 1 \\
	\end{bmatrix}.
	</me>
	This way of writing <m>A=LU</m> as the product of a lower and
	an upper triangular matrix is known as an <m>LU</m>
	factorization of <m>A</m>, and its usefulness will be explored
	in <xref ref="sec-gaussian-revisited" />.
	</p>

      </statement>
    </example>


  </subsection>

  <subsection>
		<title> Summary </title>
		<p> In this section, we learned that the elementary row operations of replacement, scaling,
	and interchange can be performed via multiplication by <term>elementary matrices</term>. 
			<ul>
				<li>
					<p> The elementary matrices for replacement are <term>lower triangular</term>. </p>
				</li>
				<li>
					<p> The elementary matrices for scaling are <term>diagonal</term>. </p>
				</li>
				<li>
					<p>
						Square triangular matrices are invertible if and only if all the
						diagonal entries are non-zero.
						Furthermore, their inverses are easily computed via back substitution.
					</p>
				</li>
			</ul>
		</p>

	</subsection>

  <xi:include href="exercises/exercises3-1b.ptx" />
  
</section>



