<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-least-squares"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Least squares methods </title>

  <introduction>
    <p>
      Suppose we collect some data when performing an experiment and
      plot it as shown on the left of <xref ref="lst-squares-intro"
      />.  Notice that there is no line on which all the points lie;
      in fact, it would be surprising if there were since we can
      expect some uncertainty in the measurements recorded.  There
      does, however, appear to be a line, as shown on the right, on which
      the points <em> almost</em> lie.
    </p>
    
    <figure xml:id="lst-squares-intro">
      <caption>
	A collection of points and a line approximating the
	linear relationship implied by them.
      </caption>
      <sidebyside widths="45% 45%">
	<image source="images/lst-squares-1" />
	<image source="images/lst-squares-2" />
      </sidebyside>
    </figure>

    <p>
      In this section, we'll explore how the techniques developed in
      this chapter enable us to find the line that best approximates
      the data.  
	  </p>
	 
	  <p>
	  More generally, that whenever
      <m>A\xvec=\bvec</m> is inconsistent, we can instead seek 
	  an approximate solution -- a solution to 
      <m>A\xvec=\bhat</m> where <m>\bhat</m> is a close as possible 
	  to <m>\bvec</m>.
      Orthogonal projection gives us just the right tool for doing
      this.
    </p>

	
    
    <exploration>
      <statement>
      <p>
	<ol marker="a.">
	  <li>
	    <p> Is there a solution to the equation
	    <m>A\xvec=\bvec</m> where <m>A</m> and <m>\bvec</m> are such
	    that 
	    <me>
	      \begin{bmatrix}
	      1 \amp 2 \\
	      2 \amp 5 \\
	      -1 \amp 0 \\
	      \end{bmatrix}
	      \xvec = \threevec5{-3}{-1}
	    </me>.
	    </p>
	    <sage language="python">
	      <input>
import numpy as np
A = np.array([1, 2, 2, 5, -1, 0]).reshape(3,2)
b = np.array([5, -3, -1)]
	      </input>
	    </sage>
	  </li>

	  <li>
	    <p>
	      We know that <m>\threevec12{-1}</m> and
	      <m>\threevec250</m> form a basis for <m>\col(A)</m>.  Find
	      an orthogonal basis for <m>\col(A)</m>. 
	    </p>
	  </li>
	  
	  <li>
	    <p>
	      Find the orthogonal projection <m>\widehat\bvec</m>
	      of <m>\bvec</m> onto <m>\col(A)</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Explain why the equation <m>A\xvec=\widehat\bvec</m>
	      must be consistent and then find its solution.
	    </p>
	  </li>
	  
	</ol>
      </p>
      </statement>
      <solution>
	<p>
	  <ol marker="a">
	    <li>
	      <p>
		The reduced row echelon form
		<me>
		  \left[
		  \begin{array}{cc|c}
		  1 \amp 2 \amp 5 \\
		  2 \amp 5 \amp -3 \\
		  -1 \amp 0 \amp -1\\
		  \end{array}
		  \right]
		  \sim
		  \left[
		  \begin{array}{cc|c}
		  1 \amp 0 \amp 0 \\
		  0 \amp 1 \amp 0 \\
		  0 \amp 0 \amp 1 \\
		  \end{array}
		  \right]
		</me>
		shows that there is no solution.
	      </p>
	    </li>
	    <li>
	      <p>
		Applying Gram-Schmidt, we find an orthogonal basis
		consisting of <m>\wvec_1=\cthreevec12{-1}</m> and
		<m>\wvec_2=\threevec012</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The projection formula gives <m>\bhat =
		\cthreevec0{-1}{-2}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The equation is consistent because <m>\bhat</m> is in
		<m>\col(A)</m>.  We find the solution
		<m>\xvec=\twovec2{-1}</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
    </exploration>

  </introduction>

  <subsection>
    <title> A first example </title>

    <p>
      When we've encountered inconsistent systems in the past, we've
      simply said there is no solution and moved on.  The preview
      activity, however, shows how we can find approximate solutions
      to an inconsistent system: if there are no solutions to
      <m>A\xvec = \bvec</m>, we instead solve the consistent system
      <m>A\xvec = \bhat</m>, the orthogonal projection of <m>\bvec</m>
      onto <m>\col(A)</m>.  As we'll see, this solution is, in a
      specific sense, the best possible.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose we have three data points <m>(1,1)</m>,
	  <m>(2,1)</m>, and <m>(3,3)</m> and that we would like to find a
	  line passing through them.
	  <ol marker="a.">
	    <li>
	      <p>
		Plot these three points in <xref ref="fig-ls-empty"
		/>.  Are you able to draw a line that passes through
		all three points? 
		  </p>
		<figure xml:id="fig-ls-empty">
		  <caption> Plot the three data points here. </caption>
		  <sidebyside width="50%">
		    <image source = "images/empty-ls" />
		  </sidebyside>
		</figure>
	    </li>
	    
	    <li>
	      <p>
		Remember that the equation of a line can be written
		as 
		<me>y = mx + b</me> 
		where <m>m</m> is the slope and <m>b</m> is the <m>y</m>-intercept.  
		Statisticans prefer the notation <me> y = \beta_0 + \beta_1 x</me>,
		and we're going to adopt statistical preferences for most of the remainder of 
		this chapter since least squares is such an important method in 
		statistics.
		  </p>

		  <p>
		To begin, we will try to find <m>\beta_0</m> and
		<m>\beta_1</m> so that the three points lie on the line with 
		equation  <m>y = \beta_0 + \beta_1 x</m>.
		The first data point <m>(1,1)</m> gives an equation
		for <m>\beta_0</m> and <m>\beta_1</m>. 
		In particular, we know that when
		<m>x=1</m>, then <m>y=1</m> so we have
		<m>\beta_0 + \beta_1(1) = 1</m> or <m>\beta_0 + \beta_1 = 1</m>.  
		  </p>
		  <p>
		Use the other two data points to create a linear system describing 
		<m>\beta_0</m> and <m>\beta_1</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		We have obtained a linear system having three equations,
		one from each data point, for the two unknowns <m>\beta_0</m> and <m>\beta_1</m>.
		Identify a matrix <m>X</m> and vector <m>\betavec</m> so
		that the system has the form <m>\yvec = X \betavec</m>, where
		<m>\betavec=\ctwovec{\beta_0}{\beta_1}</m>.  
	      </p>

	      <p>
		Notice that the unknown vector <m>\betavec=\ctwovec {\beta_0}{\beta_1}</m>
		specifies the intercept and slope of the line that we seek.
	      </p>
	    </li>

	    <li>
	      <p>
		Is there a solution to this linear system?  How does
		this question relate to your attempt to draw a line
		through the three points above?
	      </p>
		<sage language="python">
		  <input>

		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Since this system is inconsistent, we know that
		<m>\yvec</m> is not in the column space <m>\col(X)</m>.
		Find an orthogonal basis for <m>\col(X)</m> and use it
		to find the orthogonal projection <m>\yhat = \proj{\yvec}{\col(X)}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Since <m>\yhat</m> is in <m>\col(X)</m>, the
		equation <m>X\betavec = \yhat</m> is consistent.  Find its
		solution, which we will denote <m>\betahat = \ctwovec{\hat\beta_0}{\hat\beta_1}</m>,
		and sketch the line <m>y=\hat{\beta}_0 + \hat{\beta}_1 x</m> in 
		<xref ref="fig-ls-empty" />.  
		This line is called the <term>least squares regression line</term>.
		<idx><h>least squares regression</h></idx>
		<idx><h>regression</h><see>least squares regression</see></idx>
		That "hat" on <m>\betahat</m> indicates that these coefficients (most likely) 
		do not fit the data exactly, but come as close as we can to doing so (in the 
		sense of minimizing the distance between <m>\yhat = X\betahat</m> and <m>\yvec</m>).
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		It's not
		possible to draw a line through all three points.
	      </p>
	    </li>
	    <li>
	      <p>
		We have the equations
		<md>
			<mrow>
			  \beta_0 + \beta_1 \amp {}={} 1
			</mrow>
			<mrow>
			  \beta_0 + 2\beta_1 \amp {}={} 1
			</mrow>
			<mrow>
			  \beta_0 + 3\beta_1 \amp {}={} 3
			</mrow>
		  </md>
	      </p>
	    </li>
	    <li>
	      <p>
			We have <m>X = \begin{bmatrix}
				1 \amp 1 \\
				1 \amp 2 \\
				1 \amp 3 \\
				\end{bmatrix}</m> and <m>\yvec=\threevec113</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		This linear system is inconsistent.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\yhat=\threevec{2/3}{5/3}{8/3}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\betahat=\twovec{-1/3}1</m>.
		This line is shown in <xref
		ref="fig-best-fit-line-ans" />.
	      </p>
	      <figure xml:id="fig-best-fit-line-ans">
		<caption>
		  The line that best approximates the three data
		  points.
		</caption>
		<sidebyside width="50%">
		  <image source="images/line-regress-2" />
		</sidebyside>
	      </figure>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		After plotting the points, we see that it's not
		possible to draw a line through all three points.
	      </p>
	    </li>
	    <li>
	      <p>
		We have the equations
		<md>
		  <mrow>
		    \beta_0 + \beta_1 \amp {}={} 1
		  </mrow>
		  <mrow>
		    \beta_0 + 2\beta_1 \amp {}={} 1
		  </mrow>
		  <mrow>
		    \beta_0 + 3\beta_1 \amp {}={} 3
		  </mrow>
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		We have <m>X = \begin{bmatrix}
		1 \amp 1 \\
		1 \amp 2 \\
		1 \amp 3 \\
		\end{bmatrix}</m> and <m>\yvec=\threevec113</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Finding the reduced row echelon form of the associated
		augmented matrix tells us this is an inconsistent
		system.  Since a solution would describe a line
		passing through the three points, we should expect
		this.
	      </p>
	    </li>
	    <li>
	      <p>
		Applying Gram-Schmidt gives us the orthogonal basis
		<m>\wvec_1=\threevec111</m> and <m>\wvec_2 =
		\threevec{-1}01</m>.  Projecting <m>\yvec</m> onto
		<m>\col(X)</m> gives
		<m>\yhat=\threevec{2/3}{5/3}{8/3}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Solving the equation <m>X\betavec = \yhat</m> gives
		<m>\betahat=\ctwovec{-1/3}1</m>, which describes a line
		having vertical intercept <m>b=-1/3</m> and the slope
		<m>=1</m>.  This line is shown in <xref
		ref="fig-best-fit-line" />.
	      </p>
	      <figure xml:id="fig-best-fit-line">
		<caption>
		  The line that best approximates the three data
		  points.
		</caption>
		<sidebyside width="50%">
		  <image source="images/line-regress-2" />
		</sidebyside>
	      </figure>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>


    <p>
      This activity illustrates the idea behind a technique known as
	  <idx><h>least squares regression</h></idx>
      <term>least squares regression</term>, which we have been working
      toward throughout this chapter.  If the data points are denoted as
      <m>(x_i, y_i)</m>, we construct the matrix <m>X</m> and vector
      <m>\yvec</m> as
      <me>
	A =
	\begin{bmatrix}
	1 \amp x_1 \\
	1 \amp x_2 \\
	1 \amp x_3 \\
	\end{bmatrix},\hspace{24pt}
	\yvec = \threevec{y_1}{y_2}{y_3}
      </me>.
      With the vector <m>\betavec=\ctwovec{\beta_0}{\beta_1}</m> 
	  representing the line with equation <m>y = \beta_0 + \beta_1 x</m>, we see that
      the equation <m>\yvec = X\betavec</m> describes a line passing
      through all the data points.  In our activity, it is visually
      apparent that there is no such line, which agrees with
      the fact that the equation <m>\yvec = X\betavec</m> is inconsistent.
    </p>

    <p>
      Remember that <m>\yhat = \proj{\yvec}{\col(X)}</m> 
	  is the closest vector in <m>\col(X)</m> to <m>\yvec</m>.  
	  Therefore, when we solve the equation <m>X\betavec=\yhat</m>, we are finding the vector
      <m>\betahat</m> so that 
	  <m>\yhat = X\betahat = \threevec{\hat{\beta}_0+ \hat{\beta_1} x_1}{\hat{\beta}_0+ \hat{\beta_1} x_2}{\hat{\beta}_0+ \hat{\beta_1} x_3}</m> 
	  is as close to <m>\yvec=\threevec{y_1}{y_2}{y_3}</m>
      as possible.  Let's think about what this means within the
      context of this problem.
    </p>

    <p>
      The difference 
	    <me>\yvec-\yhat =
      \threevec{y_1-(\hat{\beta_0} +\hat{\beta_1} x_1)}{y_2-(\hat{\beta_0} + 
	                 \hat{\beta_1} x_2)}{y_3-(\hat{\beta_0} +\hat{\beta_1} x_3)}
		</me> 
	  so that the square of the distance between <m> \yhat = X\betahat</m> and
      <m>\yvec</m> is
      <md>
	<mrow>
	  \len{\yvec - \yhat}^2 \amp =
	</mrow>
	<mrow>
	  \amp 
	  \left(y_1-(\hat{\beta}_0 +\hat{\beta}_1 x_1)\right)^2 + 
	  \left(y_2-(\hat{\beta}_0 +\hat{\beta}_1 x_2)\right)^2 +
	  \left(y_3-(\hat{\beta}_0 +\hat{\beta}_1 x_3)\right)^2
	</mrow>
      </md>.
	  <idx><h>least squares regression</h></idx>
      Our approach finds the values for <m>\beta_0</m> and <m>\beta_1</m> that
      make this sum of squares as small as possible, which is
      why we call this a <term>least squares</term> problem.
    </p>

    <p>
		Usually the least squares regression line does not pass through all of the data points.
		Statisticians call <m> y_i - \hat{y}_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i)</m> the 
		<term>residual</term> for observation <m>i</m>.
        As shown in <xref ref="fig-least-squares-def" />,  
		residuals measure the vertical distance between the observed response  
		value <m>y_i</m> and the predicted response value <m>\hat y_i</m>:
		<me>
			\mbox{residual} = \mbox{observed} - \mbox{expected}
		</me>.
	  Seen in this way, the square of the distance <m>\len{\yvec-\hat y}^2</m> is a
      measure of how much the line defined by the vector <m>\betahat</m>
      misses the data points.  The solution to the least squares
      problem is the line that misses the data points by the smallest
      amount possible (when measured in this way).
    </p>

    <figure xml:id="fig-least-squares-def">
      <caption>
	The solution of the least squares problem and the vertical
	distances between the line and the data points.
      </caption>
      <sidebyside width="50%">
	<image source = "images/line-regress-1" />
      </sidebyside>
    </figure>
</subsection>

<subsection xml:id="subsec-linear-model-framework">
	<title>The linear model framework</title>
	<idx><h>linear model</h></idx>
	<p>
		The previous example is an example of <term>simple linear regression</term>. In simple linear 
		regression there is a single quantitative predictor and the model proposes a linear 
		relationship between the explanatory variable and the response.
		<idx><h>linear model</h></idx>
		Least squares regression can be used with multiple explanatory variables just as easily as with one 
		-- at least if we are willing to let the computer take care of the tedious arithmetic involved.
		In this section we describe a general framework called <term>linear models</term>.
		Linear models and their generalizations are arguably the most important and commonly used method 
		of data analysis.
	</p>
	<p>
		Suppose we are looking for a relationship of the form 
		<me>
			y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
		</me>.
		We mean for this to hold for every subject in the data, each of whom has (likely different) 
		values of <m>y</m> and the <m>x_i</m>'s.  But we want the values of the <m>\beta's </m> to be the 
		same for everyone.  Let's rewrite our equation to emphasize that we are really dealing 
		with vectors here. 
		<me>
			\yvec = \beta_0 \onevec + \beta_1 \xvec_1 + \beta_2 \xvec_2 + \cdots + \beta_p \xvec_p
		</me>.
		Notice that we snuck in a vector of 1's to make the "constant" term look like all the others.
	</p>

	<p>
		Now let's go one more step and express this model using matrices. 
		<idx><h>data matrix</h></idx>
		The vector of ones 
		and the <m>\xvec_i</m> vectors form the columns of the <m>n \by (p+1)</m> 
		matrix <m>X</m>.  
		This matrix is usually refered to as the <term>model matrix</term>.
		<idx><h>coefficients</h><h>of a regression equation</h></idx>
		<idx><h>model matrix</h></idx>
		The <term>coefficients</term> <m>\beta_i</m> are arranged into a column vector. 
		Then the entire relationship is expressed as a simple equation of the form 
		<m>A\xvec = \bvec</m>, but with different letters. And typically statisticians prefer to 
		swap the left and right sides of the equation as well. So the model is exprssed like this:

		<md>
			<mrow>
			X \amp = \begin{bmatrix} \onevec \; \xvec_1 \; \xvec_2 \; \cdots \; \xvec_p \end{bmatrix} \; , \quad
			\betavec = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix}
			</mrow>
			<mrow>
				\yvec \amp = X \betavec
			</mrow>
		</md>.
		This equation will almost never have an exact solution because in typical applications  
		<m>X</m> will have many more rows than columns.
	</p>

	<p>
		Since there is typically no exact solution, we seek an approximate solution, a solution to 
		<me>
			\yhat = X \betahat\; \mbox{where} \; \yhat = \proj{\yvec}{\col(X)}
		</me>. 
		<idx><h>model space</h></idx>
		In this context, <m>\col(X)</m> is called the <term>model space</term>. It will be important below  to 
		note that this means that 
		<me>\yvec - \yhat \mbox{ is orthogonal to (every vector in) the model space}</me>.
	</p>

	<p>
		Whether we express our equation as <m>A \xvec = \bvec</m>, as we have mostly done to this point, 
		or as <m>\yvec = X \betavec</m>, as we will typically do in statistical applications, or using 
		some other letters, the linear algebra is the same (and familiar): 
		orthogonal projection, solving sytems of linear equations, etc.  
	</p>

	<example>
		<statement>
			<p>
				Add example of setting up a multiple regression problem.
				Include at least a binary categorical predictor.
			</p>
		</statement>
	</example>

	<p>
	We conclude this section with a note about the intercept term.
	</p>

	<note>
	<title>Models without an intercept</title>	
	<p> It is possible to fit models without an intecept term.  In this case the column of 1's will be  
		omitted from the model matrix <m>X</m>. Algebraically, a few things, like the defnition and 
		interpretation of <m>R^2</m> below do not work out as well in that case. And statistically, omitting 
		the intercept makes a strong assumption about the nature of the relationship.  In most statistical 
		software, the default it to always include an intercept, but there are options to fit models without 
		the intercept term if so desired.
	</p>
	</note>

  </subsection>

  <subsection>
    <title> Solving least squares problems </title>
    
    <p> Now that we've discussed least squares approximate solutions to 
		<m>A \xvec = \bvec</m> and seen an important application of this 
		method in linear models, usually expressed as <m> \yvec = X \betavec</m>,
		it is time to turn our attention to some of the details involved in solving 
		least squares problems. We'll continue with the statistical notation for this.
    </p>

	<p> We already know one way to solve a least squares problem <m>\yvec = X \betavec</m>, namely
		<ol>
			<li>
				<title>Project <m>y</m> into the model space</title>
				<p>
					Compute <m>\yhat = \proj{\yvec}{\col(X)}</m>.
				</p>
			</li>
			<li>
				<title>Solve the new equation <m>\yhat = X \betavec</m> for <m>\betavec</m></title>
				<p>
					Because <m>\yhat \in \col(X)</m>, we know this equation is consistent.
					If the columns of <m>X</m> are linearly independent (as will usually be the case
					in linear model applications),
					then there is exactly one solution.
					We denote the solution to this  as <m>\betahat</m>. The entries in 
					<m>\betahat</m> are called the (estiamted) coeffients of the model.
				</p>
				<p>Because we can measure how close <m>\yhat</m> is to <m>\yvec</m> using an expression 
					that involves a sum of squares, and <m>\betahat</m> makes this expression as small as possible,
					<m>\betahat</m> is called a <term>least squares approximate solution</term> 
					to the original equation <m>\yvec = X\betavec</m>.
				</p>
			</li>
		</ol>
	</p>

    <p>
		That is the method we have outlined above. But there are other methods 
		that are usually used in practice, because they are more efficient and more 
		stable numerically. 
	</p>
	
	<p>
		We begin by describing a method 
	  for finding <m>\betahat</m> that does
      not involve first finding the orthogonal projection <m>\yhat</m>.

      Remember that <me>\yhat = \proj{\yvec}{\col(X)}</me>, so
      <m>\yvec - \yhat</m> is orthogonal to <m>\col(A)</m>.  
	  In other words, <m>\yvec - \yhat</m> is in the
      orthogonal complement <m>\col(A)^\perp</m>, which <xref
      ref="prop-col-orthog" /> tells us is the same as
      <m>\nul(X^{\transpose})</m>.  Since <m>\yvec - \yhat</m> is in
      <m>\nul(X^{\transpose})</m>, it follows that
      <me>
	X^{\transpose}(\yvec -\yhat) = \zerovec
      </me>.
	  This is just another way of writing down that <m>\yvec - \yhat</m> is orthogonal to 
	  each column of <m>X</m>.
	</p>

	<p>
      Because the least squares approximate
      solution is the vector <m>\betahat</m> such that 
	  <m>X\betahat = \yhat</m>, we can rearrange this equation to see that
      <md>
	<mrow>
	  X^{\transpose}(X\betahat - \yvec) \amp = \zerovec
	</mrow>
	<mrow>
	  X^{\transpose}X\betahat - X^{\transpose}\yvec \amp = \zerovec
	</mrow>
	<mrow>
	  X^{\transpose}X\betahat \amp = X^{\transpose}\yvec
	</mrow>
      </md>.
      <idx> <h>normal equation</h> </idx>
      This equation is called the <term>normal equation</term>, and we
      have the following proposition.
    </p>

    <proposition>
      <statement>
	<p> If the columns of <m>X</m> are linearly independent, then
	there is a unique least squares approximate solution
	<m>\betahat</m> to the equation <m>X\betahat=\yvec</m> given by the
	normal equation
	<me>
	  X^{\transpose}X\betahat = X^{\transpose}\yvec
	</me>.
	</p>
      </statement>
    </proposition>

	<p>
	The next example demonstrates how we can use the normal equation to 
	find the least squares approximate solution.
	</p>

    <example xml:id="example-toy-normal-equation">
      <p>
	Consider the equation
	<me>
		\begin{array}{cccc}
	  \threevec{16}{-1}7 \amp = \amp
	  \begin{bmatrix}
	  2 \amp 1 \\
	  2 \amp 0 \\
	  -1 \amp 3 \\
	  \end{bmatrix}
	  \amp
	  \betavec
	  \\
	  \yvec \amp = \amp X \amp \betavec  
	  \end{array}
	</me>.
	Since this equation is inconsistent, we will find the least squares
	approximate solution <m>\betahat</m> by solving the normal
	equation <m>X^{\transpose}X\betahat = X^{\transpose}\yvec</m>, which has the form
	<md>
		<mrow>
	  X^{\transpose}X\betahat \amp =  X^{\transpose}\yvec 
		</mrow>
		<mrow> 
	  \begin{bmatrix} 9 \amp -1 \\ -1 \amp 10 \\ \end{bmatrix} \betahat \amp = \twovec{23}{37} 
		</mrow>
	</md>.
	Solving this yields <m>\betahat=\twovec34</m>.
      </p>
    </example>

	<p>
	You may wonder why the approach in <xref ref="example-toy-normal-equation"/> is better than 
	the original appraoch.  Here's one reason why.  Suppose we have a larger example and <m>X</m> is
	<m>n\by m</m> with <m>n</m> much larger than <m>p</m>, as is often the case. Then 
	<m>X^{\transpose} X</m> is <m> p\by p</m>, which is small. But <m> X  X^\transpose</m> 
	is <m>n \by n</m>, which is large -- much larger than <m>X</m>.  So computing 
	<m>\yhat = \proj{\yvec}{\col(X)} = X X^{\transpose} \yvec</m> is expensive.
	Working with <m>X^\transpose \yvec</m> is comparatively much less computationally intensive.
	</p>

    <activity xml:id="activity-crickets">
      <statement>
	<p>
	  The rate at which a cricket chirps is related to the outdoor
	  temperature, as reflected in some experimental data that we'll
	  study in this activity.  The
	  chirp rate <m>C</m> is expressed in chirps per second while the
	  temperature <m>T</m> is in degrees Fahrenheit.  Evaluate the
	  following cell to load the data:
	</p>

	  <sage language="python">
	    <input>
import pandas as pd 
import numpy as np 
import seaborn.objects as so 
url = 'https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/crickets.csv'
Crickets = pd.read_csv(url)
print(Crickets)
so.Plot(data = Crickets, x = "Chirps", "Temperature").add(so.Dot()).show()
	    </input>
	  </sage>

	<p>
	  We would like to represent this relationship by a linear
	  function
	  <me>
	    \beta_0 + \beta_1 C = T
	  </me>.
	  <ol marker="a.">
	    <li>
	      <p>
		Use the first data point <m>(C_1,T_1)=(20.0,88.6)</m>
		to write an equation involving <m>\beta_0</m> and
		<m>\beta_1</m>.
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Suppose that we represent the coefficients using a vector
		<m>\betavec = \twovec{\beta_0}{\beta_1}</m>.  
		Use the 15 data points to create the matrix
		<m>X</m> and vector <m>\yvec</m> 
		so that the linear system
		<m>\yvec = X \betavec</m> describes the desired relationship.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Write the normal equations 
		<me>X^{\transpose}X\betahat = X^{\transpose}\yvec</me>; 
		that is, find the matrix <m>X^{\transpose}X</m> and the
		vector <m>X^{\transpose}\yvec</m>.
	      </p>
	    </li>

	    <li>
	      <p> Solve the normal equations to find <m>\betahat</m>, the
	      least squares approximate solution to the equation
	      <m>\yvec = X \betavec</m>.  Call your solution <m>\betahat</m>.
	      </p>

	      <sage language="python">
		<input>
		</input>
	      </sage>

	      <p>
		What are the values of <m>\beta_0</m> and <m>\beta_1</m>
		that you found?
	      </p>
	    </li>

	    <li>
	      <p>
		If the chirp rate is 22 chirps per second, what is
		your prediction for the temperature?
	      </p>

	      <p>
			Plot the data and your least squares regression line.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>
	    
	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>\beta_0 + 20.0\beta_1 = 88.6</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>X</m> is the <m>15\by2</m> matrix whose first
		column consists only of 1's and whose second column is
		the vector of chirp rates.  The vector <m>\yvec</m> is
		the vector of temperatures.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>X^{\transpose}X=\begin{bmatrix}
		15.0 \amp 248.5 \\
		248.5 \amp 4157.9 \\
		\end{bmatrix}</m> and <m>X^{\transpose}\yvec =
		\twovec{1190.2}{19857.7}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\xhat = \twovec{\beta_0}{\beta_1} =
		\twovec{22.8}{3.4}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>97.9</m> degrees.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have the equation <m>\beta_0 + 20.0\beta_1 =
		88.6</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>X</m> is the <m>15\by2</m> matrix whose first
		column consists only of 1's and whose second column is
		the vector of chirp rates.  The vector <m>\yvec</m> is
		the vector of temperatures.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>X^{\transpose}X=\begin{bmatrix}
		15.0 \amp 248.5 \\
		248.5 \amp 4157.9 \\
		\end{bmatrix}</m> and <m>X^{\transpose}\yvec =
		\twovec{1190.2}{19857.7}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\betahat = \twovec{\beta_0}{\beta_1} =
		\twovec{22.8}{3.4}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The predicted temperature is <m>\beta_0 + 22\beta_1 = 97.9</m> degrees.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
		<!-- <xref ref="activity-crickets" />
      demonstrates an approach, called <term>linear regression</term>, 
	  in which a collection of data is modeled using
      a linear function found by solving a least squares problem.
		<idx><h>least squares regression</h></idx>
		<idx><h>linear regression</h></idx>
		<idx><h>linear regression</h><seealso>least squares regression</seealso></idx> -->
      Once we have the linear function that best fits the data,
      we can make predictions about situations that we haven't
      encountered in the data.  
      If we're going to use our function to make predictions, it's
      natural to ask how much confidence we have in these
      predictions.  This is a statistical question that leads to a
      rich and well-developed theory, which we won't explore in much
      detail here.  However, there is one simple measure of how well our
      linear function fits the data that is known as the coefficient
      of determination and denoted by <m>R^2</m>.
    </p>

    <p>
      We have seen that the predicted values from our model are given by 
	  <m>\yhat = X \betahat</m> and that the square of the distance
      <m>\len{\yvec-\yhat}^2</m> measures the amount by which the
      line fails to pass through the data points.  When the line is close
      to the data points, we expect this number to be small.  However,
      the size of this measure depends on the scale of the data.  For
      instance, the two lines shown in <xref
      ref="fig-regression-scale" /> seem to fit the data equally well,
      but <m>|\yvec-\yhat|^2</m> is 100 times larger on
      the right.
    </p>

    <figure xml:id="fig-regression-scale">
      <caption>
	The lines appear to fit equally well in spite of the fact that
	<m>\len{\yvec-\yhat}^2</m> differs by a factor of 100.
      </caption>
      <sidebyside widths="45% 45%">
	<image source="images/line-regress-1" />
	<image source="images/line-regress-10" />
      </sidebyside>
    </figure>

	<p> We can create a measure of fit that is indpendent of scale if we consider the 
		relationship among three important vectors:
		<ul>
			<li>
				<p>
					<m> \yvec - \ybar</m> 
					holds the differences between the observed response values and their mean value. 
					The (square of the) length of this vector is a measure of the total 
					variability in the response variable.
				</p>
			</li>
			<li>
				<p>
					<m> \yvec - \yhat</m> 
					holds the differences between the observed response values and model fitted values. 
					These differences are called <term>residuals</term>
					This vector is orthogonal to the <term>model space </term>, <m>\col(X)</m>.
				</p>
			</li>
			<li>
				<p>
					<m> \yhat - \ybar</m>
					holds the differences between the fitted values and the mean response. 
					Importantly, this vector <em>is in the model space</em>.  We can see this as follows.
					The vector <m>\yhat</m> is in the model space by definition.
					If our model includes an intercept (so the first column of <m>X</m> is <m>\onevec</m>),
					then 
					<me>\ybar = \overline{y} \onevec = \overline{y}\onevec + 0 \xvec_1 + 0 \xvec_2 + \cdots + 0 \xvec_p
						= X \begin{bmatrix}\overline{y} \\0\\\vdots\\0\end{bmatrix}
					</me>,
					so <m>\ybar</m> is also in the model space.  This implies that <m>\yhat - \ybar</m> 
					is in the model space.
				</p>
				<p>
					Because <m>\yhat - \ybar</m> is in the model space and <m>y - \yhat</m> is orthogonal to the 
					model space, we know that these vectors are orthogonal.
				</p>
			</li>
		</ul>
		Putting this together we see that 
		<md>
			<mrow> \yvec - \ybar \amp = (\yvec - \yhat ) + (\yhat - \ybar) </mrow>
			<mrow> \len{\yvec - \ybar}^2 \amp = \len{\yvec - \yhat}^2 + \len{\yhat - \ybar}^2 </mrow>
		</md>.
		The second equation is the just the Pythagorean Theorem applied to the triangle formed by our 
		three vectors.
	</p>

	<p>
		This provides a natural way to measure how well our model fits the data:
	</p>

    <!-- <p>
      The coefficient of determination <m>R^2</m> is defined by
      normalizing <m>|\yvec-\yhat|^2</m> so that it is
      independent of the scale.  Recall that we described how to
      demean a vector in
      <xref ref="sec-dot-product" />:  given a vector <m>\yvec</m>, we
      obtain <m>\widetilde{\yvec}</m> by subtracting the average of the
      components from each component.  
	  That is, <m> \widetilde{\yvec} = \yvec - \overline{\yvec}</m>.
    </p> -->

    <definition xml:id="def-rsquared">
      <title> Coefficient of determination </title>
      <idx> coefficient of determination </idx>
      <idx> <h> <m> R^2</m> </h><see>coefficient of determination</see> </idx>
	  <notation>
		<usage><m>R^2</m></usage>
		<description>the coefficient of determination</description>
	  </notation>
      <statement>
	<p>
		For any model that determines predictions <m>\yhat</m> for a response  
		variable <m>\yvec</m>, we can define 
		the <term>coefficient of determination</term> as 
		<md>
			<mrow>
			R^2 \amp = \frac{ \len{\yhat - \ybar}^2}{\len{\yvec - \ybar}^2}
			</mrow>
		</md>.    
	</p>
	<p>
		For linear model with an intercept, we additionally have
		<m>\yvec - \yhat \perp \yhat - \ybar</m>, so 
		<md>
			<mrow>
			R^2 \amp = \frac{ \len{\yhat - \ybar}^2}{\len{\yvec - \ybar}^2}
			</mrow>
			<mrow>
			 \amp = \frac{ \len{\yhat - \ybar}^2}{\len{\yvec - \ybar}^2}
			 	\frac{ \len{\yhat - \ybar}^2}{\len{\yvec - \yhat}^2 + \len{\yhat - \ybar}^2 }
			</mrow>
		</md>.    
	  <!-- <me>
	    R^2 = 1 - \frac{\len{\yvec - \yhat}^2}{\len{\yvec - \overline{\yvec}}}
	  </me>. -->
	</p>
      </statement>
    </definition>


	<p>It is clear from the definition that for linear models with an intercept,
		<me>0 \le R^2 \le 1</me>. 
		So one way to interpret <m>R^2</m>
		is as the proportion of the variation in <m>\yvec</m> that is explained by the model 
		(i.e., by <m>\yhat</m>). If <m>\yhat = \yvec</m>, the fit is perfect and <m>R^2 = 1</m>. 
		At the other extreme, if our model predicts <m>\yhat = \ybar</m> (everyone is average),
		then <m>R^2 = 0</m>.
	</p>

    <p>
      A more complete explanation of this definition relies on the
      concept of variance, which we explore in <xref
      ref="ex-r2-meaning"/> and the next chapter.  For the time being,
      it's enough to know that <m>0\leq R^2 \leq 1</m> and that the
      closer <m>R^2</m> is to 1, the better the line fits the data.
      In our original example, illustrated in <xref
      ref="fig-regression-scale" />, we find that <m>R^2 = 0.75</m>,
      and in our study of cricket chirp rates, we have
      <m>R^2=0.69</m>.  However, assessing the confidence we have in
      predictions made by solving a least squares problem can require
      considerable thought, and it would be naive to rely only on the
      value of <m>R^2</m>.
    </p>

	<p>There is also a connection between the correlation coefficient and the coefficient of  
		of determination.  For a simple linear model <m>\yvec = \beta_0 + \beta_1 \xvec</m>, 
		<me>
			\corr(\xvec, \yvec) = R^2
		</me>.
		For this model, an orthogonal basis for the model space is  
		<md>
			<mrow> \wvec_1 \amp = \onevec </mrow>
			<mrow> \wvec_2 \amp = \xvec - \proj{\xvec}{\onevec} = \xvec - \xmean </mrow>
		</md>,
		and 
		<me>
			\yhat = \proj{\yvec}{\onevec} + \proj{\yvec}{\xvec - \xmean} = ???
		</me>
		
	</p>
		
	
  </subsection>

  <subsection>
    <title> Using <m>QR</m> factorizations </title>

    <p>
      As we've seen, the least squares approximate solution
      <m>\xhat</m> to <m>A\xvec=\bvec</m> may be found by solving the
      normal equation <m>A^{\transpose}A\xhat = A^{\transpose}\bvec</m>, and this can be a
      practical strategy for some problems.  However, this approach
      can be problematic as small rounding errors can accumulate and
      lead to inaccurate final results.
    </p>

    <p>
      As the next activity demonstrates, there is an third method
      for finding the least squares approximate solution <m>\xhat</m>
      using a <m>QR</m> factorization of the matrix <m>A</m>, and this
      method is preferable as it is both computatinoally efficient and 
	  numerically more reliable. This is the method implemented in most 
	  statistical software packages.
    </p>

    <activity xml:id="activity-BFI">
      <statement>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Suppose we are interested in finding the least squares
		approximate solution to the equation <m>A\xvec =
		\bvec</m> and that we have the <m>QR</m> factorization
		<m>A=QR</m>.  Explain why the least
		squares approximation solution is given by solving
		<md>
		  <mrow>
		    A\xhat \amp = QQ^{\transpose}\bvec \\
		  </mrow>
		  <mrow>
		    QR\xhat \amp = QQ^{\transpose}\bvec \\
		  </mrow>
		</md>
	      </p>
	    </li>

	    <li>
	      <p>
		Multiply both sides of the second expression by
		<m>Q^{\transpose}</m> and explain why
		<me>
		  R\xhat = Q^{\transpose}\bvec.
		</me>
	      </p>

	      <p>
		Since <m>R</m> is upper triangular, this is a relatively
		simple equation to solve using back substitution, as we
		saw in <xref ref="sec-gaussian-revisited" />.  We will
		therefore write the least squares approximate solution as
		<me>
		  \xhat = R^{-1}Q^{\transpose}\bvec,
		</me>
		and put this to use in the following context.
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Brozakâ€™s formula, which is used to calculate a person's body fat
		index <m>BFI</m>, is
		<me>BFI = 100 \left(\frac{4.57}{\rho} - 4.142\right)
		</me>
		where <m>\rho</m> denotes a person's body density in
		grams per cubic centimeter.  Obtaining an accurate
		measure of <m>\rho</m> is difficult, however, because it
		requires submerging the person in water and measuring
		the volume of water displaced.  Instead, we will gather
		several other body measurements, which are more easily
		obtained, and use it to predict <m>BFI</m>.
	      </p>

	      <p>
		For
		instance, suppose we take 10 patients and measure their
		weight <m>w</m> in pounds, height <m>h</m> in inches,
		abdomen <m>a</m> in centimeters, wrist circumference
		<m>r</m> in centimeters, neck circumference <m>n</m> in
		centimeters, and <m>BFI</m>.  Evaluating the following
		cell loads and displays the data.
		  </p>

		<sage>
		  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/bfi.csv')
weight = vector(df['Weight'])
height = vector(df['Height'])
abdomen = vector(df['Abdomen'])
wrist = vector(df['Wrist'])
neck = vector(df['Neck'])
BFI = vector(df['BFI'])
print(df)
		  </input>
		</sage>

		<p>
		In addition, that cell provides:
		<ol>
		  <li>
		    <p>
		      vectors <c>weight</c>, <c>height</c>,
		      <c>abdomen</c>, <c>wrist</c>, <c>neck</c>, and
		      <c>BFI</c> 
		      formed from the columns of the dataset.
		    </p>
		  </li>
		  <li>
		    <p>
		      the command <c>onesvec(n)</c>, which returns an
		      <m>n</m>-dimensional vector whose entries are all
		      one.
		    </p>
		  </li>
		  <li>
		    <p>
		      the command <c>QR(A)</c> that returns the
		      <m>QR</m> factorization of <m>A</m> as
		      <c>Q, R = QR(A)</c>.
		    </p>
		  </li>
		  <li>
		    <p>
		      the command <c>demean(v)</c>, which returns the
		      demeaned vector <m>\widetilde{\vvec}</m>.
		    </p>
		  </li>
		</ol>
	      </p>

	      <p>
		We would like to find the linear function
		<me>
		  \beta_0 + \beta_1w + \beta_2h + \beta_3a + \beta_4r +
		  \beta_5n = BFI
		</me>
		that best fits the data.
	      </p>

	      <p>
		Use the first data point to write an equation for 
		the parameters <m>\beta_0,\beta_1,\ldots,\beta_5</m>. 
	      </p>
	    </li>

	    <li>
	      <p>
		Describe the linear system
		<m>A\xvec = \bvec</m> for 
		these parameters.  More specifically, describe how 
		the matrix <m>A</m> and the vector <m>\bvec</m> are
		formed. 
	      </p>
	    </li>

	    <li>
	      <p>
		Construct the matrix <m>A</m> and find its <m>QR</m>
		factorization in the cell below.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the least squares approximate solution
		<m>\xhat</m> by solving the equation <m>R\xhat =
		Q^{\transpose}\bvec</m>.  You may want to use <c>N(xhat)</c> to
		display a decimal approximation of the vector.
		What are the parameters
		<m>\beta_0,\beta_1,\ldots,\beta_5</m> that best fit the
		data?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the coefficient of determination <m>R^2</m> for
		your parameters.  What does this imply about the quality
		of the fit?
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Suppose a person's measurements are: weight 190, height
		70, abdomen 90, wrist 18, and neck 35.  Estimate this
		person's
		<m>BFI</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Use the fact that <m>\bhat=QQ^{\transpose}\bvec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Use the fact that <m>Q^{\transpose}Q=I</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\beta_0 + 154\beta_1 + 68\beta_2 + 85\beta_3 +
		17\beta_4 + 36\beta_5 = 13</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A</m> is the <m>10\by6</m> matrix whose columns
		are a vector of all 1's followed by the vectors of
		weights, heights, abdominal, wrist, and neck
		measurements.  The vector <m>\bvec</m> is the vector
		of BFI readings.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>Q</m> is a <m>10\by6</m> matrix and <m>R</m> is
		a <m>6\by6</m> upper triangular matrix.
	      </p>
	    </li>
	    <li>
	      <p>
		We find that
		<me>
		  \beta_0 = 54.08, \beta_1 = 0.19, \beta_2 =
		    -2.62,
		    \beta_3 = 0.92, \beta_4 = 2.70, \beta_5 =
		    -0.41
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.95</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Evaluating <m>\beta_0 + 190\beta_1 + 70\beta_2 +
		90\beta_3 + 18\beta_4 + 35\beta_5 = 22.9</m>
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The columns of <m>Q</m> form an orthonormal basis for
		<m>\col(A)</m> so that <m>\bhat=QQ^{\transpose}\bvec</m>.  The
		equation <m>A\xhat=\bhat</m> then becomes
		<m>QR\xhat=QQ^{\transpose}\bvec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>Q^{\transpose}Q=I</m>, we have <m>Q^{\transpose}QR\xhat =
		Q^{\transpose}QQ^{\transpose}\bvec</m>, which gives <m>R\xhat =
		Q^{\transpose}\bvec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\beta_0 + 154\beta_1 + 68\beta_2 + 85\beta_3 +
		17\beta_4 + 36\beta_5 = 13</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A</m> is the <m>10\by6</m> matrix whose columns
		are a vector of all 1's followed by the vectors of
		weights, heights, abdominal, wrist, and neck
		measurements.  The vector <m>\bvec</m> is the vector
		of BFI readings.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>Q</m> is a <m>10\by6</m> matrix and <m>R</m> is
		a <m>6\by6</m> upper triangular matrix.
	      </p>
	    </li>
	    <li>
	      <p>
		We find that
		<me>
		  \beta_0 = 54.08, \beta_1 = 0.19, \beta_2 =
		    -2.62,
		    \beta_3 = 0.92, \beta_4 = 2.70, \beta_5 =
		    -0.41
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.95</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Evaluating <m>\beta_0 + 190\beta_1 + 70\beta_2 +
		90\beta_3 + 18\beta_4 + 35\beta_5 = 22.9</m>
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
      To summarize, we have seen that
    </p>

    <proposition>
      <statement>
	<p>
	  If the columns of <m>A</m> are linearly independent and we
	  have the <m>QR</m> factorization <m>A=QR</m>, then the least
	  squares approximate solution <m>\xhat</m> to the equation
	  <m>A\xvec=\bvec</m> is given by
	  <me>
	    \xhat = R^{-1}Q^{\transpose}\bvec
	  </me>.
	</p>
      </statement>
    </proposition>

  </subsection>

  <subsection>
    <title> Polynomial Regression </title>

    <p>
      In the examples we've seen so far, we have fit a linear function
      to a dataset.  Sometimes, however, a polynomial, such as a
      quadratic function, may be more appropriate.  It turns out that the
      techniques we've developed in this section are still useful as
      the next activity demonstrates.
    </p>

    <activity>
      <statement>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Suppose that we have a small dataset containing the
		points <m>(0,2)</m>, <m>(1,1)</m>, <m>(2,3)</m>, and
		<m>(3,3)</m>, such as appear when the following cell is
		evaluated.
	      </p>

		<sage language="python">
		  <input>
import numpy as np
import seaborn.objects as so 
small_data = np.array([[0, 2], [1, 1], [2, 3], [3, 3]])
so.Plot(x=small_data[:,0], y = small_data[:, 1]).add(so.Dot())
list_plot(data, color='blue', size=40)		  
		  </input>
		</sage>

	      <p>
		Let's fit a quadratic function of the form
		<me>
		  \yvec = \beta_0 + \beta_1 \xvec + \beta_2 \xvec^2 
		</me>
		to this dataset.
	      </p>

	      <p>
		Write four equations, one for each data point, that
		describe the 
		coefficients <m>\beta_0</m>, <m>\beta_1</m>, and
		<m>\beta_2</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Express these four equations as a linear system
		<m>\yvec = X \betavec </m> where 
		<m>\betavec = \threevec{\beta_0}{\beta_1}{\beta_2}</m>.
	      </p>

	      <p>
		Find the <m>QR</m> factorization of <m>X</m> and use it
		to find the least squares approximate solution
		<m>\betahat</m>.
	      </p>

		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Use the parameters <m>\beta_0</m>, <m>\beta_1</m>, and <m>\beta_2</m> that you found to
		write the quadratic function that fits the data.  
		Creat a plot that incluedes the raw data and the quadratic fit.
	      </p>

		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		What is your predicted <m>\hat{y}</m> value when <m>x=1.5</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Find the coefficient of determination <m>R^2</m> for the
		quadratic 
		function.  What does this say about the quality of the
		fit?
	      </p>
	    </li>

	    <li>
	      <p>
		Now fit a cubic polynomial of the form
		<me>
		  \yvec = \beta_0 \onevec + \beta_1 \xvec  + \beta_2 \xvec^2 + \beta_3 \xvec ^3
		</me>
		to this dataset.
	      </p>

		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the coefficient of determination <m>R^2</m> for the
		cubic function.  What does this say about the quality of
		the fit?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		What do you notice when you plot the cubic function
		along with the data?  How does this reflect the value of
		<m>R^2</m> that you found?
	      </p>

		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have the equations
		<md>
		  <mrow>
		    \beta_0 + 0\beta_1 + 0^2\beta_2 \amp {}={} 2
		  </mrow>
		  <mrow>
		    \beta_0 + 1\beta_1 + 1^2\beta_2 \amp {}={} 1
		  </mrow>
		  <mrow>
		    \beta_0 + 2\beta_1 + 2^2\beta_2 \amp {}={} 3
		  </mrow>
		  <mrow>
		    \beta_0 + 3\beta_1 + 3^2\beta_2 \amp {}={} 3
		  </mrow>
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\betahat = \threevec{7/4}{-1/4}{1/4}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\frac74 - \frac14 x + \frac14x^2 = y</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>1.9375</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>0.54</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\yhat = 2-\frac{25}{6} \xvec +4 \xvec ^2-\frac56 \xvec^3 </m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=1</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The graph of the cubic function passes through each
		data point.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have the equations
		<md>
		  <mrow>
		    \beta_0 + 0\beta_1 + 0^2\beta_2 \amp {}={} 2
		  </mrow>
		  <mrow>
		    \beta_0 + 1\beta_1 + 1^2\beta_2 \amp {}={} 1
		  </mrow>
		  <mrow>
		    \beta_0 + 2\beta_1 + 2^2\beta_2 \amp {}={} 3
		  </mrow>
		  <mrow>
		    \beta_0 + 3\beta_1 + 3^2\beta_2 \amp {}={} 3
		  </mrow>
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		With <m>X=\begin{bmatrix}
		1 \amp 0 \amp 0 \\
		1 \amp 1 \amp 1 \\
		1 \amp 2 \amp 4 \\
		1 \amp 3 \amp 9 \\
		\end{bmatrix}
		</m> and <m>\yvec=\fourvec2133</m>, we find
		<m>\betahat = \threevec{7/4}{-1/4}{1/4}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The quadratic function is <m>\frac74 - \frac14 x +
		\frac14x^2 = y</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The predicted value is <m>y=\frac74 -
		\frac14(1.5)+\frac14(1.5)^2 = 1.9375</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2 = 0.54</m>
	      </p>
	    </li>
	    <li>
	      <p>
		We find <m>\yhat = 2-\frac{25}{6} \xvec +4 \xvec ^2-\frac56 \xvec^3 </m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=1</m>, which means that we have a perfect fit.
	      </p>
	    </li>
	    <li>
	      <p>
		The graph of the cubic function passes through each
		data point.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
      The matrices <m>X</m> that you created in the last activity when
      fitting a quadratic and cubic function to a dataset have
      a special form.  In particular, if the data points are labeled
      <m>(x_i, y_i)</m> and we seek a degree <m>k</m> polynomial, then
      <me>
	X =
	\begin{bmatrix}
	1 \amp x_1 \amp x_1^2 \amp \ldots \amp x_1^k \\
	1 \amp x_2 \amp x_2^2 \amp \ldots \amp x_2^k \\
	\vdots \amp \vdots \amp \vdots \amp \ddots \amp \vdots \\
	1 \amp x_m \amp x_m^2 \amp \ldots \amp x_m^k \\
	\end{bmatrix}.
      </me>
	  <idx><h>Vandermonde matrix</h></idx>
      This is called a <term>Vandermonde matrix</term> of degree <m>k</m>.   
	  You can use <c>numpy.polynomial.polynomial.polyvander()</c> to create  
	  <idx><h> <c>numpy.polynomial.polynomial.polyvander()</c> </h> </idx>
	  <idx><h>Vandermonde matrix</h><seealso> <c>numpy.polynomial.polynomial.polyvander()</c> </seealso> </idx>
	  these matrices for a specified vector <m>\xvec</m> and degree.
	</p>
<sage language="python">
	<input>
import numpy as np 
from numpy.polynomial.polynomial import polyvander
x = np.arange(5)
print(polyvander(x, 5))
	</input>
	<output>
[[1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]
[1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00]
[1.000e+00 2.000e+00 4.000e+00 8.000e+00 1.600e+01 3.200e+01]
[1.000e+00 3.000e+00 9.000e+00 2.700e+01 8.100e+01 2.430e+02]
[1.000e+00 4.000e+00 1.600e+01 6.400e+01 2.560e+02 1.024e+03]]
	</output>
</sage>
<p>
Notice that <m>0^0</m> is treated as <m>1</m> for the purposes of this matrix.
    </p>

    <activity>
      <statement>
	<p>
	  This activity explores a dataset describing 
	  Arctic sea ice and that comes from
	  <url href="http://sustainabilitymath.org/"
	       visual="sustainabilitymath.org">
	    Sustainability Math.
	  </url>
	</p>
	
	<p>
	  Evaluating the cell below will plot the extent of Arctic sea
	  ice, in millions of square kilometers, during the twelve
	  months of 1980, 2012, and 2017. We will focus primarily on 2012.
	</p>
	  <sage language="python">
	    <input>
import pandas as pd
import numpy as np
import seaborn.objects as so
ice = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/sea_ice.csv')
print(ice)
so.Plot(data = ice, x = "Month", y = "2012").add(so.Dot())
	    </input>
	  </sage>

	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Find the vector <m>\betahat</m>, the least squares
		approximate solution to the linear system that results
		from fitting a degree 5 polynomial to the data.
	      </p>

		<sage language="python">
		  <input>

		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
			Plot the data along with the fitted polynomial model.
	      </p>

		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the coefficient of determination <m>R^2</m> for
		this polynomial fit.
	      </p>
	    </li>

	    <li>
	      <p>
		Repeat these steps to fit a degree 8 polynomial to the
		data, plot the polynomial with the data, and find
		<m>R^2</m>.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Repeat one more time by fitting a degree 11 polynomial
		to the data, creating a plot, and finding <m>R^2</m>.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>

	      <p>
		It's certainly true that higher degree polynomials fit
		the data better, as seen by the increasing values of
		<m>R^2</m>, but that's not always a good thing. For
		instance, when <m>k=11</m>, you may notice that the
		graph of the polynomial wiggles a little more than we
		would expect. In this case, the polynomial is trying too
		hard to fit the data, which usually contains some
		uncertainty, especially if it's obtained from
		measurements.  The error built in to the data is called
		<term>noise</term>, and its presence means that we shouldn't
		expect our polynomial to fit the data perfectly.  When
		we choose a polynomial whose degree is too high, we give
		the noise too much influence over the fit of the model, which leads to
		some undesirable behavior, like the wiggles in the
		graph.
	      </p>

	      <p>
		Fitting the data with a function that is too flexible and fits  
		the training data better than it can be expected to fit new data  
		high is called <term>overfitting</term>, a phenomenon that
		can appear in many machine learning applications.
		Generally speaking, we would like to choose <m>k</m>
		large enough to capture the essential features of the
		data but not so large that we overfit and build the
		noise into the model.  What we really need is a method for selecting 
		a good value for <m>k</m> and a better way to measure how well we should 
		expect the model to fit <em>new</em> data, not the data used to train the model.
		That discussion would take us too far afield for the moment, but it is an 
		important discussion.
	      </p>
	    </li>

	    <li>
	      <p>
		Choosing a reasonable value of <m>k</m>, estimate the
		extent of Arctic sea ice at month 6.5, roughly at the
		Summer Solstice.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
      
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>\xhat = \begin{bmatrix}
		17.4\\-7.1\\4.5\\-1.1\\0.1\\-0.003
		\end{bmatrix}
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The fifth degree polynomial fits the data fairly well.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.99</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.9997</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=1</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>8.7</m> million square kilometers
		of sea ice.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
	    
      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>\xhat = \begin{bmatrix}
		17.4\\-7.1\\4.5\\-1.1\\0.1\\-0.003
		\end{bmatrix}
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The fifth degree polynomial fits the data fairly well.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.99</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=0.9997</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>R^2=1</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>k=5</m> seems like a good choice, and this gives
		the prediction of <m>8.7</m> million square kilometers
		of sea ice.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
	    
    </activity>
	

  </subsection>

  <subsection xml:id="subsec-skl-lm">
	<title>Fitting linear models with standard tools</title>

	<p>
		Coming soon.
	</p>
	

  </subsection>


  <subsection>
    <title> Summary </title>

    <p>
      This section introduced some types of least squares problems and
      a framework for working with them.
      <ul>
	<li>
	  <p>
	    Given an inconsistent system <m>A\xvec=\bvec</m>, we find
	    <m>\xhat</m>, the <term>least squares approximate solution</term> 
		by requiring that <m>A\xhat</m> be as close
	    to <m>\bvec</m> as possible.  In other words, we solve 
	    <m>A\xhat = \bhat</m> where <m>\bhat = \proj{\bvec}{\col(A)}</m>
	  </p>
	</li>

	<li>
		<p>
			One important application of this is fitting <term>linear models</term> to data.
			In that context, we typically use different letters. Instead of  
			<m>A \xvec = \bvec</m>, you are more likely to see  
			<m> \yvec = X \betavec</m>. Here  
			<ol>
				<li>
					<title>	<m>\yvec</m> represents the <term>response variable</term>.</title>
					<p>
						the variable we are trying to predict or estimate from other available data.
					</p>
				</li>
				<li>
					<title>
						<m>X</m> represents the <term>data matrix</term>.  
					</title>
					<p>
						Each row of <m>X</m> represents an observation unit.
						Each column represents a data variable.  
						Often we include a column of 1's in <m>X</m>.  
						This  allows us to model an <term>intercept</term> which represents a 
						baseline amount that is  part of every prediction. 
						<m>X</m> may include the results of applying a function to 
						some of the "raw data", after all, that's just another variable.
					</p>
				</li>
				<li>
					<title>
						<m>\betavec = \begin{bmatrix}\beta_0\\\beta_1\\ \vdots \\ \beta_p \\\end{bmatrix}</m> 
						represents the coefficients of the model.
					</title>
					<p>
						
					</p>
				</li>
			</ol>

		</p>
	</li>

	<li>
	  <p>
	    One way to find <m>\xhat</m> with <m>A \xhat = \bhat</m> is by solving 
		the normal equations <me>A^{\transpose}A\xhat = A^{\transpose}\bvec</me>.  
		This is not our preferred method since numerical problems can arise.
	  </p>
	  <p>
		The statistical version of the normal equation is 
		<me>X^{\transpose}X\betahat = A^{\transpose}\yvec</me>.
	  </p>
	</li>

	<li>
	  <p>
	    A second way to find <m>\xhat</m> with <m>A \xhat = \bhat</m> 
		uses a <m>QR</m> factorization of <m>A</m>.  If <m>A=QR</m>, 
		then <m>\xhat = R^{-1}Q^{\transpose}\bvec</m> and finding <m>R^{-1}</m> is
	    computationally feasible since <m>R</m> is upper triangular.  Alternatively, we can 
		use backsubstitution to solve <m>R \xhat = Q^{\transpose} \bvec</m>.
	  </p>
	  <p>
		The statistical version of this is <m>X = QR</m> and  
		<m>R \betahat = Q^{\transpose} \yvec</m>.
	  </p>
	</li>

	<li>
	  <p>
	    This technique may be applied widely and is useful for
	    modeling data.  
		We saw examples in this section where linear functions of 
		several input variables and polynomials provided 
		effective models for different datasets.
	  </p>
	</li>

	<li>
	  <p>
	    A simple measure of the quality of the fit is the coefficient of
	    determination <m>R^2</m> though some care must be used in interpreting 
		this number in context.  In particular, as models become more complex, <m>R^2</m> 
		generally increases because more flexible models can fit the data better. 
		But they may be prone to overfitting. Our goal is generally not to fit the 
		data at hand but to learn something of value about other data.
	  </p>
	</li>
      </ul>
    </p>

  </subsection>

  <xi:include href="exercises/exercises6-5.ptx" />
  
</section>

