<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-dot-product"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> The dot product  </title>

  <introduction>
		<p> In this section, we introduce a new operation on vectors, known as the <term>dot product</term>.
			Our interest in this opearation is based on <term>linear projections</term>.
	Geometrically, the dot product is related to the lengths of vectors and the angles between pairs
	of vectors. Importantly, the dot product is also easily computed from the list-of-numbers
	representation of vectors. It also has many nice algebraic properties. So this is another
	situation where each of our three presepectives of vectors provides valuable insight. </p>
	</introduction>

	<subsection xml:id="sec-projections-and-dot-products">
		<title>Projections and dot products</title>
	
	<p>	
		We will motivate the defintion of the dot product by investigating the <term>projection</term>
		of a vector <m>\bvec</m> in the direction of a vector <m>\vvec</m>, which we will denote as 
		<m>\proj{\bvec}{\vvec}</m>. 
		<notation>
			<usage> <m>\proj{\yvec}{\xvec}</m> </usage>
		  <description>the projection of <m>\yvec</m> in the direction of <m>\xvec</m></description>
		</notation>

		The projection vector <m>\proj{\bvec}{\vvec}</m> is the scalar multiple <m>k \vvec</m> of <m>\vvec</m> 
		that is as close as possible to <m>\bvec</m>. That means to find the projection vector,
		we must find the scalar <m>k</m> that makes the vector <m>\bvec - k \vvec</m> as short as possible.
		As is illustrated in <xref ref="fig-projection-illustrated" />,
		this happens when <m>k \vvec</m> and <m>(\bvec - k \vvec)</m> form a 90 degree angle. 
	</p>

	<figure xml:id="fig-projection-illustrated">
		<caption>
			Illustrating the projection <m>\proj{\bvec}{\vvec}</m>.  
			We are looking for a vector <m>k \vvec</m> along the blue line
			that is as close as possible to <m>\bvec</m>.  This will occur (upper right) when <m>k \vvec</m>
			and <m> \bvec - k \vvec</m> form a right angle.  If <m>k \vvec</m> is shorter (lower left) 
			or longer (lower right), then the angle will not be a right angle, and so a closer vector exists
			(because the legs of a right triangle are shorter than the hypotenuse).
		</caption>
		<sbsgroup>
		<sidebyside widths="45% 45%" margins="5% 5%" valign="bottom">
		<image source="images/projection-line-relabel1" />
		<image source="images/projection-line-relabel10" />
		</sidebyside>
		<sidebyside widths="45% 45%" margins="5% 5%" valign="bottom">
		<image source="images/projection-line-relabel8" />
		<image source="images/projection-line-relabel12" />
		</sidebyside>
	</sbsgroup>
		
	</figure>

	<p>
		<idx>projection</idx>
		By the definition of cosine, this means that 
		<me>
			\frac{k \len{\bvec}}{\len{\vvec}} = \cos \theta
		</me>
		where <m>\len{\vvec}</m> denotes the (Euclidean) length of the vector <m>\vvec</m> and 
		<m>\theta</m> is the angle between the two vectors.
		Solving for <m>k</m>, we find that 
		<me>
			k = \frac{\len{\bvec} \cos \theta}{\len{\vvec}}
		</me>.
		So
		<me>
			  \proj{\bvec}{\vvec} = \frac{\len{\bvec} \cos \theta}{\len{\vvec}} \vvec
		</me>.
	</p>

	<p>
		Notice that 
		<me>
			  \proj{\bvec}{\vvec} 
			  = \frac{\len{\bvec} \cos \theta}{\len{\vvec}} \vvec 
			  = \frac{\len{\vvec}\len{\bvec} \cos \theta}{\len{\vvec}^2} \vvec 
		</me>.
		This leads to our geometric definition of the dot product, which is similar to the 
		peojection, but is (a) symmetric in <m>\bvec</m> and <m>\vvec</m>, (b) avoids the denominator, and 
		(c) results in a scalar rather than a vector.
		The result will be easier to work with algebraically and also easier to compute.
	</p>

    <figure xml:id="fig-dot-angle">
      <caption>
	The dot product <m>\vvec\cdot\wvec</m> measures the angle  
	<m>\theta</m>.
      </caption>
      <sidebyside width="50%">
	<image source="images/dot-angle" />
      </sidebyside>
    </figure>


		<definition xml:id="def-geometric-dot-product">
		<idx>dot product</idx>
			<statement>
				<p>
					For any two non-zero vectors <m>\xvec</m> and <m>\yvec</m> in <m>\real^n</m> with an angle <m>\theta</m> 
					between them, we define their  
					<term>dot product</term>, denoted <m>\xvec \cdot \yvec</m>, as 
					<me>
						\xvec \cdot \yvec = \len{\xvec} \len{\yvec} \cos \theta
					</me>.
					If either <m>\xvec</m> or <m>\yvec</m> is the zero vector, then <m>\xvec \cdot \yvec = 0</m>.
				</p>
			</statement>
		</definition>

	<note>
		<p>
		Note that the dot product of two vectors is a <em>scalar</em>. For this reason, the dot product is sometimes  
		called the <term>scalar product</term>. 
		</p>
	</note>

	<p>
	We can easily re-express the projection in terms of the dot product.	
	</p>

	<proposition xml:id="prop-projection-in-terms-of-dot-product">
		<idx>projection</idx>
		<statement>
			<p>
					For any two vectors <m>\xvec</m> and <m>\yvec</m> in <m>\real^n</m>,
				<me>\proj{\yvec}{\xvec} = \frac{\xvec \cdot \yvec}{\len{\xvec}^2} \xvec </me>.
			</p>
		</statement>
	</proposition>

	<p>
		The formulas for the dot product and for projection become simpler for <term>unit vectors</term>,
		vectors with length 1. If <m>\uvec</m> and <m>\vvec</m> are unit vectors in <m>\real^n</m> and 
		<m>\xvec</m> and <m>\yvec</m> are any vectors in <m>\real^n</m>, then 
		<idx><h>unit vector</h></idx>
		<idx><h>vector</h><h>unit</h><see>unit vector</see></idx>
		<me>
			\uvec \cdot \vvec = \len{\uvec} \len{\vvec} \cos \theta = \cos \theta
		</me>,
		and 
		<me>
			\proj{\xvec}{\uvec} = \frac{\xvec \cdot \uvec}{\len{\uvec}^2} \uvec = (\xvec \cdot \uvec) \uvec
		</me>.
		And since <m>\frac{\xvec}{\len{\xvec}}</m> is a unit vector, this gives us another way to think about 
		<m>\proj{\yvec}{\xvec}</m>, namely,
		<me>
		  \proj{\yvec}{\xvec} 
		  = \proj{\yvec}{\frac{\xvec}{\len{\xvec}}}
		  <!-- = \frac{\yvec \cdot \frac{\xvec}{\len{\xvec}}}{ \len{\frac{\xvec}{\len{\xvec}}}^2} \frac{\xvec}{\len{\xvec}} -->
		  = \underbrace{\left(\yvec \cdot \frac{\xvec}{\len{\xvec}}\right)}_{\mathrm{length}} 
		  		\underbrace{\frac{\xvec}{\len{\xvec}} }_{\mathrm{direction}}</me>.
		In other words, we obtain the length of the projection by taking the dot product of <m>\yvec</m> with a 
		<em>unit</em> vector in the same direction as <m>\xvec</m>.
	</p>

	<proposition xml:id="prop-unit-projection">
		<statement>
			<p>
				If <m>\uvec</m> is a unit vector and <m>\xvec</m> is any vector of the same dimension, 
				then 
				<ol>
					<li>
						<p>
				<me>
					\proj{\xvec}{\uvec} = (\xvec \cdot \uvec) \uvec
				</me>.
							
						</p>
					</li>
					<li>
						<p>
							<m>\len{\proj{\xvec}{\uvec}} = \xvec \cdot \uvec</m>.
						</p>
					</li>
				</ol>
			</p>
		</statement>
	</proposition>

	<p>
		A number of properties of the dot product follow from the definition.
	</p>

		<proposition xml:id="prop-properties-of-dot-product">
			<statement>
				<p>
					For any vectors <m>\xvec, \yvec, \zvec \in \real^n</m> and scalar <m>k \in \real</m>, 
					the following properties hold.
					<ol>
						<li>
							<title>Commutativity</title>
							<p>
								<m>\xvec \cdot \yvec = \yvec \cdot \xvec</m>
							</p>
						</li>
						<li>
							<title>Scalar multiples</title>
							<p>
								<m>k (\xvec \cdot \yvec) = (k \xvec) \cdot \yvec = \xvec \cdot (k \yvec)</m>
							</p>
						</li>
						<li>
							<title>Length</title>
							<p>
								<m>\xvec \cdot \xvec = \len{\xvec}^2</m>
							</p>
						</li>
						<li>
							<title>Distributivity</title>
							<p>
							   <m>\xvec \cdot(\yvec + \zvec) = (\xvec \cdot \yvec) + (\xvec \cdot \zvec)</m>
							</p>
						</li>
						<li>
							<title>Linear Combinations</title>
								<p>
									<ul>
										<li>
											<p>
												<m>
													(c_1 \xvec_1 + c_2 \xvec_2) \cdot \yvec
													= c_1\xvec_1\cdot\yvec + c_2\xvec_2\cdot\yvec
												</m>

											</p>
										</li>
										<li>
											<p>
												<m>
													(c_1 \xvec_1 + c_2 \xvec_2) \cdot (d_1\yvec_1 +
													d_2\yvec_2)
													= c_1 d_1 \xvec_1\cdot\yvec_1 + c_2 d_1
													\xvec_2\cdot\yvec_1 +
													c_2 d_1 \xvec_2\cdot\yvec_1 + c_2 d_2
													\xvec_2\cdot\yvec_2
												</m>

											</p>
										</li>
									</ul>
								
							</p>
						</li>
					</ol>
				</p>
			</statement>

			<proof>
				<p> 
					The first two statements follow directly from the definition. 
				</p>
				<p>
					The third statement follows by observing that <m>\cos \theta = \cos 0 = 1</m> for two vectors  
					that point in the same direction.
				</p>
				<p>
					The fourth property deserves some explanation. 
					First, we observe that a similar property holds for projections, namely,
					<me>
						\proj{(\yvec + \zvec)}{\xvec}) =
						\proj{\yvec}{\xvec} + \proj{\zvec}{\xvec}
					</me>. This is illustrated in <xref ref="fig-sum-of-projections" />.
					The orange vector is the sum of the red and blue vectors, and the projections  
					onto one of the axes is indicated.  You can drag the background to get a different 
					perspective and to see that this holds even if 
					<m>\avec</m>, <m>\bvec</m>, and the axis of projection 
					do not lie in a plane.  Projecting onto an axis makes the visualization a little bit 
					simpler, but changing the perspective demonstrates that this is not essential.
					It will always be the case that the projection of a sum is the sum of the projections.
				</p>

				<p>
					The final property follows by (repeated) application of scalar multiples and distributivity.
				</p>


					<!-- <example hide-type="true">
						<title>Interactive: A vector in <m>\real^3</m>, by coordinates</title>
					  </example> -->
					<figure xml:id="fig-sum-of-projections">
						<caption>
						The projection of a sum is the sum of projections. 
							Drag the arrow heads to change the vectors. Drag on the background change  
							the perspective. Projections are illustrated with thinner, lighter lines of  
							the same color as the vectors being projected. 
							Each of the red, blue, and orange triangles  
							is a right triangle.
							The blue projection vector is  
							indicated twice, once as part of the blue triangle, and a second time  
							translated so that it begins where the red projection vector ends.
						</caption>
					<interactive iframe="build-demos/projection-dot.html" height="600px" />
						<!-- <mathbox source="build-demos/vector.html" height="400px"/> -->
					</figure>

				<p>
					From this it follows that 
					<md>
						<mrow> 
						\frac{(\yvec + \zvec)\cdot \xvec}{\len{\xvec}^2} \xvec
							\amp =
						\frac{\yvec \cdot \xvec}{\len{\xvec}^2} \xvec +
						\frac{\zvec \cdot \xvec}{\len{\xvec}^2} \xvec 
						</mrow>
						<mrow> 
						\left( (\yvec + \zvec) \cdot \xvec \right) \frac{\xvec}{\len{\xvec}^2}
							\amp  = 
						\left( \yvec \cdot \xvec  +  \zvec \cdot \xvec \right) \frac{\xvec}{\len{\xvec}^2}
						</mrow>
						<mrow> 
						(\yvec + \zvec) \cdot \xvec  
							\amp  =  \yvec \cdot \xvec + \zvec \cdot \xvec 
						</mrow>
					</md>
				</p>
			</proof>
		</proposition>

	<p>
		These properties of the dot product allow us to algebraically rearrange or simplify many 
		expressions involving dot products.
	</p>

    <example>
      <statement>
	<p> Suppose that <m>\vvec_1\cdot\wvec = 4</m> and
	<m>\vvec_2\cdot\wvec = -7</m>.  Then
	<me>
	  \begin{aligned}
	  (2\vvec_1)\cdot\wvec \amp {}={} 2(\vvec_1\cdot\wvec) =
	  2(4) = 8 \\
	  (-3\vvec_1+ 2\vvec_2)\cdot\wvec \amp {}={}
	  -3(\vvec_1\cdot\wvec) + 2(\vvec_2\cdot\wvec) = -3(4)+2(-7) =
	  -26
	  \end{aligned}
	</me>.
	</p>
      </statement>
    </example>

    <p> As we move forward, it will be important for us to recognize
    when vectors are perpendicular to one another.  For instance, when
    vectors <m>\vvec</m> and <m>\wvec</m> are perpendicular, the angle
    between them <m>\theta=90^\circ</m> and we have
    <me>
      \vvec\cdot\wvec=\len{\vvec}\len{\wvec}\cos\theta =
      \len{\vvec}\len{\wvec}\cos90^\circ = 0
    </me>.
    Therefore, the dot product between perpendicular vectors must be
    zero. 
    This leads to the following definition.
    </p>

    <definition>
	<idx><h>orthogonal vectors</h></idx>
	<notation>
		<usage><m>\vvec \perp \wvec</m></usage>
	  <description>the vectors <m>\vvec</m> and <m>\wvec</m> are orthogonal</description>
	</notation>
      <statement>
		<p>
	We say that vectors <m>\vvec</m> and <m>\wvec</m> are
	<term>orthogonal</term> if <m>\vvec\cdot\wvec=0</m>.
	We can denote this with <m>\vvec \perp \wvec</m>.
		</p>
      </statement>
    </definition>

    <p> In practical terms, two perpendicular vectors are
    orthogonal.  However, the concept of orthogonality is somewhat
    more general because it allows one or both of the vectors to
    be the zero vector <m>\zerovec</m>.
    </p>

	<p>We turn next to the important question of how to compute dot products.
	</p>

	</subsection>
	
	<subsection xml:id="subsec-computing-dot-products">
		<title>Computing dot products</title>
	
		<p>
		The geometric definition of the dot product motivates many of its applications, but 
		it can be difficult to compute the dot product from this definition because 
		<m>\cos \theta</m> may be difficult to obtain, espcially in high dimensions. 
		Fortunately, there is a computational shortcut that follows directly from 
		<xref ref="prop-properties-of-dot-product" />.
		</p>


		<p>Let <me>
				\vvec =
				\left[
				\begin{array}{c}
				v_1 \\ v_2 \\ \vdots \\ v_n \\
				\end{array}
				\right]
				\text{ and }
				\wvec = \left[
				\begin{array}{c}
				w_1 \\ w_2 \\ \vdots \\ w_n \\
				\end{array}
				\right]
			</me>
			be two vectors in <m>\real^n</m>. Then we can write 
			<md>
				<mrow>
				\vvec \amp = v_1 \evec_1 + v_2 \evec_2 + \cdots + v_n \evec_n
				</mrow>
				<mrow>
				\wvec \amp = w_1 \evec_1 + w_2 \evec_2 + \cdots + w_n \evec_n
				</mrow>
			</md>.
			From this it follows that  
			<md>
				<mrow> \vvec \cdot \wvec \amp =
				(v_1 \evec_1 + v_2 \evec_2 + \cdots + v_n \evec_n) \cdot
				(w_1 \evec_1 + w_2 \evec_2 + \cdots + w_n \evec_n) 
				</mrow>
				<mrow> 
					\amp 
					\sum_{i=1}^{n} \sum_{j=1}^{n} v_i w_j \evec_i \cdot \evec_j 
				</mrow>
				<mrow> 
					\amp 
					\sum_{i = j} v_i w_i 1 + \sum_{i \neq j} v_i w_i 0
				</mrow>
				<mrow> 
					\amp 
					\sum_{i=1}^{n} v_i w_i 
				</mrow>
			</md>
			because
			<me>
				\evec_i \cdot \evec_j = \begin{cases} 0 \amp i \neq j \\ 1 \amp i = j \\ \end{cases} \;\;
			</me>,
			so the only terms in the sum that are not 0 are the ones where <m>i</m> and <m>j</m> match.
		</p>
		<p>
			In other words, the dot product can be computed as the <em>sum of products</em> of corresponding entries  
			in the two vectors. It will be useful, whenever you see a sum of products to ask how it might be  
			interpreted as a dot product.
		</p>

		<proposition xml:id="prop-computing-dot-product">
			<statement>
				<p>Let <m>\vvec</m> and <m>\wvec</m> be two vectors in <m>\real^n</m>. Then <me>
					\vvec\cdot\wvec =
						\left[
						\begin{array}{c}
						v_1 \\ v_2 \\ \vdots \\ v_n \\
						\end{array}
						\right]
						\cdot
						\left[
						\begin{array}{c}
						w_1 \\ w_2 \\ \vdots \\ w_n \\
						\end{array}
						\right]
						= v_1w_1 + v_2w_2 + \cdots + v_n w_n
					</me>.
				</p>
			</statement>
		</proposition>

		<note>
				<p>
					<me>
					\vvec\cdot\wvec =
						\left[
						\begin{array}{c}
						v_1 \\ v_2 \\ \vdots \\ v_n \\
						\end{array}
						\right]
						\cdot
						\left[
						\begin{array}{c}
						w_1 \\ w_2 \\ \vdots \\ w_n \\
						\end{array}
						\right]
						= v_1w_1 + v_2w_2 + \cdots + v_n w_n
						= 
						\begin{bmatrix} v_1 \amp v_2 \amp \cdots \amp v_n \\ \end{bmatrix}
						\begin{bmatrix} w_1 \\ w_2 \\ \cdots \\ w_n \\ \end{bmatrix}
					</me>.
					So a dot product can be comuted as a matrix product of a <m>1 \by n</m> matrix and an <m>n \by 1</m> matrix.
				</p>
				<p>
					This means that for a fixed dimension, the dot product is a <term>linear transformation</term> from 
					<m>\real^n \to \real^1</m>.
					There is a 
					<url href="https://youtu.be/LyGKycYT2v0?si=f9R2mjWaAYPllbcs" visual="youtu.be/LyGKycYT2v0?si=f9R2mjWaAYPllbcs">
					3Blue1Brown video</url> that illustrates the connection
					between the dot product and this linear transformation. We
					recommend that you view it.
				</p>
				<p>
					We will write
					<idx><h>transpose of a matrix</h></idx>
					<me>
						\begin{bmatrix} v_1 \amp v_2 \amp \cdots \amp v_n \\ \end{bmatrix} = 
						\begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n \\ \end{bmatrix}^\transpose
					</me>
					and call the wide version the <term>transpose</term> of the tall version.  
					For more about matrix transposes see <xref ref="sec-transpose"/>.
				</p>
		</note>

		<example>
			<statement>
				<p> For two-dimensional vectors <m>\xvec</m> and <m>\yvec</m>, their dot product <m>
					\xvec\cdot\yvec</m> is <me>
						\xvec\cdot\yvec = \twovec{x_1}{x_2}\cdot\twovec{y_1}{y_2} =
						x_1y_1 + x_2y_2
					</me>.
					For instance, <me>
						\twovec{2}{-3}\cdot\twovec{4}{1} = 2\cdot 4 + (-3)\cdot 1 = 5
					</me>.
				</p>
			</statement>

		</example>
		<example>
			<statement>
				<p> We compute the dot product between two four-dimensional vectors as <me>
						\left[
						\begin{array}{c}
						2 \\ 0 \\ -3 \\ 1 \\
						\end{array}
						\right]
						\cdot
						\left[
						\begin{array}{c}
						-1 \\ 3 \\ 1 \\ 2 \\
						\end{array}
						\right]
						= 2(-1) + 0(3) + (-3)(1) + 1(2) = -3
					</me>.
				</p>
			</statement>
		</example>


	<example>
			<statement>
				<p>
				From the
				<xref ref="prop-properties-of-dot-product" />
				we know that
				<m>\len{\vvec}^2 = \vvec \cdot \vvec</m>. We can also compute the
				length of a vector using the Pythagorean Theorem. When using our
				computationa shortcut, we see that these are the same calculation.
				</p>
				<p> For example, consider the vector <m>\vvec = \twovec32</m> as shown in <xref
						ref="fig-dot-length" />. </p>

				<figure xml:id="fig-dot-length">
					<caption> The vector <m>\vvec=\twovec32</m>. </caption>
					<sidebyside width="50%">
						<image source="images/dot-length" />
					</sidebyside>
				</figure>

				<p> We may find the length of this vector using the Pythagorean theorem since the
				vector forms the hypotenuse of a right triangle having a horizontal leg of length 3 and a
				vertical leg of length 2, so <m>\len{\vvec}^2 = 3^2 + 2^2 = 13</m>. Now notice that the dot
				product of <m>\vvec</m> with itself performs the identical arithmetic: 
				<me>\vvec\cdot\vvec = 3(3) + 2(2) = 13 = \len{\vvec}^2</me>. 
				</p>
			</statement>
		</example>
    <activity>
		
      <statement>
      <p>
	<ol marker="a.">
	  <li><p> Compute the dot product
	  <me>\twovec{3}{4}\cdot\twovec{2}{-2}
	  </me>.
	  </p></li>

	  <li><p> Sketch the vector <m>\vvec=\twovec{3}{4}</m> below.  Then
	  use the Pythagorean theorem to find the length of <m>\vvec</m>.
	  </p>
	  <figure>
	    <caption>
	      Sketch the vector <m>\vvec</m> and find its length.
	    </caption>
	    <sidebyside width="50%">
										<image source="images/empty-6" />
	    </sidebyside>
	  </figure>
	  </li>

	  <li><p> Compute the dot product <m>\vvec\cdot\vvec</m>.  How
	  is the dot product related to the length of <m>\vvec</m>?
	  </p></li>

	  <li><p> Remember that the matrix <m>\mattwo0{-1}10</m>
	  represents the matrix transformation that rotates vectors
	  counterclockwise by <m>90^\circ</m>.  Beginning with the vector
	  <m>\vvec = \twovec34</m>, find <m>\wvec</m>, the result
	  of rotating <m>\vvec</m> by <m>90^\circ</m>, and sketch it above.
	  </p></li>

	  <li><p> What is the dot product <m>\vvec\cdot\wvec</m>?
	  </p></li>

	  <li><p> Suppose that <m>\vvec=\twovec ab</m>.  Find the
	  vector <m>\wvec</m> that results from rotating <m>\vvec</m>
	  by <m>90^\circ</m> and find the dot product
	  <m>\vvec\cdot\wvec</m>.  </p></li>

						<!-- <li><p> Suppose that <m>\vvec</m> and <m>\wvec</m> are two
	  perpendicular vectors.  What do you think their dot product
	  <m>\vvec\cdot\wvec</m> is?
	  </p></li> -->
	</ol>
      </p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>\twovec34\cdot\twovec2{-2} = 3\cdot2+4\cdot(-2) =
		-2</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The length of <m>\vvec</m> is 5.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\vvec\cdot\vvec = 25</m>, which is the square of
		the length of <m>\vvec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\wvec=\twovec{-4}3</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\vvec\cdot\wvec=0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\vvec\cdot\wvec=0</m>.
	      </p>
	    </li>
						<!-- <li>
	      <p>
		The dot product should be zero.
	      </p>
	    </li> -->
	  </ol>
	</p>
      </solution>
	      
	</activity>





		<!-- <p> To see this, we will apply the Law of Cosines, which says that
    <me>
      \begin{aligned}
      \len{\wvec-\vvec}^2 \amp = \len{\vvec}^2 + \len{\wvec}^2 -
      2\len{\vvec}\len{\wvec}\cos\theta \\
      (\wvec-\vvec)\cdot(\wvec-\vvec) \amp =
      \vvec\cdot\vvec +
      \wvec\cdot\wvec - 2\len{\vvec}\len{\wvec}\cos\theta \\
      \wvec\cdot\wvec + \vvec\cdot\vvec- 2\vvec\cdot\wvec \amp =
      \vvec\cdot\vvec +
      \wvec\cdot\wvec - 2\len{\vvec}\len{\wvec}\cos\theta \\
      -2\vvec\cdot\wvec \amp = -2\len{\vvec}\len{\wvec}\cos\theta \\
      \vvec\cdot\wvec \amp = \len{\vvec}\len{\wvec}\cos\theta \\
      \end{aligned}
    </me>
    The upshot of this reasoning is that
    <me>
      \vvec\cdot\wvec = \len{\vvec}\len{\wvec}\cos\theta\text{.}
    </me>
    </p>

    <p> To summarize:
    </p> -->



    <activity>
      <statement>
	<p>
	  <ol marker="a.">
	    <li>
	      <p> Sketch the vectors <m>\vvec=\twovec32</m> and
	      <m>\wvec=\twovec{-1}3</m> using <xref
	      ref="fig-dot-empty" />.
	      </p>
	
	      <figure xml:id="fig-dot-empty">
		<caption>
		  Sketch the vectors <m>\vvec</m> and <m>\wvec</m> here.
		</caption>
		<sidebyside width="50%">
		  <image source="images/empty-4" />
		</sidebyside>
	      </figure>
	    </li>
	    
	    <li><p> Find the lengths <m>\len{\vvec}</m> and
	    <m>\len{\wvec}</m> using the dot product. </p></li>
	    
	    <li><p> Find the dot product <m>\vvec\cdot\wvec</m> and use it
	    to find the angle between <m>\vvec</m> and
	    <m>\wvec</m>.
	    </p></li>
	    
	    <li><p> Consider the vector <m>\xvec = \twovec{-2}{3}</m>.
	    Include it in your sketch in <xref ref="fig-dot-empty" /> and
	    find the angle between <m>\vvec</m> and <m>\xvec</m>.
	    </p></li>
	    
	    <li><p> If two vectors are perpendicular, what can you say
	    about their dot product?  Explain your thinking.
	    </p></li>
	    
	    <li><p> For what value of <m>k</m> is the vector
	    <m>\twovec6k</m> perpendicular to <m>\wvec</m>?
	    </p></li>
	    
	    <li><p> Python can be used to find lengths of vectors and their
	    dot products.  
		<idx><h><c>np.linalg.norm()</c></h></idx>
		<idx><h>norm</h><h>Euclidean</h></idx>
		For instance, if <c>v</c> and <c>w</c> are
	    vectors, then <c>np.linalg.norm()</c> gives the length of <c>v</c> and
	    <c>v @ w</c> gives <m>\vvec\cdot\wvec</m>.
	</p>
	    
	    <p> Suppose that
	    <me>
	      \vvec=\fourvec203{-2}, \hspace{24pt}
	      \wvec=\fourvec1{-3}41
	    </me>.
	    Use the Python cell below to find <m>\len{\vvec}</m>,
	    <m>\len{\wvec}</m>, <m>\vvec\cdot\wvec</m>, and the angle
	    between <m>\vvec</m> and <m>\wvec</m>.  You may use
	    <c>math.acos()</c> to find the angle's measure expressed in
	    radians.  
	</p>
	    <sage language="python">
			<input> 
import math  
import numpy as np
			</input>
	    </sage>
	    </li>
	</ol></p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <sidebyside width="50%">
		<image source="images/ex-6-1-1-plot" />
	      </sidebyside>
	    </li>

	    <li>
	      <p>
		<m>\len{\vvec} = \sqrt{13}</m> and <m>\len{\wvec} =
		\sqrt{10}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		<m> 
		  \theta =
		  = 52.1^\circ.
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>
		  \theta =
		  = 90^\circ.
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Their dot product
		must be zero.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>k=2</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>55.9^\circ</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <sidebyside width="50%">
		<image source="images/ex-6-1-1-plot" />
	      </sidebyside>
	    </li>

	    <li>
	      <p>
		We find that
		<md>
		  <mrow>
		    \vvec\cdot\vvec {}={} 13
		  </mrow>
		  <mrow>
		    \wvec\cdot\wvec {}={} 10
		  </mrow>
		</md>
		so that
		<m>\len{\vvec} = \sqrt{13}</m> and <m>\len{\wvec} =
		\sqrt{10}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		<m>\vvec\cdot\wvec = 3</m> so that
		<me>
		  \theta =
		  \arccos\left(\frac{\vvec\cdot\wvec}{\len{\vvec}\len{\wvec}}\right)
		  = 52.1^\circ.
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\vvec\cdot\xvec = 0</m> so that
		<me>
		  \theta =
		  \arccos\left(\frac{\vvec\cdot\xvec}{\len{\vvec}\len{\xvec}}\right)
		  = 90^\circ.
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		If two vectors are perpendicular, then the angle
		between them is <m>90^\circ</m>.  Since
		<m>\cos(90^\circ) = 0</m>, their dot product
		must be zero.
	      </p>
	    </li>
	    <li>
	      <p>
		The dot product is <m>-6 + 3k = 0</m> so <m>k=2</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		We find that <m>\len{\vvec} = \sqrt{17}</m>,
		<m>\len{\wvec} = 3\sqrt{3}</m>, <m>\vvec\cdot\wvec =
		12</m>, and the angle between these vectors is
		<m>55.9^\circ</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>


    <p>
      As we have seen, we can use the dot product to compute the angle 
	  between two vectors. This angle (or the cosine of the angle) is often taken as a 
	  measure of "similarity" between two vectors. 
	  For example, consider the vectors <m>\uvec</m>,
      <m>\vvec</m>, and <m>\wvec</m>, shown in <xref
      ref="fig-similar-vectors" />.  The vectors <m>\vvec</m> and
      <m>\wvec</m> seem somewhat similar as the directions they define
      are nearly the same.  By comparison, <m>\uvec</m> appears rather
      dissimilar to both <m>\vvec</m> and <m>\wvec</m>.  We will
      measure the similarity of vectors by finding the angle between
      them; the smaller the angle, the more similar the vectors.
	  This is especially useful in contexts where the direction of a vector conveys 
	  more important information than its magnitude.
    </p>

    <figure xml:id="fig-similar-vectors">
      <caption>
	Which of the vectors are most similar?
      </caption>
      <sidebyside width="50%">
	<image source="images/similar-vectors" />
      </sidebyside>
    </figure>

    <activity>
      <statement>
	<p> This activity explores two uses of the dot product
	as a way to compute the "similarity" of vectors.
	<ol marker="a.">
	  <li>
	    <p>
	      Our first task is to assess the similarity between various
	      Wikipedia articles by forming vectors from each of five
	      articles.  
		  <idx><h>document vector</h></idx>
		  In particular, one may download the text from a
	      Wikipedia article, remove common words, such as <q>the</q>
	      and <q>and</q>, count the number of times the
	      remaining words appear in the article, and represent
	      these counts in a vector, called the <term>document vector</term> 
		  for each article.
	    </p>

	    <p>
	      For example, evaluate the following cell that loads 
		  a matrix constructed from the Wikipedia articles on Veteran's Day, 
		  Memorial Day, Labor Day, the Golden Globe Awards, and the Super
	      Bowl.  Each row of the matrix represents one of 604 words and each 
		  column represents one of the articles as a document vector.  
	      For instance, the word <q>act</q> appears 3 times in the
	      Veteran's Day article and 0 times in the Labor Day
	      article.
		</p>
	      <sage language="python">
		<input>
import pandas as pd
import numpy as np

events = pd.read_csv("https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/events.csv", index_col=0)
print(events.shape) 
print(head(events, 5))
		</input>
		<output> 
shape:  (604, 5)
           Veterans Day  Memorial Day  Labor Day  Golden Globe Awards  \
according             0             4          3                    0   
across                0             3          1                    0   
act                   3             2          0                    0   
acting                1             0          0                    9   
active                0             1          0                    2   

           Super Bowl  
according           3  
across              0  
act                 2  
acting              0  
active              3  
		</output>
	      </sage>

	    <p>
	      <ol marker="1.">
		<li>
		  <p> Suppose that two articles have no words in
		  common.  What is the value of the dot product between
		  their corresponding vectors?  What does this say
		  about the angle between these vectors?
		  </p>
		</li>

		<li>
		  <p>
		    Suppose there are two articles on the same subject,
		    yet one article is twice as long.  What approximate
		    relationship would you expect to hold between the
		    two vectors?  What does this say about the angle
		    between them?
		  </p>
		</li>

		<li>
		  <p> Use the Python cell below to find the angle between
		  the document vector for the Veteran's Day article and the other four
		  articles.  To express the angle in degrees, multiply radians by 
		  <c>180.0 / math.pi</c>.
		  </p>

		  <sage language="python">
		    <input>
import numpy as np
import math 
		    </input>
		  </sage>
		</li>

		<li>
		  <p>
		    Compare the four angles you have found and discuss
		    what they mean about the similarity between the
		    Veteran's Day article and the other four.  How do
		    your findings reflect the nature of these five events?
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>

	  <li>
	    <p>
	      Vectors are often used to represent how a quantity changes
	      over time.  For instance, the vector
	      <m>\svec=\fourvec{78.3}{81.2}{82.1}{79.0}</m> might
	      represent the value of a company's stock on four
	      consecutive days.  When interpreted in this way, we call the
	      vector a <term>time series.</term> 
		  <idx><h>time series</h></idx>
		  Evaluate the Python cell
	      below to see a representation of time series
	      for four different stocks over 10 days. 
		  </p>
	      <sage language="python">
		<input>
import numpy as np
import pandas as pd
import seaborn.objects as so
stocks = pd.read_csv("https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/time-series.csv", header=None)
stocks1 = stocks  
stocks1['day'] = stocks1.index
# convert to "long" format for plotting with seaborn.objects
stocks1_long = pd.melt(stocks1, id_vars="day", var_name = "stock", value_name="price")
print(stocks1)
# print(stocks1_long)
so.Plot(stocks1_long, x = "day", y = "price", color = "stock").add(so.Lines()).show()
		</input>
	      </sage>
		<p>
	      Notices that although one stock has a higher value, stocks 0 and 3 appear to 
		  be related since they seem to rise and
	      fall at roughly similar ways.  We often say that such series are 
	      <term>correlated</term>, and we would like to measure the degree
	      to which they are correlated.

	      <ol marker="1.">
		<li>
		  <p>
		    In order to compare the ways in
		    which they rise and fall, we will first
			<idx>demean</idx>
		    <term>demean</term> each time series;  that is, for each
		    time series, we will subtract its average value to
		    obtain a new time series.  
		  </p>
		    <sage language="python">
		      <input>
stocks0 = stocks - stocks.mean(axis = 0)
stocks0_long = pd.melt(stocks0, id_vars="day", var_name = "stock", value_name="price")
so.Plot(stocks_long, x = "day", y = "price", color = "stock").add(so.Lines()).show()
		      </input>
		    </sage>
		</li>

		<li>
		  <p>
		    If the demeaned series are <m>\tilde{\svec}_1</m> and
		    <m>\tilde{\svec}_2</m>, then the <term>correlation</term> between
		    <m>\svec_1</m> and <m>\svec_2</m> is defined to be
			<idx>correlation</idx>
		    <me>
		      \corr(\svec_1, \svec_2) =
		      \frac{\tilde{\svec}_1\cdot\tilde{\svec}_2}
		      {\len{\tilde{\svec}_1}\len{\tilde{\svec}_2}} =
		      \frac{\len{\tilde{\svec}_1}\len{\tilde{\svec}_2} \cos \theta}
		      {\len{\tilde{\svec}_1}\len{\tilde{\svec}_2}} = \cos \theta
		    </me>
			where <m>\theta</m> is the angle between 
			<m>\tilde{\svec_1}</m> and 
			<m>\tilde{\svec_2}</m>.
		    That is, the correlation equals the cosine of the
		    angle between the demeaned time series. Among other things, 
		    this implies that <m>\corr(\svec_1,\svec_2)</m> is always 
			between -1 and 1.
		  </p>

		  <p>
		    Find the correlation between each pair of stocks. 
		  </p>
		    <sage language="python">
		      <input>

		      </input>
		    </sage>
		</li>

		<li>
		  <p>
		    Suppose that two time series are such that their
		    demeaned time series are scalar multiples of one
		    another, as in <xref ref="fig-correlation" />
		  </p>

		  <figure xml:id="fig-correlation">
		    <caption>
		      On the left, the demeaned time series are positive
		      scalar multiples of one another.  On the right,
		      they are negative scalar multiples.
		    </caption>
		    <sidebyside widths="40% 40%">
		      <image source="images/correlation-pos" />
		      <image source="images/correlation-neg" />
		    </sidebyside>
		  </figure>

		  <p>
		    For instance, suppose we have time series
		    <m>\tvec_1</m> and 
		    <m>\tvec_2</m> whose
		    demeaned time series <m>\tilde{\tvec}_1</m>
		    and <m>\tilde{\tvec}_2</m> are positive scalar
		    multiples of one another.  What is the angle between
		    the demeaned vectors?  What does this say about the
		    correlation <m>\corr(\tvec_1, \tvec_2)</m>?
		  </p>
		</li>

		<li>
		  <p>
		    Suppose the 
		    demeaned time series <m>\tilde{\tvec}_1</m>
		    and <m>\tilde{\tvec}_2</m> are negative scalar
		    multiples of one another, what is the angle between
		    the demeaned vectors?  What does this say about the
		    correlation <m>\corr(\tvec_1, \tvec_2)</m>?
		  </p>
		</li>

		<li>
			<p> Which pair of stocks had the largest correlation?  How is this reflected in 
				the plot?
			</p>
		</li>
		<li>
			<p> Which pair of stocks had the smallest (most negative) correlation?  
				How is this reflected in the plot?
			</p>
		</li>
		<li>
			<p> Which pair of stocks had a correlation closest to 0? 
				How is this reflected in the plot?
			</p>
		</li>
		<li>
			<p>
				The correlation is important enough that numpy can do all the work of computing correlations for each 
				pair of rows (default) or columns (what we want here) in a matrix.  The result is a matrix containing 
				all the pairwise correlations.
				<idx><h><c>np.corrcoef()</c></h></idx>
			</p>
				<sage language="python">
					<input>
print("correlations"); print() 
print(np.corrcoef(stocks, rowvar = False))
					</input>
					<output>
correlations

[[ 1.         -0.87380249 -0.09502163  0.97826418]
 [-0.87380249  1.          0.09375034 -0.91146418]
 [-0.09502163  0.09375034  1.         -0.08984275]
 [ 0.97826418 -0.91146418 -0.08984275  1.        ]]
					</output>
				</sage>
				<p>
			<ul>
				<li>
					<p>
						Explain why the shape of the correlation matrix is what it is.
						What shape would it have been if we had not used <c>rowvar = False</c>?
					</p>
				</li>
				<li>
					<p>
						Explain why this matrix is symmetric.
					</p>
				</li>
				<li>
					<p>
						Explain why the diagonal values are what they are.
					</p>
				</li>
			</ul>
		</p>
		</li>
	      </ol>
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p> They are perpendicular.
		  </p>
		</li>
		<li>
		  <p>
		    The angle should be close to 0.
		  </p>
		</li>
		<li>
		  <p>
		    The angle <c>veterans</c> makes with
		    <c>memorial</c> is <m>46.4^\circ</m>, with
		    <c>labor</c> is <m>59.2^\circ</m>, with
		    <c>golden</c> is <m>86.2^\circ</m>, and with
		    <c>super</c> is <m>86.4^\circ</m>.
		  </p>
		</li>
		<li>
		  <p>
		    The articles on Veteran's Day and
		    Memorial Day are most similar.
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    The graphs are now lowered so that their averages
		    are zero.
		  </p>
		</li>
		<li>
		  <p>
		  </p>
		</li>
		<li>
		  <p>
		  </p>
		</li>
		<li>
		  <p>
		  </p>
		</li>
		<li>
		  <p>
		  </p>
		</li>
		<li>
		  <p>
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p> If there are no words in common, then the dot
		  product between the two vectors will be zero.  This
		  means that they are perpendicular to one another.
		  </p>
		</li>
		<li>
		  <p>
		    The vectors should be, at least approximately,
		    scalar multiples of one another, which means that
		    the angle between them is zero.
		  </p>
		</li>
		<li>
		  <p>
		    The angle <c>veterans</c> makes with
		    <c>memorial</c> is <m>46.4^\circ</m>, with
		    <c>labor</c> is <m>59.2^\circ</m>, with
		    <c>golden</c> is <m>86.2^\circ</m>, and with
		    <c>super</c> is <m>86.4^\circ</m>.
		  </p>
		</li>
		<li>
		  <p>
		    It appears that the articles on Veteran's Day and
		    Memorial Day are most similar.  This makes sense
		    because both are U.S. national holidays that honor
		    military service.  The second most similar article is
		    Labor Day, which is also a national holiday.  The
		    other two are quite dissimilar as they are
		    entertainment events.
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    The graphs are now lowered so that their averages
		    are zero.
		  </p>
		</li>
		<li>
		  <p>
		    See below for all of the correlations.
		  </p>
		</li>
		<li>
		  <p>
		    The angle should be zero, which means that the
		    correlation will be <m>1</m>.
		  </p>
		</li>
		<li>
		  <p>
		    The angle should be <m>180^\circ</m>, which means
		    that the correlation should be <m>-1</m>.
		  </p>
		</li>
		<li>
		  <p>
		    The largest correlation is <m>0.97</m> for stocks 0 and 3, which are most similar.
		  </p>
		</li>
		<li>
		  <p>
		    The smallest correlation is <m>-0.91</m> for stocks 1 and 3, which are least similar.
			When one goes up, the other tends to go down.
		  </p>
		</li>
		<li>
		  <p>
		    The closest correlation to 0 is <m>-0.090</m> for stocks 3 and 4. These stocks seem 
			unrelated.  When one goes up, the other sometimes goes up, sometimes down.  
		  </p>
		</li>
		<li>
			<p>
				Correlation matrices are square and have a row and column for each of the vectors we computed 
				correlations with.  Since we have 4 column vectors here, we get a <m>4 \by 4</m> matrix.
				It would be <m>10 \by 10</m> if we did row-wise correlations.
				These matrices are always symmetric because the definition of correlation is symmetric.
				The diagonal entries are 1 because 
				<me>\corr(\xvec, \xvec) = 
					\frac{\xvec \cdot \xvec}{\vlen{\xvec}\vlen{\xvec}} =
					\frac{\xvec \cdot \xvec}{\len{\xvec}^2} = 1</me>.
			</p>
		</li>
	      </ol>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>

  </activity>
 
  <p>We conclude this section by listing once more the important properties of the dot produt.</p>

		<assemblage xml:id="assemblage-dot-product">
			<title> Properties of the dot product </title>

			<p> Let
			<m>\xvec</m>,
			<m>\yvec</m>, and
			<m> \wvec </m> be vectors in
			<m>\real^n</m>,
			let
			<m>\theta</m> be the angle between
			<m>\xvec</m> and
			<m>\yvec</m>,
			and let
			<m> a </m> and
			<m>b</m> be scalars. Then

			<ol>
				<li>
					<p> <m>\xvec\cdot\yvec = \len{\xvec}\len{\yvec}\cos\theta</m>. </p>
				</li>
				<li>
					<p> <m>\xvec\cdot\xvec = \len{\xvec}^2 </m>. </p>
				</li>
				<li>
					<p> <m>\proj{\yvec}{\xvec} = \frac{\yvec \cdot \xvec}{\len{\xvec}^2} \xvec </m>. </p>
				</li>
				<li>
					<p> <m> \xvec\cdot\yvec = \yvec\cdot\xvec </m>. </p>
				</li>
				<li>
					<p> <m> (a\xvec)\cdot\yvec = a(\xvec\cdot\yvec)</m>. </p>
				</li>
				<li>
					<p> <m>(a\xvec+b\yvec)\cdot\wvec = a\xvec\cdot\wvec + b\yvec\cdot\wvec</m>.</p>
				</li>
				<li>
					<p>
						<m>\xvec \cdot \yvec = \sum_{i=1}^n x_i y_i</m>.
					</p>
				</li>
			</ol>
			</p>
		</assemblage>

</subsection>

  <subsection>
    <title> <m>k</m>-means clustering </title>
	<idx><h>clustering</h></idx>

    <p>
      A typical problem in data science is to find some underlying
      patterns in a dataset.  Suppose, for instance, that we have the
      set of 177 data points plotted in <xref ref="fig-clusters" />.
      Notice that the points are not scattered around haphazardly;
      instead, they seem to form clusters.  Our goal here is to
      develop a strategy for detecting the clusters.
    </p>

    <figure xml:id="fig-clusters">
      <caption>
	A set of 177 data points.
      </caption>
      <sidebyside width="80%">
	<image source="images/cluster-plot" />
      </sidebyside>
    </figure>

    <p>
      To see how this could be useful, suppose we have medical data
      describing a group of patients, some of whom have been diagnosed
      with a specific condition, such as diabetes.  Perhaps we have a
      record of age, weight, blood sugar, cholesterol, and other
      attributes for each patient.  It could be that the data points
      for the group diagnosed as having the condition form a cluster
      that is somewhat distinct from the rest of the data.  Suppose
      that we are able to identify that cluster and that we are then
      presented with a new patient that has not been tested for the
      condition.  If the attributes for that patient place them in
      that cluster, we might identify them as being at risk for the
      condition and prioritize them for appropriate screenings.
    </p>

    <p>
      If there are many attributes for each patient, the data may be
      high-dimensional and not easily visualized.  We would therefore
      like to develop an algorithm that separates the data points into
      clusters without human intervention.  
	  <idx><h>clustering</h></idx>
	  We call the result a <term>clustering</term>.
    </p>

    <p>
      The next activity introduces a technique, called <m>k</m>-means
      clustering, that helps us find clusterings.  To do so, we will
      view the data points as vectors so that the distance between two
      data points equals the length of the vector joining them.  That
      is, if two points are represented by the vectors <m>\vvec</m>
      and <m>\wvec</m>, then the distance between the points is
      <m>\len{\vvec-\wvec}</m>. 
    </p>

    <activity>
      <statement>
	<p>
		<idx><h>centroid</h></idx>
	  To begin, we identify the <term>centroid</term>, or the
	  average, of a set of vectors
	  <m>\vvec_1, \vvec_2, \cdots,\vvec_n</m> 
	  as
	  <me>
	    \frac1n\left(\vvec_1+\vvec_2+\cdots+\vvec_n\right)
	  </me>.

	  <ol marker="a.">
	    <li>
	      <p>
		Find the centroid of the vectors
		<me>
		  \vvec_1=\twovec11,
		  \vvec_2=\twovec41,
		  \vvec_3=\twovec44.
		</me>
		and sketch the vectors and the centroid using <xref
		ref="fig-centroid-plot" />.  You may wish to simply plot
		the points represented by the tips of the vectors rather
		than drawing the vectors themselves.
	      </p>

	      <figure xml:id="fig-centroid-plot">
		<caption>
		  The vectors <m>\vvec_1</m>, <m>\vvec_2</m>,
		  <m>\vvec_3</m> and their centroid.
		</caption>
		<sidebyside width="50%">
		  <image source="images/empty-5-k" />
		</sidebyside>
	      </figure>

	      <p>
		Notice that the centroid lies in the center of the
		points defined by the vectors.
	      </p>
	    </li>

	    <li>
	      <p>
		Now we'll illustrate an algorithm
		that forms clusterings.  To begin, consider the
		following 
		points, represented as vectors,
		<me>
		  \vvec_1=\twovec{-2}{1},
		  \vvec_2=\twovec11,
		  \vvec_3=\twovec12,
		  \vvec_4=\twovec32,
		</me>
		which are shown in <xref ref="fig-clustering-ex" />.
	      </p>

	      <figure xml:id="fig-clustering-ex">
		<caption>
		  We will group this set of four points into two
		  clusters.
		</caption>
		<sidebyside width="50%">
		  <image source="images/kmeans-data" />
		</sidebyside>
	      </figure>

	      <p> 
		Suppose that we would like to group these points into
		<m>k=2</m> clusters. (Later on, we'll see how to choose
		an appropriate value for <m>k</m>, the number of
		clusters.)  We begin by choosing two points
		<m>c_1</m> and <m>c_2</m> at random and declaring them
		to be the <q>centers</q>' of the two clusters.
	      </p>

	      <p>
		For example, suppose we randomly choose
		<m>c_1=\vvec_2</m> and <m>c_2=\vvec_3</m> as the center
		of two clusters.  The cluster centered on
		<m>c_1=\vvec_2</m> will be the set of points that are
		closer to <m>c_1=\vvec_2</m> than to <m>c_2=\vvec_3</m>.
		Determine which of the four data points are in this
		cluster, which we denote by <m>C_1</m>, and circle them
		in <xref ref="fig-clustering-ex" />.
	      </p>
	    </li>

	    <li>
	      <p>
		The second cluster will consist of the data points that
		are closer to <m>c_2=\vvec_3</m> than
		<m>c_1=\vvec_2</m>.  Determine which of the four points
		are in this cluster, which we denote by <m>C_2</m>, and
		circle them in <xref ref="fig-clustering-ex" />.
	      </p>
	    </li>

	    <li>
	      <p>
		We now have a clustering with two clusters, but we will
		try to improve upon it in the following way.  First,
		find the centroids of the two clusters;  that is,
		redefine <m>c_1</m> to be the centroid of cluster
		<m>C_1</m> and <m>c_2</m> to be the centroid of
		<m>C_2</m>. 
		Find those centroids and
		indicate them in <xref ref="fig-clustering-ex-2" />
	      </p>

	      <figure xml:id="fig-clustering-ex-2">
		<caption>
		  Indicate the new centroids and clusters.
		</caption>
		<sidebyside width="50%">
		  <image source="images/kmeans-data" />
		</sidebyside>
	      </figure>

	      <p>
		Now update the cluster <m>C_1</m> to be
		the set of points closer to <m>c_1</m> than <m>c_2</m>.
		Update the cluster <m>C_2</m> in a similar way and
		indicate the clusters in <xref ref="fig-clustering-ex-2"
		/>.
	      </p>
	    </li>

	    <li>
	      <p>
		Let's perform this last step again.  That is, update the
		centroids <m>c_1</m> and <m>c_2</m> from the new
		clusters and then update the clusters <m>C_1</m> and
		<m>C_2</m>.  Indicate your centroids and clusters in 
		<xref ref="fig-clustering-ex-3" />.
	      </p>

	      <figure xml:id="fig-clustering-ex-3">
		<caption>
		  Indicate the new centroids and clusters.
		</caption>
		<sidebyside width="50%">
		  <image source="images/kmeans-data" />
		</sidebyside>
	      </figure>

	      <p>
		Notice that this last step produces the same set of
		clusters so there is no point in repeating it.  We
		declare this to be our final clustering.
	      </p>
	    </li>

	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The centroid is <m>c = \twovec32</m>.
	      </p>
	      <sidebyside width="50%">
		<image source="images/k-means-6-1" />
	      </sidebyside>
	    </li>

	    <li>
	      <p>
		<m>C_1 = \{\vvec_1,
		\vvec_2\}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		<m>C_2=\{\vvec_3,\vvec_4\}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		<m>c_1 = \twovec{-1/2}{0}</m> and <m>c_2 =
		\twovec22</m>
	      </p>
	      <p>
		<m>C_1 = \{\vvec_1\}</m> and <m>C_2=\{\vvec_2, \vvec_3,
		\vvec_4\}</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		<m>c_1 = \twovec{-2}1</m> and
		<m>c_2 = \twovec{5/3}{5/3}</m>.  The clusters
		<m>C_1</m> and <m>C_2</m> are unchanged.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The centroid is <m>c = \twovec32</m>.
	      </p>
	      <sidebyside width="50%">
		<image source="images/k-means-6-1" />
	      </sidebyside>
	    </li>

	    <li>
	      <p>
		The first cluster is <m>C_1 = \{\vvec_1,
		\vvec_2\}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		The second cluster is 
		<m>C_2=\{\vvec_3,\vvec_4\}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		We redefine <m>c_1 = \twovec{-1/2}{0}</m> and <m>c_2 =
		\twovec22</m>. This leads to new clusters <m>C_1 =
		\{\vvec_1\}</m> and <m>C_2=\{\vvec_2, \vvec_3,
		\vvec_4\}</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		We have new centroids <m>c_1 = \twovec{-2}1</m> and
		<m>c_2 = \twovec{5/3}{5/3}</m>, and the clusters
		<m>C_1</m> and <m>C_2</m> are unchanged.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
      This activity demonstrates our algorithm for finding a
      clustering.  We first choose a value <m>k</m> and seek to break
      the data points into <m>k</m> clusters. The algorithm 
      proceeds in the following way:
      <ul>
	<li>
	  <p>
	    Choose <m>k</m> points <m>c_1, c_2, \ldots, c_k</m> at
	    random from our data set. 
	  </p>
	</li>
	<li>
	  <p>
	    Construct the cluster <m>C_1</m> as the set of data points
	    closest to <m>c_1</m>, <m>C_2</m> as the set of data
	    points closest to <m>c_2</m>, and so forth.
	  </p>
	</li>
	<li>
	  <p> Repeat the following until the clusters no longer
	  change: 
	  <ul>
	    <li>
	      <p>
		Find the centroids <m>c_1, c_2,\ldots,c_k</m> of the
		current clusters.
	      </p>
	    </li>
	    <li>
	      <p>
		Update the clusters <m>C_1,C_2,\ldots,C_k</m>.
	      </p>
	    </li>
	  </ul>
	  </p>
	</li>
      </ul>
    </p>

    <p>
      The clusterings we find depend on the initial random choice of
      points <m>c_1, c_2,\ldots, c_k</m>.  For instance, in the
      previous activity, we arrived, with the initial choice <m>c_1=
      \vvec_2</m> and <m>c_2=\vvec_3</m>, at the clustering:
      <me>
	\begin{array}{rcl}
	C_1 \amp {}={} \amp \{\vvec_1\} \\
	C_2 \amp {}={} \amp \{\vvec_2, \vvec_3,\vvec_4\}
	\end{array}
      </me>.
    </p>

    <p>
      If we instead choose the initial points to be <m>c_1 =
      \vvec_3</m> and <m>c_2=\vvec_4</m>, we eventually find the
      clustering:
      <me>
	\begin{array}{rcl}
	C_1 \amp {}={} \amp \{\vvec_1, \vvec_2, \vvec_3\} \\
	C_2 \amp {}={} \amp \{\vvec_4\}
	\end{array}
      </me>.
    </p>

    <p>
      Is there a way that we can determine which clustering is the
      better of the two?  It seems like a better clustering will be
      one for which the points in a cluster are, on average, closer to
      the centroid of their cluster.  If we have a clustering, we
      therefore define a function, called the <term>objective</term>,
      which measures the average of the square of the distance from
      each point to the centroid of the cluster to which that point
      belongs.  A clustering with a smaller objective will have
      clusters more tightly centered around their centroids, which
      should result in a better clustering.
    </p>

    <p>
      For example, when we obtain the clustering:
      <me>
	\begin{array}{rcl}
	C_1 \amp {}={} \amp \{\vvec_1, \vvec_2, \vvec_3\} \\
	C_2 \amp {}={} \amp \{\vvec_4\}
	\end{array}
      </me>.
      with centroids <m>c_1=\ctwovec{0}{4/3}</m> and
      <m>c_2=\vvec_4=\twovec32</m>, we find the objective to
      be
      <me>
	\frac14\left(
	\len{\vvec_1-c_1}^2 + 
	\len{\vvec_2-c_1}^2 + 
	\len{\vvec_3-c_1}^2 + 
	\len{\vvec_4-c_2}^2
	\right)
	= \frac 53
      </me>.
    </p>

    <activity>
      <statement>
	<p>
	  We'll now use the objective to compare clusterings and to
	  choose an appropriate value of <m>k</m>.
	  <ol marker="a.">
	    <li>
	      <p>
		In the previous activity, one initial choice of
		<m>c_1</m> and <m>c_2</m> led to the
		clustering:
		<me>
		  \begin{array}{rcl}
		  C_1 \amp {}={} \amp \{\vvec_1\} \\
		  C_2 \amp {}={} \amp \{\vvec_2, \vvec_3,\vvec_4\}
		  \end{array}
		</me>
		with centroids <m>c_1=\vvec_1</m> and
		<m>c_2=\twovec{5/3}{5/3}</m>.  
		Find the objective of this clustering.
	      </p>
	    </li>
	    <li>
	      <p>
		We have now seen two clusterings and computed their
		objectives.  Recall that our data set is shown in <xref
		ref="fig-clustering-ex" />.  Which of the two
		clusterings feels like the better fit?  How is this fit
		reflected in the values of the objectives?
	      </p>
	    </li>

	    <li xml:id="li-guess-clusters">
	      <p>
		Evaluating the following cell will load and display a
		data set consisting of 177 data points.  This data set
		has the name <c>data</c>.
		  </p>
		<sage>
		  <input>
sage.repl.load.load("https://raw.githubusercontent.com/davidaustinm/ula_modules/master/k_means.py", globals())
list_plot(data, color='blue', size=20, aspect_ratio=1)
		  </input>
		</sage>
		<p>
		Given this plot of the data, what would seem like a
		reasonable number of clusters?
	      </p>
	    </li>

	    <li>
	      <p>
		In the following cell, you may choose a value of
		<m>k</m> and then run the algorithm to determine and
		display a clustering and its objective.  If you run the
		algorithm a few 
		times with the same value of <m>k</m>, you will likely
		see different clusterings having different objectives.
		This is natural since our algorithm starts by making a
		random choice of points <m>c_1,c_2,\ldots,c_k</m>, and a
		different choices may lead to different clusterings.
		Choose a value of <m>k</m> and run the algorithm a few
		times.  Notice that clusterings having lower objectives
		seem to fit the data better.  Repeat this experiment
		with a few different values of <m>k</m>.
	      </p>
		<sage>
		  <input>
k = 2  # you may change the value of k here
clusters, centroids, objective = kmeans(data, k)
print('Objective =', objective)
plotclusters(clusters, centroids)
		  </input>
		</sage>
	    </li>
	    
	    <li>
	      <p>
		For a given value of <m>k</m>, our strategy is to run
		the algorithm several times and choose the clustering
		with the smallest objective.  After choosing a value of
		<m>k</m>, the following cell will run the
		algorithm 10 times and display the clustering having the
		smallest objective.
	      </p>
		<sage>
		  <input>
k = 2  # you may change the value of k here
clusters, centroids, objective = minimalobjective(data, k)
print('Objective =', objective)
plotclusters(clusters, centroids)
		  </input>
		</sage>
	      
	      <p>
		For each value of <m>k</m> between 2 and 9, find the
		clustering having the smallest objective and plot your
		findings in <xref ref="fig-clustering-elbow" />.
	      </p>
	      
	      <figure xml:id="fig-clustering-elbow">
		<caption>
		  Construct a plot of the minimal objective as it
		  depends on the choice of <m>k</m>.
		</caption>
		<sidebyside width="80%">
		  <image source="images/clustering-elbow" />
		</sidebyside>
	      </figure>

	      <p>
		This plot is called an <term>elbow plot</term> due to its
		shape.  Notice how the objective decreases
		sharply when <m>k</m> is small and then flattens out.
		This leads to a location, called the elbow, where the
		objective transitions from being sharply decreasing to
		relatively flat.  This means that increasing <m>k</m>
		beyond the elbow does not significantly decrease the
		objective, which makes the elbow a good choice for
		<m>k</m>.
	      </p>

	      <p>
		Where does the elbow occur in your plot above?  How does
		this compare to the best value of <m>k</m> that you
		estimated by simply looking at the data in <xref
		ref="li-guess-clusters" />.
	      </p>
	    </li>
	  </ol>
	</p>

	<p>
	  Of course, we could increase <m>k</m> until each data
	  point is its own cluster.  However, this defeats the
	  point of the technique, which is to group together
	  nearby data points in the hope that they share common
	  features, thus providing insight into the structure
	  of the data.
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The objective is <m>5/6</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		The clustering <m>C_1=\{\vvec_1\}</m> and <m>C_2
		= \{\vvec_2, \vvec_3, \vvec_4\}</m> has a smaller
		objective. 
	      </p>
	    </li>
	    <li>
	      <p>
		<m>k=6</m> or <m>k=7</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		With a fixed value of <m>k</m>, running the algorithm
		several times leads to different clusterings with
		different objectives.  If we increase <m>k</m>, the
		objective generally decreases.
	      </p>
	    </li>
	    <li>
	      <p>
		The elbow occurs around <m>k=6</m> or <m>k=7</m>.
	      </p>
	      <sidebyside width="90%">
		<image source="images/clustering-elbow-plot" />
	      </sidebyside>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The objective is
		<me> \frac14\left(\len{(\vvec_1 - c_1)}^2
		+ \len{(\vvec_2 - c_2)}^2
		+ \len{(\vvec_3 - c_2)}^2
		+ \len{(\vvec_4 - c_2)}^2\right)
		= \frac56
		</me>.
	      </p>
	    </li>

	    <li>
	      <p>
		The clustering with <m>C_1=\{\vvec_1\}</m> and <m>C_2
		= \{\vvec_2, \vvec_3, \vvec_4\}</m> appears to be a
		tighter clustering and has a smaller objective.
	      </p>
	    </li>
	    <li>
	      <p>
		It appears that the best clustering is either
		<m>k=6</m> or <m>k=7</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		With a fixed value of <m>k</m>, running the algorithm
		several times leads to different clusterings with
		different objectives.  If we increase <m>k</m>, the
		objective generally decreases.
	      </p>
	    </li>
	    <li>
	      <p>
		The elbow occurs around <m>k=6</m> or <m>k=7</m>,
		which are the values that we felt led to the best
		clusterings.
	      </p>
	      <sidebyside width="90%">
		<image source="images/clustering-elbow-plot" />
	      </sidebyside>
	    </li>
	  </ol>
	</p>
      </solution>
	    
    </activity>

    <p>
      We have now seen how our algorithm and the objective identify
      a reasonable value for <m>k</m>, the number of the clusters, and
      produce a good clustering having <m>k</m> clusters.  Notice that
      we don't claim to have found the best clustering as the true
      test of any clustering will be in how it helps us understand the
      dataset and helps us make predictions about any new data that we
      may encounter.
    </p>

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section introduced the dot product and the ability to
      investigate geometric relationships between vectors.
      <ul>
	<li>
	  <p>
	    The dot product of two vectors <m>\vvec</m> and
	    <m>\wvec</m> satisfies these properties:
	    <me>
	      \begin{array}{rcl}
	      \vvec\cdot\vvec \amp {}={} \amp \len{\vvec}^2 \\
	      \vvec\cdot\wvec \amp {}={} \amp
	      \len{\vvec}\len{\wvec}\cos\theta \\
	      \end{array}
	    </me>
	    where <m>\theta</m> is the angle between <m>\vvec</m> and
	    <m>\wvec</m>.
	  </p>
	</li>

	<li>
	  <p>
	    The vectors <m>\vvec</m> and <m>\wvec</m> are orthogonal
	    when <m>\vvec\cdot\wvec= 0</m>.
	  </p>
	</li>

	<li>
	  <p>
	    We explored some applications of the dot product to the
	    similarity of vectors, correlation of time series, and
	    <m>k</m>-means clustering.
	  </p>
	</li>
      </ul>
    </p>
  </subsection>

  <xi:include href="exercises/exercises6-1.ptx" />
  
</section>
