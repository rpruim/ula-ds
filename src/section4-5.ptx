<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-determinants"
	 xmlns:xi="http://www.w3.org/2001/XInclude">

  <title> Determinants </title>

  <introduction>
    <p> As invertibility plays a central role in this chapter, we need
    a criterion that tells us when a matrix is invertible.  
    We already know that a square matrix is invertible if and only if
    it is row equivalent to the identity matrix.  In this section, we
    will develop a second, numerical criterion that tells us when a
    square matrix is invertible.
    </p>

    <p> To begin, let's consider a <m>2\by2</m> matrix <m>A</m>
    whose columns are vectors <m>\vvec_1</m> and <m>\vvec_2</m>.  We
    have frequently drawn the vectors and studied the linear
    combinations they form using a figure such as <xref
    ref="fig-intro-dets" />.
    </p>

    <figure xml:id="fig-intro-dets">
      <caption>
	Linear combinations of two vectors <m>\vvec_1</m> and <m>\vvec_2</m> 
	form a collection of congruent parallelograms.
      </caption>
      <sidebyside width="40%">
	<image source="images/basis-1" />
      </sidebyside>
    </figure>

    <p> Notice how the linear combinations form a set of congruent
    parallelograms in the plane.  In this section, we will use the
    area of these parallelograms to define a numerical
    quantity called the determinant that tells us whether the matrix
    <m>A</m> is invertible.  
    </p>

    <sidebyside widths="60% 40%" valign="middle">
      <p> To recall, the area of parallelogram
      is found by multiplying the length of one side by the
      perpendicular distance to its parallel side.  Using the notation
      in the figure, the area of the
      parallelogram is <m>bh</m>.
      </p>
      <image source="images/parallelogram-area" />
    </sidebyside>

    <exploration>
      <statement>
      <p> We will explore the area formula in this preview
      activity. 

      <ol marker="a.">
	<li><p> Find the area of the following parallelograms. </p>

	<sbsgroup>
	  <sidebyside widths="3% 30% 3% 30% 3% 30%">
	    <p> 1. </p>
	    <image source="images/parallelogram-a" /> 
	    <p> 2. </p>
	    <image source="images/parallelogram-b" /> 
	    <p> 3. </p>
	    <image source="images/parallelogram-c" /> 
	  </sidebyside>
	  <sidebyside widths="3% 30% 3% 30% 3% 30%">
	    <p> 4. </p>
	    <image source="images/parallelogram-d" /> 
	    <p> 5. </p>
	    <image source="images/parallelogram-e" />
	    <p> 
	    </p>
	    <p>
	    </p>
	  </sidebyside>
	</sbsgroup>
	</li>

	<li>
	  <sidebyside widths="50% 40%" valign="middle">
	    <p> Explain why the area of the parallelogram formed by 
	      the vectors <m>\vvec</m> and <m>\wvec_1</m> is the same as
	    that formed by <m>\vvec</m> and <m>\wvec_2</m>. </p>
	    <image source="images/area-shear" />
	  </sidebyside>
	</li>
      </ol></p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p> We find the following areas.
	  <ol marker="1.">
	    <li><p> A <m>1\by1</m> square has area 1. </p></li>
	    <li><p> A <m>2\by3</m> rectangle has area 6. </p></li>
	    <li><p> The square has side length <m>\sqrt{2}</m> giving
	    an area of 2. </p></li>
	    <li><p> If we consider the horizontal length as the base,
	    we see that <m>b=h=2</m> so that the area is 4. </p></li>
	    <li><p> In the same way, we can consider both the base and
	    height to be 2 so that the area is 4. </p></li>
	  </ol>
	  </p></li>

	  <li><p> If we consider the base to be the length of
	  <m>\vvec</m>, then the height, which is the perpendicular
	  distance to its parallel side, is the same in both
	  parallelograms. </p></li>
	</ol></p>
      </solution>
	    
    </exploration>

  </introduction>

  <subsection>
    <title> Determinants of <m>2\by2</m> matrices </title>

    <p> Determinants are defined for any square matrix, 
		but we will begin by defining
    the determinant of a <m>2\by2</m> matrix <m>A =
    \left[\begin{array}{rr} \vvec_1 \amp \vvec_2
    \end{array}\right]</m>.  First, however, we need to define the
    <term>orientation</term> of an ordered pair of vectors.  As shown in <xref
    ref="fig-det-orientation" />, an ordered pair of vectors
    <m>\vvec_1</m> and <m>\vvec_2</m> is called <term>positively oriented</term>
    if the angle, measured in the counterclockwise direction,
    from <m>\vvec_1</m> to <m>\vvec_2</m> is less than
    <m>180^\circ</m>; we say the pair is <term>negatively oriented</term> if
    it is more than <m>180^\circ</m>.
	<idx> <h>orientation</h> <h>of a pair of vectors</h> </idx>
    </p>

    <figure xml:id="fig-det-orientation">
      <caption>
	The vectors on the left are positively oriented while the ones
	on the right are negatively oriented.
      </caption>
      <sidebyside width="60%">
	<image source="images/det-orientation" />
      </sidebyside>
    </figure>

    <definition>
	<idx> determinant </idx>
      <statement>
	<p> Suppose a <m>2\by2</m> matrix <m>A</m> has columns
	<m>\vvec_1</m> and <m>\vvec_2</m>.  If the pair of vectors is
	positively oriented, then the <term>determinant</term> of
	<m>A</m>, denoted <m>\det(A)</m>, is the area of the
	parallelogram formed by <m>\vvec_1</m> and <m>\vvec_2</m>.
	If the pair is negatively oriented, then <m>\det(A)</m> is
	minus the area of the parallelogram. </p>
      </statement>
    </definition>

    <example xml:id="example-det-identity">
      <statement>
	<p>
	  Consider the determinant of the identity matrix
	  <me>I =
	  \left[\begin{array}{rr} 1\amp 0 \\ 0 \amp 1 \\
	  \end{array}\right]
	  =
	  \left[\begin{array}{rr} \evec_1 \amp \evec_2 \\
	  \end{array}\right]
	  </me>.
	  As seen on the left of <xref ref="fig-det-identity" />,
	  the vectors <m>\vvec_1 = \evec_1</m> 
	  and <m>\vvec_2=\evec_2</m> form a positively oriented pair.
	  Since the parallelogram they form is a <m>1\by1</m> square,
	  we have <m>\det(I) = 1.</m>
	</p>

	<figure xml:id="fig-det-identity">
	  <caption>
	    The determinant <m>\det(I) = 1</m>, as seen on the left.
	    On the right, we see that <m>\det(A) = -2</m>
	    where <m>A</m> is the matrix whose columns are shown.
	  </caption>
	  <sidebyside width="75%">
	    <image source="images/det-identity" />
	  </sidebyside>
	</figure>

	<p> Now consider the matrix
	  <me>A = 
	  \left[\begin{array}{rr} -2\amp 0 \\ 0 \amp 1 \\
	  \end{array}\right]
	  =
	  \left[\begin{array}{rr} \vvec_1 \amp \vvec_2 \\
	  \end{array}\right]
	  </me>.
	  As seen on the right of <xref ref="fig-det-identity" />, the
	  vectors <m>\vvec_1</m> and <m>\vvec_2</m> form a negatively
	  oriented pair.  The parallelogram they define is a
	  <m>2\by1</m> rectangle so we have <m>\det(A) = -2</m>.
	  </p>
      </statement>
    </example>

    <activity xml:id="activity-2-by-2-determinants">
      <statement>
      <p> In this activity, we will find the determinant of some
      simple <m>2\by2</m> matrices and discover some important
      properties of determinants.
      </p>

      <figure xml:id="js-det">
	<caption>
	  The geometric meaning of the determinant of a matrix.
	</caption>

	<interactive xml:id="interactive-det"
		     platform="javascript" width="100%"
		     aspect="100:80"
		     source="jslibrary/figures.js
			     jslibrary/det.js"
		     preview="preview/det-preview.png">
	  <sbsgroup>
	    <sidebyside width="60%">
	      <slate xml:id="det-sliders"
		     aspect="9:2"
		     surface="canvas" />
	    </sidebyside>
	    <sidebyside width="60%">
	      <slate xml:id="det-canvas" aspect="1:1" surface="canvas" />
	    </sidebyside>
	  </sbsgroup>
	  <instructions>
	    <p>
	      The sliders in the diagram below allow you to choose a
	      matrix <m>A=\begin{bmatrix}a \amp b \\
	      c \amp d \\
	      \end{bmatrix}</m>.  The two vectors representing the columns
	      of the matrix, along with the parallelograms they define,
	      are shown below.
	    </p>
	  </instructions>
	  <static>
	    <sidebyside width="100%">
	      <p>
		There is an interactive diagram at
		<url href="http://gvsu.edu/s/0J9"
		     visual="gvsu.edu/s/0J9"/>
		that accompanies this activity.
	      </p>
	    </sidebyside>
	    <sidebyside width="50%">
	      <image source="preview/det-preview.png"/>
	    </sidebyside>
	  </static>
	</interactive>
      </figure>

      <p> <ol marker="a.">
	<li><p>
	  Use the diagram to find the determinant of the matrix
	  <m>\left[\begin{array}{rr} -\frac12 \amp 0 \\ 0 \amp 2
	  \end{array}\right]</m>.
	  Along with <xref ref="example-det-identity"/>, what
	  does this lead you to believe is generally true about 
	  the determinant of a diagonal matrix?
	</p></li>

	<li><p> Use the diagram to find the determinant of the matrix
	<m>\left[\begin{array}{rr} 0 \amp 1 \\
	1 \amp 0 \\
	\end{array}\right]</m>.  What is the geometric effect of the
	matrix transformation defined by this matrix?
	</p></li>

	<li><p> Use the diagram to find the determinant of the matrix
	<m>\left[\begin{array}{rr} 2 \amp 1 \\
	0 \amp 1 \\
	\end{array}\right]</m>.  
	More generally, what do you notice about the determinant of any matrix
	of the form
	<m>\left[\begin{array}{rr} 2 \amp k \\
	0 \amp 1 \\
	\end{array}\right]</m>?  What does this say about the
	determinant of an upper triangular matrix?
	</p></li>

	<li><p> Use the diagram to find the determinant of any matrix
	of the form
	<m>\left[\begin{array}{rr} 2 \amp 0 \\
	k \amp 1 \\
	\end{array}\right]</m>.  What does this say about the
	determinant of a 
	lower triangular matrix?
	</p></li>

	<li><p> Use the diagram to find the determinant of the matrix
	<m>\left[\begin{array}{rr} 1 \amp -1 \\
	-2 \amp 2 \\
	\end{array}\right]</m>.   In general,
	what is the determinant of a matrix whose columns are linearly
	dependent? 
	</p></li>

	<li><p> Consider the matrices
	<me>
	  A = \left[\begin{array}{rr}
	  2 \amp 1 \\
	  2 \amp -1 \\
	  \end{array}\right],~~~
	  B = \left[\begin{array}{rr}
	  1 \amp 0 \\
	  0 \amp 2 \\
	  \end{array}\right]
	  </me>.
	  Use the diagram to find the determinants of <m>A</m>,
	  <m>B</m>, and <m>AB</m>.  What does this suggest is
	  generally true about the 
	  relationship of <m>\det(AB)</m> to <m>\det(A)</m> and
	  <m>\det(B)</m>?
	</p></li>
      </ol></p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p> The determinant is <m>-1</m> because the vectors are
	  negatively oriented and the rectangle has sides of length
	  <m>\frac12</m> and <m>2</m>.  
	  The determinant of a diagonal matrix seems to be
	  the product of the diagonal entries.
	  </p></li>

	  <li><p> The matrix transformation is a reflection over the
	  line <m>y=x</m> and we see that the determinant is
	  <m>-1</m>.
	  </p></li>

	  <li><p> The determinant will continue to be <m>2</m> for any
	  value of <m>k</m>.  This illustrates the fact that the
	  determinant of an upper triangular matrix equals the product
	  of its diagonal entries.
	  </p></li>

	  <li><p> The same reasoning tells us that this determinant is
	  <m>2</m> and, in fact, the determinant of a lower triangular
	  matrix equals the product of its diagonal entries.
	  </p></li>

	  <li><p> The determinant of this matrix is <m>0</m> because
	  the parallelogram formed by the vector has no area.  This
	  suggests that the determinant of a matrix whose columns are
	  linearly dependent is <m>0</m>.  </p></li>

	  <li><p> We find that <m>\det(A) = -4</m>, <m>\det(B) = 2</m>,
	  and <m>\det(AB) = -8</m>.  This suggests that
	  <m>\det(AB) = \det(A) \det(B)</m>.
	  </p></li>
	</ol></p>
      </solution>
	  
    </activity>


    <p> <xref ref="activity-2-by-2-determinants" /> illustrates some 
		important properties of determinants that are true more generally,
		including for larger matrices.

    <ul>
      <li><p> If <m>A</m> is a diagonal matrix, then <m>\det(A)</m>
      equals the product of the entries on the diagonal.  
	  A <m>2 \by 2</m> diagonal matrix represents a transformation that scales horizontally 
	  and vertically by the scaling factors on the diagonal.  
	  The area of the transformed unit square is the product of the diagonal entries.
	  If exactly one of the two is negative, the orientation is reversed and the determinant will be  
	  negative.  We saw an example of this in <xref ref="fig-det-identity" />.
	  </p>
	  </li>
		
      <li><p> If <m>A</m> is a triangular matrix, then <m>\det(A)</m>
      equals the product of the entries on the diagonal.  A <m>2 \by 2</m> 
	  triangular matrix represents a shear transformation.  The diagonal entries 
	  represent the horizontal and vertical scaling.  The non-zero off-diagonal entry 
	  represents the amount of shear.  This distorts a rectangle into a parallelogram, but 
	  leaves the area unchanged.
	  For example,
      <me>
	\det\left[\begin{array}{rr} 2 \amp 2 \\
	0 \amp 3 \\ \end{array}\right] = 2\cdot 3 = 6
	</me>, since the two parallelograms in <xref
	ref="fig-parallelogram-f" /> have equal area.
      </p>

      <figure xml:id="fig-parallelogram-f">
	<caption> The determinant of a triangular matrix equals the
	product of its diagonal entries.
	</caption>
	<sidebyside widths="30% 30%">
	  <image source="images/parallelogram-f" />
	  <image source="images/parallelogram-b" />
	</sidebyside>
      </figure>
      </li>

      <li><p> We also saw that
      <me>
	\det \left[\begin{array}{rr}
	0 \amp 1 \\
	1 \amp 0 \\
	\end{array}\right] = -1
      </me>
      because the columns form a negatively oriented pair.
      You may remember from <xref ref="subsec-triangular-invertible"/>
      that a matrix such as this is obtained by interchanging two rows
      of the identity matrix.  
      </p></li>

      <li><p> The determinant satisfies a multiplicative property,
      which says that
      <me>
	\det(AB) = \det(A)\det(B).
      </me>
      Rather than simply thinking of the
      determinant as the area of a parallelogram, we may also think
      of it as a factor by which areas are scaled under the matrix
      transformation defined by the matrix.  Applying the matrix
      transformation defined by <m>B</m> will scale area by
      <m>\det(B)</m>.  If we then compose <m>B</m> with the matrix
      transformation defined by <m>A</m>, area will scale a second
      time by the factor <m>\det(A)</m>.  The net effect is that the
      matrix transformation defined by <m>AB</m> scales area by
      <m>\det(A)\det(B)</m> so that <m>\det(AB)=\det(A)\det(B)</m>.
      </p></li>
    </ul>
    </p>

	<p>As we will see, these properties hold for larger matrices as well.</p>

    

  </subsection>


  <subsection xml:id="subsec-determinants-larger-matrices">
	<title>Determinants of larger matrices</title>


    <p> 
		<idx><h>parallelepiped</h></idx>
	We can define determinants for <m>n\by n</m> matrices by measuring the "volume"
	of a "box" (technically a <term>parallelepiped</term>) defined by the columns of the matrix, 
	even if this box resides in <m>\real^n</m> for some very large <m>n</m>.  
    </p>

    <sidebyside widths="60% 40%">
      <p> For example, the columns of a <m>3\by3</m> matrix
      <m>A</m> will form a parallelpiped, like the one shown here, and
      there is a means by which we can classify sets of such vectors
      as either positively or negatively oriented.  Therefore, we can
      define the determinant <m>\det(A)</m> in terms of the volume of
      the parallelpiped, but we will not worry about the details here.
      </p>
      <image source="images/det-3d" />
    </sidebyside>

	<p>
		Soon we will learn general methods for computing the determinant of a matrix, 
		but some determinants are easy to calculate directly from the definition. 
	</p>

	<proposition xml:id="prop-row-of-zeros-determinant">
		<statement>
			<p>
			<ol>
				<li>
					<p>
						<m>\det(I) = 1</m> for any identity matrix <m>I</m>.
					</p>
				</li>
				<li>
					<p>
				If <m>A</m> is a square matrix with a row of 0s, then <m>\det(A) = 0</m>.
					</p>
				</li>
			</ol>
			</p>
		</statement>

		<proof>
			<p>
			<ol>
				<li>
					<p>
				The identity transformation doesn't change anything, so the scaling factor is 1.
					</p>
				</li>
				<li>
					<p>
						If row <m>r</m> of <m>A</m> consists entirely of 0s, then the <m>r</m>th component 
						of <m>A \xvec</m> will be 0.  This means that the transformation associated with 
						<m>A</m> collapses into a smaller space, and so the volume of any transformed 
						shape will be 0.
					</p>
				</li>
			</ol>
		</p>
		</proof>
	</proposition>

	<p>Several properties established for <m>2\by2</m> matrices hold for larger matrices as well.
		In particular the following proposition holds by the same argument given for <m>2 \by 2</m> matrices.
	</p>

	<proposition xml:id="proposition-det-product">
		<statement>
	  <p>
		For any <m>n \by n</m> matrices <m>A</m> and <m>B</m>, 
		  <me>\det(AB) = \det(A)\det(B)</me>.
		</p>
	</statement>
	</proposition>

	<p>
		<xref ref="proposition-det-product" /> allows us to establish a relationship between the
		determinant of an invertible matrix and the determinant of its inverse. For an invertible
		matrix <m>A</m> we know that <m>\det(A) \det(A^{-1}) = \det(A A^{-1}) = \det(I) = 1</m>.
		This leads to the following proposition. 
	</p>

	<proposition xml:id="proposition-det-of-inverse">
		<statement>
	  <p>
		If <m>A</m> is an invertible matrix, then
		<m>\det(A^{-1}) = 1/\det(A)</m>.
	  </p>
		</statement>
	  </proposition>


  </subsection>

  <subsection>
    <title> Determinants of elementary matrices</title>

    <p>
      Perhaps the most important property of determinants also
      appeared in <xref ref="activity-2-by-2-determinants" />.
	  We saw that when the columns of the matrix <m>A</m> are linearly dependent, 
	  the parallelogram formed by those vectors folds down onto a line.
      For instance, if <m>A=\begin{bmatrix} 1 \amp 2 \\ -1 \amp -2 \\
      \end{bmatrix}</m>, then the resulting parallelogram, as shown
      in
      <xref ref="figure-linear-dep-det"/>, has zero area, which means
      that <m>\det(A)=0</m>. 
    </p>

    <figure xml:id="figure-linear-dep-det">
      <caption>
	When the columns of <m>A</m> are linearly dependent, we find
	that <m>\det(A) = 0</m>.
      </caption>
      <sidebyside width="40%">
	<image source="images/parallelogram-g"/>
      </sidebyside>
    </figure>

    <p>
      The condition that the columns of <m>A</m> are linearly
      dependent is equivalent same to the condition that <m>A</m>
      is not invertible.  This leads us to believe that <m>A</m> is
      not invertible if and only if its determinant is zero.  
      The following proposition expresses this thought.
    </p>

    <proposition xml:id="prop-invertible-det">
      <statement>
	<p> 
		The matrix <m>A</m> is invertible if and only if <m>\det(A) \neq 0</m>.
	</p>
      </statement>
    </proposition>

    <p> To understand this proposition more fully, let's remember that the
    matrix <m>A</m> is invertible if and only if it is row equivalent
    to the identity matrix <m>I</m>.  We will therefore consider how
    the determinant changes when we perform row operations on a matrix.
    Along the way, we will discover an effective means to compute the
    determinant.
    </p>


    <p> In <xref ref="subsec-triangular-invertible" />, we saw how to
    describe the three row operations, scaling, interchange, and 
    replacement, using matrix multiplication.  
	Now we would like to compute the determinants of each of these matrices.
			<!-- If we perform a row
    operation on the matrix <m>A</m> to obtain the matrix <m>A'</m>,
    we would like to relate <m>\det(A)</m> and <m>\det(A')</m>.  To do
    so, 
    remember that -->
    <ul>
      <li> <title>Scaling</title> 
		<p> Scalings are performed by multiplying a matrix <m>A</m>
      by a diagonal matrix, such as 
      <me>
	S = \left[\begin{array}{rrr}
	1 \amp 0 \amp 0 \\
	0 \amp 3 \amp 0 \\
	0 \amp 0 \amp 1 \\
	\end{array}\right],
      </me>
      which has the effect of multiplying the second row of <m>A</m>
      by <m>3</m> to obtain <m>A'</m>.  The associated transformation scales 
	  by a factor of three in one dimension, so it scales the volumne by 3 as well. 
	  Thus <m>\det(S) = 3</m>.
						<!-- Since <m>S</m> is diagonal, we
      know that its determinant is the product of its diagonal entries
      so that <m>\det(S) = 3</m>.   -->
						<!-- This means that <m>A'=SA</m> and therefore
      <me>\det(A')=\det(S)\det(A) = 3\det(A).</me> 
	  In general, if we -->
	  More generally, if we scale any row of <m>A</m>
      <m>k</m>, we have <me>\det(S) = k</me>.
	  Note that when <m>k</m> is negative, the orientation is reversed by scaling.
	</p></li>

      <li><title>Interchanges</title>
	  
	  <p> Interchanges are performed by matrices <m>P</m> which result from interchanging two rows of 
		the identity matrix. For example,
      <me>
	P = \left[\begin{array}{rrr}
	0 \amp 1 \amp 0 \\
	1 \amp 0 \amp 0 \\
	0 \amp 0 \amp 1 \\
	\end{array}\right],
      </me>
      interchanges the first and second rows of <m>I_3</m>.  
	  Note that for any such matrix <m>P</m>, <m>P P = I</m>, so <m>\det(P) \det(P) = 1</m>.
	  This means that <m>\det(P)</m> must either be <m>1</m> or <m>-1</m>. As a transformation, 
	  an interchange "swaps" two axes.  This reverses the orientation, so   
	  <me>
		\det(P) = -1
	  </me>
	  for any interchange matrix <m>P</m>.

						<!-- 
	  As we saw in <xref ref="proposition-det-properties"/>, <m>\det(P) = -1</m>.
      Therefore, when <m>PA=A'</m>, we have <me>\det(A')=\det(P)
      \det(A) = -\det(A).</me> In other words, <m>\det(A') =
      -\det(A)</m> when we perform an interchange.  -->

	</p></li> 

      <li><p> Row replacement operations are performed by matrices
      such as
      <me>
	R = \left[\begin{array}{rrr}
	1 \amp 0 \amp 0 \\
	0 \amp 1 \amp 0 \\
	-2 \amp 0 \amp 1 \\
	\end{array}\right],
      </me>
      which multiplies the first row by <m>-2</m> and adds the result
      to the third row.  Each such matrix has exactly one off-diagonal entry that is non-zero. 
	  This represents a shear along one axis of <m>\real^n</m>, so 
	  <me>
		\det(R) = 1
	  </me>
	  for any replacement matrix <m>R</m>.
						<!-- Since this is a lower triangular matrix, we
      know that the determinant is the product of the diagonal entries,
      which says that <m>\det(R) = 1</m>.  This means that when
      <m>RA = A'</m>,
      we have <m>\det(A')=\det(R)\det(A) = \det(A)</m>.  In
      other words, a row replacement does not change the determinant. -->
      </p>
	</li>
    </ul>
    </p>

    <proposition xml:id="proposition-det-elementary-matrices">
      <title> The determinants of elementary matrices</title>
      <statement>
	<p>
	  <ul>
	    <li>
	      <p>
		If <m>S</m> is a scaling matrix with scaling factor <m>k</m>, then  
		<m>\det(S) = k</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>P</m> is an interchange matrix, then  <m>\det(P) = -1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>R</m> is a replacement matrix, then <m>\det(R) = 1</m>.
	      </p>
	    </li>
	  </ul>
	</p>
      </statement>
    </proposition>

	<p>
	<xref ref="proposition-det-elementary-matrices"/> allows us to compute the determinant of any matrix.  
		If <m>A</m> is invertible, then <m>A\sim I</m>. But this means that 
		<me>
		E_k E_{k-1} \cdots E_2 E_1 A = I	
		</me>
		for some elementary matrices <m>E_1, E_2, \dots, E_k</m>.
		By <xref ref="prop-" /> this implies
		<me>
		\det(E_k) \det(E_{k-1}) \cdots \det(E_2) \det(E_1) \det(A) = \det(I) = 1
		</me>,
		so
		<me>
		\det(A) = \frac{1}{\det(E_k) \det(E_{k-1}) \cdots \det(E_2) \det(E_1)}
		</me>.
		Recall that <m>\det(R) = 1</m> for an replacement matrices, so most of these terms are 1.
		Also <m>\det(P) = -1</m>, so we only need to know if there we have an even or odd number of  
		interchanges to know if the determinant is positive or negative. The abosulte value of the determinant of  
		<m>A</m> is the reciprocal of the product of the scaling factors used in any scaling operations involved in 
		computing the RREF.
	</p>

	<p>
		If <m>A</m> is not invertible, then the RREF of <m>A</m> will have a row of zeros, so  
		by <xref ref="prop-row-of-zeros-determinant"/>, 
		<me>
		\det(E_k) \det(E_{k-1}) \cdots \det(E_2) \det(E_1) \det(A) = 0
		</me>,
		from which we see that <m>\det(A) = 0</m>.
	</p>
	<p>
		So with just a little bookkeeping, we can compute the determinant of a matrix while we are calculting its reduced  
		row echelon form.
	</p>


    <proposition xml:id="proposition-det-row-operations">
      <title> The effect of row operations on the determinant </title>
      <statement>
	<p>
	  <ul>
	    <li>
	      <p>
		If <m>A'</m> is obtained from <m>A</m> by scaling a
		row by <m>k</m>, then <m>\det(A') = k\det(A)</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>A'</m> is obtained from <m>A</m> by
		interchanging two rows, then <m>\det(A') =
		-\det(A)</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>A'</m> is obtained from <m>A</m> by performing a
		replacement operation, then <m>\det(A') = \det(A)</m>.
	      </p>
	    </li>
	  </ul>
	</p>
      </statement>
    </proposition>

	<p>
		Elemenatary matrices also provide us a tool for demonstrating 
		<xref ref="proposition-det-triangular" /> and 
		<xref ref="prop-invertible-det"/>.
	</p>

	<proposition xml:id="proposition-det-triangular">
		<statement>
			<p>
		  The determinant of a square triangular matrix equals the
		  product of its diagonal entries.  
			</p>
		</statement>
	<proof>
		<p>
		If <m>A</m> is a square triangular matrix, then we can reduce <m>A</m> to a square diagonal matrix<m>D</m> 
		using only replacement operations.  Thus <m>\det(A) = \det(D)</m>.  Also, the diagonal entries of <m>D</m> 
		are the same as the diagonal entries of <m>A</m>. 
		</p>

		<p>
			A square diagonal matrix <m>D</m> can be reduced to <m>I</m> by a sequence of scaling operations. 
			If the diagonal entires of <m>D</m> are <m>d_1, d_2, \dots d_n</m>, then we must scale by
			<m>1/d_1, 1/d_2, \dots 1/d_n</m>, so      
			<me>
				\frac{1}{d_1} \cdot \frac{1}{d_2} \dots frac{1}{d_n} \det(D) = \det(I) = 1
			</me>
			and 
			<me>
				\det(D) = d_1 d_2 \cdots d_n
			</me>.
		</p>
	</proof>
	  </proposition>



	<proof>
		<title>Proof of <xref ref="prop-invertible-det"/></title>
		<p>
		If <m>A</m> is invertible, then <m>A \sim I</m>. So <m>\det(A)</m> is a nonzero multiple of <m>\det(I) = 1</m>.  
		Thus <m>\det(A) \neq 0</m>. 
		</p>
		<p>
			If <m>A</m> is not invertible, then <m>A \sim B</m> for some matrix <m>B</m> that contains a row of 0s.
			This means <m>\det(A)</m> is a nonzero multiple of <m>\det(B) = 0</m>.  Thus <m>\det(A) = 0</m>.
		</p>
	</proof>


    <activity xml:id="act-determinant-gaussian-elimination">
      <statement>
      <p> We will investigate the connection between the determinant
      of a matrix and its invertibility using Gaussian elimination.
      <ol marker="a.">
	<li><p> Consider the two upper triangular matrices
	<me>
	  U_1 =
	  \left[\begin{array}{rrr}
	  1 \amp -1 \amp 2 \\
	  0 \amp 2 \amp 4 \\
	  0 \amp 0 \amp -2 \\
	  \end{array}\right],~~~
	  U_2 =
	  \left[\begin{array}{rrr}
	  1 \amp -1 \amp 2 \\
	  0 \amp 2 \amp 4 \\
	  0 \amp 0 \amp 0 \\
	  \end{array}\right].
	</me>
	Remembering <xref
	ref="proposition-triangular-invertibility"/>, 
	which of the matrices <m>U_1</m> and <m>U_2</m> are
	invertible?  What are the determinants <m>\det(U_1)</m> and
	<m>\det(U_2)</m>? 
	</p></li>

	<li><p> Explain why an upper triangular matrix is invertible
	if and only if its determinant is not zero.
	</p></li>

	<li><p> 
	  Let's now consider the matrix
	  <me>
	    A = 
	    \left[\begin{array}{rrr}
	    1 \amp -1 \amp 2 \\
	    -2 \amp 2 \amp -6 \\
	    3 \amp -1 \amp 10 \\
	    \end{array}\right]
	  </me> and begin the Gaussian elimination process
	  with a row replacement operation
	  <me>
	    A = 
	    \left[\begin{array}{rrr}
	    1 \amp -1 \amp 2 \\
	    -2 \amp 2 \amp -6 \\
	    3 \amp -1 \amp 10 \\
	    \end{array}\right]
	    \sim
	    \left[\begin{array}{rrr}
	    1 \amp -1 \amp 2 \\
	    0 \amp 0 \amp -2 \\
	    3 \amp -1 \amp 10 \\
	    \end{array}\right]
	    = A_1
	    </me>.
	    What is the relationship between <m>\det(A)</m> and
	    <m>\det(A_1)</m>?
	</p></li>

	<li><p> Next we perform another row replacement operation:
	<me>
	  A_1=
	\left[\begin{array}{rrr}
	1 \amp -1 \amp 2 \\
	0 \amp 0 \amp -2 \\
	3 \amp -1 \amp 10 \\
	\end{array}\right]
	\sim
	\left[\begin{array}{rrr}
	1 \amp -1 \amp 2 \\
	0 \amp 0 \amp -2 \\
	0 \amp 2 \amp 4 \\
	\end{array}\right]
	= A_2
	</me>.
	What is the relationship between <m>\det(A)</m> and
	<m>\det(A_2)</m>? 
	</p></li>

	<li><p> Finally, we perform an interchange:
	<me>
	  A_2 =
	  \left[\begin{array}{rrr}
	1 \amp -1 \amp 2 \\
	0 \amp 0 \amp -2 \\
	0 \amp 2 \amp 4 \\
	\end{array}\right]
	\sim
	  \left[\begin{array}{rrr}
	  1 \amp -1 \amp 2 \\
	  0 \amp 2 \amp 4 \\
	  0 \amp 0 \amp -2 \\
	  \end{array}\right]
	  = U
	</me>
	to arrive at an upper triangular matrix <m>U</m>.  What is the
	relationship between <m>\det(A)</m> and <m>\det(U)</m>?
	</p></li>

	<li><p> Since <m>U</m> is upper
	triangular, we can compute its determinant, which allows us to
	find <m>\det(A)</m>.  What is <m>\det(A)</m>?  Is <m>A</m>
	invertible?
	</p></li>

	<li><p> Now consider the matrix
	<me>
	  A = 
	  \left[\begin{array}{rrr}
	  1 \amp -1 \amp 3 \\
	  0 \amp 2 \amp -2 \\
	  2 \amp 1 \amp 3 \\
	  \end{array}\right].
	</me>
	Perform a sequence of row operations to find an upper
	triangular matrix <m>U</m> that is row equivalent to
	<m>A</m>.  Use this to determine <m>\det(A)</m> and whether
	<m>A</m> invertible?
	</p></li>

	<li><p> Suppose we apply a sequence of row operations on a
	matrix <m>A</m> to obtain <m>A'</m>.  Explain why <m>\det(A)
	\neq 0</m> if and only if <m>\det(A') \neq 0</m>.
	</p></li>

	<li><p> Explain why an <m>n\by n</m> matrix
	<m>A</m> is invertible if and only if <m>\det(A) \neq 0</m>.
	</p></li>

      </ol></p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p> The matrix <m>U_1</m> is invertible because we see
	  there is a pivot position in every row and column.  The
	  matrix <m>U_2</m>, however, is not invertible because there
	  is not a pivot position in the third row.
	  </p></li>

	  <li><p> The determinant of an upper triangular matrix equals
	  the product of its diagonal entries.  Consequently, if the
	  determinant of an upper triangular matrix is not zero, then
	  each of its diagonal entries must be nonzero.  In this case,
	  there is a pivot position in every row and every column so
	  that the matrix is invertible.  </p></li>

	  <li><p> Row replacement operations do not change the
	  determinant so <m>\det(A) = \det(A_1)</m>.
	  </p></li>

	  <li><p> In the same way, <m>\det(A) =
	  \det(A_2)</m>. </p></li> 

	  <li><p> Interchanges change the sign of the determinant so
	  <m>\det(A) = -\det(U)</m>.
	  </p></li>

	  <li><p> The determinant <m>\det(U) = -4</m> since it is the
	  product of the diagonal entries of <m>U</m>.  This means
	  that <m>\det(A) = 4</m>.  We see that <m>A</m> is invertible
	  because <m>U</m>, which has a pivot position in every row
	  and every column, is invertible.
	  </p></li>

	  <li><p> Beginning with a row replacement operation, we
	  arrive at
	  <me>A_1=\left[\begin{array}{rrr}
	  1 \amp - 1 \amp 3 \\
	  0 \amp 2 \amp -2 \\
	  0 \amp -3 \amp 3 \\
	  \end{array}\right]</me>.  We next scale the second row by
	  <m>\frac12</m> to obtain
	  <me>A_2=\left[\begin{array}{rrr}
	  1 \amp - 1 \amp 3 \\
	  0 \amp 1 \amp -1 \\
	  0 \amp -3 \amp 3 \\
	  \end{array}\right]</me>.  Another row replacement operation
	  gives
	  <me>A_3=\left[\begin{array}{rrr}
	  1 \amp - 1 \amp 3 \\
	  0 \amp 1 \amp -1 \\
	  0 \amp 0 \amp 0 \\
	  \end{array}\right]</me>.  Putting these operations together,
	  we see that <m>\det(A) = 2\det(U) = 0</m>.  In this case,
	  <m>A</m> is not invertible because <m>U</m>, which has a row
	  without a pivot position, is not invertible.
	  </p></li>

	  <li><p> Performing one of the three row operations either
	  leaves the determinant unchanged (row replacement), changes
	  its sign (interchange), or multiplies it by a nonzero number
	  (scaling).  Therefore, if we begin with a matrix whose
	  determinant is not zero, the determinant remains not zero
	  after any row operation is applied.
	  </p></li>

	  <li><p> If we apply a sequence of row operations to <m>A</m>
	  to find a
	  row equivalent matrix <m>U</m> that is upper triangular, we
	  know that <m>\det(A)\neq0</m> if and only if <m>\det(U) \neq 0</m>.
	  We also know that <m>A</m> is invertible if and only if
	  <m>U</m> is invertible.  Putting these facts together, we
	  conclude that <m>\det(A)\neq 0</m> if and only if <m>A</m> is
	  invertible. </p></li>

	</ol></p>
      </solution>
	    
    </activity>

    <p> As seen in this activity, row operations can be used to
    compute the determinant of a matrix.  More specifically, 
    applying the forward substitution phase of Gaussian
    elimination to the matrix <m>A</m> requires only replacement operations and interchanges and leads 
	us to an upper triangular matrix <m>U</m> so that <m>A\sim U</m> and <m>\det(A) = \pm \det(U)</m>.
	And <m>\det(U)</m> is easily calculated using <xref ref="proposition-det-triangular" />. So we  
	see that we can calculate determinates without completing the RREF algorithm.  If suffices to 
	do the forward substitutions phase.
    </p>

    <!-- <p>
      We know that <m>U</m> is
      invertible when all of its diagonal entries are nonzero.  We also
      know that <m>\det(U)\neq 0</m> under the same condition.  This
      tells us <m>U</m> is invertible if and only if <m>\det(U) \neq 0</m>.
    </p>

    <p>
      Now if <m>\det(A)\neq 0</m>, we also have <m>\det(U)\neq 0</m>
      since applying a sequence of row operations to <m>A</m> only
      multiplies the determinant by a nonzero number.  It then follows
      that <m>U</m> is invertible so <m>U\sim I</m>.  Therefore, we
      also know that <m>A\sim I</m> and so <m>A</m> must also be
      invertible.
    </p>

    <p>
      This explains <xref ref="prop-invertible-det"/> and so we know
      that <m>A</m> is invertible
      if and only if <m>\det(A)\neq 0</m>.
    </p>

    <p>
      Finally, notice that if <m>A</m> is invertible, we have
      <m>A^{-1}A = I</m>, 
      which tells us that
      <me>
	\det(A^{-1}A) = \det(A^{-1})\det(A) = 1.
      </me>
      Therefore, <m>\det(A^{-1}) = 1/\det(A)</m>.
    </p>

    <proposition>
      <statement>
	<p>
	  If <m>A</m> is an invertible matrix, then
	  <m>\det(A^{-1}) = 1/\det(A)</m>.
	</p>
      </statement>
    </proposition> -->
	    
  </subsection>


  <subsection>
    <title> Cofactor expansions </title>
	<idx><h>cofactor expansion</h></idx>
	<idx><h>determinant</h><h>via cofactor expansion</h></idx>

    <p> We now have a technique for computing the determinant of a
    matrix using row operations.  There is another way to compute
    determinants, using what are called <term>cofactor expansions</term>,
    that will be important for us in the next chapter. 
    We will describe this method here.
    </p>

    <p> To begin, let's show how to compute the determinant of a <m>2\by2</m> matrix.</p>

		<assemblage xml:id="assemblage-det-2-by-2">
			<title>Determinant of a <m>2 \by 2 </m> matrix</title>
			<idx><h>determinant</h><h> of a <m>2 \by 2</m> matrix</h></idx>
			<p>
				Let <m> 
      A = \left[\begin{array}{rr}
      a \amp b \\
      c \amp d \\
      \end{array}\right]
				</m>.
				We can create an equivalent triangular matrix using <m> -\frac{c}{a} R_1 + R_2 \to R_2</m>.
				This gives us  
				<me>
      \det A = \det \left[\begin{array}{rr}
      a \amp b \\
      c \amp d \\
      \end{array}\right] 
	  = 
      \det \left[\begin{array}{rr}
      a \amp b \\
      0 \amp -\frac{c}{a} b + d \\
      \end{array}\right] 
	  = 
	  a (-\frac{c}{a} b + d) 
	  = 
      ad-bc
				</me>.
			</p>
		</assemblage>

		<p>
			Note that the derivation of a formula for the determinant of a <m> 2 \by 2 </m> matrix relied on two  
			facts: (a) replacement doesn't change the determinant, and (b) the determinant of a triangular matrix   
			is the product of the diagonal elements.
			<!-- This same general approach could be used on any square matrix: keep applying row operations until we  
			obtain a triangular matrix.  If we use scaling or or interchange, we need to keep track of how our determinant 
			changes.  But we can obtain a triangular matrix without using scaling, so  this just amounts to 
			keeping track of a possible sign change when we use interchange. -->
		</p>

    <p> Now that we have a formula for the determinant of a <m> 2 \by 2 </m> matrix in hand, we  
		can develop the method of cofactor expansion.
		Using a cofactor expansion to find the determinant of a more
    general <m>n\by n</m> matrix is a little more work so we will
    demonstrate it with an example.
    </p>

    <example>
      <statement>
	<p> We illustrate how to use a cofactor expansion to find the
	determinant of <m>A</m> where
	<me>
	  A = 
	  \left[\begin{array}{rrr}
	  1 \amp -1 \amp 2 \\
	  -2 \amp 2 \amp -6 \\
	  3 \amp -1 \amp 10 \\
	  \end{array}\right].
	</me>
	</p>

	<p> To begin, we choose one row or column.  It doesn't matter
	which we choose because the result will be the same in any
	case.  Here, we choose the second row
	<me>
	  \left[\begin{array}{rrr}
	  \lgray{1} \amp \lgray{-1} \amp \lgray{2} \\
	  -2 \amp 2 \amp -6 \\
	  \lgray{3} \amp \lgray{-1} \amp \lgray{10} \\
	  \end{array}\right]
	</me>.
	</p>

	<p> The determinant will be found by creating a sum of terms,
	one for each entry in the row we have chosen.  For each entry
	in the row, we form its term by
	multiplying
	<ul>
	  <li><p> <m>(-1)^{i+j}</m> where <m>i</m> and <m>j</m> are
	  the row and column numbers, respectively, of the
	  entry, </p></li>
	  <li><p> the entry itself, and </p></li>
	  <li><p> the determinant of the entries left over when we
	  have crossed out the row and column containing the
	  entry. </p> </li>
	</ul>
	</p>

	<p> Since we are computing the determinant of this matrix
	<me>
	  \left[\begin{array}{rrr}
	  \gray{1} \amp \gray{-1} \amp \gray{2} \\
	  -2 \amp 2 \amp -6 \\
	  \gray{3} \amp \gray{-1} \amp \gray{10} \\
	  \end{array}\right]
	</me>
	using the second row, the entry in the first column of this
	row is <m>-2</m>.  Let's see 
	how to form the term from this entry.
	</p>

	<p> The term itself is <m>-2</m>, and the matrix that is left
	over when we cross out the second row and first column is
	<me>
	  \left[\begin{array}{rrr}
	  \gray{1} \amp {-1} \amp {2} \\
	  \gray{-2} \amp \gray{2} \amp \gray{-6} \\
	  \gray{3} \amp {-1} \amp {10} \\
	  \end{array}\right]
	</me>
	whose determinant is
	<me>
	  \det\left[\begin{array}{rr}
	  -1 \amp 2 \\
	  -1 \amp 10 \\
	  \end{array}\right] =
	  -1(10) - 2 (-1) = -8
	</me>.
	Since this entry is in the second row and first column, the
	term we construct is
	<m> (-1)^{2+1}(-2)(-8) = -16
	</m>.
	</p>

	<p> Putting this together, we find the determinant to be
	<me>
	  \begin{aligned}
	  \left[\begin{array}{rrr}
	  {1} \amp {-1} \amp {2} \\
	  -2 \amp {2} \amp {-6} \\
	  {3} \amp {-1} \amp {10} \\
	  \end{array}\right] 
	  {}={}
	  \amp
	  (-1)^{2+1}(-2)\det\left[\begin{array}{rr}
	  -1 \amp 2 \\
	  -1 \amp 10 \\
	  \end{array}\right] \\
	  \amp
	  {}+{}
	  (-1)^{2+2}(2)\det\left[\begin{array}{rr}
	  1 \amp 2 \\
	  3 \amp 10 \\
	  \end{array}\right] \\
	  \amp
	  {}+{}
	  (-1)^{2+3}(-6)\det\left[\begin{array}{rr}
	  -1 \amp -1 \\
	  3 \amp -1 \\
	  \end{array}\right] \\
	  \\
	  {}={}
	  \amp
	  (-1)(-2)(-1(10)-2(-1)) \\
	  \amp + (1)(2)(1(10)-2(3)) \\
	  \amp + (-1)(-6)((-1)(-1)-(-1)3) \\ \\
	  {}={}
	  \amp
	  -16 + 8 + 12 \\
	  {}={}
	  \amp
	  4 \\

	  \end{aligned}
	</me>.
	Notice that this agrees with the determinant that we found for
	this matrix using row operations in the <xref ref="act-determinant-gaussian-elimination" />.
	</p>	  
	  
      </statement>
    </example>

    <activity>
      <statement>
      <p> We will explore cofactor expansions through some
      examples.
      <ol marker="a.">
	<li><p> Using a cofactor expansion, show that the determinant
	of the following matrix 
	<me> \det
	  \left[\begin{array}{rrr}
	  2 \amp 0 \amp -1 \\
	  3 \amp 1 \amp 2 \\
	  -2 \amp 4 \amp -3 \\
	  \end{array}\right] = -36
	</me>.  Remember that you can choose any row or column to
	create the expansion, but the choice of a particular row or
	column may simplify 
	the computation.
	</p></li>

	<li><p> Use a cofactor expansion to find the determinant of
	<me>
	  \left[\begin{array}{rrrr}
	  -3 \amp 0 \amp 0 \amp 0 \\
	  4 \amp 1 \amp 0  \amp 0 \\
	  -1 \amp 4 \amp -4 \amp 0\\
	  0 \amp 3 \amp 2 \amp 3 \\
	  \end{array}\right]
	</me>.  (Which row should you choose to make your work especially easy?) 
	Explain how the cofactor expansion technique shows
	that the determinant of a triangular matrix is equal to the
	product of its diagonal entries.
	</p></li>

	<li><p> Use a cofactor expansion to determine whether the
	following vectors form a basis of <m>\real^3</m>:
	<me>
	  \threevec{2}{-1}{-2},
	  \threevec{1}{-1}{2},
	  \threevec{1}{0}{-4}
	</me>.
	</p></li>
	
	<li><p> NumPy or SciPy will compute the determinant of a matrix <c>A</c>
	with the command <c>numpy.linalg.det()</c> or <c>scipy.linalg.det</c>.  
	Use Python to find the determinant of the matrix
	<idx><h><c>numpy.linalg.det()</c></h> <see><c>linalg.det()</c></see></idx>
	<idx><h><c>scipy.linalg.det()</c></h> <see><c>linalg.det()</c></see></idx>
	<idx><h><c>linalg.det()</c></h></idx>
	<idx><h>determinant</h> <h>in Python</h></idx>
	<me>
	  \left[\begin{array}{rrrr}
	  2 \amp 1 \amp -2 \amp -3 \\
	  3 \amp 0 \amp -1  \amp -2 \\
	  -3 \amp 4 \amp 1 \amp 2\\
	  1 \amp 3 \amp 3 \amp -1 \\
	  \end{array}\right]
	</me>.
</p>
	<sage language="python">
	  <input>
import numpy as np 
from scipy import linalg
	  </input>
	</sage>
	  
	</li>

      </ol></p>
      </statement>

      <solution>
	<p><ol marker="a.">
	  <li><p> We will using a cofactor expansion along the first
	  row so that
	  <me>
	    \begin{array}{rl}
	    \det\left[\begin{array}{rrr}
	    2 \amp 0 \amp -1 \\
	    3 \amp 1 \amp 2 \\
	    -2 \amp 4 \amp -3 \\
	    \end{array}\right]
	    = \amp
	    (-1)^{1+1}\cdot 2 \det\left[\begin{array}{rr}
	    1 \amp 2 \\
	    4 \amp -3 \\
	    \end{array}\right] \\
	    \amp 
	    + (-1)^{1+3}\cdot (-1) \det\left[\begin{array}{rr}
	    3 \amp 1 \\
	    -2 \amp 4 \\
	    \end{array}\right] \\
	    = \amp
	    2(-11) - 14 = -36
	    \end{array}
	  </me>.
	  </p></li>

	  <li><p> Expanding along the first row gives
	  <me>
	    \begin{array}{rl}
	    \det
	    \left[\begin{array}{rrrr}
	    -3 \amp 0 \amp 0 \amp 0 \\
	    4 \amp 1 \amp 0  \amp 0 \\
	    -1 \amp 4 \amp -4 \amp 0\\
	    0 \amp 3 \amp 2 \amp 3 \\
	    \end{array}\right] \amp
	    = 3\det
	    \left[\begin{array}{rrr}
	    1 \amp 0  \amp 0 \\
	    4 \amp -4 \amp 0\\
	    3 \amp 2 \amp 3 \\
	    \end{array}\right] \\
	    \amp
	    =-3(1)\det
	    \left[\begin{array}{rr}
	    -4 \amp 0 \\
	    2 \amp 3 \\
	    \end{array}\right] \\
	    \amp
	    =-3(1)(-4)(3) = 36
	    \end{array}
	  </me>.
	  </p></li>

	  <li><p> We form the matrix <m>A</m> whose columns are the
	  three given vectors.  Expanding along either the second row or third
	  column to take advantage of the zero in the <m>(2,3)</m>
	  entry, we see that <m>\det(A) = 0</m>, which means that
	  <m>A</m> is not invertible.  Therefore, the vectors do not
	  form a basis for <m>\real^3</m>.
	  </p></li>

	  <li><p> Python tells us that <m>\det(A) = 72</m>.
	  </p></li>
	</ol></p>
      </solution>
	    
    </activity>

  </subsection>

  <subsection>
    <title> Summary </title>

    <p> In this section, we associated a numerical quantity, the determinant, to a square matrix and
		showed how it tells us whether the matrix is invertible. <ul>
				<li>
					<p> The determinant of a matrix has a geometric interpretation. In particular,
		when <m>n=2</m>, the determinant is the signed area of the parallelogram formed by the two
		columns of the matrix. </p>
				</li>

				<li>
					<p> The determinant satisfies many properties, including the facts that <ul>
							<li>
								<p>
									<m>\det(AB) = \det(A) \det(B)</m>, and </p>
							</li>
							<li>
								<p>
									the determinant of a triangular matrix is equal to the
									product of its diagonal entries.
								</p>
							</li>
						</ul>
					</p>
				</li>

				<li>
					<p> These properties helped us compute the determinant of a
						matrix using row operations. This also led to the important
						observation that the determinant of a matrix is nonzero if and
						only if the matrix is invertible. </p>
				</li>

				<li>
					<p> Finally, we learned how to compute the determinant of a matrix using <term>cofactor
		expansions</term>, which will be a valuable tool for us in the next chapter. </p>
				</li>
			</ul></p>

    <p> We have seen three ways to compute the
    determinant: by interpreting the determinant as a signed area or
    volume; by applying appropriate row operations; and by using a
    cofactor expansion.  It's worth spending a moment to think about
    the relative merits of these approaches.
    </p>

    <p> The geometric definition of the determinant tells us that the
    determinant is measuring a natural geometric quantity, an insight
    that does not easily come through the other two approaches.  The
    intuition we gain by thinking about the determinant geometrically
    makes it seem reasonable that the determinant should be zero for
    matrices that are not invertible: if the columns are linearly
    dependent, the vectors cannot create a positive volume.
    </p>

    <p> Approaching the determinant through row operations provides an
    effective means of computing the determinant.  In fact, this is
    what most computer programs do behind the scenes when they
    compute a determinant.  This approach is also a useful theoretical
    tool for explaining why the determinant tells us whether a matrix
    is invertible.
    </p>

    <p> The cofactor expansion method will be useful to us in the next
    chapter when we look at eigenvalues and eigenvectors.  It is not,
    however, a practical way to compute a determinant.  To see why,
    consider the fact that the determinant of a <m>2\by2</m>
    matrix, written as <m>ad-bc</m>,
    requires us to compute two terms, <m>ad</m> and <m>bc</m>.  To
    compute the determinant of a <m>3\by3</m> matrix, we need to
    compute three <m>2\by2</m> determinants, which involves
    <m>3\cdot 2 = 6</m> terms.  For a <m>4\by4</m> matrix, we need
    to compute four <m>3\by3</m> determinants, which produces
    <m>4\cdot3\cdot2 = 24</m> terms.  Continuing in this way, we see
    that the cofactor expansion of a <m>10\by10</m> matrix would
    involve <m>10\cdot9\cdot8\ldots3\cdot2 = 10! = 3628800</m> terms.
    </p>

    <p> By contrast, we have seen that the number of steps required to
    perform Gaussian elimination on an <m>n\by n</m> matrix is
    proportional to <m>n^3</m>.  When <m>n=10</m>, we have <m>n^3 =
    1000</m>, which points to the fact that finding the determinant
    using Gaussian elimination is considerably less work.
    </p>

  </subsection>
  
  <xi:include href="exercises/exercises4-5.ptx" />

</section>
