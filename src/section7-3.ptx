<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-pca"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Principal Component Analysis </title>

  <introduction>

    <p>
      We are sometimes presented with a dataset that includes many 
	  variables for each observational unit.   When this happens, 
	  the case vectors (rows of the column-variate data matrix or columns  
	  of the row-variate data matrix) live in a high dimensional space.
	  For instance, we looked at a dataset describing body fat index (BFI) in
      <xref ref="activity-BFI" /> where each case vector is
      six-dimensional.  Developing an intuitive understanding of such
      data is hampered by the fact that it is challenging to visualize.
    </p>

    <p>
      This section explores a technique called <term> principal
      component analysis</term>, which enables us to reduce the
      dimension of a dataset so that it may be visualized or studied
      in a way that makes interesting features more readily stand out.
      Our previous work with variance and the orthogonal
      diagonalization of symmetric matrices provides the key ideas.
    </p>

    <exploration>
      <statement>
	<p>
	  We will begin by recalling our earlier discussion of
	  variance.  Suppose we have a dataset that leads to the
	  covariance matrix
	  <me>
	    S = \begin{bmatrix}
	    7 \amp -4 \\
	    -4 \amp 13
	    \end{bmatrix}.
	  </me>
	  <ol marker="a.">
	    <li>
	      <p>
		Suppose that <m>\uvec</m> is a unit eigenvector of
		<m>S</m> with eigenvalue <m>\lambda</m>.  What is the
		variance <m>V_{\uvec}</m> in the <m>\uvec</m>
		direction?
	      </p>
	    </li>

	    <li>
	      <p>
		Find an orthogonal diagonalization of <m>S</m>.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		What is the total variance?
	      </p>
	    </li>

	    <li>
	      <p>
		In
		which direction is the variance greatest and what is
		the variance in this direction?  If we project the
		data onto this line, how much variance is lost?
	      </p>
	    </li>

	    <li>
	      <p>
		In which direction is the variance smallest and how is
		this direction related to the direction of maximum
		variance? 
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>V_{\uvec} = \uvec\cdot(S\uvec) =
		\lambda\uvec\cdot\uvec = \lambda</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		We can write <m>S=QDQ^{\transpose}</m> where
		<me>
		  D=\begin{bmatrix}
		  15 \amp 0 \\
		  0 \amp 5 \\
		  \end{bmatrix},~~~
		  Q = \begin{bmatrix}
		  \frac1{\sqrt{5}} \amp \frac2{\sqrt{5}} \\
		  -\frac2{\sqrt{5}} \amp \frac1{\sqrt{5}} \\
		  \end{bmatrix}.
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		The total variance is the sum of the eigenvalues,
		<m>V=\lambda_1 + \lambda_2 = 15 + 5 = 20</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The variance is greatest in the direction of the
		eigenvector associated to the largest eigenvalue.
		This direction is defined by
		<m>\twovec{\frac{1}{\sqrt{5}}}{-\frac2{\sqrt{5}}}</m>,
		and the variance is 15 in this direction.  
	      </p>
	    </li>
	    <li>
	      <p>
		The variance is smallest in the direction defined by
		<m>\twovec{\frac2{\sqrt{5}}}{\frac1{\sqrt{5}}}</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
		
    </exploration>

    <p>
      Here are some ideas we've seen previously that will be
      particularly useful for us in this section.  Remember that 
      the covariance matrix of a dataset is <m>S=\frac 1{n-1} X^{\transpose}X</m>
      where <m>X</m> is the column-variate matrix of <m>n</m> demeaned
      data.
      <ul>
	<li>
	  <p>
	    When <m>\uvec</m> is a unit vector, the variance of the
	    demeaned data after projecting onto the line defined by
	    <m>\uvec</m> is given by the quadratic form <m>V_{\uvec} =
	    \uvec\cdot(S\uvec)</m>.
	  </p>
	</li>
	<li>
	  <p>
	    In particular, if <m>\uvec</m> is a unit eigenvector of
	    <m>S</m> with associated eigenvalue <m>\lambda</m>, then
	    <m>V_{\uvec} = \lambda</m>.
	  </p>
	</li>
	<li>
	  <p>
	    Moreover, variance is additive, as we recorded in <xref
	    ref="prop-variance-additivity" />: if <m>W</m> is a subspace
	    having an orthonormal basis
	    <m>\uvec_1,\uvec_2,\ldots,\uvec_n</m>, then the variance
	    <me>
	      V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots +
	      V_{\uvec_n}\text{.} 
	    </me>
	  </p>
	</li>
      </ul>
    </p>

  </introduction>

  <subsection>
    <title> Themes of Principal Component Analysis </title>

    <p>
      Let's begin by looking at an example that illustrates the
      central theme of this technique.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose that we work with a dataset having 100 observations of 5 variables.
	  The demeaned column-variate data matrix <m>X</m> is therefore <m>100 \by 5</m> 
	  and leads to the covariance matrix <m>S=\frac1{99}~X^{\transpose}X</m>, which is a
	  <m>5\by5</m> matrix.  Because <m>S</m> is symmetric, the
	  Spectral Theorem tells us it is orthogonally diagonalizable
	  so suppose that <m>S = QDQ^{\transpose}</m> where
	  <me>
	    Q = \begin{bmatrix}
	    \uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4 \amp \uvec_5
	    \end{bmatrix},\hspace{24pt}
	    D = \begin{bmatrix}
	    13 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 10 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 2 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0
	    \end{bmatrix}.
	  </me>
	  <ol marker="a.">
	    <li>
	      <p>
		What is <m>V_{\uvec_2}</m>, the variance in the
		<m>\uvec_2</m> direction?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Find the variance of the data projected onto the line
		defined by <m>\uvec_4</m>.  What does this say about the
		data?
	      </p>
	    </li>

	    <li>
	      <p>
		What is the total variance of the data?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Consider the 2-dimensional subspace spanned by
		<m>\uvec_1</m> and <m>\uvec_2</m>.  If we project the
		data onto this subspace, what fraction 
		of the total variance is represented by the variance of the
		projected data?
	      </p>
	    </li>
	    
	    <li>
	      <p>	    
		How does this question change if we project onto the
		3-dimensional subspace 
		spanned by <m>\uvec_1</m>, <m>\uvec_2</m>, and
		<m>\uvec_3</m>?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		What does this tell us about the data?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>10</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>0</m>, which tells us
		every case vector is in the
		orthogonal complement of <m>\uvec_4</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>25</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>92\%</m> of the variance
	      </p>
	    </li>
	    <li>
	      <p>
		<m>100\%</m> of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		All of the data lies in the <m>3</m>-dimensional
		subspace spanned by
		<m>\uvec_1</m>, <m>\uvec_1</m>, and <m>\uvec_1</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
	  
	<solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>V_{\uvec_2} = \lambda_2 = 10</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V_{\uvec_4} = \lambda_4 = 0</m>, which tells us
		there is no variance in the <m>\uvec_4</m> direction.
		Therefore, when we project onto the line defined by
		<m>\uvec_4</m>, every case vector projects to
		<m>\zerovec</m> so every case vector is in the
		orthogonal complement of <m>\uvec_4</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V = V_{\uvec_1} + V_{\uvec_2} + V_{\uvec_3} +
		V_{\uvec_4} + V_{\uvec_5}  = 13+10+2+0+0 = 25</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The variance of the data projected onto this subspace
		is <m>13+10=23</m>, which represents <m>23/25=92\%</m>
		of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		Projecting onto this 3-dimensional subspace retains
		all of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		All of the data lies in the <m>3</m>-dimensional
		subspace spanned by
		<m>\uvec_1</m>, <m>\uvec_1</m>, and <m>\uvec_1</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
      
    </activity>

    <p>
      This activity demonstrates how the eigenvalues of the covariance
      matrix can tell us when data are clustered around, or even wholly
      contained within, a smaller dimensional subspace.  In
      particular, the original data is 5-dimensional, but we see
      that it actually lies in a 3-dimensional subspace of
      <m>\real^5</m>.  Later in this section, we'll see how to use
      this observation to work with the data as if it were
      three-dimensional, an idea known as <term>dimensional reduction</term>.
    </p>

    <p>
      <idx> principal components</idx> 
	  The eigenvectors <m>\uvec_j</m>
      of the covariance matrix are called <term>principal
      components</term>, and we will order them so that their associated
      eigenvalues decrease.  Generally speaking, we hope that the
      first few principal components retain most of the variance, as
      the example in the activity demonstrates.  In that example, we
      have the sequence of subspaces
      <ul>
	<li>
	  <p>
	    <m>W_1</m>, the 1-dimensional subspace spanned by
	    <m>\uvec_1</m>, which retains <m>13/25 = 52\%</m> of the
	    total variance,
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_2</m>, the 2-dimensional subspace spanned by
	    <m>\uvec_1</m> and <m>\uvec_2</m>, which retains <m>23/25
	    = 92\%</m> of the variance, and
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_3</m>, the 3-dimensional subspace spanned by
	    <m>\uvec_1</m>, <m>\uvec_2</m>, and <m>\uvec_3</m>, which
	    retains all of the variance.
	  </p>
	</li>
      </ul>
    </p>

    <p>
      Notice how we retain more of the total variance as we increase
      the dimension of the subspace onto which the data are projected.
      Eventually, projecting the data onto <m>W_3</m> retains all the
      variance, which tells us the data must lie in <m>W_3</m>, a
      smaller dimensional subspace of <m>\real^5</m>.
    </p>

    <p>
      In fact, these subspaces are the best possible.  We know that
      the first principal component <m>\uvec_1</m> is the eigenvector
      of <m>S</m> associated to the largest eigenvalue.  This means
      that the variance is as large as possible in the <m>\uvec_1</m>
      direction.  In other words, projecting onto any other line
      will retain a smaller amount of variance.  Similarly, projecting
      onto any other 2-dimensional subspace besides <m>W_2</m> will
      retain less variance than projecting onto <m>W_2</m>.  The
      principal components have the wonderful ability to pick out the
      best possible subspaces to retain as much variance as
      possible.
    </p>

    <p>
      Of course, this is a contrived example.  Typically, the presence
      of noise in a dataset means that we do not expect all the points
      to be wholly contained in a smaller dimensional subspace. One situation 
	  where this does occur, however, is when some of the variables in the data 
	  set are computed as linear combinations of other variables. This could 
	  happen, for example, if our data had measurements in two different units 
	  (say, Fahrenheit and Celsius temperatures) or if the data included both 
	  subtotals and totals in a sum (e.g., before tax bill, tax, and total with 
	  tax included). 
	  <!-- And it always happens if there are more variables than 
	  observational units, since the number of linearly independent columns in 
	  <m>X</m> is bounded by the number of rows. -->
	</p>

	<p>
      In this exaple, the 2-dimensional subspace <m>W_2</m> retains
      <m>92\%</m> of the variance.  Depending on the situation, we may
      want to write off the remaining <m>8\%</m> of the variance as
      noise in exchange for the convenience of working with a smaller
      dimensional subspace.  As we'll see later, we will seek a
      balance using a number of principal components large enough to
      retain most of the variance but small enough to be easy to work
      with. 
    </p>

    <activity>
      <statement>
	<p>
	  We will work here with a demeaned dataset having 100 observations of 3
	  variables.  Evaluating the following cell will create the demeaned column-variate 
	  data matrix <m> X</m> and plot the data as a 3-d scatter plot.
	</p>
	<sage language="python" auto-evaluate="yes">
		<input>
import numpy as np 
import pandas as pd
		</input>
		<output>
		</output>
	</sage>
	
<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_demo.py', globals()) -->
	  <sage language="python">
	    <input>
import matplotlib.pyplot as plt
data = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/pca-demo.csv', index_col=0)
X = data.to_numpy()
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
ax.scatter(X[:, 0], X[:, 1], X[:, 2])                                
plt.show()


	    </input>
	  </sage>
	  <p>
	  Notice that the data appears to cluster around a plane
	  though it does not seem to be wholly contained within that
	  plane.

	  <ol marker="a.">
	    <li>
	      <p>
		Use the matrix <c>X</c> to construct the covariance
		matrix <m>S</m>.  Then determine the variance in the
		direction of 
		<m>\uvec=\threevec{1/3}{2/3}{2/3}</m>?
		  </p>
		<sage language="python">
		  <input>

		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the eigenvalues of <m>S</m> and determine the
		total variance.
		  </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
		<p>
		Notice that Python does not necessarily sort the
		eigenvalues in decreasing order.
	      </p>
	    </li>

	    <li>
	      <p>
		Use the <c>numpy.linalg.eig()</c> command to find the
		eigenvectors of <m>S</m>.  
		Define vectors <c>u1</c>, <c>u2</c>, and <c>u3</c>
		representing the three principal components in order of
		decreasing eigenvalues.  How can you check if these
		vectors are an orthonormal basis for <m>\real^3</m>?
	      </p>  
	    </li>

	    <li>
	      <p>
		What fraction of the total variance is retained by
		projecting the data onto <m>W_1</m>, the subspace
		spanned by <m>\uvec_1</m>?  What fraction of the total
		variance is retained by projecting onto <m>W_2</m>,
		the subspace spanned by <m>\uvec_1</m> and
		<m>\uvec_2</m>?  What fraction of the total variance
		do we lose by projecting onto <m>W_2</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Each column of <m>X^{\transpose}</m> (each row of <m>X</m>) represents
		one observational unit in our data.  
		<idx><h>case vector</h></idx>
		We will refer to these as
		<term>case vectors</term>.
		In a traditional scatter plot (or cloud plot in 3 dimensions), each 
		case vector is represented by one of the dots. 
		If we project each case vector  <m>\xvec</m> 
		onto <m>W_2</m>, the Projection Formula tells us we obtain 
		<me>
		  \xhat = (\uvec_1\cdot\xvec) \uvec_1 +
		  (\uvec_2\cdot\xvec) \uvec_2.
		</me>
		Rather than viewing the projected data in
		<m>\real^3</m>, we will record the coordinates of
		<m>\xhat</m> in the basis defined by <m>\uvec_1</m>
		and <m>\uvec_2</m>;  that is, we will record the
		coordinates
		<me>
		  \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.  
		</me>
		Construct the matrix <m>Q</m> so that <m>Q^{\transpose}\xvec =
		\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>.
		
		</p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		    Since each column of <m>X^{\transpose}</m> represents one observational unit,
		    the matrix <m>Q^{\transpose}X^{\transpose}</m> is a <em>row-variate</em> representation 
			of the data projected onto a lower-dimensional subspace identified by 
			PCA. We can transpose again to get the column-variate represention <m>X Q</m>.
		  </p>
		<sage language="python">
		  <input>
PCA = X @ Q
fig, ax = plt.subplots()
ax.scatter(PCA[:, 0], PCA[:, 1])
plt.show()
		  </input>
		</sage>
		<p>
		Notice how this plot enables us to view the data as if
		it were two-dimensional.
		Why is this plot wider than it is tall?
	      </p>
	    </li>
		<li>
			<p>
				ScikitLearn provides another way to compute the first (highest-variance) 
				specified number of principle componenets from a data matrix.
			</p>
			<sage language="python">
				<input>
from sklearn.decomposition import PCA
pca3 = PCA(n_components=3)
pcaX = pca3.fit_transform(X)
print(pcaX.shape)
fig, ax = plt.subplots()
ax.scatter(pcaX[:, 0], pcaX[:, 1])
plt.show()
				</input>
			</sage>
			
		</li>
	  </ol>

	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>V_{\uvec} = 7885</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V=12195</m>
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>P</m> is the matrix of eigenvectors, evaluate
		<m>P^{\transpose}P</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>W_1</m> retains 
		<m>83\%</m> of the total variance, and 
		<m>W_2</m> retains <m>98\%</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Because the variance in the <m>\uvec_1</m> direction
		is greater than the variance in the <m>\uvec_2</m>
		direction.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
		<sage language="python">
			<input>
import numpy as np  
import pandas as pd
np.set_printoptions(precision = 2, suppress = True)
data = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/pca-demo.csv', index_col=0)
X = data.to_numpy()
S = 1/99 * (np.transpose(X) @ X)
print(S)
evals, P = np.linalg.eig(S)
print(evals, "\n", P)
print(np.transpose(P) @ P)
u = [P[:, i] for i in [0, 2, 1]]
V = [np.transpose(uu) @ S @  for uu in u]
print(V)
			</input>
			<output>
			</output>
		</sage>
		
	<p>
		<!-- Check computations here: n vs n-1 -->
	  <ol marker="a.">
	    <li>
	      <p>
		After constructing the covariance matrix <m>S =
		\frac{1}{99}X^{\transpose}X</m>, we find that <m>V_{\uvec} =
		\uvec\cdot(S\uvec) \approx 7965</m>.
	      </p>
		  <sage language="python">
			<input>
np.set_printoptions(precision = 2, suppress = True)
S = 1/99 * (np.transpose(X) @ X)
print("\nS"); print(S)
u = np.array((1,2,2))/3
print(np.transpose(u) @ S @ u)
print("\nVu"); print(u @ (S @ u))
			</input>
			<output>
			</output>
		  </sage>
		  
	    </li>
	    <li>
	      <p>
		The total variance <m>V</m> is the sum of the
		eigenvalues of <m>S_{XX}</m> so we obtain
		<m>V=12195</m>.
	      </p>
		  <sage language="python">
			<input>
evals, P = np.linalg.eig(S)
print("total variance"); print (np.sum(evals))
			</input>
			<output>
			</output>
		  </sage>
		  
	    </li>
	    <li>
	      <p>
		If we obtain <m>P</m>, the matrix of eigenvectors,
		from Python, computing <m>P^{\transpose}P</m> evaluates the dot
		products between the columns.  Since <m>P^{\transpose}P=I</m>,
		the basis provided by Sage is orthonormal.
	      </p>
		  <sage language="python">
			<input>
# put u1, u2, u3 into a list; ordered by eigen value
u = [P[:, i] for i in [0, 2, 1]]
print("\nu"); print(u)

# if orthogonormal, this should be I
print(np.transpose(P) @ P)

			</input>
			<output>
			</output>
		  </sage>
		  
	    </li>
	    <li>
	      <p>
		Projecting onto <m>W_1</m>, we see that
		<m>\lambda_1/V = 0.83</m> so <m>W_1</m> retains about
		<m>83\%</m> of the total variance.  The subspace
		<m>W_2</m> retains <m>(\lambda_1+\lambda_2)/V=0.98</m>
		or <m>98\%</m> of the total variance.  If we project
		onto <m>W_2</m> we lose less than <m>2\%</m> of the
		variance. 
	      </p>
		  <sage language="python">
			<input>
# compute variance in direction of each u
V = [np.transpose(uu) @ S @ uu for uu in u]
print("\nvariance")
print(V); print(np.sum(V))		
			</input>
			<output>
			</output>
		  </sage>
		  

	    </li>
	    <li>
	      <p>
		<m>Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}</m>
	      </p>
		<sage language="python">
			<input>
Q = np.column_stack(u[0], u[1])
print("Q"); print(Q)
			</input>
			<output>
			</output>
		</sage>
	    </li>
		
	    <li>
	      <p>
		The plot is wider because the variance in the
		<m>\uvec_1</m> direction, which corresponds to the
		horizontal coordinate, is greater than the variance in
		the <m>\uvec_2</m> direction.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
      This example is a more realistic illustration of principal
      component analysis.  The plot of the 3-dimensional data
      appears to show that the data lies close to a plane, and the
      principal components will identify this plane.  Starting with
      the <m>100\by3</m> matrix of demeaned data <m>X</m>, we
      construct the covariance matrix <m>S=\frac{1}{99} ~X^{\transpose}X</m> 
	  and study its eigenvalues.  Notice that the first two principal
      components account for more than 98% of the variance, which
      means we can expect the case vectors to lie close to <m>W_2</m>, the
      two-dimensional subspace spanned by <m>\uvec_1</m> and
      <m>\uvec_2</m>.
    </p>

    <p>
      Since <m>W_2</m> is a subspace of <m>\real^3</m>, 
      projecting the case vectors onto <m>W_2</m> gives a list of 100
      points in <m>\real^3</m>.  In order to visualize them more
      easily, we instead consider the coordinates of the projections
      in the basis defined by <m>\uvec_1</m> and <m>\uvec_2</m>.  For
      instance, we know that the projection of a case vector 
      <m>\xvec</m> is
      <me>
	\xhat = (\uvec_1\cdot\xvec)\uvec_1 +
	(\uvec_2\cdot\xvec)\uvec_2,
      </me>
      which is a three-dimensional vector.  Instead, we can record the
      coordinates <m>\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>
      and plot them in the two-dimensional coordinate plane, as
      illustrated in <xref ref="fig-pca-coords" />.
    </p>

    <figure xml:id="fig-pca-coords">
      <caption>
	The projection <m>\xhat</m> of a case vector <m>\xvec</m> onto
	<m>W_2</m> is a three-dimensional vector, which may be
	represented by the two coordinates describing this vector as a
	linear combination of <m>\uvec_1</m> and <m>\uvec_2</m>.
      </caption>
      <sidebyside widths="50% 40%">
	<image source = "images/pca-proj" />
	<image source = "images/pca-coords" />
      </sidebyside>
    </figure>

    <p>
      If we form the matrix <m>Q=\begin{bmatrix}\uvec_1 \amp \uvec_2
      \end{bmatrix}</m>, then we have
      <me>
	Q^{\transpose}\xvec = \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.
      </me>
      This means that the columns of <m>Q^{\transpose}X^{\transpose}</m> represent the
      coordinates of the projected case vectors, which may now be plotted in
      the plane. Transposing again gives the column-variate version of the 
	  PCA-projected data: <m>XQ</m>.
    </p>

    <p>
      In this plot, the first coordinate, represented by the
      horizontal coordinate, represents the projection of a case vector 
      onto the line defined by <m>\uvec_1</m> while the second
      coordinate represents the projection onto the line defined by
      <m>\uvec_2</m>.  Since <m>\uvec_1</m> is the first principal
      component, the variance in the <m>\uvec_1</m> direction is
      greater than the variance in the <m>\uvec_2</m> direction.  For
      this reason, the plot will be more spread out in the horizontal
      direction than in the vertical.
    </p>

  </subsection>

  <subsection>
    <title> Using Principal Component Analysis </title>

    <p>
      Now that we've explored the ideas behind principal component
      analysis, we will look at a few examples that illustrate its use.
    </p>

    <activity>
      <statement>
	<p>
	  The next cell will load a dataset describing the average
	  consumption of various food groups for citizens in each of
	  the four nations of the United Kingdom.  The units for each
	  entry are grams per person per week.
	</p>
	  <sage language="python">
	    <input>
np.set_printoptions(precision = 5, suppress = True)
Food = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/uk-diet.csv', index_col=0)
print(Food.info)
FoodX = np.transpose(Food.to_numpy())
data_mean = FoodX.mean(axis = 0)
FoodX = FoodX - data_mean
print(FoodX.mean(axis = 0))
# one row per country; 17 variables
print(FoodX.shape)
	    </input>
	  </sage>
	  <p>
	  We will view this as a dataset consisting of four case vectors in
	  <m>\real^{17}</m>.  Since we have 17 variables measured for each observation, 
	  we can't easily visualize this with a scatter plot, which would consist of
	  four points in 17-dimensional space. 
	  Studying the numbers themselves doesn't lead to much insight either.
	</p>

	<p>
	  In addition to loading the data, evaluating the cell above
	  created a vector <c>data_mean</c>, which is the componentwise mean of the
	  four case vectors, and <c>FoodX</c>, the <m>4 \by 17</m> column-variate 
	  matrix of demeaned data.
	  <ol marker="a.">
	    <li>
	      <p>
		What is the average consumption of Beverages across
		the four nations?
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>S</m> and its
		eigenvalues.  Because there are four points in
		<m>\real^{17}</m> whose mean is zero, there are only
		three nonzero eigenvalues.  
	      </p>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance does the
		first principal component account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the first principal component <m>\uvec_1</m> and
		project the four demeaned case vectors onto the line
		defined by <m>\uvec_1</m>.  Plot those vectors as points 
		on <xref ref="fig-pca-1d" />
	      </p>

	      <figure xml:id="fig-pca-1d">
		<caption>
		  A plot of the demeaned data projected onto the
		  first principal component.
		</caption>
		<sidebyside width="90%">
		  <image source = "images/pca-plot-1" />
		</sidebyside>
	      </figure>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance do the first
		two principal components account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the coordinates of the demeaned case vectors 
		projected onto <m>W_2</m>, the two-dimensional
		subspace of <m>\real^{17}</m> spanned by
		the first two principal components.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>

	      <p>
		Plot these coordinates in <xref ref="fig-pca-2d" />.
	      </p>

	      <figure xml:id="fig-pca-2d">
		<caption>
		  The coordinates of the demeaned case vectors 
		  projected onto the first two principal components.
		</caption>
		<sidebyside width="90%">
		  <image source="images/pca-plot-2" />
		</sidebyside>
	      </figure>
	    </li>

	    <li>
	      <p>
		What information do these plots reveal that is not
		clear from consideration of the original case vectors?
	      </p>
	    </li>

	    <li>
	      <p>
		Study the first principal component <m>\uvec_1</m>
		and find the first component of <m>\uvec_1</m>, which
		corresponds to the dietary category Alcoholic Drinks.
		(To do this, you may wish to use
		<c>N(u1, digits=2)</c> for a result that's easier to read.)
		If a case vector lies on the far right side of the plot
		in <xref ref="fig-pca-2d" />, what does it mean about
		that nation's consumption of Alcoholic Drinks?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>57.5</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>78805</m>,
		<m>33946</m>, and <m>4093</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>67\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		  </p>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinate </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>-145</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>477</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>-92</m> </cell>
		    </row>
		    <row>
		      <cell> Wales </cell>
		      <cell> <m>-241</m> </cell>
		    </row>
		  </tabular>
	    </li>

	    <li>
	      <p>
		<m>96\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		  </p>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinates </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-145, 3)</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>(477, 59)</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>(-92, -286)</m> </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-241, 225)</m> </cell>
		    </row>
		  </tabular>
	    </li>

	    <li>
	      <p>
		Northern Ireland appears to be significantly different
		from the other three nations.
	      </p>
	    </li>
	    <li>
	      <p>
		The average consumption of Alcoholic Drinks will
		be less than the mean.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Beverages is the second category so this would be the
		second component of the <c>data_mean</c> vector,
		which is <m>57.5</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The three nonzero eigenvalues are <m>78805</m>,
		<m>33946</m>, and <m>4093</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The total variance <m>V=116844</m> is the sum of the
		eigenvalues so the first principal component accounts
		for <m>\lambda_1/V = 67\%</m> of the total variance.
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
	      </p>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinate </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>-145</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>477</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>-92</m> </cell>
		    </row>
		    <row>
		      <cell> Wales </cell>
		      <cell> <m>-241</m> </cell>
		    </row>
		  </tabular>
	    </li>

	    <li>
	      <p>
		The first two principal components account for
		<m>96\%</m> of the total variance.
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
	      </p>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinates </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-145, 3)</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>(477, 59)</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>(-92, -286)</m> </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-241, 225)</m> </cell>
		    </row>
		  </tabular>
	    </li>

	    <li>
	      <p>
		Northern Ireland appears to be significantly different
		from the other three nations.  There are several possible
		reasons for this, both historical and geographical,
		that we might explore.
	      </p>
	    </li>
	    <li>
	      <p>
		The first component of <m>\uvec_1</m> is negative.
		Therefore, if a nation is on the right side of this
		plot, the average consumption of Alcoholic Drinks will
		be less than the mean.  This can be confirmed by
		looking at the original data.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>

    <p>
      This activity demonstrates how principal component analysis
      enables us to extract information from a dataset that may not be
      easily obtained otherwise.  As in our previous example, we see
      that the case vectors lie quite close to a
      two-dimensional subspace of <m>\real^{17}</m>.  In fact,
      <m>W_2</m>, the subspace spanned by the first two principal
      components, accounts for more than 96% of the variance.
      More importantly, when we project the data onto <m>W_2</m>, it
      becomes apparent that Northern Ireland is fundamentally
      different from the other three nations.
    </p>

    <p>
      With some additional thought, we can determine more specific
      ways in which Northern Ireland is different.  On the
      <m>2</m>-dimensional plot, Northern Ireland lies far to the right
      compared to the other three nations.  Since the data has been
      demeaned, the origin <m>(0,0)</m> in this plot corresponds to
      the average of the four nations.  The coordinates of the point
      representing Northern Ireland are about <m>(477, 59)</m>,
      meaning that the projected data point differs from the mean by
      about <m>477\uvec_1+59\uvec_2</m>.
    </p>

    <p>
      Let's just focus on the contribution from <m>\uvec_1</m>.  We
      see that the ninth component of <m>\uvec_1</m>, the one that
      describes Fresh Fruit, is about <m>-0.63</m>.  This means that
      the ninth component of <m>477\uvec_1</m> differs from the mean
      by about <m>477(-0.63) = -300</m> grams per person per week.  So
      roughly speaking, people in Northern Ireland are eating about
      300 fewer grams of Fresh Fruit than the average across the four
      nations.  This is borne out by looking at the original data,
      which show that the consumption of Fresh Fruit in Northern
      Ireland is significantly less than in the other nations.  Examing
      the other components of <m>\uvec_1</m> shows other ways in
      which Northern Ireland differs from the other three nations.
    </p>

    <activity xml:id="activity-pca-iris">
      <statement>
	<p>
	  In this activity, we'll look at a
	  <url href="https://archive.ics.uci.edu/ml/datasets/Iris"
	       visual="archive.ics.uci.edu">
	  well-known dataset 
	  </url>
	  that describes 150 irises representing three species of
	  iris: <em>iris setosa</em>, <em>iris versicolor</em>, and <em>iris virginica</em>.
	  For each flower, the length and width of
	  its sepal and the length and width of its petal, all in
	  centimeters, are recorded. 
	</p>

	<figure xml:id="fig-iris">
	  <caption>
	    One of the three species, <em>iris versicolor</em>, represented
	    in the dataset showing three shorter petals and three
	    longer sepals.  
	    (Source:
	    <url
		href="https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg"
		visual="gvsu.edu/s/21D">
	      Wikipedia
	      </url>,
	      License:
	      <url
		  href="https://commons.wikimedia.org/wiki/Commons:GNU_Free_Documentation_License,_version_1.2"
		  visual="gvsu.edu/s/21E">
		GNU Free DOcumetation License</url>)	      
	  </caption>
	  <sidebyside width="70%">
	    <image
		source="images/Iris_versicolor.jpg"/>
	  </sidebyside>
	</figure>

	<p>
	  Evaluating the following cell will load the dataset, which
	  consists four physcial measuremnts for each of 150 iris plants.  
	  In addition, we compute a vector <c>data_mean</c>, a four-dimensional
	  vector holding the means of the four measurements (which is the same as the mean  
	  of the 150 case vectors), and <c>irisX</c>,
	  the <m>150 \by 4</m> demeaned data matrix.
	</p>
<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_iris.py', globals()) -->
	  <sage language="python">
	    <input>
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

iris = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/iris.data')
iris4 = iris[iris.columns[:4]]
irisX = iris4.to_numpy()
iris_mean = irisX.mean(axis = 0)
irisX = irisX - iris_mean
	    </input>
	  </sage>
	  <p>
	  Since the data is four-dimensional, we are not able to
	  visualize it easily.  Of course, we could forget about two of the
	  measurements and plot a scatter plot of just two of the four variables,
	  say, just the sepal length and sepal width.
	  </p>

	  <sage language="python">
	    <input>
import seaborn.objects as so 
( 
	so.Plot(iris, x = 'sepal length', y = 'sepal width', color = 'species')
    .add(so.Dot())
    .show()
)
	    </input>
	  </sage>

	  <p>
	  <ol marker="a.">
	    <li>
	      <p>
		What is the mean sepal width?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>S</m> and its
		eigenvalues.
		  </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Find the fraction of variance for which the first two
		principal components account.
	      </p>
	    </li>

	    <li>
	      <p>
		Construct the first two principal components
		<m>\uvec_1</m> and <m>\uvec_2</m> along with the
		matrix <m>Q</m> whose columns are <m>\uvec_1</m> and
		<m>\uvec_2</m>.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		As we have seen, the columns of the matrix <m>Q^{\transpose}X^{\transpose}</m> or 
		rows of <m>XQ</m> hold the coordinates of the demeaned case vectors after
		projecting onto <m>W_2</m>, the subspace spanned by
		the first two principal components.  Evaluating the
		following cell shows a plot of these coordinates.
		  </p>
		<sage language="python">
		  <input>
irisPCA = X @ Q
( 
	so.Plot(iris, x = irisPCA[:, 0], y = irisPCA[:, 1], color = 'species')
	.add(so.Dot())
	.show()
)
		  </input>
		</sage>
		<p>
		Suppose we have a flower whose coordinates in this
		plane are <m>(-2.5, -0.75)</m>.  To what species does
		this iris most likely belong?  Find an estimate of the
		sepal length, sepal width, petal length, and petal
		width for this flower.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose you have an iris, but you only know that its
		sepal length is 5.65 cm and its sepal width is 2.75
		cm.  Knowing only these two measurements, determine
		the coordinates <m>(c_1, c_2)</m> in the plane where
		this iris lies.  To what species does this iris most
		likely belong?  Now estimate the petal length and petal
		width of this iris.
		  </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	    <li>
	      <p>
		Suppose you find another iris whose sepal width is 3.2
		cm and whose petal width is 2.2 cm.  Find the
		coordinates <m>(c_1, c_2)</m> of this iris and
		determine the species to which it most likely
		belongs.  Also, estimate the sepal length and the
		petal length.
	      </p>
		<sage language="python">
		  <input>
		  </input>
		</sage>
	    </li>

	  </ol>

      
	</p>
      </statement>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>3.05</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>4.20</m>, <m>0.24</m>,
		<m>0.08</m>, <m>0.02</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>97.8\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The columns of <m>Q</m> are for the first two
		principal components.
	      </p>
	    </li>
	    <li>
	      <p>
		<em>Iris setosa</em> and the vector of measurements is
		<m>\fourvec{5.43}{3.81}{1.49}{0.25}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The petal length is
		<m>3.99</m> and the petal width is <m>1.29</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The sepal length is <m>7.23</m> and the petal length
		is <m>6.15</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
		
      <solution>
	  <sage language="python">
		<input>
np.set_printoptions(precision = 3, suppress = True)
S = np.transpose(irisX) @ irisX / 149.0
irisEvals, irisE = np.linalg.eig(S)
Q = irisE[:, 0:2]
print("Q"); print(Q)
irisPCA = irisX @ Q 
print("irisPCA"); print(irisPCA[0:10, :])
( 
	so.Plot(iris, x = irisPCA[:, 0], y = irisPCA[:, 1], color = 'species')
	.add(so.Dot())
    .add(so.Dot(x=-2.5, y=-.075, color = "red"))
	.show()
)

pca_flower = np.array((-2.5, -0.75))
natural_flower = Q @ pca_flower + iris_mean
print(natural_flower)

		</input>
		<output>
		</output>
	  </sage>
	  
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The second component of <c>data_mean</c>, which is the
		one corresponding to sepal width, is <m>3.05</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The eigenvalues are <m>4.20</m>, <m>0.24</m>,
		<m>0.08</m>, and <m>0.02</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The first two principal components account for
		<m>97.8\%</m> of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>P</m> is the matrix whose columns are an
		orthonormal basis of eigenvectors, then <m>Q</m> is
		formed from the first two columns of <m>P</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		This would most likely belong to <em>iris setosa</em>.  To find
		its measurements, we evaluate <m>-2.5\uvec_1 -
		0.75\uvec_2 + \mvec</m> where <m>\mvec</m> is the
		vector of means.  This is the same as
		<m>Q\twovec{-2.5}{-0.75} + \mvec</m>, which
		gives the vector of
		measurements <m>\fourvec{5.43}{3.81}{1.49}{0.25}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Subtracting the mean sepal length and sepal width, we
		have <m>(-0.19, -0.30)</m>.  Then the first two
		components of <m>c_1\uvec_1+c_2\uvec_2 =
		Q\twovec{c_1}{c_2} = \twovec{-0.19}{-0.30}</m>.  This
		gives <m>(c_1, c_2) = (0.18, 0.40)</m>.  This looks
		like an <em>iris versicolor</em>.  As in the
		previous part, we can now find the petal length to be
		<m>3.99</m> and the petal width to be <m>1.29</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Using the same approach as the last part, we find
		<m>(c_1,c_2)=(2.90, -0.53)</m>, which gives a sepal
		length of <m>7.23</m> and a petal length of
		<m>6.15</m>.  Most likely, this flower belongs to 
		<em>iris virginica</em>. 
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

    </activity>
      

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section has explored <term>principal component analysis</term> (PCA) as a
      technique to reduce the dimension of a dataset.  From the
      demeaned column-variate data matrix <m>X</m>, we form the covariance matrix
      <m>S_{XX} = \frac1{n-1} X^{\transpose}X</m>, where <m>n</m> is the number of observational units.
      <ul>
	<li>
	  <p>
	    The eigenvectors <m>\uvec_1, \uvec_2, \ldots \uvec_m</m>,
	    of <m>S_{XX}</m> are called the <term>principal components</term>.  
		We arrange
	    them so that their corresponding eigenvalues are in
	    decreasing order.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>W_p</m> is the subspace spanned by the first
	    <m>p</m> principal components, then the variance of the
	    demeaned data projected onto <m>W_p</m> is the sum of the
	    first <m>p</m> eigenvalues of <m>S_{XX}</m>.  No other
	    <m>p</m>-dimensional subspace retains more variance when
	    the data are projected onto it.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>Q</m> is the matrix whose columns are the first
	    <m>p</m> principal components, then <m>XQ</m> contains the 
		column-variate data matrix, 
		expressed in the basis <m>\uvec_1,\ldots,\uvec_p</m>, of the data once
	    projected onto <m>W_p</m>.
	  </p>
	</li>

	<li>
	  <p>
	    Our goal is to use a number of principal components that
	    is large enough to retain most of the variance in the
	    dataset but small enough to be manageable.
	  </p>
	</li>
	<li>
		<p>
			The advantage of principal components analysis is the resulting data reudction.
			The primary disadvantage is the prinicpal components may not be easily or naturally 
			interpretable.
		</p>
	</li>
      </ul>
    </p>

  </subsection>

  <xi:include href="exercises/exercises7-3.ptx" />
  
</section>
