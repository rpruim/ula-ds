<?xml version="1.0" encoding="UTF-8"?>

<exercises>
	<introduction>
  <p>
	Here are some useful Python commands you may like to use in these exercises.
    <ul>
      <li>
	<p>
	  <c>Q, R = scipy.linalg.qr(A)</c> returns the <m>QR</m> factorization of a matrix 
	  <m>A</m> as <c>Q</c> and <c>R</c>,
	</p>
      </li>
      <li>
	<p>
	  <c>np.ones(n)</c> returns the <m>n</m>-dimensional
	  vector whose entries are all 1,
	</p>
      </li>
      <li>
	<p>
	  <c>x - x.mean()</c> demeans the vector <c>x</c>,
	</p>
      </li>
      <li>
	<p>
	  <c>numpy.vander(x, N)</c> returns the Vandermonde matrix
	  of degree <c>N - 1</c> (<c>N</c> columns) 
	  formed from the components of the vector <c>x</c>, and
	</p>
      </li>
	  <li>
		<p>
			<c>pandas.read_csv()</c> reads csv data into a pandas data frame.
		</p>
	  </li>
	  <li>
		<p>
			<c>x.to_numpy()</c> will convert pandas data frames and columns of data frames into NumPy arrays.
		</p>
	  </li>
	  <li>
		<p>
			<c>numpy.set_printoptions(precision = k, suppress = True)</c> 
			sets the precision used for NumPy's numerical output and suppresses the use of  
			scientific notation for small values. 
		</p>
	  </li>
	  <li>
		<p>
			<c>pprint.pprint()</c> sometimes offeres a nicer display of output.
		</p>
	  </li>
    </ul>
	The following packages and modules have been pre-loaded on this page.
  </p>
    <!-- <sage>
      <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
      </input>
    </sage> -->

	<sage language="python" auto-evaluate="yes">
		<input>
import numpy as np   
import pandas as pd 
import seaborn.objects as so
from pprint import pprint
np.set_printoptions(precision = 4, suppress = True)
		</input>
	</sage>
	
</introduction>

  <exercise>
    <statement>
      <p>
	Suppose we write the linear system
	<me>
	  \begin{bmatrix}
	  1 \amp -1 \\
	  2 \amp -1 \\
	  -1 \amp 3 
	  \end{bmatrix}
	  \xvec = \threevec{-8}5{-10}
	</me>
	as <m>A\xvec=\bvec</m>.
	<ol marker="a.">
	  <li>
	    <p>
	      Find an orthogonal basis for <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find <m>\bhat</m>, the orthogonal projection of
	      <m>\bvec</m> onto <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find a solution to the linear system <m>A\xvec =
	      \bhat</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\wvec_1=\threevec12{-1}</m> and <m>
		\wvec_2=\threevec012</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\bhat = \threevec21{-8}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xvec=\twovec{-1}{-3}</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Applying Gram-Schmidt gives the orthogonal basis
	      <me>
		\wvec_1=\threevec12{-1},\hspace{24pt}
		\wvec_2=\threevec012
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      Applying the Projection Formula gives
	      <m>\bhat = \threevec21{-8}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Solving the linear system gives <m>\xvec=\twovec{-1}{-3}</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
  </exercise>

  <exercise xml:id="ex-lst-squares-line">
    <statement>
      <p>
	Consider the data in <xref ref="table-lst-squares-line" />.
	  </p>
	<table xml:id="table-lst-squares-line">
	  <title>
	    A data set with four points.
	  </title>
	  <tabular halign="center">
            <row bottom="minor">
	      <cell> <m>x</m> </cell>
	      <cell> <m>y</m> </cell>
	    </row>
	    <row>
	      <cell> 1 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 2 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 3 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 4 </cell>
	      <cell> 2 </cell>
	    </row>
	  </tabular>
	</table>
	<sage language="python">
	  <input>

	  </input>
	</sage>
	<p>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up the matrix equation <m>\yvec = X \betavec</m> for a simple linear model (intercept and slope)
		  for these data. That is, specify <m>X</m> and <m>\yvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Write the normal equation that describes the least
	      squares approximate solution to <m>\yvec = X\betavec</m>. 
		  Write the equation symbolically first, then plug in the matrices for
		  this problem and simplify.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution <m>\betahat</m> and create a plot  showing 
		  the original data and the least squares regression line.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x=3.5</m>? 
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>X=\begin{bmatrix}
	      1 \amp 1 \\
	      1 \amp 2 \\
	      1 \amp 3 \\
	      1 \amp 4 \\
	      \end{bmatrix}</m>,
	      <m>\yvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      4 \amp 10 \\
	      10 \amp 30 \\
	      \end{bmatrix}
	      \betahat = \twovec5{14}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\betahat = \twovec{1/2}{3/10}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>1.55</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.6</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>X=\begin{bmatrix}
	      1 \amp 1 \\
	      1 \amp 2 \\
	      1 \amp 3 \\
	      1 \amp 4 \\
	      \end{bmatrix}</m>
	      and the vector <m>\yvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      4 \amp 10 \\
	      10 \amp 30 \\
	      \end{bmatrix}
	      \betahat = \twovec5{14}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\betahat = \twovec{1/2}{3/10}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      The predicted value is <m>y=1/2 + 3/10*3.5 = 1.55</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.6</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Consider the four points in
	<xref ref="table-lst-squares-line" />.
	  </p>
	<sage>
	  <input>

	  </input>
	</sage>
	<p>
	<ol marker="a.">
	  <li>
	    <p>
		  Set up the matrix equation <m>\yvec = X \betavec</m> for a quadratic model
		  (<m>y = \beta_0 + \beta_1 x + \beta_2 x^2</m>) for these data. That
		  is, specify <m>X</m> and <m>\yvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Use a <m>QR</m> factorization to find the least squares
	      approximate solution <m>\betahat</m> and plot the data and
	      the graph of the resulting quadratic function.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x=3.5</m>? 
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      We have the Vandermonde matrix <m>A = \begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 2 \amp 4 \\
	      1 \amp 3 \amp 9 \\
	      1 \amp 4 \amp 16\\
	      \end{bmatrix}</m> and vector <m>\bvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The quadratic function is <m>1.75 - 0.95x + 0.25x^2</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This gives the predicted value <m>1.75 - 0.95(3.5) +
	      0.25(3.5)^2=1.488</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.93</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Consider the data in <xref ref="table-lst-squares-multi" />.
	  </p>
	<table xml:id="table-lst-squares-multi">
	  <title>
	    A simple data set
	  </title>
	  <tabular halign="center">
            <row bottom="minor">
              <cell> <m>x_1</m> </cell>
              <cell> <m>x_2</m> </cell>
              <cell> <m>y</m> </cell>
            </row>
            <row>
              <cell> 1</cell>
              <cell> 1 </cell>
              <cell> 4.2 </cell>
            </row>
            <row>
              <cell> 1 </cell>
              <cell> 2 </cell>
              <cell> 3.3 </cell>
            </row>
            <row>
              <cell> 2 </cell>
              <cell> 1 </cell>
              <cell> 5.9 </cell>
            </row>
            <row>
              <cell> 2 </cell>
              <cell> 2 </cell>
              <cell> 5.1 </cell>
            </row>
            <row>
              <cell> 3 </cell>
              <cell> 2 </cell>
              <cell> 7.5 </cell>
            </row>
            <row>
              <cell> 3 </cell>
              <cell> 3 </cell>
              <cell> 6.3 </cell>
            </row>
          </tabular>
	</table>

	<sage>
	  <input>

	  </input>
	</sage>
	<p>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up a linear system <m>\yvec = X\betavec</m> that
	      describes the relationship
	      <me>
		y = \beta_0 + \beta_1 x_1 + \beta_2 x_2
	      </me>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution
	      <m>\betahat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x_1 =
	      2.4</m> and <m>x_2=2.9</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>

	  <li>
		<p>
			Repeat for a quadratic model <m>y = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2</m>.
		</p>
	  </li>

	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 1 \amp 2 \\
	      1 \amp 2 \amp 1 \\
	      1 \amp 2 \amp 2 \\
	      1 \amp 3 \amp 2 \\
	      1 \amp 3 \amp 3 \\
	      \end{bmatrix}
	      \xvec = \begin{bmatrix}
	      4.2 \\ 3.3 \\ 5.9 \\ 5.1 \\ 7.5 \\ 6.3
	      \end{bmatrix}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat=\threevec{2.95}{2.00}{-0.85}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>2.95+2.00(2.4)-0.85(2.9) = 5.27</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.99</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 1 \amp 2 \\
	      1 \amp 2 \amp 1 \\
	      1 \amp 2 \amp 2 \\
	      1 \amp 3 \amp 2 \\
	      1 \amp 3 \amp 3 \\
	      \end{bmatrix}
	      \xvec = \begin{bmatrix}
	      4.2 \\ 3.3 \\ 5.9 \\ 5.1 \\ 7.5 \\ 6.3
	      \end{bmatrix}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat=\threevec{2.95}{2.00}{-0.85}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>2.95+2.00(2.4)-0.85(2.9) = 5.27</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.99</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Determine whether the following statements are true or false
	and explain your thinking.
	<ol marker="a.">
	  <li>
	    <p>
	      If <m>A\xvec=\bvec</m> is consistent, <m>\bhat = \proj{\bvec}{\col(A)}</m>, and 
		  <m>A\xhat = \bhat</m>, then <m>\xhat</m> is a solution to <m>A\xvec=\bvec</m>. 
	    </p>
	  </li>
	  <li>
	    <p>
	      If <m>R^2=1</m>, then the least squares approximate
	      solution <m>\xhat</m> is also a solution to the original
	      equation <m>A\xvec=\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Given the <m>QR</m> factorization <m>A=QR</m>, we have
	      <m>A\xhat=Q^{\transpose}Q\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      A <m>QR</m> factorization provides a 
	      method for finding the approximate least squares
	      solution to <m>A\xvec=\bvec</m> that is more reliable
	      than solving the normal equations.
	    </p>
	  </li>
	  <li>
	    <p>
	      A solution to <m>AA^{\transpose}\xvec = A\bvec</m> is the least
	      squares approximate solution to <m>A\xvec = \bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <answer>
      <p>
	<ol marker="a.">
	  <li><p> True </p></li>
	  <li><p> True </p></li>
	  <li><p> False </p></li>
	  <li><p> True </p></li>
	  <li><p> False </p></li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      True.  If <m>A\xvec=\bvec</m>, then <m>\bvec</m> is in
	      <m>\col(A)</m> so <m>A\xhat = \bhat = \bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      True. If <m>R^2 = 1 -
	      \frac{|A\xhat - \bvec|^2}
	      {|\widetilde{\bvec}|^2} = 1</m>, then
	      <m>|A\xhat-\bvec| = 0</m>.  Therefore, <m>A\xhat =
	      \bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      False.  The product <m>Q^{\transpose}Q = I</m> rather than the
	      matrix that projects vectors orthogonally onto
	      <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      True, numerical issues are more likely to arise when
	      solving the normal equations.
	    </p>
	  </li>
	  <li>
	    <p>
	      False.  The normal equations <m>A^{\transpose}A\xhat = A^{\transpose}\bvec</m>
	      gives the least squares approximate solution.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>
	    
  <exercise>
    <statement>
      <p>
	Explain your response to the following questions.
	<ol marker="a.">
	  <li>
	    <p>
			In a least squares regression model, 
	      if <m>\betahat=\zerovec</m>, what does this say about the
	      vector <m>\yvec</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>X</m> are orthonormal, how can you
	      easily find the least squares approximate solution to
	      <m>\yvec = X\betavec</m>?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\bvec</m> is in <m>\col(A)^\perp = \nul(A^{\transpose})</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat = A^{\transpose}\bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Since <m>A\xhat = \bhat</m> and <m>\xhat = \zerovec</m>,
	      we know that <m>\bhat=\zerovec</m>.  This says that
	      <m>\bvec</m> is in <m>\col(A)^\perp = \nul(A^{\transpose})</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      In this case, <m>\bhat = AA^{\transpose}\bvec</m>, which means we
	      have <m>A\xhat = AA^{\transpose}\bvec</m>.  If we multiply both
	      sides by <m>A^{\transpose}</m>, we have <m>\xhat = A^{\transpose}\bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
  </exercise>

  <exercise>
    <statement>
      <p>
	The following cell loads in some data showing the number of
	people in Bangladesh living without electricity over 27 years.
	It also defines vectors <c>year</c>, which records the years in the
	data set, and <c>people</c>, which records the number of people living without electricity.
	  </p>
<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals()) -->
	<sage language="python">
	  <input>
Bangladesh = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/bangladesh.csv')
print(Bangladesh.head(5))
( 
    so.Plot(Bangladesh, x="Year", y="People")
    .add(so.Dot())
    .show()
)
# extract columns as numpy vectors
year = Bangladesh['Year'].to_numpy()
people = Bangladesh['People'].to_numpy()

	  </input>
	</sage>
	<p>
	<ol marker="a.">
	  <li>
	    <p>
	      Suppose we want to write
	      <me>
		N = \beta_0 + \beta_1 t
	      </me>
	      where <m>t</m> is the year and <m>N</m> is the number of
	      people living without electricity. 
	      Construct the matrix <m>X</m> and vector
	      <m>\yvec</m> so that the linear system
	      <m>X\betavec=\yvec</m> describes the vector
	      <m>\betahat = \twovec{\beta_0}{\beta_1}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Using a <m>QR</m> factorization of <m>A</m>, find
	      the values of <m>\beta_0</m> and <m>\beta_1</m>
	      in the least squares approximate solution
	      <m>\betahat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is the coefficient of determination <m>R^2</m> and
	      what does this tell us about the quality of the
	      approximation?
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your prediction for the number of people living
	      without electricity in 1985?
	    </p>
	  </li>
	  <li>
	    <p>
	      Estimate the year in which there will be no people
	      living without electricity.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the <m>27\by 2</m> matrix
	      whose first column is all 1's and whose second column is
	      the vector of years.  The vector <m>\bvec</m> is the
	      vector that records the number of people.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>N = 3.88\by10^9 - 1.90\by10^6t</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.90</m>
	    </p>
	  </li>
	  <li>
	    <p>
	    <m>1.14\by10^8</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      2045
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the <m>27\by 2</m> matrix
	      whose first column is all 1's and whose second column is
	      the vector of years.  The vector <m>\bvec</m> is the
	      vector that records the number of people.
	    </p>
	  </li>
	  <li>
	    <p>
	      We obtain <m>N = 3.88\by10^9 - 1.90\by10^6t</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.90</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>3.88\by10^9 - 1.90\by10^6(1985) =
	      1.14\by10^8</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Solve <m>N = 3.88\by10^9 - 1.90\by10^6t=0</m> to
	      obtain 2045.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	    
  </exercise>

  <exercise>
    <statement>
      <p>
	This problem concerns a data set describing planets in our
	Solar system.  
	For each planet, we have 
	the length <m>L</m> of the semi-major axis, essentially the
	distance from the planet to the Sun in AU (astronomical units),
	and the period <m>P</m>, the length of time in years required
	to complete one orbit around the Sun.
      </p>

      <p>
	We would like to model this data using the function
	<m>P = CL^r</m> where <m>C</m> and <m>r</m> are parameters we
	need to determine.  Since this isn't a linear function, we
	will transform this relationship by taking the natural
	logarithm of both sides to obtain
	<me>
	  \text{Model 1: } \log(P) = \log(C_1) + r_1\log(L).
	</me>
	Equivalently, 
	<me>
	  \text{Model 2: } \log(L) = \log(C_2) + r_2\log(L).
	</me>
      </p>

      <p>
	Evaluating the following cell loads the data set and defines
	two vectors <c>logaxis</c>, whose
	components are <m>\log(L)</m>, and <c>logperiod</c>, whose
	components are <m>\log(P)</m>.	
	  </p>
<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals()) -->
	<sage language="python">
	  <input>
planets = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/planets.csv',index_col=0)
log_axis = np.log(planets['Semi-major axis'].to_numpy())
log_period = np.log(planets['Period'].to_numpy())
print(planets)

	  </input>
	</sage>

	<p>
	<ol marker="a.">
	  <li>
	    <p>
	      Construct the matrix <m>X</m> and vector <m>\yvec</m> form Model 1 so that
	      the solution to <m>X\betavec=\yvec</m> is the vector
	      <m>\betahat = \ctwovec{\log(C)}r</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution
	      <m>\betahat</m>.  What does this give for the values of
	      <m>C</m> and <m>r</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.  What
	      does this tell us about the quality of the
	      approximation?
	    </p>
	  </li>
	  <li>
		<p>
			Repeat the process above for Model 2.
		</p>
	  </li>
	  <li>
	    Suppose that the orbit of an asteroid has a semi-major
	    axis whose length is <m>L=4.0</m> AU.  Estimate the period
	    <m>P</m> of the asteroid's orbit. 
		Which model should you use?
		Do you get the same result if you use the other model?
	  </li>

	  <li>
	    <p>
	      Halley's Comet has a period of <m>P=75</m> years.
	      Estimate the length of its semi-major axis.
		  Which model should you use?
		  Do you get the same result if you use the other model?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>X</m> is the <m>9\by2</m> matrix whose
	      first column is all 1's and whose second column is the
	      vector of the logarithms of the semi-major axes.  The
	      vector <m>\yvec</m> is the vector that records the
	      logarithms of the periods.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\hat C_1=0.996</m> and <m>\hat r_1=1.50</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=1.000</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>7.99</m> years.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>17.9</m> AU.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>X</m> is the <m>9\by2</m> matrix whose
	      first column is all 1's and whose second column is the
	      vector of the logarithms of the semi-major axes.  The
	      vector <m>\yvec</m> is the vector that records the
	      logarithms of the periods.
	    </p>
	  </li>
	  <li>
	    <p>
	      We find <m>\betahat=\twovec{\log(C_1)}{r_1} =
	      \twovec{-0.0036}{1.501}</m> so that <m>C=e^{-0.0036} =
	      0.996</m> and <m>r=1.50</m>.  This means that <m>P =
	      0.996L^{1.5}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=1.000</m>, which means that we have a very good
	      fit because this data reflects a physical
	      law.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>P = 0.996(4.0)^{1.5}=7.99</m> years.
	    </p>
	  </li>
	  <li>
	    <p>
	      The expression <m>\log(P) = \log(C) + r\log(L)</m> means
	      that <m>\log(76) = \log(C) + r\log(L)</m>.  Solving for
	      <m>L = 17.9</m> AU.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
  </exercise>
  
  <exercise>
    <statement>
      <p>
	Evaluating the following cell loads a data set describing the
	temperature in the Earth's atmosphere at various altitudes.
	There are also two vectors <c>altitude</c>, expressed in kilometers,
	and <c>temperature</c>, in degrees Celsius.
	  </p>
<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals()) -->
	<sage language="python">
	  <input>
weather = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/altitude-temps.csv')
altitude = weather['Altitude'].to_numpy()
temperature = weather['Temperature'].to_numpy()
print(weather)
(
    so.Plot(weather, x = "Altitude", y = "Temperature")
	.add(so.Dot())
	.show()
)
	  </input>
	</sage>
	<p>
	<ol marker="a.">
	  <li>
	    <p>
	      Describe how to form the matrix <m>X</m> and vector
	      <m>\yvec</m> so that the linear system
	      <m>X\betavec=\yvec</m> describes a degree <m>k</m>
	      polynomial fitting the data.
	    </p>
	  </li>
	  <li>
	    <p>
	      After choosing a value of <m>k</m>,
	      construct the matrix <m>X</m> and vector <m>\yvec</m>,
	      and find the least squares approximate solution
	      <m>\betahat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Plot the polynomial by sampling some points on the curve and plotting them.
	    </p>
	  </li>

	  <li>
	    <p>
	      Now examine what happens as you vary the
	      degree of the polynomial <m>k</m>.  Choose an
	      appropriate value of <m>k</m> that seems to capture the
	      most important features of the data while avoiding
	      overfitting, and explain your choice.
	    </p>
	  </li>

	  <li>
	    <p>
	      Use your value of <m>k</m> to estimate the
	      temperature at an altitude of 55 kilometers.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>X</m> will be the Vandermonde matrix of
	      degree <m>k</m> using the altitude readings while
	      <m>\yvec</m> is the vector of temperatures.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> appears to give a good fit without
	      overfitting.
	    </p>
	  </li>
	  <li>
	    <p>
	      We have
	      <me>
	      18.0-10.3x+0.35x^2+0.0003x^3-0.0001x^4+1.5\by10^{-6}x^5
	      -4.7\by 10^{-9}x^6
	      </me>.
	    </p>
	  </li>
	  <li>
	    <p>
	      A small value of <m>k</m> does not adequately track
	      important features of the data.
	      If we increase <m>k</m> too far, the graph tries too
	      hard to match the data, and we see spurious features in
	      the graph.
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate the temperature to be <m>-7</m> degrees
	      Celsius.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>X</m> will be the Vandermonde matrix of
	      degree <m>k</m> using the altitude readings while
	      <m>\yvec</m> is the vector of temperatures.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> appears to give a good fit without
	      overfitting.
	    </p>
	  </li>
	  <li>
	    <p>
	      We have
	      <me>
	      18.0-10.3x+0.35x^2+0.0003x^3-0.0001x^4+1.5\by10^{-6}x^5
	      -4.7\by 10^{-9}x^6\text{.} 
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      A small value of <m>k</m> does not adequately track
	      important features of the data.
	      If we increase <m>k</m> too far, the graph tries too
	      hard to match the data, and we see spurious features in
	      the graph.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> seems to work well.  With this function, we
	      estimate the temperature to be <m>-7</m> degrees
	      Celsius.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	The following cell loads some data describing 1057 houses in a
	particular real estate market.  For each house, we record the
	living area in square feet, the lot size in acres, the age
	in years, and the price in dollars.  
	  </p>

<!-- sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals()) -->
	<sage language="python">
	  <input>
housing = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv',index_col=0)
print(housing)
A = housing.to_numpy()
pprint(A)
	  </input>
	</sage>

	<p>
	We will use linear regression to predict the price of
	a house given its living area, lot size, and age:
	<me>
	  \text{Price} = 
	  \beta_0 + \beta_1~\text{Living Area} +
	  \beta_2~\text{Lot Size} + \beta_3~\text{Age} 
	</me>
	<ol marker="a.">
	  <li>
	    <p>
	      Use a <m>QR</m> factorization to find the least squares
	      approximate solution <m>\xhat</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Discuss what the of the signs of <m>\hat\beta_1</m>,
	      <m>\hat\beta_2</m>, and <m>\hat\beta_3</m> tell you about prices of houses.
	    </p>
	  </li>

	  <li>
	    <p>
	      If two houses are
	      identical except for differing in age by one year, how
	      would you predict that their prices compare to each another?
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.  What
	      does this say about the quality of the fit?
	    </p>
	  </li>
	  <li>
	    <p>
	      Predict the price of a house whose living area is 2000
	      square feet, lot size is 1.5 acres, and age is 50 years.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\xhat = \fourvec{\beta_0}{\beta_1}{\beta_2}{\beta_3}
	      =
	      \fourvec{20823}{84.7}{588.4}{-263.6}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      Increasing the living area or the lot size
	      will cause the house price to increase, but an older
	      house will cost less.
	    </p>
	  </li>
	  <li>
	    <p>
	      A house
	      that's one year older will cost about <m>\$263.60</m> less.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.59</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate <m>\$177966</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\xhat = \fourvec{\beta_0}{\beta_1}{\beta_2}{\beta_3}
	      =
	      \fourvec{20823}{84.7}{588.4}{-263.6}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\beta_1</m> and <m>\beta_2</m> are positive, which
	      means that increasing the living area or the lot size
	      will cause the house price to increase.  However,
	      <m>\beta_3</m> is negative, which means that an older
	      house will cost less.
	    </p>
	  </li>
	  <li>
	    <p>
	      Because <m>\beta_3=-263.6</m>, we would expect a house
	      that's one year older to cost <m>\$263.60</m> less.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.59</m>, which is far from perfect without being
	      terrible.  There are other factors that are not included
	      in the dataset that can also influence the house price,
	      such as number of bedrooms and the location.
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate <m>\$177966</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	We observed that if the columns of <m>A</m> are linearly
	independent, then there is a unique least squares approximate
	solution to the equation <m>A\xvec=\bvec</m> because the equation
	<m>A\xhat=\bhat</m> has a unique solution.  We also said that
	<m>\xhat</m> is the unique solution to the normal equation
	<m>A^{\transpose}A\xhat = A^{\transpose}\bvec</m> without explaining why this
	equation has a unique solution.  This exercise offers an
	explanation.
      </p>

      <p>
	Assuming that the columns of <m>A</m> are linearly
	independent, we would like to conclude that the equation
	<m>A^{\transpose}A\xhat=A^{\transpose}\bvec</m> has a unique solution.
	<ol marker="a.">
	  <li>
	    <p>
	      Suppose that <m>\xvec</m> is a vector for which
	      <m>A^{\transpose}A\xvec = \zerovec</m>.  Explain why the following
	      argument is valid and allows us to conclude that
	      <m>A\xvec = \zerovec</m>.
	      <me>
		\begin{aligned}
		A^{\transpose}A\xvec \amp = \zerovec \\
		\xvec\cdot A^{\transpose}A\xvec \amp = \xvec\cdot\zerovec = 0 \\
		(A\xvec)\cdot(A\xvec) \amp = 0 \\
		\len{A\xvec}^2 \amp = 0. \\
		\end{aligned}
	      </me>
	      In other words, if <m>A^{\transpose}A\xvec = \zerovec</m>, we know
	      that <m>A\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>A</m> are linearly independent and
	      <m>A\xvec = \zerovec</m>, what do we know about the
	      vector <m>\xvec</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Explain why <m>A^{\transpose}A\xvec = \zerovec</m> can only happen
	      when <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Assuming that the columns of <m>A</m> are linearly
	      independent, explain why <m>A^{\transpose}A\xhat=A^{\transpose}\bvec</m> has a
	      unique solution.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Use the fact that <m>\xvec\cdot(A^{\transpose}A\xvec)=
	      (A\xvec)^{\transpose}(A\xvec)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      It must be true that <m>\xvec=\zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follows from the previous two parts of this
	      exercise.
	    </p>
	  </li>
	  <li>
	    <p>
	      The columns of <m>A^{\transpose}A</m> must be linearly independent.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Starting with the assumption that <m>A^{\transpose}A\xvec =
	      \zerovec</m>, we dot both sides of the equation with
	      <m>\xvec</m> to obtain <m>\xvec \cdot A^{\transpose}A\xvec =
	      0</m>.  From here, we write
	      <me>
		\begin{aligned}
		\xvec\cdot A^{\transpose}A\xvec \amp = 0 \\
		\xvec^{\transpose}A^{\transpose}A\xvec \amp = 0 \\
		(A\xvec)^{\transpose}(A\xvec) \amp = 0 \\
		(A\xvec)\cdot (A\xvec) \amp = 0 \\
		\len{A\xvec}^2 \amp = 0. \\
		\end{aligned}
	      </me>
	      This says that the length of <m>A\xvec</m> must be zero,
	      which tells us that <m>A\xvec=\zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>A</m> are linearly independent, we
	      know that the only solution to the homogeneous equation
	      <m>A\xvec = \zerovec</m> is <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The previous two parts of this exercise tell us that
	      <m>A^{\transpose}A\xvec = \zerovec</m> implies
	      that <m>A\xvec = \zerovec</m>, which implies that
	      <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Since the only solution to the homogeneous equation
	      <m>A^{\transpose}A\xvec = \zerovec</m> is <m>\xvec=\zerovec</m>,
	      the columns of <m>A^{\transpose}A</m> are linearly independent,
	      which means that <m>A^{\transpose}A\xhat=A^{\transpose}\bvec</m> has only one
	      solution. 
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	  
  </exercise>
	    

  <exercise xml:id="ex-r2-meaning">
    <statement>
      <p>
	This problem is about the meaning of the coefficient of
	determination <m>R^2</m> and its connection to variance, a
	topic that appears in the next section.  Throughout this
	problem, we consider the linear system <m>\yvec = X\betavec</m> and
	the approximate least squares solution <m>\betahat</m>, where
	<m>\yhat = X\betahat</m>.  We suppose that <m>X</m> is an
	<m>m\by n</m> matrix, and we will denote the
	<m>m</m>-dimensional vector <m>\onevec = \fourvec11{\vdots}1</m>.
      </p>

      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Explain why <m>\ybar</m>, the
	      mean of the components of <m>\yvec</m>, can be found as
	      the dot product
	      <me>
		\ybar = \frac 1m \yvec\cdot\onevec.
	      </me>
	    </p>
	  </li>

	  <li>
	    <p>
	      In the examples we have seen in this section, 
	      explain why <m>\onevec</m> is in <m>\col(A)</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      If we write <m>\yvec = \yhat + \yvec^\perp</m>, 
	      explain why
	      <me>
		\yvec^\perp\cdot\onevec = 0
	      </me>
	      and hence why the mean of the components of
	      <m>\yvec^\perp = \yvec - \yhat</m> is zero.
	    </p>
	  </li>

	  <li>
	    <p>
	      The variance of an <m>m</m>-dimensional vector
	      <m>\vvec</m> is
	      <m>\var(\vvec) = \frac1m \len{\widetilde{\vvec}}^2</m>, where
	      <m>\widetilde{\vvec}</m> is the vector obtained by demeaning
	      <m>\vvec</m>.
	    </p>

	    <p>
	      Explain why
	      <me>
		\var(\yvec) = \var(\yhat) + \var(\yvec^\perp).
	      </me>
	    </p>
	  </li>

	  <li>
	    <p>
	      Explain why
	      <me>
		\frac{\len{\yvec - X\yhat}^2}{\len{\widetilde{\yvec}}^2}
		= \frac{\var(\yvec^\perp)}{\var(\yvec)}
	      </me>
	      and hence
	      <me>
		R^2 = \frac{\var(\yhat)}{\var(\yvec)} =
		\frac{\var(X\betahat)}{\var(\yvec)}.
	      </me>
	    </p>

	    <p>
	      These expressions indicate why 
	      it is sometimes said that <m>R^2</m> measures the
	      <q>fraction of the variance explained </q> by the model we
	      are using to fit the data.  As seen in the previous
	      exercise, there may be other features that are not
	      recorded in the dataset that influence the quantity we
	      wish to predict.
	      
	    </p>
	  </li>

	  <li>
	    <p>
	      Explain why <m>0\leq R^2 \leq 1</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\yvec\cdot\onevec</m> simply sums the components of
	      <m>\yvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\onevec</m> is a column of <m>A</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\yperp</m> is in <m>\col(A)^\perp</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Use the fact that <m>\yhat</m> and <m>\yperp</m> are orthogonal.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follows from <m>\yvec = \yhat + \yperp</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follows from <m>\var(\yvec) = \var(\yhat) + \var(\yperp)</m>. 
	    </p>
	  </li>
	</ol>
      </p>
    </answer>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\yvec\cdot\onevec</m> simply sums the components of
	      <m>\yvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The examples we've seen fit the data using functions
	      that have a constant term such as
	      <m>y=\beta_0+\beta_1x</m>.  This means that
	      <m>\onevec</m> will be a column of the matrix <m>X</m>
	      and hence in <m>\col(X)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Since <m>\onevec</m> is in <m>\col(X)</m>, any vector in
	      <m>\col(X)^\perp</m>, such as <m>\yperp</m>, will be
	      orthogonal to <m>\onevec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Notice that
	      <me>\yvec\cdot\onevec = \yhat\cdot\onevec +
	      \yperp\cdot\onevec = \yhat\cdot\onevec</me>
	      which says that
	      <me>
		\widetilde{\yvec} = \widetilde{\yhat} + \yperp
	      </me>.
	      Since <m>\yhat</m> and <m>\yperp</m> are orthogonal, we
	      have
	      <m>\widetilde{\yhat}\cdot\yperp = 0</m>.  This means
	      that
	      <me>
		|\widetilde{\yvec}|^2 =
		(\widetilde{\yhat}+\yperp)\cdot 
		(\widetilde{\yhat}+\yperp)
		= |\widetilde{\yhat}|^2 + |\yperp|^2
	      </me>
	      so that <m>\var(\yvec) = \var(\yhat)+\var(\yperp)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>|\yvec-X\betahat|^2 = |\yvec-\yhat|^2 = |\yperp|^2</m>,
	      which explains why
	      <me>
		\frac{\len{\yvec - X\betahat}^2}{\len{\widetilde{\yvec}}^2}
		= \frac{\var(\yvec^\perp)}{\var(\yvec)}
	      </me>.
	      Then
	      <me>
		R^2 = 1 - 
		\frac{\len{\yvec - X\betahat}^2}{\len{\widetilde{\yvec}}^2}
		= 1-\frac{\var(\yvec^\perp)}{\var(\yvec)} =
		\frac{\var(\yvec) - \var(\yperp)}{\var(\yvec)} =
		\frac{\var(\yhat)}{\var(\yvec)}
	      </me>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The variances are nonnegative so we have <m>R^2 \geq
	      0</m>.  Also,
	      <me>
		1 = \frac{\var(\yvec)}{\var(\yvec)} =
		\frac{\var(\yhat)+\var(\yperp)}{\var(\yvec)} \geq
		\frac{\var(\yhat)}{\var(\yvec)}=R^2
	      </me>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>

  </exercise>

</exercises>

