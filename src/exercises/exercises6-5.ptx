<?xml version="1.0" encoding="UTF-8"?>

<exercises>
  <p>
    Evaluating the following cell loads in some commands that will be
    helpful in the following exercises.  In particular, there are
    commands
    <ul>
      <li>
	<p>
	  <c>QR(A)</c> that returns the <m>QR</m> factorization of
	  <c>A</c> as <c>Q, R = QR(A)</c>,
	</p>
      </li>
      <li>
	<p>
	  <c>onesvec(n)</c> that returns the <m>n</m>-dimensional
	  vector whose entries are all 1,
	</p>
      </li>
      <li>
	<p>
	  <c>demean(v)</c> that demeans the vector <c>v</c>,
	</p>
      </li>
      <li>
	<p>
	  <c>vandermonde(x, k)</c> that returns the Vandermonde matrix
	  of degree <m>k</m>
	  formed from the components of the vector <c>x</c>, and
	</p>
      </li>
      <li>
	<p>
	  <c>plot_model(xhat, data)</c> that plots the <c>data</c> and
	  the model <c>xhat</c>.
	</p>
      </li>
    </ul>
    <sage>
      <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
      </input>
    </sage>
  </p>

  <exercise>
    <statement>
      <p>
	Suppose we write the linear system
	<me>
	  \begin{bmatrix}
	  1 \amp -1 \\
	  2 \amp -1 \\
	  -1 \amp 3 
	  \end{bmatrix}
	  \xvec = \threevec{-8}5{-10}
	</me>
	as <m>A\xvec=\bvec</m>.
	<ol marker="a.">
	  <li>
	    <p>
	      Find an orthogonal basis for <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find <m>\bhat</m>, the orthogonal projection of
	      <m>\bvec</m> onto <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find a solution to the linear system <m>A\xvec =
	      \bhat</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Applying Gram-Schmidt gives the orthogonal basis
	      <me>
		\wvec_1=\threevec12{-1},\hspace{24pt}
		\wvec_2=\threevec012
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      Applying the Projection Formula gives
	      <m>\bhat = \threevec21{-8}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Solving the linear system gives <m>\xvec=\twovec{-1}{-3}</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\wvec_1=\threevec12{-1}</m> and <m>
		\wvec_2=\threevec012</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\bhat = \threevec21{-8}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xvec=\twovec{-1}{-3}</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

  <exercise xml:id="ex-lst-squares-line">
    <statement>
      <p>
	Consider the data in <xref ref="table-lst-squares-line" />.
	<table xml:id="table-lst-squares-line">
	  <title>
	    A data set with four points.
	  </title>
	  <tabular halign="center">
            <row bottom="minor">
	      <cell> <m>x</m> </cell>
	      <cell> <m>y</m> </cell>
	    </row>
	    <row>
	      <cell> 1 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 2 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 3 </cell>
	      <cell> 1 </cell>
	    </row>
	    <row>
	      <cell> 4 </cell>
	      <cell> 2 </cell>
	    </row>
	  </tabular>
	</table>
	<sage>
	  <input>

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up the linear system <m>A\xvec=\bvec</m> that
	      describes the line <m>b + mx = y</m> passing through
	      these points.
	    </p>
	  </li>
	  <li>
	    <p>
	      Write the normal equations that describe the least
	      squares approximate solution to <m>A\xvec=\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution <m>\xhat</m> 
	      and plot the data and the resulting line.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x=3.5</m>? 
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>
	</ol>
	      
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A=\begin{bmatrix}
	      1 \amp 1 \\
	      1 \amp 2 \\
	      1 \amp 3 \\
	      1 \amp 4 \\
	      \end{bmatrix}</m>
	      and the vector <m>\bvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      4 \amp 10 \\
	      10 \amp 30 \\
	      \end{bmatrix}
	      \xhat = \twovec5{14}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat = \twovec{1/2}{3/10}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      The predicted value is <m>y=1/2 + 3/10*3.5 = 1.55</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.6</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>A=\begin{bmatrix}
	      1 \amp 1 \\
	      1 \amp 2 \\
	      1 \amp 3 \\
	      1 \amp 4 \\
	      \end{bmatrix}</m>,
	      <m>\bvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      4 \amp 10 \\
	      10 \amp 30 \\
	      \end{bmatrix}
	      \xhat = \twovec5{14}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat = \twovec{1/2}{3/10}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>1.55</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.6</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Consider the four points in
	<xref ref="table-lst-squares-line" />.
	<sage>
	  <input>

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up a linear system <m>A\xvec = \bvec</m> that
	      describes a quadratic function
	      <me>
		\beta_0+\beta_1x+\beta_2x^2 = y
	      </me>
	      passing through the points.
	    </p>
	  </li>
	  <li>
	    <p>
	      Use a <m>QR</m> factorization to find the least squares
	      approximate solution <m>\xhat</m> and plot the data and
	      the graph of the resulting quadratic function.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x=3.5</m>? 
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      We have the Vandermonde matrix <m>A = \begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 2 \amp 4 \\
	      1 \amp 3 \amp 9 \\
	      1 \amp 4 \amp 16\\
	      \end{bmatrix}</m> and vector <m>\bvec=\fourvec1112</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The quadratic function is <m>1.75 - 0.95x + 0.25x^2</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This gives the predicted value <m>1.75 - 0.95(3.5) +
	      0.25(3.5)^2=1.488</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.93</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Consider the data in <xref ref="table-lst-squares-multi" />.
	<table xml:id="table-lst-squares-multi">
	  <title>
	    A simple data set
	  </title>
	  <tabular halign="center">
            <row bottom="minor">
              <cell> <m>x_1</m> </cell>
              <cell> <m>x_2</m> </cell>
              <cell> <m>y</m> </cell>
            </row>
            <row>
              <cell> 1</cell>
              <cell> 1 </cell>
              <cell> 4.2 </cell>
            </row>
            <row>
              <cell> 1 </cell>
              <cell> 2 </cell>
              <cell> 3.3 </cell>
            </row>
            <row>
              <cell> 2 </cell>
              <cell> 1 </cell>
              <cell> 5.9 </cell>
            </row>
            <row>
              <cell> 2 </cell>
              <cell> 2 </cell>
              <cell> 5.1 </cell>
            </row>
            <row>
              <cell> 3 </cell>
              <cell> 2 </cell>
              <cell> 7.5 </cell>
            </row>
            <row>
              <cell> 3 </cell>
              <cell> 3 </cell>
              <cell> 6.3 </cell>
            </row>
          </tabular>
	</table>
	<sage>
	  <input>

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up a linear system <m>A\xvec = \bvec</m> that
	      describes the relationship
	      <me>
		\beta_0 + \beta_1 x_1 + \beta_2 x_2 = y.
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution
	      <m>\xhat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your predicted <m>y</m>-value when <m>x_1 =
	      2.4</m> and <m>x_2=2.9</m>?
	    </p>
	  </li>

	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 1 \amp 2 \\
	      1 \amp 2 \amp 1 \\
	      1 \amp 2 \amp 2 \\
	      1 \amp 3 \amp 2 \\
	      1 \amp 3 \amp 3 \\
	      \end{bmatrix}
	      \xvec = \begin{bmatrix}
	      4.2 \\ 3.3 \\ 5.9 \\ 5.1 \\ 7.5 \\ 6.3
	      \end{bmatrix}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat=\threevec{2.95}{2.00}{-0.85}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>2.95+2.00(2.4)-0.85(2.9) = 5.27</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.99</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\begin{bmatrix}
	      1 \amp 1 \amp 1 \\
	      1 \amp 1 \amp 2 \\
	      1 \amp 2 \amp 1 \\
	      1 \amp 2 \amp 2 \\
	      1 \amp 3 \amp 2 \\
	      1 \amp 3 \amp 3 \\
	      \end{bmatrix}
	      \xvec = \begin{bmatrix}
	      4.2 \\ 3.3 \\ 5.9 \\ 5.1 \\ 7.5 \\ 6.3
	      \end{bmatrix}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat=\threevec{2.95}{2.00}{-0.85}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>2.95+2.00(2.4)-0.85(2.9) = 5.27</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.99</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Determine whether the following statements are true or false
	and explain your thinking.
	<ol marker="a.">
	  <li>
	    <p>
	      If <m>A\xvec=\bvec</m> is consistent, then <m>\xhat</m>
	      is a solution to <m>A\xvec=\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If <m>R^2=1</m>, then the least squares approximate
	      solution <m>\xhat</m> is also a solution to the original
	      equation <m>A\xvec=\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Given the <m>QR</m> factorization <m>A=QR</m>, we have
	      <m>A\xhat=Q^TQ\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      A <m>QR</m> factorization provides a 
	      method for finding the approximate least squares
	      solution to <m>A\xvec=\bvec</m> that is more reliable
	      than solving the normal equations.
	    </p>
	  </li>
	  <li>
	    <p>
	      A solution to <m>AA^T\xvec = A\bvec</m> is the least
	      squares approximate solution to <m>A\xvec = \bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      True.  If <m>A\xvec=\bvec</m>, then <m>\bvec</m> is in
	      <m>\col(A)</m> so <m>A\xhat = \bhat = \bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      True. If <m>R^2 = 1 -
	      \frac{|A\xhat - \bvec|^2}
	      {|\widetilde{\bvec}|^2} = 1</m>, then
	      <m>|A\xhat-\bvec| = 0</m>.  Therefore, <m>A\xhat =
	      \bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      False.  The product <m>Q^TQ = I</m> rather than the
	      matrix that projects vectors orthogonally onto
	      <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      True, numerical issues are more likely to arise when
	      solving the normal equations.
	    </p>
	  </li>
	  <li>
	    <p>
	      False.  The normal equations <m>A^TA\xhat = A^T\bvec</m>
	      gives the least squares approximate solution.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li><p> True </p></li>
	  <li><p> True </p></li>
	  <li><p> False </p></li>
	  <li><p> True </p></li>
	  <li><p> False </p></li>
	</ol>
      </p>
    </answer>
	      
  </exercise>
	    
  <exercise>
    <statement>
      <p>
	Explain your response to the following questions.
	<ol marker="a.">
	  <li>
	    <p>
	      If <m>\xhat=\zerovec</m>, what does this say about the
	      vector <m>\bvec</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>A</m> are orthonormal, how can you
	      easily find the least squares approximate solution to
	      <m>A\xvec=\bvec</m>?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Since <m>A\xhat = \bhat</m> and <m>\xhat = \zerovec</m>,
	      we know that <m>\bhat=\zerovec</m>.  This says that
	      <m>\bvec</m> is in <m>\col(A)^\perp = \nul(A^T)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      In this case, <m>\bhat = AA^T\bvec</m>, which means we
	      have <m>A\xhat = AA^T\bvec</m>.  If we multiply both
	      sides by <m>A^T</m>, we have <m>\xhat = A^T\bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\bvec</m> is in <m>\col(A)^\perp = \nul(A^T)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat = A^T\bvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
	The following cell loads in some data showing the number of
	people in Bangladesh living without electricity over 27 years.
	It also defines vectors <c>year</c>, which records the years in the
	data set, and <c>people</c>, which records the number of people.
	<sage>
	  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/bangladesh.csv')
data = [vector(row) for row in df.values]
year = vector(df['Year'])
people = vector(df['People'])
print(df)
list_plot(data, size=40, color='blue')


	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Suppose we want to write
	      <me>
		N = \beta_0 + \beta_1 t
	      </me>
	      where <m>t</m> is the year and <m>N</m> is the number of
	      people. 
	      Construct the matrix <m>A</m> and vector
	      <m>\bvec</m> so that
	      the linear system
	      <m>A\xvec=\bvec</m> describes the vector
	      <m>\xvec=\twovec{\beta_0}{\beta_1}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Using a <m>QR</m> factorization of <m>A</m>, find
	      the values of <m>\beta_0</m> and <m>\beta_1</m>
	      in the least squares approximate solution
	      <m>\xhat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      What is the coefficient of determination <m>R^2</m> and
	      what does this tell us about the quality of the
	      approximation?
	    </p>
	  </li>
	  <li>
	    <p>
	      What is your prediction for the number of people living
	      without electricity in 1985?
	    </p>
	  </li>
	  <li>
	    <p>
	      Estimate the year in which there will be no people
	      living without electricity.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the <m>27\times 2</m> matrix
	      whose first column is all 1's and whose second column is
	      the vector of years.  The vector <m>\bvec</m> is the
	      vector that records the number of people.
	    </p>
	  </li>
	  <li>
	    <p>
	      We obtain <m>N = 3.88\times10^9 - 1.90\times10^6t</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.90</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>3.88\times10^9 - 1.90\times10^6(1985) =
	      1.14\times10^8</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Solve <m>N = 3.88\times10^9 - 1.90\times10^6t=0</m> to
	      obtain 2045.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the <m>27\times 2</m> matrix
	      whose first column is all 1's and whose second column is
	      the vector of years.  The vector <m>\bvec</m> is the
	      vector that records the number of people.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>N = 3.88\times10^9 - 1.90\times10^6t</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.90</m>
	    </p>
	  </li>
	  <li>
	    <p>
	    <m>1.14\times10^8</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      2045
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	    
  </exercise>

  <exercise>
    <statement>
      <p>
	This problem concerns a data set describing planets in our
	Solar system.  
	For each planet, we have 
	the length <m>L</m> of the semi-major axis, essentially the
	distance from the planet to the Sun in AU (astronomical units),
	and the period <m>P</m>, the length of time in years required
	to complete one orbit around the Sun.
      </p>

      <p>
	We would like to model this data using the function
	<m>P = CL^r</m> where <m>C</m> and <m>r</m> are parameters we
	need to determine.  Since this isn't a linear function, we
	will transform this relationship by taking the natural
	logarithm of both sides to obtain
	<me>
	  \ln(P) = \ln(C) + r\ln(L).
	</me>
      </p>

      <p>
	Evaluating the following cell loads the data set and defines
	two vectors <c>logaxis</c>, whose
	components are <m>\ln(L)</m>, and <c>logperiod</c>, whose
	components are <m>\ln(P)</m>.	
	<sage>
	  <input>
import numpy as np	    
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/planets.csv',index_col=0)
logaxis = vector(np.log(df['Semi-major axis']))
logperiod = vector(np.log(df['Period']))
print(df)

	  </input>
	</sage>

	<ol marker="a.">
	  <li>
	    <p>
	      Construct the matrix <m>A</m> and vector <m>\bvec</m> so that
	      the solution to <m>A\xvec=\bvec</m> is the vector
	      <m>\xvec=\ctwovec{\ln(C)}r</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the least squares approximate solution
	      <m>\xhat</m>.  What does this give for the values of
	      <m>C</m> and <m>r</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.  What
	      does this tell us about the quality of the
	      approximation?
	    </p>
	  </li>
	  <li>
	    Suppose that the orbit of an asteroid has a semi-major
	    axis whose length is <m>L=4.0</m> AU.  Estimate the period
	    <m>P</m> of the asteroid's orbit.
	  </li>

	  <li>
	    <p>
	      Halley's Comet has a period of <m>P=75</m> years.
	      Estimate the length of its semi-major axis.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> is the <m>9\times2</m> matrix whose
	      first column is all 1's and whose second column is the
	      vector of the logarithms of the semi-major axes.  The
	      vector <m>\bvec</m> is the vector that records the
	      logarithms of the periods.
	    </p>
	  </li>
	  <li>
	    <p>
	      We find <m>\xhat=\twovec{\ln(C)}{r} =
	      \twovec{-0.0036}{1.501}</m> so that <m>C=e^{-0.0036} =
	      0.996</m> and <m>r=1.50</m>.  This means that <m>P =
	      0.996L^{1.5}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=1.000</m>, which means that we have a very good
	      fit because this data reflects a physical
	      law.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>P = 0.996(4.0)^{1.5}=7.99</m> years.
	    </p>
	  </li>
	  <li>
	    <p>
	      The expression <m>\ln(P) = \ln(C) + r\ln(L)</m> means
	      that <m>\ln(76) = \ln(C) + r\ln(L)</m>.  Solving for
	      <m>L = 17.9</m> AU.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> is the <m>9\times2</m> matrix whose
	      first column is all 1's and whose second column is the
	      vector of the logarithms of the semi-major axes.  The
	      vector <m>\bvec</m> is the vector that records the
	      logarithms of the periods.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>C=0.996</m> and <m>r=1.50</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=1.000</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>7.99</m> years.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>17.9</m> AU.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>
  
  <exercise>
    <statement>
      <p>
	Evaluating the following cell loads a data set describing the
	temperature in the Earth's atmosphere at various altitudes.
	There are also two vectors <c>altitude</c>, expressed in kilometers,
	and <c>temperature</c>, in degrees Celsius.
	<sage>
	  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/altitude-temps.csv')
data = [vector(row) for row in df.values]
altitude = vector(df['Altitude'])
temperature = vector(df['Temperature'])
print(df)
list_plot(data, size=40, color='blue')

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Describe how to form the matrix <m>A</m> and vector
	      <m>\bvec</m> so that the linear system
	      <m>A\xvec=\bvec</m> describes a degree <m>k</m>
	      polynomial fitting the data.
	    </p>
	  </li>
	  <li>
	    <p>
	      After choosing a value of <m>k</m>,
	      construct the matrix <m>A</m> and vector <m>\bvec</m>,
	      and find the least squares approximate solution
	      <m>\xhat</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Plot the polynomial and data using
	      <c>plot_model(xhat, data)</c>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Now examine what happens as you vary the
	      degree of the polynomial <m>k</m>.  Choose an
	      appropriate value of <m>k</m> that seems to capture the
	      most important features of the data while avoiding
	      overfitting, and explain your choice.
	    </p>
	  </li>

	  <li>
	    <p>
	      Use your value of <m>k</m> to estimate the
	      temperature at an altitude of 55 kilometers.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the Vandermonde matrix of
	      degree <m>k</m> using the altitude readings while
	      <m>\bvec</m> is the vector of temperatures.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> appears to give a good fit without
	      overfitting.
	    </p>
	  </li>
	  <li>
	    <p>
	      We have
	      <me>
	      18.0-10.3x+0.35x^2+0.0003x^3-0.0001x^4+1.5\times10^{-6}x^5
	      -4.7\times 10^{-9}x^6\text{.} 
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      A small value of <m>k</m> does not adequately track
	      important features of the data.
	      If we increase <m>k</m> too far, the graph tries too
	      hard to match the data, and we see spurious features in
	      the graph.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> seems to work well.  With this function, we
	      estimate the temperature to be <m>-7</m> degrees
	      Celsius.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix <m>A</m> will be the Vandermonde matrix of
	      degree <m>k</m> using the altitude readings while
	      <m>\bvec</m> is the vector of temperatures.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>k=6</m> appears to give a good fit without
	      overfitting.
	    </p>
	  </li>
	  <li>
	    <p>
	      We have
	      <me>
	      18.0-10.3x+0.35x^2+0.0003x^3-0.0001x^4+1.5\times10^{-6}x^5
	      -4.7\times 10^{-9}x^6\text{.} 
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      A small value of <m>k</m> does not adequately track
	      important features of the data.
	      If we increase <m>k</m> too far, the graph tries too
	      hard to match the data, and we see spurious features in
	      the graph.
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate the temperature to be <m>-7</m> degrees
	      Celsius.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	The following cell loads some data describing 1057 houses in a
	particular real estate market.  For each house, we record the
	living area in square feet, the lot size in acres, the age
	in years, and the price in dollars.  The cell also defines
	variables <c>area</c>, <c>size</c>, <c>age</c>, and <c>price</c>.

	<sage>
	  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/orthogonality.py', globals())
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv',index_col=0)
df = df.fillna(df.mean())
area = vector(df['Living.Area'])
size = vector(df['Lot.Size'])
age = vector(df['Age'])
price = vector(df['Price'])
df


	  </input>
	</sage>
	We will use linear regression to predict the price of
	a house given its living area, lot size, and age:
	<me>
	  \beta_0 + \beta_1~\text{Living Area} +
	  \beta_2~\text{Lot Size} + \beta_3~\text{Age} = \text{Price}.
	</me>
	<ol marker="a.">
	  <li>
	    <p>
	      Use a <m>QR</m> factorization to find the least squares
	      approximate solution <m>\xhat</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      Discuss the
	      significance of the signs of <m>\beta_1</m>,
	      <m>\beta_2</m>, and <m>\beta_3</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      If two houses are
	      identical except for differing in age by one year, how
	      would you predict that their prices compare to each another?
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the coefficient of determination <m>R^2</m>.  What
	      does this say about the quality of the fit?
	    </p>
	  </li>
	  <li>
	    <p>
	      Predict the price of a house whose living area is 2000
	      square feet, lot size is 1.5 acres, and age is 50 years.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\xhat = \fourvec{\beta_0}{\beta_1}{\beta_2}{\beta_3}
	      =
	      \fourvec{20823}{84.7}{588.4}{-263.6}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\beta_1</m> and <m>\beta_2</m> are positive, which
	      means that increasing the living area or the lot size
	      will cause the house price to increase.  However,
	      <m>\beta_3</m> is negative, which means that an older
	      house will cost less.
	    </p>
	  </li>
	  <li>
	    <p>
	      Because <m>\beta_3=-263.6</m>, we would expect a house
	      that's one year older to cost <m>\$263.60</m> less.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.59</m>, which is far from perfect without being
	      terrible.  There are other factors that are not included
	      in the dataset that can also influence the house price,
	      such as number of bedrooms and the location.
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate <m>\$177966</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\xhat = \fourvec{\beta_0}{\beta_1}{\beta_2}{\beta_3}
	      =
	      \fourvec{20823}{84.7}{588.4}{-263.6}
	      </m>
	    </p>
	  </li>
	  <li>
	    <p>
	      Increasing the living area or the lot size
	      will cause the house price to increase, but an older
	      house will cost less.
	    </p>
	  </li>
	  <li>
	    <p>
	      A house
	      that's one year older will cost about <m>\$263.60</m> less.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>R^2=0.59</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      We estimate <m>\$177966</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	We observed that if the columns of <m>A</m> are linearly
	independent, then there is a unique least squares approximate
	solution to the equation <m>A\xvec=\bvec</m> because the equation
	<m>A\xhat=\bhat</m> has a unique solution.  We also said that
	<m>\xhat</m> is the unique solution to the normal equation
	<m>A^TA\xhat = A^T\bvec</m> without explaining why this
	equation has a unique solution.  This exercise offers an
	explanation.
      </p>

      <p>
	Assuming that the columns of <m>A</m> are linearly
	independent, we would like to conclude that the equation
	<m>A^TA\xhat=A^T\bvec</m> has a unique solution.
	<ol marker="a.">
	  <li>
	    <p>
	      Suppose that <m>\xvec</m> is a vector for which
	      <m>A^TA\xvec = \zerovec</m>.  Explain why the following
	      argument is valid and allows us to conclude that
	      <m>A\xvec = \zerovec</m>.
	      <me>
		\begin{aligned}
		A^TA\xvec \amp = \zerovec \\
		\xvec\cdot A^TA\xvec \amp = \xvec\cdot\zerovec = 0 \\
		(A\xvec)\cdot(A\xvec) \amp = 0 \\
		\len{A\xvec}^2 \amp = 0. \\
		\end{aligned}
	      </me>
	      In other words, if <m>A^TA\xvec = \zerovec</m>, we know
	      that <m>A\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>A</m> are linearly independent and
	      <m>A\xvec = \zerovec</m>, what do we know about the
	      vector <m>\xvec</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Explain why <m>A^TA\xvec = \zerovec</m> can only happen
	      when <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Assuming that the columns of <m>A</m> are linearly
	      independent, explain why <m>A^TA\xhat=A^T\bvec</m> has a
	      unique solution.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Starting with the assumption that <m>A^TA\xvec =
	      \zerovec</m>, we dot both sides of the equation with
	      <m>\xvec</m> to obtain <m>\xvec \cdot A^TA\xvec =
	      0</m>.  From here, we write
	      <me>
		\begin{aligned}
		\xvec\cdot A^TA\xvec \amp = 0 \\
		\xvec^TA^TA\xvec \amp = 0 \\
		(A\xvec)^T(A\xvec) \amp = 0 \\
		(A\xvec)\cdot (A\xvec) \amp = 0 \\
		\len{A\xvec}^2 \amp = 0. \\
		\end{aligned}
	      </me>
	      This says that the length of <m>A\xvec</m> must be zero,
	      which tells us that <m>A\xvec=\zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If the columns of <m>A</m> are linearly independent, we
	      know that the only solution to the homogeneous equation
	      <m>A\xvec = \zerovec</m> is <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The previous two parts of this exercise tell us that
	      <m>A^TA\xvec = \zerovec</m> implies
	      that <m>A\xvec = \zerovec</m>, which implies that
	      <m>\xvec = \zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Since the only solution to the homogeneous equation
	      <m>A^TA\xvec = \zerovec</m> is <m>\xvec=\zerovec</m>,
	      the columns of <m>A^TA</m> are linearly independent,
	      which means that <m>A^TA\xhat=A^T\bvec</m> has only one
	      solution. 
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Use the fact that <m>\xvec\cdot(A^TA\xvec)=
	      (A\xvec)^T(A\xvec)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      It must be true that <m>\xvec=\zerovec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follows from the previous two parts of this
	      exercise.
	    </p>
	  </li>
	  <li>
	    <p>
	      The columns of <m>A^TA</m> must be linearly independent.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	  
  </exercise>
	    

  <exercise xml:id="ex-r2-meaning">
    <statement>
      <p>
	This problem is about the meaning of the coefficient of
	determination <m>R^2</m> and its connection to variance, a
	topic that appears in the next section.  Throughout this
	problem, we consider the linear system <m>A\xvec=\bvec</m> and
	the approximate least squares solution <m>\xhat</m>, where
	<m>A\xhat=\bhat</m>.  We suppose that <m>A</m> is an
	<m>m\times n</m> matrix, and we will denote the
	<m>m</m>-dimensional vector <m>\onevec =
	\fourvec11{\vdots}1</m>.
      </p>

      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Explain why <m>\bbar</m>, the
	      mean of the components of <m>\bvec</m>, can be found as
	      the dot product
	      <me>
		\bbar = \frac 1m \bvec\cdot\onevec.
	      </me>
	    </p>
	  </li>

	  <li>
	    <p>
	      In the examples we have seen in this section, 
	      explain why <m>\onevec</m> is in <m>\col(A)</m>.
	    </p>
	  </li>

	  <li>
	    <p>
	      If we write <m>\bvec = \bhat + \bvec^\perp</m>, 
	      explain why
	      <me>
		\bvec^\perp\cdot\onevec = 0
	      </me>
	      and hence why the mean of the components of
	      <m>\bvec^\perp</m> is zero.
	    </p>
	  </li>

	  <li>
	    <p>
	      The variance of an <m>m</m>-dimensional vector
	      <m>\vvec</m> is
	      <m>\var(\vvec) = \frac1m \len{\widetilde{\vvec}}^2</m>, where
	      <m>\widetilde{\vvec}</m> is the vector obtained by demeaning
	      <m>\vvec</m>.
	    </p>

	    <p>
	      Explain why
	      <me>
		\var(\bvec) = \var(\bhat) + \var(\bvec^\perp).
	      </me>
	    </p>
	  </li>

	  <li>
	    <p>
	      Explain why
	      <me>
		\frac{\len{\bvec - A\xhat}^2}{\len{\widetilde{\bvec}}^2}
		= \frac{\var(\bvec^\perp)}{\var(\bvec)}
	      </me>
	      and hence
	      <me>
		R^2 = \frac{\var(\bhat)}{\var(\bvec)} =
		\frac{\var(A\xhat)}{\var(\bvec)}.
	      </me>
	    </p>

	    <p>
	      These expressions indicate why 
	      it is sometimes said that <m>R^2</m> measures the
	      <q>fraction of variance explained</q> by the function we
	      are using to fit the data.  As seen in the previous
	      exercise, there may be other features that are not
	      recorded in the dataset that influence the quantity we
	      wish to predict.
	      
	    </p>
	  </li>

	  <li>
	    <p>
	      Explain why <m>0\leq R^2 \leq 1</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\bvec\cdot\onevec</m> simply sums the components of
	      <m>\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The examples we've seen fit the data using functions
	      that have a constant term such as
	      <m>y=\beta_0+\beta_1x</m>.  This means that
	      <m>\onevec</m> will be a column of the matrix <m>A</m>
	      and hence in <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Since <m>\onevec</m> is in <m>\col(A)</m>, any vector in
	      <m>\col(A)^\perp</m>, such as <m>\bperp</m>, will be
	      orthogonal to <m>\onevec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Notice that
	      <me>\bvec\cdot\onevec = \bhat\cdot\onevec +
	      \bperp\cdot\onevec = \bhat\cdot\onevec</me>
	      which says that
	      <me>
		\widetilde{\bvec} = \widetilde{\bhat} + \bperp\text{.} 
	      </me>
	      Since <m>\bhat</m> and <m>\bperp</m> are orthogonal, we
	      have
	      <m>\widetilde{\bhat}\cdot\bperp = 0</m>.  This means
	      that
	      <me>
		|\widetilde{\bvec}|^2 =
		(\widetilde{\bhat}+\bperp)\cdot 
		(\widetilde{\bhat}+\bperp)
		= |\widetilde{\bhat}|^2 + |\bperp|^2
	      </me>
	      so that <m>\var(\bvec) = \var(\bhat)+\var(\bperp)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>|\bvec-A\xhat|^2 = |\bvec-\bhat|^2 = |\bperp|^2</m>,
	      which explains why
	      <me>
		\frac{\len{\bvec - A\xhat}^2}{\len{\widetilde{\bvec}}^2}
		= \frac{\var(\bvec^\perp)}{\var(\bvec)}\text{.}
	      </me>
	      Then
	      <me>
		R^2 = 1 - 
		\frac{\len{\bvec - A\xhat}^2}{\len{\widetilde{\bvec}}^2}
		= 1-\frac{\var(\bvec^\perp)}{\var(\bvec)} =
		\frac{\var(\bvec) - \var(\bperp)}{\var(\bvec)} =
		\frac{\var(\bhat)}{\var(\bvec)}\text{.}
	      </me> 
	    </p>
	  </li>
	  <li>
	    <p>
	      The variances are nonnegative so we have <m>R^2 \geq
	      0</m>.  Also,
	      <me>
		1 = \frac{\var(\bvec)}{\var(\bvec)} =
		\frac{\var(\bhat)+\var(\bperp)}{\var(\bvec)} \geq
		\frac{\var(\bhat)}{\var(\bvec)}=R^2\text{.}
	      </me>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\bvec\cdot\onevec</m> simply sums the components of
	      <m>\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\onevec</m> is a column of <m>A</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\bperp</m> is in <m>\col(A)^\perp</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Use the fact that <m>\bhat</m> and <m>\bperp</m> are orthogonal.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follows from <m>\bvec = \bhat + \bperp</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      This follow from <m>\var(\bvec) = \var(\bhat) +
	      \var(\bperp)</m>. 
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

</exercises>

