<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Using Singular Value Decompositions</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" Randall Pruim ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math",
    "renderActions": {
      "findScript": [
        10,
        function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        },
        ""
      ]
    }
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-sage",
  "linked": true,
  "linkKey": "linked-sage",
  "autoeval": false,
  "languages": [
    "sage"
  ],
  "evalButtonText": "Evaluate (Sage)"
});
</script><script>// Make *any* pre with class 'sagecell-python' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({
  "inputLocation": "pre.sagecell-python",
  "linked": true,
  "linkKey": "linked-python",
  "autoeval": false,
  "languages": [
    "python"
  ],
  "evalButtonText": "Evaluate (Python)"
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.3/pretext_search.js"></script><link href="https://pretextbook.org/css/0.7/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.3</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.3/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.3/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.3/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link href="https://pretextbook.org/css/0.7/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.7/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra:</span> <span class="subtitle">Data Science Edition</span></a></h1>
<p class="byline">Randall Pruim</p>
</div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" class="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms" class="searchterms"></span>
</h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" aria-label="Show or hide table of contents"><span class="icon">‚ò∞</span><span class="name">Contents</span></button><a class="index-button button" href="index-1.html" title="Index"><span class="name">Index</span></a><button id="user-preferences-button" class="user-preferences-button button" title="Modify user preferences"><span id="avatarbutton" class="avatarbutton name">You!</span><div id="preferences_menu_holder" class="preferences_menu_holder hidden"><ol id="preferences_menu" class="preferences_menu" style="font-family: 'Roboto Serif', serif;">
<li data-env="avatar" tabindex="-1">Choose avatar<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden avatar">
<li data-val="You!" tabindex="-1">
<span id="theYou!" class="avatarcheck">‚úîÔ∏è</span>You!</li>
<li data-val="üò∫" tabindex="-1">
<span id="theüò∫" class="avatarcheck"></span>üò∫</li>
<li data-val="üë§" tabindex="-1">
<span id="theüë§" class="avatarcheck"></span>üë§</li>
<li data-val="üëΩ" tabindex="-1">
<span id="theüëΩ" class="avatarcheck"></span>üëΩ</li>
<li data-val="üê∂" tabindex="-1">
<span id="theüê∂" class="avatarcheck"></span>üê∂</li>
<li data-val="üêº" tabindex="-1">
<span id="theüêº" class="avatarcheck"></span>üêº</li>
<li data-val="üåà" tabindex="-1">
<span id="theüåà" class="avatarcheck"></span>üåà</li>
</ol>
</li>
<li data-env="fontfamily" tabindex="-1">Font family<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fontfamily">
<li data-val="face" data-change="OS" tabindex="-1" style="font-family: 'Open Sans'">
<span id="theOS" class="ffcheck">‚úîÔ∏è</span><span class="name">Open Sans</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
<li data-val="face" data-change="RS" tabindex="-1" style="font-family: 'Roboto Serif'">
<span id="theRS" class="ffcheck"></span><span class="name">Roboto Serif</span><span class="sample">AaBbCc 123 PreTeXt</span>
</li>
</ol>
</li>
<li data-env="font" tabindex="-1">Adjust font<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden fonts">
<li>Size</li>
<li><span id="thesize">12</span></li>
<li data-val="size" data-change="-1" tabindex="-1" style="font-size: 80%">Smaller</li>
<li data-val="size" data-change="1" tabindex="-1" style="font-size: 110%">Larger</li>
<li>Width</li>
<li><span id="thewdth">100</span></li>
<li data-val="wdth" data-change="-5" tabindex="-1" style="font-variation-settings: 'wdth' 60">narrower</li>
<li data-val="wdth" data-change="5" tabindex="-1" style="font-variation-settings: 'wdth' 150">wider</li>
<li>Weight</li>
<li><span id="thewght">400</span></li>
<li data-val="wght" data-change="-50" tabindex="-1" style="font-weight: 200">thinner</li>
<li data-val="wght" data-change="50" tabindex="-1" style="font-weight: 700">heavier</li>
<li>Letter spacing</li>
<li>
<span id="thelspace">0</span><span class="byunits">/200</span>
</li>
<li data-val="lspace" data-change="-1" tabindex="-1">closer</li>
<li data-val="lspace" data-change="1" tabindex="-1">f a r t h e r</li>
<li>Word spacing</li>
<li>
<span id="thewspace">0</span><span class="byunits">/50</span>
</li>
<li data-val="wspace" data-change="-1" tabindex="-1">smaller‚ÄÖgap‚ÄÉ</li>
<li data-val="wspace" data-change="1" tabindex="-1">larger‚ÄÉgap</li>
<li>Line Spacing</li>
<li>
<span id="theheight">135</span><span class="byunits">/100</span>
</li>
<li data-val="height" data-change="-5" tabindex="-1" style="line-height: 1">closer<br>together</li>
<li data-val="height" data-change="5" tabindex="-1" style="line-height: 1.75">further<br>apart</li>
</ol>
</li>
<li data-env="atmosphere" tabindex="-1">Light/dark mode<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden atmosphere">
<li data-val="default" tabindex="-1">
<span id="thedefault" class="atmospherecheck">‚úîÔ∏è</span>default</li>
<li data-val="pastel" tabindex="-1">
<span id="thepastel" class="atmospherecheck"></span>pastel</li>
<li data-val="darktwilight" tabindex="-1">
<span id="thedarktwilight" class="atmospherecheck"></span>twilight</li>
<li data-val="dark" tabindex="-1">
<span id="thedark" class="atmospherecheck"></span>dark</li>
<li data-val="darkmidnight" tabindex="-1">
<span id="thedarkmidnight" class="atmospherecheck"></span>midnight</li>
</ol>
</li>
<li data-env="ruler" tabindex="-1">Reading ruler<div class="wrap_to_submenu"><span class="to_submenu">‚ñª</span></div>
<ol class="hidden ruler">
<li data-val="none" tabindex="-1">
<span id="thenone" class="rulercheck">‚úîÔ∏è</span>none</li>
<li data-val="underline" tabindex="-1">
<span id="theunderline" class="rulercheck"></span>underline</li>
<li data-val="lunderline" tabindex="-1">
<span id="thelunderline" class="rulercheck"></span>L-underline</li>
<li data-val="greybar" tabindex="-1">
<span id="thegreybar" class="rulercheck"></span>grey bar</li>
<li data-val="lightbox" tabindex="-1">
<span id="thelightbox" class="rulercheck"></span>light box</li>
<li data-val="sunrise" tabindex="-1">
<span id="thesunrise" class="rulercheck"></span>sunrise</li>
<li data-val="sunriseunderline" tabindex="-1">
<span id="thesunriseunderline" class="rulercheck"></span>sunrise underline</li>
<li class="moveQ">Motion by:</li>
<li data-val="mouse" tabindex="-1">
<span id="themouse" class="motioncheck">‚úîÔ∏è</span>follow the mouse</li>
<li data-val="arrow" tabindex="-1">
<span id="thearrow" class="motioncheck"></span>up/down arrows - not yet</li>
<li data-val="eye" tabindex="-1">
<span id="theeye" class="motioncheck"></span>eye tracking - not yet</li>
</ol>
</li>
</ol></div></button><span class="treebuttons"><a class="previous-button button" href="sec-svd-intro.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="up-button button" href="chap7.html" title="Up"><span class="icon">^</span><span class="name">Up</span></a><a class="next-button button" href="backmatter.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a></span><div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" class="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\boldsymbol a}}
\newcommand{\bvec}{{\boldsymbol b}}
\newcommand{\cvec}{{\boldsymbol c}}
\newcommand{\dvec}{{\boldsymbol d}}
\newcommand{\dtil}{\widetilde{\boldsymbol d}}
\newcommand{\evec}{{\boldsymbol e}}
\newcommand{\fvec}{{\boldsymbol f}}
\newcommand{\mvec}{{\boldsymbol m}}
\newcommand{\nvec}{{\boldsymbol n}}
\newcommand{\pvec}{{\boldsymbol p}}
\newcommand{\qvec}{{\boldsymbol q}}
\newcommand{\rvec}{{\boldsymbol r}}
\newcommand{\svec}{{\boldsymbol s}}
\newcommand{\tvec}{{\boldsymbol t}}
\newcommand{\uvec}{{\boldsymbol u}}
\newcommand{\vvec}{{\boldsymbol v}}
\newcommand{\wvec}{{\boldsymbol w}}
\newcommand{\xvec}{{\boldsymbol x}}
\newcommand{\yvec}{{\boldsymbol y}}
\newcommand{\zvec}{{\boldsymbol z}}
\newcommand{\betavec}{{\boldsymbol \beta}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\zerovec}{{\boldsymbol 0}}
\newcommand{\onevec}{{\boldsymbol 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\xmean}{\overline{\xvec}}
\newcommand{\yhat}{\widehat{\yvec}}
\newcommand{\ymean}{\overline{\yvec}}
\newcommand{\betahat}{\widehat{\betavec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\by}{\times}
\newcommand{\transpose}{\top}
\newcommand{\proj}[2]{\operatorname{proj}\left(#1 \to #2\right)}
\newcommand{\projsub}[2]{\operatorname{proj}_{#2}(#1)}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural">
<li>
<div class="toc-item"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="colophon-1.html" class="internal"><span class="title">Colophon</span></a></div></li>
<li><div class="toc-item"><a href="preface-1.html" class="internal"><span class="title">Our goals -- Preface to David Austin‚Äôs original edition</span></a></div></li>
<li><div class="toc-item"><a href="preface-2.html" class="internal"><span class="title">What‚Äôs different in the data science edition?</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap1.html" class="internal"><span class="codenumber">1</span> <span class="title">Scalars, Vectors and Matrices</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-vectors.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsection-1" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Three ways to think about vectors</span></a></div></li>
<li>
<div class="toc-item"><a href="sec-vectors.html#subsection-2" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Vector operations: scalar multiplication and vector addition.</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-scalar-multiplication" class="internal"><span class="codenumber">1.1.2.1</span> <span class="title">Scalar Multiplication</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-addition" class="internal"><span class="codenumber">1.1.2.2</span> <span class="title">Vector addition</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsubsec-vector-properties" class="internal"><span class="codenumber">1.1.2.3</span> <span class="title">Mathematical properties of vector operations</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-3" class="internal"><span class="codenumber">1.1.3</span> <span class="title">The (Euclidean) length of a vector</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors.html#subsection-4" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Summary</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-vectors-in-python.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Vectors in Python</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-intro-to-python" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Introduction to Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-numpy-vectors" class="internal"><span class="codenumber">1.2.2</span> <span class="title"><code class="code-inline tex2jax_ignore">numpy</code> vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsec-vector-length-numpy" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Vector length</span></a></div></li>
<li><div class="toc-item"><a href="sec-vectors-in-python.html#subsection-8" class="internal"><span class="codenumber">1.2.4</span> <span class="title">Plotting vectors</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-combos-of-vectors.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Linear combinations of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#subsection-9" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-combos-of-vectors.html#exercises-1" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-matrices.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Matrices</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrices.html#subsec-matrices-and-their-uses" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Matrices and their uses</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-11" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Scalar multiplication and addition of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-12" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Matrix-vector multiplication and linear combinations</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-13" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Matrix-vector multiplication and linear systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#sec-matrices-in-python" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Matrices in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-15" class="internal"><span class="codenumber">1.4.6</span> <span class="title">Matrix-matrix products</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsec-special-matrices" class="internal"><span class="codenumber">1.4.7</span> <span class="title">Some special types of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#subsection-17" class="internal"><span class="codenumber">1.4.8</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrices.html#exercises-2" class="internal"><span class="codenumber">1.4.9</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-tensors.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Tensors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-tensors.html#subsec-numpy-tensors" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Tensors in NumPy</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-axes" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Aggregation and Axes</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-ndarray-append" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expanding an array</span></a></div></li>
<li><div class="toc-item"><a href="sec-tensors.html#subsec-broadcasting" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Broadcasting</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap2.html" class="internal"><span class="codenumber">2</span> <span class="title">Systems of equations: Solving <span class="process-math">\(A \xvec = \bvec\)</span></span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-expect.html" class="internal"><span class="codenumber">2.1</span> <span class="title">What can we expect</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-expect.html#subsection-22" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Some simple examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-23" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Systems of linear equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#subsection-24" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-expect.html#exercises-3" class="internal"><span class="codenumber">2.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-finding-solutions.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Finding solutions to linear systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-25" class="internal"><span class="codenumber">2.2.1</span> <span class="title">Gaussian elimination</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-26" class="internal"><span class="codenumber">2.2.2</span> <span class="title">Augmented matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-27" class="internal"><span class="codenumber">2.2.3</span> <span class="title">Reduced row echelon form</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsec-solving-matrix-equations" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Solving matrix equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#subsection-29" class="internal"><span class="codenumber">2.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-finding-solutions.html#exercises-4" class="internal"><span class="codenumber">2.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-python-introduction.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Computational Linear Algebra</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-30" class="internal"><span class="codenumber">2.3.1</span> <span class="title">Reduced row echelon form in Python</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-linalg-solve" class="internal"><span class="codenumber">2.3.2</span> <span class="title">np.linalg.solve()</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsec-compute-effort" class="internal"><span class="codenumber">2.3.3</span> <span class="title">Computational effort</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#subsection-33" class="internal"><span class="codenumber">2.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-5" class="internal"><span class="codenumber">2.3.5</span> <span class="title">Exercises</span></a></div></li>
<li><div class="toc-item"><a href="sec-python-introduction.html#exercises-6" class="internal"><span class="codenumber">2.3.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pivots.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Pivots and their relationship to solution spaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pivots.html#subsection-34" class="internal"><span class="codenumber">2.4.1</span> <span class="title">The existence of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-35" class="internal"><span class="codenumber">2.4.2</span> <span class="title">The uniqueness of solutions</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#subsection-36" class="internal"><span class="codenumber">2.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pivots.html#exercises-7" class="internal"><span class="codenumber">2.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap3.html" class="internal"><span class="codenumber">3</span> <span class="title">Linear combinations and transformations</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-span.html" class="internal"><span class="codenumber">3.1</span> <span class="title">The span of a set of vectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-span.html#subsection-37" class="internal"><span class="codenumber">3.1.1</span> <span class="title">The span of a set of vectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-38" class="internal"><span class="codenumber">3.1.2</span> <span class="title">Pivot positions and span</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#subsection-39" class="internal"><span class="codenumber">3.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-span.html#exercises-8" class="internal"><span class="codenumber">3.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-dep.html" class="internal"><span class="codenumber">3.2</span> <span class="title">Linear independence</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-40" class="internal"><span class="codenumber">3.2.1</span> <span class="title">Linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-41" class="internal"><span class="codenumber">3.2.2</span> <span class="title">How to recognize linear dependence</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-42" class="internal"><span class="codenumber">3.2.3</span> <span class="title">Homogeneous equations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#subsection-43" class="internal"><span class="codenumber">3.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-dep.html#exercises-9" class="internal"><span class="codenumber">3.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-linear-trans.html" class="internal"><span class="codenumber">3.3</span> <span class="title">Matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-44" class="internal"><span class="codenumber">3.3.1</span> <span class="title">Matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-45" class="internal"><span class="codenumber">3.3.2</span> <span class="title">Composing matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsec-dynamical-systems" class="internal"><span class="codenumber">3.3.3</span> <span class="title">Discrete Dynamical Systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#subsection-47" class="internal"><span class="codenumber">3.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-linear-trans.html#exercises-10" class="internal"><span class="codenumber">3.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transforms-geom.html" class="internal"><span class="codenumber">3.4</span> <span class="title">The geometry of matrix transformations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-48" class="internal"><span class="codenumber">3.4.1</span> <span class="title">The geometry of <span class="process-math">\(2\by2\)</span> matrix transformations</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-49" class="internal"><span class="codenumber">3.4.2</span> <span class="title">Matrix transformations and computer animation</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#subsection-50" class="internal"><span class="codenumber">3.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transforms-geom.html#exercises-11" class="internal"><span class="codenumber">3.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap4.html" class="internal"><span class="codenumber">4</span> <span class="title">Invertibility, bases, and coordinate systems</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-matrix-inverse.html" class="internal"><span class="codenumber">4.1</span> <span class="title">Invertibility</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-51" class="internal"><span class="codenumber">4.1.1</span> <span class="title">Invertible matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-52" class="internal"><span class="codenumber">4.1.2</span> <span class="title">Solving equations with an inverse</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#subsection-53" class="internal"><span class="codenumber">4.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-matrix-inverse.html#exercises-12" class="internal"><span class="codenumber">4.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="subsec-triangular-invertible.html" class="internal"><span class="codenumber">4.2</span> <span class="title">Triangular matrices and Gaussian elimination</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-triangular-matrices" class="internal"><span class="codenumber">4.2.1</span> <span class="title">Triangular matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsec-elementary-matrices" class="internal"><span class="codenumber">4.2.2</span> <span class="title">Elementary matrices</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#subsection-56" class="internal"><span class="codenumber">4.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="subsec-triangular-invertible.html#exercises-13" class="internal"><span class="codenumber">4.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-bases.html" class="internal"><span class="codenumber">4.3</span> <span class="title">Bases and coordinate systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-bases.html#subsection-57" class="internal"><span class="codenumber">4.3.1</span> <span class="title">Bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-58" class="internal"><span class="codenumber">4.3.2</span> <span class="title">Coordinate systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-59" class="internal"><span class="codenumber">4.3.3</span> <span class="title">Examples of bases</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#subsection-60" class="internal"><span class="codenumber">4.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-bases.html#exercises-14" class="internal"><span class="codenumber">4.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-jpeg.html" class="internal"><span class="codenumber">4.4</span> <span class="title">Image compression</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-61" class="internal"><span class="codenumber">4.4.1</span> <span class="title">Color models</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-62" class="internal"><span class="codenumber">4.4.2</span> <span class="title">The JPEG compression algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#subsection-63" class="internal"><span class="codenumber">4.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-jpeg.html#exercises-15" class="internal"><span class="codenumber">4.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-determinants.html" class="internal"><span class="codenumber">4.5</span> <span class="title">Determinants</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-determinants.html#subsection-64" class="internal"><span class="codenumber">4.5.1</span> <span class="title">Determinants of <span class="process-math">\(2\by2\)</span> matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-65" class="internal"><span class="codenumber">4.5.2</span> <span class="title">Determinants and invertibility</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-66" class="internal"><span class="codenumber">4.5.3</span> <span class="title">Cofactor expansions</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#subsection-67" class="internal"><span class="codenumber">4.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-determinants.html#exercises-16" class="internal"><span class="codenumber">4.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-subspaces.html" class="internal"><span class="codenumber">4.6</span> <span class="title">Subspaces</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-68" class="internal"><span class="codenumber">4.6.1</span> <span class="title">Subspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-69" class="internal"><span class="codenumber">4.6.2</span> <span class="title">The column space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-70" class="internal"><span class="codenumber">4.6.3</span> <span class="title">The null space of <span class="process-math">\(A\)</span></span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#subsection-71" class="internal"><span class="codenumber">4.6.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-subspaces.html#exercises-17" class="internal"><span class="codenumber">4.6.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gaussian-revisited.html" class="internal"><span class="codenumber">4.7</span> <span class="title">Partial pivoting and LU factorizations</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal"><span class="codenumber">4.7.1</span> <span class="title">Partial pivoting</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-73" class="internal"><span class="codenumber">4.7.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#subsection-74" class="internal"><span class="codenumber">4.7.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gaussian-revisited.html#exercises-18" class="internal"><span class="codenumber">4.7.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap5.html" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-eigen-intro.html" class="internal"><span class="codenumber">5.1</span> <span class="title">An introduction to eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-75" class="internal"><span class="codenumber">5.1.1</span> <span class="title">A few examples</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsec-eigen-use" class="internal"><span class="codenumber">5.1.2</span> <span class="title">The usefulness of eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#subsection-77" class="internal"><span class="codenumber">5.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-intro.html#exercises-19" class="internal"><span class="codenumber">5.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-find.html" class="internal"><span class="codenumber">5.2</span> <span class="title">Finding eigenvalues and eigenvectors</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-78" class="internal"><span class="codenumber">5.2.1</span> <span class="title">The characteristic polynomial</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-79" class="internal"><span class="codenumber">5.2.2</span> <span class="title">Finding eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-80" class="internal"><span class="codenumber">5.2.3</span> <span class="title">The characteristic polynomial and the dimension of eigenspaces</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-81" class="internal"><span class="codenumber">5.2.4</span> <span class="title">Using Python to find eigenvalues and eigenvectors</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#subsection-82" class="internal"><span class="codenumber">5.2.5</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-find.html#exercises-20" class="internal"><span class="codenumber">5.2.6</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-eigen-diag.html" class="internal"><span class="codenumber">5.3</span> <span class="title">Diagonalization, similarity, and powers of a matrix</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-83" class="internal"><span class="codenumber">5.3.1</span> <span class="title">Diagonalization of matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-84" class="internal"><span class="codenumber">5.3.2</span> <span class="title">Powers of a diagonalizable matrix</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-85" class="internal"><span class="codenumber">5.3.3</span> <span class="title">Similarity and complex eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#subsection-86" class="internal"><span class="codenumber">5.3.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-eigen-diag.html#exercises-21" class="internal"><span class="codenumber">5.3.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-dynamical.html" class="internal"><span class="codenumber">5.4</span> <span class="title">Dynamical systems</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-87" class="internal"><span class="codenumber">5.4.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-88" class="internal"><span class="codenumber">5.4.2</span> <span class="title">Classifying dynamical systems</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-89" class="internal"><span class="codenumber">5.4.3</span> <span class="title">A <span class="process-math">\(3\by3\)</span> system</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#subsection-90" class="internal"><span class="codenumber">5.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dynamical.html#exercises-22" class="internal"><span class="codenumber">5.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-stochastic.html" class="internal"><span class="codenumber">5.5</span> <span class="title">Markov chains and Google‚Äôs PageRank algorithm</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-91" class="internal"><span class="codenumber">5.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-92" class="internal"><span class="codenumber">5.5.2</span> <span class="title">Markov chains</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsec-google" class="internal"><span class="codenumber">5.5.3</span> <span class="title">Google‚Äôs PageRank algorithm</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#subsection-94" class="internal"><span class="codenumber">5.5.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-stochastic.html#exercises-23" class="internal"><span class="codenumber">5.5.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-power-method.html" class="internal"><span class="codenumber">5.6</span> <span class="title">Finding eigenvectors numerically</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-power-method.html#subsection-95" class="internal"><span class="codenumber">5.6.1</span> <span class="title">The power method</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-96" class="internal"><span class="codenumber">5.6.2</span> <span class="title">Finding other eigenvalues</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#subsection-97" class="internal"><span class="codenumber">5.6.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-power-method.html#exercises-24" class="internal"><span class="codenumber">5.6.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap6.html" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-dot-product.html" class="internal"><span class="codenumber">6.1</span> <span class="title">The dot product</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-dot-product.html#sec-projections-and-dot-products" class="internal"><span class="codenumber">6.1.1</span> <span class="title">Projections and dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsec-computing-dot-products" class="internal"><span class="codenumber">6.1.2</span> <span class="title">Computing dot products</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-100" class="internal"><span class="codenumber">6.1.3</span> <span class="title"><span class="process-math">\(k\)</span>-means clustering</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#subsection-101" class="internal"><span class="codenumber">6.1.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-dot-product.html#exercises-25" class="internal"><span class="codenumber">6.1.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-transpose.html" class="internal"><span class="codenumber">6.2</span> <span class="title">Orthogonal complements and the matrix transpose</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-transpose.html#subsection-102" class="internal"><span class="codenumber">6.2.1</span> <span class="title">Orthogonal complements</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-103" class="internal"><span class="codenumber">6.2.2</span> <span class="title">The matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-104" class="internal"><span class="codenumber">6.2.3</span> <span class="title">Properties of the matrix transpose</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#subsection-105" class="internal"><span class="codenumber">6.2.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-transpose.html#exercises-26" class="internal"><span class="codenumber">6.2.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-orthogonal-bases.html" class="internal"><span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-106" class="internal"><span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-107" class="internal"><span class="codenumber">6.3.2</span> <span class="title">Orthogonal projections</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#subsection-108" class="internal"><span class="codenumber">6.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-orthogonal-bases.html#exercises-27" class="internal"><span class="codenumber">6.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-gram-schmidt.html" class="internal"><span class="codenumber">6.4</span> <span class="title">Finding orthogonal bases</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-109" class="internal"><span class="codenumber">6.4.1</span> <span class="title">Gram-Schmidt orthogonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-110" class="internal"><span class="codenumber">6.4.2</span> <span class="title"><span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#subsection-111" class="internal"><span class="codenumber">6.4.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-gram-schmidt.html#exercises-28" class="internal"><span class="codenumber">6.4.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-least-squares.html" class="internal"><span class="codenumber">6.5</span> <span class="title">Least squares methods</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-112" class="internal"><span class="codenumber">6.5.1</span> <span class="title">A first example</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-linear-model-framework" class="internal"><span class="codenumber">6.5.2</span> <span class="title">The linear model framework</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-114" class="internal"><span class="codenumber">6.5.3</span> <span class="title">Solving least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-115" class="internal"><span class="codenumber">6.5.4</span> <span class="title">Using <span class="process-math">\(QR\)</span> factorizations</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-116" class="internal"><span class="codenumber">6.5.5</span> <span class="title">Polynomial Regression</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsec-skl-lm" class="internal"><span class="codenumber">6.5.6</span> <span class="title">Fitting linear models with standard tools</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#subsection-118" class="internal"><span class="codenumber">6.5.7</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-least-squares.html#exercises-29" class="internal"><span class="codenumber">6.5.8</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="chap7.html" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a></div>
<ul class="structural">
<li>
<div class="toc-item"><a href="sec-symmetric-matrices.html" class="internal"><span class="codenumber">7.1</span> <span class="title">Symmetric matrices and variance</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-119" class="internal"><span class="codenumber">7.1.1</span> <span class="title">Symmetric matrices and orthogonal diagonalization</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-120" class="internal"><span class="codenumber">7.1.2</span> <span class="title">Variance</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#subsection-121" class="internal"><span class="codenumber">7.1.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-symmetric-matrices.html#exercises-30" class="internal"><span class="codenumber">7.1.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-quadratic-forms.html" class="internal"><span class="codenumber">7.2</span> <span class="title">Quadratic forms</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-122" class="internal"><span class="codenumber">7.2.1</span> <span class="title">Quadratic forms</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-123" class="internal"><span class="codenumber">7.2.2</span> <span class="title">Definite symmetric matrices</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#subsection-124" class="internal"><span class="codenumber">7.2.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-quadratic-forms.html#exercises-31" class="internal"><span class="codenumber">7.2.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-pca.html" class="internal"><span class="codenumber">7.3</span> <span class="title">Principal Component Analysis</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-pca.html#subsection-125" class="internal"><span class="codenumber">7.3.1</span> <span class="title">Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-126" class="internal"><span class="codenumber">7.3.2</span> <span class="title">Using Principal Component Analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#subsection-127" class="internal"><span class="codenumber">7.3.3</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-pca.html#exercises-32" class="internal"><span class="codenumber">7.3.4</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li>
<div class="toc-item"><a href="sec-svd-intro.html" class="internal"><span class="codenumber">7.4</span> <span class="title">Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-128" class="internal"><span class="codenumber">7.4.1</span> <span class="title">Finding singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-129" class="internal"><span class="codenumber">7.4.2</span> <span class="title">The structure of singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-130" class="internal"><span class="codenumber">7.4.3</span> <span class="title">Reduced singular value decompositions</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#subsection-131" class="internal"><span class="codenumber">7.4.4</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-intro.html#exercises-33" class="internal"><span class="codenumber">7.4.5</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
<li class="active">
<div class="toc-item"><a href="sec-svd-uses.html" class="internal"><span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-132" class="internal"><span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-133" class="internal"><span class="codenumber">7.5.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-134" class="internal"><span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-135" class="internal"><span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-136" class="internal"><span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#subsection-137" class="internal"><span class="codenumber">7.5.6</span> <span class="title">Summary</span></a></div></li>
<li><div class="toc-item"><a href="sec-svd-uses.html#exercises-34" class="internal"><span class="codenumber">7.5.7</span> <span class="title">Exercises</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li>
<div class="toc-item"><a href="backmatter.html" class="internal"><span class="title">Back Matter</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="app-notation.html" class="internal"><span class="codenumber">A</span> <span class="title">Notation</span></a></div></li>
<li>
<div class="toc-item"><a href="app-python-reference.html" class="internal"><span class="codenumber">B</span> <span class="title">Python Reference</span></a></div>
<ul class="structural">
<li><div class="toc-item"><a href="subsection-138.html" class="internal"><span class="codenumber">B.1</span> <span class="title">Accessing Python</span></a></div></li>
<li><div class="toc-item"><a href="subsection-139.html" class="internal"><span class="codenumber">B.2</span> <span class="title">Packages and libraries for data science</span></a></div></li>
<li><div class="toc-item"><a href="subsec-frequently-used-python.html" class="internal"><span class="codenumber">B.3</span> <span class="title">Frequently used Python commands</span></a></div></li>
</ul>
</li>
<li><div class="toc-item"><a href="index-1.html" class="internal"><span class="title">Index</span></a></div></li>
<li><div class="toc-item"><a href="colophon-2.html" class="internal"><span class="title">Colophon</span></a></div></li>
</ul>
</li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content">
<section class="section" id="sec-svd-uses"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">7.5</span><span class="space"> </span><span class="title">Using Singular Value Decompositions</span>
</h2>
<section class="introduction" id="introduction-41"><div class="para" id="p-8418">We‚Äôve now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces.  This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.</div> <div class="para" id="p-8419">Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions.  Therefore, we‚Äôll take some time in this section to revisit some familiar issues, such as least squares problems and principal component analysis, while also looking at some new applications.</div> <article class="exploration project-like" id="exploration-29"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h3>
<div class="para logical" id="p-8420">
<div class="para">Suppose that <span class="process-math">\(A = U\Sigma V^{\transpose}\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
\Sigma = \begin{bmatrix}
13 \amp 0 \amp 0 \amp 0 \\
0 \amp 8 \amp 0 \amp 0 \\
0 \amp 0 \amp 2 \amp 0 \\
0 \amp0 \amp 0 \amp 0 \\
0 \amp0 \amp 0 \amp 0
\end{bmatrix},
\end{equation*}
</div>
<div class="para">vectors <span class="process-math">\(\uvec_j\)</span> form the columns of <span class="process-math">\(U\text{,}\)</span> and vectors <span class="process-math">\(\vvec_j\)</span> form the columns of <span class="process-math">\(V\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-5736"><div class="para" id="p-8421">What are the shapes of the matrices <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(U\text{,}\)</span> and <span class="process-math">\(V\text{?}\)</span>
</div></li>
<li id="li-5737"><div class="para" id="p-8422">What is the rank of <span class="process-math">\(A\text{?}\)</span>
</div></li>
<li id="li-5738"><div class="para" id="p-8423">Describe how to find an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5739"><div class="para" id="p-8424">Describe how to find an orthonormal basis for <span class="process-math">\(\nul(A)\text{.}\)</span>
</div></li>
<li id="li-5740"><div class="para" id="p-8425">If the columns of <span class="process-math">\(Q\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> what is <span class="process-math">\(Q^{\transpose}Q\text{?}\)</span>
</div></li>
<li id="li-5741"><div class="para" id="p-8426">How would you form a matrix that projects vectors orthogonally onto <span class="process-math">\(\col(A)\text{?}\)</span>
</div></li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-419" id="solution-419"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-419"><div class="solution solution-like"><div class="para logical" id="p-8427"><ol class="lower-alpha">
<li id="li-5742"><div class="para" id="p-8428">
<span class="process-math">\(A\)</span> is <span class="process-math">\(5\by4\text{,}\)</span> <span class="process-math">\(U\)</span> is <span class="process-math">\(5\by5\text{,}\)</span> and <span class="process-math">\(V\)</span> is <span class="process-math">\(4\by4\text{.}\)</span>
</div></li>
<li id="li-5743"><div class="para" id="p-8429">
<span class="process-math">\(\rank(A)=3\)</span> since there are three nonzero singular values.</div></li>
<li id="li-5744"><div class="para" id="p-8430">The first three columns, <span class="process-math">\(\uvec_1\text{,}\)</span> <span class="process-math">\(\uvec_2\text{,}\)</span> and <span class="process-math">\(\uvec_3\text{,}\)</span> of <span class="process-math">\(U\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5745"><div class="para" id="p-8431">The last column <span class="process-math">\(\vvec_4\)</span> of <span class="process-math">\(V\)</span> is a basis for <span class="process-math">\(\nul(A)\text{.}\)</span>
</div></li>
<li id="li-5746"><div class="para" id="p-8432">
<span class="process-math">\(Q^{\transpose}Q=I\)</span> since each entry is a dot product of two vectors in an orthonormal set.</div></li>
<li id="li-5747"><div class="para" id="p-8433">
<span class="process-math">\(QQ^{\transpose}\)</span> projects vectors orthogonally onto <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsection-132"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="space"> </span><span class="title">Least squares problems</span>
</h3>
<div class="para" id="p-8434">Least squares problems, which we explored in <a href="sec-least-squares.html" class="internal" title="Section 6.5: Least squares methods">Section¬†6.5</a>, arise when we are confronted with an inconsistent linear system <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>  Since there is no solution to the system, we instead find the vector <span class="process-math">\(\xvec\)</span> minimizing the distance between <span class="process-math">\(\bvec\)</span> and <span class="process-math">\(A\xvec\text{.}\)</span> That is, we find the vector <span class="process-math">\(\xhat\text{,}\)</span> the least squares approximate solution, by solving <span class="process-math">\(A\xhat=\bhat\)</span> where <span class="process-math">\(\bhat\)</span> is the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto the column space of <span class="process-math">\(A\text{.}\)</span>
</div>
<div class="para" id="p-8435">If we have a singular value decomposition <span class="process-math">\(A=U\Sigma V^{\transpose}\text{,}\)</span> then the number of nonzero singular values <span class="process-math">\(r\)</span> tells us the rank of <span class="process-math">\(A\text{,}\)</span> and the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(U\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>  This basis may be used to project vectors onto <span class="process-math">\(\col(A)\)</span> and hence to solve least squares problems.</div>
<div class="para" id="p-8436">Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions.  One new feature is that we need to declare our matrix to consist of floating point entries.  We do this by including <code class="code-inline tex2jax_ignore">RDF</code> inside the matrix definition, as illustrated in the following cell.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-279"><script type="text/x-sage">import numpy as np
A = np.array(
	[[1.0, 0, -1],
     [1, 1, 1]])
U, Sigma, V = np.linalg.svd(A)
print(U)
print('---------')
print(Sigma)
print('---------')
print(V)
</script></pre>
<article class="activity project-like" id="activity-105"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.2</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-8437">
<div class="para">Consider the equation <span class="process-math">\(A\xvec=\bvec\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{bmatrix}
1 \amp 0 \\
1 \amp 1 \\
1 \amp 2
\end{bmatrix}
\xvec = \threevec{-1}36
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5748">
<div class="para" id="p-8438">Find a singular value decomposition for <span class="process-math">\(A\)</span> using the Python cell below.  What are singular values of <span class="process-math">\(A\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-280"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5749"><div class="para" id="p-8439">What is <span class="process-math">\(r\text{,}\)</span> the rank of <span class="process-math">\(A\text{?}\)</span>  How can we identify an orthonormal basis for <span class="process-math">\(\col(A)\text{?}\)</span>
</div></li>
<li id="li-5750">
<div class="para" id="p-8440">Form the reduced singular value decomposition <span class="process-math">\(U_r\Sigma_rV_r^{\transpose}\)</span> by constructing the matrix <span class="process-math">\(U_r\text{,}\)</span> consisting of the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(U\text{,}\)</span> the matrix <span class="process-math">\(V_r\text{,}\)</span> consisting of the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(V\text{,}\)</span> and <span class="process-math">\(\Sigma_r\text{,}\)</span> a square <span class="process-math">\(r\by r\)</span> diagonal matrix.  Verify that <span class="process-math">\(A=U_r\Sigma_r V_r^{\transpose}\text{.}\)</span>
</div>
<div class="para" id="p-8441">You may find it convenient to remember that if <code class="code-inline tex2jax_ignore">B</code> is a matrix defined in Python, then <code class="code-inline tex2jax_ignore">B[:, slice]</code> and <code class="code-inline tex2jax_ignore">B[slice, :]</code> can be used to extract columns or rows from <code class="code-inline tex2jax_ignore">B</code> where <code class="code-inline tex2jax_ignore">slice</code> enumerates the desired columns or rows.  For instance, <code class="code-inline tex2jax_ignore">B[0:2, :])</code> provides a matrix formed from the first three rows of <code class="code-inline tex2jax_ignore">B</code>.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-281"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5751"><div class="para" id="p-8442">How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for <span class="process-math">\(\col(A)\text{?}\)</span>
</div></li>
<li id="li-5752"><div class="para logical" id="p-8443">
<div class="para">Explain why a least squares approximate solution <span class="process-math">\(\xhat\)</span> satisfies</div>
<div class="displaymath process-math">
\begin{equation*}
A\xhat = U_rU_r^{\transpose}\bvec.
\end{equation*}
</div>
</div></li>
<li id="li-5753"><div class="para" id="p-8444">What is the product <span class="process-math">\(V_r^{\transpose}V_r\)</span> and why does it have this form?</div></li>
<li id="li-5754">
<div class="para logical" id="p-8445">
<div class="para">Explain why</div>
<div class="displaymath process-math">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^{\transpose}\bvec
\end{equation*}
</div>
<div class="para">is the least squares approximate solution, and use this expression to find <span class="process-math">\(\xhat\text{.}\)</span>
</div>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-282"><script type="text/x-sage">
</script></pre>
</li>
</ol>
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-340" id="answer-340"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-340"><div class="answer solution-like"><div class="para logical" id="p-8446"><ol class="lower-alpha">
<li id="li-5755"><div class="para" id="p-8447">
<span class="process-math">\(\sigma_1 = 2.67\)</span> and <span class="process-math">\(\sigma_2 = 0.92\text{.}\)</span>
</div></li>
<li id="li-5756"><div class="para" id="p-8448">
<span class="process-math">\(\rank(A) = 2\)</span> and the first two columns of <span class="process-math">\(U\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5757"><div class="para logical" id="p-8449">
<div class="para">We have</div>
<div class="displaymath process-math">
\begin{equation*}
U_r = \begin{bmatrix}
-0.22 \amp 0.89 \\
-0.52 \amp 0.25 \\
-0.83 \amp -0.39
\end{bmatrix},\hspace{24pt}
\Sigma_r = \begin{bmatrix}
2.7 \amp 0.00 \\
0.00 \amp 0.92
\end{bmatrix},\hspace{24pt}
V_r = \begin{bmatrix}
-0.58 \amp 0.81 \\
-0.81 \amp -0.58
\end{bmatrix}\text{.}
\end{equation*}
</div>
</div></li>
<li id="li-5758"><div class="para" id="p-8450">The columns of <span class="process-math">\(U_r\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5759"><div class="para" id="p-8451"><span class="process-math">\(\displaystyle \bhat=U_rU_r^{\transpose}\bvec\)</span></div></li>
<li id="li-5760"><div class="para" id="p-8452"><span class="process-math">\(\displaystyle V_r^{\transpose}V_r = I\)</span></div></li>
<li id="li-5761"><div class="para" id="p-8453"><span class="process-math">\(\displaystyle \xhat=\twovec{-0.83}{3.50}\)</span></div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-420" id="solution-420"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-420"><div class="solution solution-like"><div class="para logical" id="p-8454"><ol class="lower-alpha">
<li id="li-5762"><div class="para" id="p-8455">The singular values are <span class="process-math">\(\sigma_1 = 2.67\)</span> and <span class="process-math">\(\sigma_2 = 0.92\text{.}\)</span>
</div></li>
<li id="li-5763"><div class="para" id="p-8456">There are two nonzero singular values so <span class="process-math">\(\rank(A) =
2\text{.}\)</span>  The first two columns of <span class="process-math">\(U\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5764"><div class="para logical" id="p-8457">
<div class="para">We have</div>
<div class="displaymath process-math">
\begin{equation*}
U_r = \begin{bmatrix}
-0.22 \amp 0.89 \\
-0.52 \amp 0.25 \\
-0.83 \amp -0.39
\end{bmatrix},\hspace{24pt}
\Sigma_r = \begin{bmatrix}
2.7 \amp 0.00 \\
0.00 \amp 0.92
\end{bmatrix},\hspace{24pt}
V_r = \begin{bmatrix}
-0.58 \amp 0.81 \\
-0.81 \amp -0.58
\end{bmatrix}\text{.}
\end{equation*}
</div>
</div></li>
<li id="li-5765"><div class="para" id="p-8458">The columns of <span class="process-math">\(U_r\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5766"><div class="para" id="p-8459">Since <span class="process-math">\(\bhat=U_rU_r^{\transpose}\bvec\text{,}\)</span> we obtain the equation <span class="process-math">\(A\xhat = U_rU_r^{\transpose}\bvec\text{.}\)</span>
</div></li>
<li id="li-5767"><div class="para" id="p-8460">
<span class="process-math">\(V_r^{\transpose}V_r = I\)</span> since this product computes the dot products of the columns.</div></li>
<li id="li-5768"><div class="para logical" id="p-8461">
<div class="para">We have the equation <span class="process-math">\(A\xhat = U_rU_r^{\transpose}\bvec\text{,}\)</span> which gives</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
A\xhat \amp = U_rU_r^{\transpose}\bvec \\
U_r\Sigma_rV_r^{\transpose}\xhat \amp = U_rU_r^{\transpose}\bvec \\
\Sigma_rV_r^{\transpose}\xhat \amp = U_r^{\transpose}\bvec \\
V_r^{\transpose}\xhat \amp = \Sigma_r^{-1}U_r^{\transpose}\bvec \\
\xhat \amp = V_r\Sigma_r^{-1}U_r^{\transpose}\bvec \\
\end{aligned}
\end{equation*}
</div>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-8462">This activity demonstrates the power of a singular value decomposition to find a least squares approximate solution for an equation <span class="process-math">\(A\xvec = \bvec\text{.}\)</span>  Because it immediately provides an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> something that we‚Äôve had to construct using the Gram-Schmidt process in the past, we can easily project <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{,}\)</span> which results in a simple expression for <span class="process-math">\(\xhat\text{.}\)</span>
</div>
<article class="proposition theorem-like" id="prop-svd-ols"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-8463">
<div class="para">If <span class="process-math">\(A=U_r\Sigma_r V_r^{\transpose}\)</span> is a reduced singular value decomposition of <span class="process-math">\(A\text{,}\)</span> then a least squares approximate solution to <span class="process-math">\(A\xvec=\bvec\)</span> is given by</div>
<div class="displaymath process-math">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^{\transpose}\bvec.
\end{equation*}
</div>
</div></article><div class="para" id="p-8464">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then the equation <span class="process-math">\(A\xhat = \bhat\)</span> has only one solution so there is a unique least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span> Otherwise, the expression in <a href="" class="xref" data-knowl="./knowl/prop-svd-ols.html" title="Proposition 7.5.1">Proposition¬†7.5.1</a> produces the solution to <span class="process-math">\(A\xhat=\bhat\)</span> having the shortest length.</div>
<div class="para" id="p-8465"> The matrix <span class="process-math">\(A^+ = V_r\Sigma_r^{-1}U_r^{\transpose}\)</span> is known as the <em class="emphasis">Moore-Penrose psuedoinverse</em> of <span class="process-math">\(A\text{.}\)</span>  When <span class="process-math">\(A\)</span> is invertible, <span class="process-math">\(A^{-1} = A^+\text{.}\)</span>
</div></section><section class="subsection" id="subsection-133"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.2</span><span class="space"> </span><span class="title">Rank <span class="process-math">\(k\)</span> approximations</span>
</h3>
<div class="para" id="p-8466">If we have a singular value decomposition for a matrix <span class="process-math">\(A\text{,}\)</span> we can form a sequence of matrices <span class="process-math">\(A_k\)</span> that approximate <span class="process-math">\(A\)</span> with increasing accuracy.  This may feel familiar to calculus students who have seen the way in which a function <span class="process-math">\(f(x)\)</span> can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.</div>
<div class="para logical" id="p-8467">
<div class="para">We‚Äôll begin with a singular value decomposition of a rank <span class="process-math">\(r\)</span> matrix <span class="process-math">\(A\)</span> so that <span class="process-math">\(A=U\Sigma V^{\transpose}\text{.}\)</span>  To create the approximating matrix <span class="process-math">\(A_k\text{,}\)</span> we keep the first <span class="process-math">\(k\)</span> singular values and set the others to zero.  For instance, if <span class="process-math">\(\Sigma = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 3 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\text{,}\)</span> we can form matrices</div>
<div class="displaymath process-math">
\begin{equation*}
\Sigma^{(1)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}, \hspace{24pt}
\Sigma^{(2)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\end{equation*}
</div>
<div class="para">and define <span class="process-math">\(A_1 = U\Sigma^{(1)}V^{\transpose}\)</span> and <span class="process-math">\(A_2 =
U\Sigma^{(2)}V^{\transpose}\text{.}\)</span>  Because <span class="process-math">\(A_k\)</span> has <span class="process-math">\(k\)</span> nonzero singular values, we know that <span class="process-math">\(\rank(A_k) = k\text{.}\)</span>  In fact, there is a sense in which <span class="process-math">\(A_k\)</span> is the closest matrix to <span class="process-math">\(A\)</span> among all rank <span class="process-math">\(k\)</span> matrices.</div>
</div>
<article class="activity project-like" id="activity-106"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.3</span><span class="period">.</span>
</h4>
<div class="para logical" id="p-8468">
<div class="para">Let‚Äôs consider a matrix <span class="process-math">\(A=U\Sigma V^{\transpose}\)</span> where</div>
<div class="displaymath process-math" id="md-49">
\begin{align*}
\amp U = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
\end{bmatrix},\hspace{10pt}
\Sigma = \begin{bmatrix}
500 \amp 0 \amp 0 \amp 0 \\
0 \amp 100 \amp 0 \amp 0 \\
0 \amp 0 \amp 20 \amp 0  \\
0 \amp 0 \amp 0 \amp 4
\end{bmatrix}\\
\amp V = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2}
\end{bmatrix}
\end{align*}
</div>
<div class="para">Evaluating the following cell will create the matrices <code class="code-inline tex2jax_ignore">U</code>, <code class="code-inline tex2jax_ignore">V</code>, and <code class="code-inline tex2jax_ignore">Sigma</code>.  Notice how the <code class="code-inline tex2jax_ignore">diagonal_matrix</code> command provides a convenient way to form the diagonal matrix <span class="process-math">\(\Sigma\text{.}\)</span>
</div>
</div> <pre class="ptx-sagecell sagecell-python" id="sage-283"><script type="text/x-sage">h = 1/2
U = matrix([[h,h,h,h],  [h,h,-h,-h],  [h,-h,h,-h],  [h,-h,-h,h]])	    
V = matrix([[h,h,h,h],  [h,-h,-h,h],  [-h,-h,h,h],  [-h,h,-h,h]])
Sigma = np.diag([500, 100, 20, 4])
</script></pre> <div class="para logical" id="p-8469"><ol class="lower-alpha">
<li id="li-5769">
<div class="para" id="p-8470">Form the matrix <span class="process-math">\(A=U\Sigma V^{\transpose}\text{.}\)</span>  What is <span class="process-math">\(\rank(A)\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-284"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5770">
<div class="para" id="p-8471">Now form the approximating matrix <span class="process-math">\(A_1=U\Sigma^{(1)}
V^{\transpose}\text{.}\)</span>  What is <span class="process-math">\(\rank(A_1)\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-285"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5771"><div class="para" id="p-8472">Find the error in the approximation <span class="process-math">\(A\approx
A_1\)</span> by finding <span class="process-math">\(A-A_1\text{.}\)</span>
</div></li>
<li id="li-5772">
<div class="para" id="p-8473">Now find <span class="process-math">\(A_2 = U\Sigma^{(2)} V^{\transpose}\)</span> and the error <span class="process-math">\(A-A_2\text{.}\)</span>  What is <span class="process-math">\(\rank(A_2)\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-286"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5773"><div class="para" id="p-8474">Find <span class="process-math">\(A_3 = U\Sigma^{(3)} V^{\transpose}\)</span> and the error <span class="process-math">\(A-A_3\text{.}\)</span>  What is <span class="process-math">\(\rank(A_3)\text{?}\)</span>
</div></li>
<li id="li-5774"><div class="para" id="p-8475">What would happen if we were to compute <span class="process-math">\(A_4\text{?}\)</span>
</div></li>
<li id="li-5775"><div class="para" id="p-8476">What do you notice about the error <span class="process-math">\(A-A_k\)</span> as <span class="process-math">\(k\)</span> increases?</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-341" id="answer-341"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-341"><div class="answer solution-like"><div class="para logical" id="p-8477"><ol class="lower-alpha">
<li id="li-5776"><div class="para" id="p-8478"><span class="process-math">\(A = \begin{bmatrix}
156 \amp 96 \amp -144 \amp -104 \\
144 \amp 104 \amp -156 \amp -96 \\
104 \amp 144 \amp -96 \amp -156 \\
96 \amp 156 \amp -104 \amp -144
\end{bmatrix}\text{.}\)</span></div></li>
<li id="li-5777"><div class="para" id="p-8479"><span class="process-math">\(A_1=\begin{bmatrix}
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125
\end{bmatrix}\text{.}\)</span></div></li>
<li id="li-5778"><div class="para" id="p-8480"><span class="process-math">\(\displaystyle A-A_1 = \begin{bmatrix}
31 \amp -29 \amp -19 \amp 21 \\
19 \amp -21 \amp -31 \amp 29 \\
-21 \amp 19 \amp 29 \amp -31 \\
-29 \amp 31 \amp 21 \amp -19
\end{bmatrix}\)</span></div></li>
<li id="li-5779"><div class="para" id="p-8481">
<span class="process-math">\(A_2 = \begin{bmatrix}
150 \amp 100 \amp -150 \amp -100 \\
150 \amp 100 \amp -150 \amp -100 \\
100 \amp 150 \amp -100 \amp -150 \\
100 \amp 150 \amp -100 \amp -150
\end{bmatrix}\)</span> and <span class="process-math">\(A-A_2 = \begin{bmatrix}
6 \amp -4 \amp 6 \amp -4 \\
-6 \amp 4 \amp -6 \amp 4 \\
4 \amp -6 \amp 4 \amp -6 \\
-4 \amp 6 \amp -4 \amp 6
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5780"><div class="para" id="p-8482">
<span class="process-math">\(A_3 = \begin{bmatrix}
155 \amp 95 \amp -145 \amp -105 \\
145 \amp 105 \amp -155 \amp -95 \\
105 \amp 145 \amp -95 \amp -155 \\
95 \amp 155 \amp -105 \amp -145
\end{bmatrix}\)</span> and <span class="process-math">\(A-A_3 = \begin{bmatrix}
1 \amp 1 \amp 1 \amp 1 \\
-1 \amp -1 \amp -1 \amp -1 \\
-1 \amp -1 \amp -1 \amp -1 \\
1 \amp 1 \amp 1 \amp 1
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5781"><div class="para" id="p-8483"><span class="process-math">\(\displaystyle A_4=A\)</span></div></li>
<li id="li-5782"><div class="para" id="p-8484">The entries get closer to <span class="process-math">\(0\text{.}\)</span>
</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-421" id="solution-421"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-421"><div class="solution solution-like"><div class="para logical" id="p-8485"><ol class="lower-alpha">
<li id="li-5783"><div class="para" id="p-8486">We find that <span class="process-math">\(A\)</span> is the rank <span class="process-math">\(4\)</span> matrix: <span class="process-math">\(\begin{bmatrix}
156 \amp 96 \amp -144 \amp -104 \\
144 \amp 104 \amp -156 \amp -96 \\
104 \amp 144 \amp -96 \amp -156 \\
96 \amp 156 \amp -104 \amp -144
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5784"><div class="para" id="p-8487">Because it has one nonzero singular value, <span class="process-math">\(A_1\)</span> has rank <span class="process-math">\(1\)</span> and has the form: <span class="process-math">\(A_1=\begin{bmatrix}
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125 \\
125 \amp 125 \amp -125 \amp -125
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5785"><div class="para" id="p-8488"><span class="process-math">\(\displaystyle A-A_1 = \begin{bmatrix}
31 \amp -29 \amp -19 \amp 21 \\
19 \amp -21 \amp -31 \amp 29 \\
-21 \amp 19 \amp 29 \amp -31 \\
-29 \amp 31 \amp 21 \amp -19
\end{bmatrix}\)</span></div></li>
<li id="li-5786"><div class="para" id="p-8489">
<span class="process-math">\(A_2\)</span> is the rank <span class="process-math">\(2\)</span> matrix <span class="process-math">\(\begin{bmatrix}
150 \amp 100 \amp -150 \amp -100 \\
150 \amp 100 \amp -150 \amp -100 \\
100 \amp 150 \amp -100 \amp -150 \\
100 \amp 150 \amp -100 \amp -150
\end{bmatrix}\)</span> and <span class="process-math">\(A-A_2 = \begin{bmatrix}
6 \amp -4 \amp 6 \amp -4 \\
-6 \amp 4 \amp -6 \amp 4 \\
4 \amp -6 \amp 4 \amp -6 \\
-4 \amp 6 \amp -4 \amp 6
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5787"><div class="para" id="p-8490">
<span class="process-math">\(A_3\)</span> is the rank <span class="process-math">\(3\)</span> matrix <span class="process-math">\(\begin{bmatrix}
155 \amp 95 \amp -145 \amp -105 \\
145 \amp 105 \amp -155 \amp -95 \\
105 \amp 145 \amp -95 \amp -155 \\
95 \amp 155 \amp -105 \amp -145
\end{bmatrix}\)</span> with <span class="process-math">\(A-A_3 = \begin{bmatrix}
1 \amp 1 \amp 1 \amp 1 \\
-1 \amp -1 \amp -1 \amp -1 \\
-1 \amp -1 \amp -1 \amp -1 \\
1 \amp 1 \amp 1 \amp 1
\end{bmatrix}\text{.}\)</span>
</div></li>
<li id="li-5788"><div class="para" id="p-8491">
<span class="process-math">\(A_4=A\)</span> since <span class="process-math">\(A\)</span> is a rank <span class="process-math">\(4\)</span> matrix.</div></li>
<li id="li-5789"><div class="para" id="p-8492">As <span class="process-math">\(k\)</span> increases, the entries in the <span class="process-math">\(A-A_k\)</span> get closer to <span class="process-math">\(0\text{.}\)</span>  This means that our approximations <span class="process-math">\(A\approx A_k\)</span> are improving.</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-8493">In this activity, the approximating matrix <span class="process-math">\(A_k\)</span> has rank <span class="process-math">\(k\)</span> because its singular value decomposition has <span class="process-math">\(k\)</span> nonzero singular values.  We then saw how the difference between <span class="process-math">\(A\)</span> and the approximations <span class="process-math">\(A_k\)</span> decreases as <span class="process-math">\(k\)</span> increases, which means that the sequence <span class="process-math">\(A_k\)</span> forms better approximations as <span class="process-math">\(k\)</span> increases.</div>
<div class="para logical" id="p-8494">
<div class="para">Another way to represent <span class="process-math">\(A_k\)</span> is with a reduced singular value decomposition so that <span class="process-math">\(A_k = U_k\Sigma_kV_k^{\transpose}\)</span> where</div>
<div class="displaymath process-math">
\begin{equation*}
U_k = \begin{bmatrix}
\uvec_1 \amp \ldots \amp \uvec_k
\end{bmatrix},\hspace{10pt}
\Sigma_k = \begin{bmatrix}
\sigma_1 \amp 0 \amp \ldots \amp 0 \\
0 \amp \sigma_2 \amp \ldots \amp 0 \\
\vdots \amp \vdots \amp \ddots \amp \vdots \\
0 \amp 0 \amp \ldots \amp \sigma_k
\end{bmatrix},\hspace{10pt}
V_k = \begin{bmatrix}
\vvec_1 \amp \ldots \amp \vvec_k
\end{bmatrix}\text{.}
\end{equation*}
</div>
<div class="para">Notice that the rank <span class="process-math">\(1\)</span> matrix <span class="process-math">\(A_1\)</span> then has the form <span class="process-math">\(A_1 = \uvec_1\begin{bmatrix}\sigma_1\end{bmatrix}
\vvec_1^{\transpose} = \sigma_1\uvec_1\vvec_1^{\transpose}\)</span> and that we can similarly write:</div>
<div class="displaymath process-math" id="md-50">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^{\transpose}\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose}\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose} + 
\sigma_3\uvec_3\vvec_3^{\transpose}\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose} +
\sigma_3\uvec_3\vvec_3^{\transpose} + 
\ldots +
\sigma_r\uvec_r\vvec_r^{\transpose}\text{.}
\end{align*}
</div>
</div>
<div class="para" id="p-8495">Given two vectors <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\text{,}\)</span> the matrix <span class="process-math">\(\uvec~\vvec^{\transpose}\)</span> is called the <em class="emphasis">outer product</em> of <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\text{.}\)</span>  (The dot product <span class="process-math">\(\uvec\cdot\vvec=\uvec^{\transpose}\vvec\)</span> is sometimes called the <em class="emphasis">inner product</em>.)  An outer product will always be a rank <span class="process-math">\(1\)</span> matrix so we see above how <span class="process-math">\(A_k\)</span> is obtained by adding together <span class="process-math">\(k\)</span> rank <span class="process-math">\(1\)</span> matrices, each of which gets us one step closer to the original matrix <span class="process-math">\(A\text{.}\)</span>
</div></section><section class="subsection" id="subsection-134"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.3</span><span class="space"> </span><span class="title">Principal component analysis</span>
</h3>
<div class="para" id="p-8496">In <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section¬†7.3</a>, we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix <span class="process-math">\(C\)</span> from a demeaned data matrix and saw that the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> tell us about the variance of the dataset in different directions.  We referred to the eigenvectors of <span class="process-math">\(C\)</span> as <em class="emphasis">principal components</em> and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset.  As we added more principal components, we retained more information about the original dataset.  This feels similar to the rank <span class="process-math">\(k\)</span> approximations we have just seen so let‚Äôs explore the connection.</div>
<div class="para logical" id="p-8497">
<div class="para">Suppose that we have a dataset with <span class="process-math">\(N\)</span> points, that <span class="process-math">\(A\)</span> represents the demeaned data matrix, that <span class="process-math">\(A = U\Sigma V^{\transpose}\)</span> is a singular value decomposition, and that the singular values are <span class="process-math">\(A\)</span> are denoted as <span class="process-math">\(\sigma_i\text{.}\)</span> It follows that the covariance matrix</div>
<div class="displaymath process-math">
\begin{equation*}
C = \frac1N AA^{\transpose} = \frac1N (U\Sigma V^{\transpose}) (U\Sigma V^{\transpose})^{\transpose} =
U\left(\frac1N \Sigma \Sigma^{\transpose}\right) U^{\transpose}.
\end{equation*}
</div>
<div class="para">Notice that <span class="process-math">\(\frac1N \Sigma\Sigma^{\transpose}\)</span> is a diagonal matrix whose diagonal entries are <span class="process-math">\(\frac1N\sigma_i^2\text{.}\)</span>  Therefore, it follows that</div>
<div class="displaymath process-math">
\begin{equation*}
C = 
U\left(\frac1N \Sigma \Sigma^{\transpose}\right) U^{\transpose}
\end{equation*}
</div>
<div class="para">is an orthgonal diagonalization of <span class="process-math">\(C\)</span> showing that</div>
<ul class="disc">
<li id="li-5790"><div class="para" id="p-8498">the principal components of the dataset, which are the eigenvectors of <span class="process-math">\(C\text{,}\)</span> are given by the columns of <span class="process-math">\(U\text{.}\)</span>  In other words, the left singular vectors of <span class="process-math">\(A\)</span> are the principal components of the dataset.</div></li>
<li id="li-5791"><div class="para logical" id="p-8499">
<div class="para">the variance in the direction of a principal component is the associated eigenvalue of <span class="process-math">\(C\)</span> and therefore</div>
<div class="displaymath process-math">
\begin{equation*}
V_{\uvec_i} = \frac1N\sigma_i^2.
\end{equation*}
</div>
</div></li>
</ul>
</div>
<article class="activity project-like" id="activity-107"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.4</span><span class="period">.</span>
</h4>
<div class="para" id="p-8500">Let‚Äôs revisit the iris data set that we studied in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section¬†7.3</a>.  Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.</div> <div class="para" id="p-8501">Evaluating the following cell will load the dataset and define the demeaned data matrix <span class="process-math">\(A\)</span> whose shape is <span class="process-math">\(150\by4\text{.}\)</span>
</div> <pre class="ptx-sagecell sagecell-python" id="sage-287"><script type="text/x-sage">from sklearn import datasets 
iris = datasets.load_iris()
X_iris = iris.data 
y_iris = iris.target
print(X_iris.shape)
print(X_iris[0:5, :])
print(X_iris.mean(axis = 0))
A_iris = X_iris - X_iris.mean(axis = 0)
print(A_iris.mean(axis = 0))    // these should all be 0
print(A_iris.shape)
</script></pre> <div class="para logical" id="p-8502"><ol class="lower-alpha">
<li id="li-5792">
<div class="para" id="p-8503">Find the singular values of <span class="process-math">\(A\)</span> using the command <code class="code-inline tex2jax_ignore">np.linalg.svd()</code> and use them to determine the variance <span class="process-math">\(V_{\uvec_j}\)</span> in the direction of each of the four principal components.  What is the fraction of variance retained by the first two principal components?</div>
<pre class="ptx-sagecell sagecell-python" id="sage-288"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5793"><div class="para logical" id="p-8504">
<div class="para">We will now write the matrix <span class="process-math">\(\Gamma = \Sigma
V^{\transpose}\)</span> so that <span class="process-math">\(X = U\Gamma\text{.}\)</span> Suppose that a demeaned data point, say, the 100th column of <span class="process-math">\(X\text{,}\)</span> is written as a linear combination of principal components:</div>
<div class="displaymath process-math">
\begin{equation*}
\xvec = c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4.
\end{equation*}
</div>
<div class="para">Explain why <span class="process-math">\(\fourvec{c_1}{c_2}{c_3}{c_4}\text{,}\)</span> the vector of coordinates of <span class="process-math">\(\xvec\)</span> in the basis of principal components, appears as 100th column of <span class="process-math">\(\Gamma\text{.}\)</span>
</div>
</div></li>
<li id="li-5794"><div class="para" id="p-8505">Suppose that we now project this demeaned data point <span class="process-math">\(\xvec\)</span> orthogonally onto the subspace spanned by the first two principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  What are the coordinates of the projected point in this basis and how can we find them in the matrix <span class="process-math">\(\Gamma\text{?}\)</span>
</div></li>
<li id="li-5795"><div class="para" id="p-8506">Alternatively, consider the approximation <span class="process-math">\(A_2=U_2\Sigma_2V_2^{\transpose}\)</span> of the demeaned data matrix <span class="process-math">\(A\text{.}\)</span> Explain why the 100th column of <span class="process-math">\(A_2\)</span> represents the projection of <span class="process-math">\(\xvec\)</span> onto the two-dimensional subspace spanned by the first two principal components, <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Then explain why the coefficients in that projection, <span class="process-math">\(c_1\uvec_1 + c_2\uvec_2\text{,}\)</span> form the two-dimensional vector <span class="process-math">\(\twovec{c_1}{c_2}\)</span> that is the 100th column of <span class="process-math">\(\Gamma_2=\Sigma_2
V_2^{\transpose}\text{.}\)</span>
</div></li>
<li id="li-5796">
<div class="para" id="p-8507">Now we‚Äôve seen that the columns of <span class="process-math">\(\Gamma_2 =
\Sigma_2 V_2^{\transpose}\)</span> form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span> In the cell below, find a singular value decomposition of <span class="process-math">\(A\)</span> and use it to form the matrix <code class="code-inline tex2jax_ignore">Gamma2</code>.  When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section¬†7.3</a>.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-289"><script type="text/x-sage"># Form the SVD of A and use it to form Gamma2

		    
Gamma2 = 

# The following will plot the projected demeaned data points
data = Gamma2.columns()
(list_plot(data[:50], color='blue', aspect_ratio=1) +
 list_plot(data[50:100], color='orange') +
 list_plot(data[100:], color='green'))
</script></pre>
</li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-342" id="answer-342"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-342"><div class="answer solution-like"><div class="para logical" id="p-8508"><ol class="lower-alpha">
<li id="li-5797"><div class="para" id="p-8509">The fraction of the total variance represented by the first two principal components is <span class="process-math">\(97.8\%\text{.}\)</span>
</div></li>
<li id="li-5798"><div class="para" id="p-8510">If <span class="process-math">\(\xvec =
c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4\text{,}\)</span> then <span class="process-math">\(\fourvec{c_1}{c_2}{c_3}{c_4}\)</span> is the corresponding column of <span class="process-math">\(U^{\transpose}A\text{.}\)</span>
</div></li>
<li id="li-5799"><div class="para" id="p-8511">We just need the first two components of the corresponding column of <span class="process-math">\(\Gamma\text{.}\)</span>
</div></li>
<li id="li-5800"><div class="para" id="p-8512">
<span class="process-math">\(\twovec{c_1}{c_2}\)</span> is the corresponding column of <span class="process-math">\(U_2^{\transpose}A\text{.}\)</span>
</div></li>
<li id="li-5801"><div class="para" id="p-8513">We can construct <span class="process-math">\(\Gamma_2=\Sigma_2 V_2^{\transpose}\)</span> or just pull out the first two rows of <span class="process-math">\(\Gamma=\Sigma
V^{\transpose}\text{.}\)</span>
</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-422" id="solution-422"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-422"><div class="solution solution-like"><div class="para logical" id="p-8514"><ol class="lower-alpha">
<li id="li-5802"><div class="para" id="p-8515">The singular values are <span class="process-math">\(\sigma_1=25.1\text{,}\)</span> <span class="process-math">\(\sigma_2 = 6.0\text{,}\)</span> <span class="process-math">\(\sigma_3=3.4\)</span> and <span class="process-math">\(\sigma_4 = 1.9\text{.}\)</span>  Since <span class="process-math">\(V_{\uvec_i} =
\frac{1}{150} \sigma_i^2\text{,}\)</span> the variances are <span class="process-math">\(4.20\text{,}\)</span> <span class="process-math">\(0.24\text{,}\)</span> <span class="process-math">\(0.08\text{,}\)</span> and <span class="process-math">\(0.02\text{.}\)</span>  The fraction of the total variance represented by the first two principal components is <span class="process-math">\(97.8\%\text{.}\)</span>
</div></li>
<li id="li-5803"><div class="para" id="p-8516">If <span class="process-math">\(\xvec\)</span> is the <span class="process-math">\(100^{th}\)</span> column of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(\fourvec{c_1}{c_2}{c_3}{c_4}\)</span> is the <span class="process-math">\(100^{th}\)</span> column of <span class="process-math">\(U^{\transpose}A = U^{\transpose}(U\Sigma
V^{\transpose}) = \Sigma V^{\transpose} = \Gamma\text{.}\)</span>
</div></li>
<li id="li-5804"><div class="para" id="p-8517">The projected data point is <span class="process-math">\(\xhat = c_1\uvec_1 +
c_2\uvec_2\)</span> since <span class="process-math">\(c_3\uvec_3+c_4\uvec_4\)</span> is orthogonal to the subspace spanned by the first two principal components.  Therefore, the coordinates of the projected data point are the first two components of the corresponding column of <span class="process-math">\(\Gamma\text{.}\)</span>  The coordinates of all the projected data points are given by the first two rows of <span class="process-math">\(\Gamma\text{.}\)</span>
</div></li>
<li id="li-5805"><div class="para" id="p-8518">Once again, suppose that <span class="process-math">\(\xvec=c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4\)</span> is a column of <span class="process-math">\(A\)</span> and that <span class="process-math">\(U_2 = \begin{bmatrix}\uvec_1 \amp \uvec_2
\end{bmatrix}\text{.}\)</span>  Then <span class="process-math">\(\twovec{c_1}{c_2}\)</span> is the corresponding column of <span class="process-math">\(U_2^{\transpose}A = U_2(U\Sigma
V^{\transpose}) = \begin{bmatrix}
1 \amp 0 \amp 0 \amp 0 \\
0 \amp 1 \amp 0 \amp 0 \\
\end{bmatrix}\Sigma V^{\transpose} = \Sigma_2 V_2^{\transpose}=\Gamma_2\text{.}\)</span>
</div></li>
<li id="li-5806"><div class="para" id="p-8519">We can construct <span class="process-math">\(\Gamma_2=\Sigma_2 V_2^{\transpose}\)</span> or just pull out the first two rows of <span class="process-math">\(\Gamma=\Sigma
V^{\transpose}\text{.}\)</span>
</div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-8520">In our first encounter with principal component analysis, we began with a demeaned data matrix <span class="process-math">\(A\text{,}\)</span> formed the covariance matrix <span class="process-math">\(C\text{,}\)</span> and used the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> to project the demeaned data onto a smaller dimensional subspace.  In this section, we have seen that a singular value decomposition of <span class="process-math">\(A\)</span> provides a more direct route: the left singular vectors of <span class="process-math">\(A\)</span> form the principal components and the approximating matrix <span class="process-math">\(A_k\)</span> represents the data points projected onto the subspace spanned by the first <span class="process-math">\(k\)</span> principal components.  The coordinates of a projected demeaned data point are given by the columns of <span class="process-math">\(\Gamma_k =
\Sigma_kV_k^{\transpose}\text{.}\)</span>
</div></section><section class="subsection" id="subsection-135"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.4</span><span class="space"> </span><span class="title">Image compressing and denoising</span>
</h3>
<div class="para" id="p-8521">In addition to principal component analysis, the approximations <span class="process-math">\(A_k\)</span> of a matrix <span class="process-math">\(A\)</span> obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in <a href="sec-jpeg.html" class="internal" title="Section 4.4: Image compression">Section¬†4.4</a>. We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.</div>
<article class="activity project-like" id="activity-108"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.5</span><span class="period">.</span>
</h4>
<div class="para" id="p-8522">Evaluating the following cell loads some data that we‚Äôll use in this activity.  To begin, it defines and displays a <span class="process-math">\(25\by15\)</span> matrix <span class="process-math">\(A\text{.}\)</span>
</div> <pre class="ptx-sagecell sagecell-python" id="sage-290"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_compress.py', globals())
print(A)
</script></pre> <div class="para logical" id="p-8523"><ol class="lower-alpha">
<li id="li-5807">
<div class="para" id="p-8524">If we interpret 0 as black and 1 as white, this matrix represents an image as shown below.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-291"><script type="text/x-sage">display_matrix(A)
</script></pre>
<div class="para logical" id="p-8525">
<div class="para">We will explore how the singular value decomposition helps us to compress this image.</div>
<ol class="decimal">
<li id="li-5808"><div class="para" id="p-8526">By inspecting the image represented by <span class="process-math">\(A\text{,}\)</span> identify a basis for <span class="process-math">\(\col(A)\)</span> and determine <span class="process-math">\(\rank(A)\text{.}\)</span>
</div></li>
<li id="li-5809">
<div class="para" id="p-8527">The following cell plots the singular values of <span class="process-math">\(A\text{.}\)</span>  Explain how this plot verifies that the rank is what you found in the previous part.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-292"><script type="text/x-sage">plot_sv(A)
</script></pre>
</li>
<li id="li-5810">
<div class="para" id="p-8528">There is a command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> that creates the approximation <span class="process-math">\(A_k\text{.}\)</span>  Use the cell below to define <span class="process-math">\(k\)</span> and look at the images represented by the first few approximations.  What is the smallest value of <span class="process-math">\(k\)</span> for which <span class="process-math">\(A=A_k\text{?}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-293"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></pre>
</li>
<li id="li-5811"><div class="para logical" id="p-8529">
<div class="para">Now we can see how the singular value decomposition allows us to compress images. Since this is a <span class="process-math">\(25\by15\)</span> matrix, we need <span class="process-math">\(25\cdot15=375\)</span> numbers to represent the image.  However, we can also reconstruct the image using a small number of singular values and vectors:</div>
<div class="displaymath process-math">
\begin{equation*}
A = A_k = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose} + \ldots +
\sigma_k\uvec_k\vvec_k^{\transpose}.
\end{equation*}
</div>
<div class="para">What are the dimensions of the singular vectors <span class="process-math">\(\uvec_i\)</span> and <span class="process-math">\(\vvec_i\text{?}\)</span>  Between the singular vectors and singular values, how many numbers do we need to reconstruct <span class="process-math">\(A_k\)</span> for the smallest <span class="process-math">\(k\)</span> for which <span class="process-math">\(A=A_k\text{?}\)</span>  This is the compressed size of the image.</div>
</div></li>
<li id="li-5812"><div class="para" id="p-8530">The <em class="emphasis">compression ratio</em> is the ratio of the uncompressed size to the compressed size.  What compression ratio does this represent?</div></li>
</ol>
</div>
</li>
<li id="li-5813"><div class="para logical" id="p-8531">
<div class="para">Next we‚Äôll explore an example based on a photograph.</div>
<ol class="decimal">
<li id="li-5814">
<div class="para" id="p-8532">Consider the following image consisting of an array of <span class="process-math">\(316\by310\)</span> pixels stored in the matrix <span class="process-math">\(A\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-294"><script type="text/x-sage">A = matrix(RDF, image)
display_image(A)
</script></pre>
<div class="para" id="p-8533">Plot the singular values of <span class="process-math">\(A\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-295"><script type="text/x-sage">plot_sv(A)
</script></pre>
</li>
<li id="li-5815">
<div class="para" id="p-8534">Use the cell below to study the approximations <span class="process-math">\(A_k\)</span> for <span class="process-math">\(k=1, 10, 20, 50, 100\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-296"><script type="text/x-sage">k = 1
display_image(approximate(A, k))
</script></pre>
<div class="para" id="p-8535">Notice how the approximating image <span class="process-math">\(A_k\)</span> more closely approximates the original image <span class="process-math">\(A\)</span> as <span class="process-math">\(k\)</span> increases.</div>
<div class="para" id="p-8536">What is the compression ratio when <span class="process-math">\(k=50\text{?}\)</span> What is the compression ratio when <span class="process-math">\(k=100\text{?}\)</span> Notice how a higher compression ratio leads to a lower quality reconstruction of the image.</div>
</li>
</ol>
</div></li>
<li id="li-5816">
<div class="para" id="p-8537">A second, related application of the singular value decomposition to image processing is called <em class="emphasis">denoising</em>.  For example, consider the image represented by the matrix <span class="process-math">\(A\)</span> below.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-297"><script type="text/x-sage">A = matrix(RDF, noise.values)		    
display_matrix(A)
</script></pre>
<div class="para logical" id="p-8538">
<div class="para">This image is similar to the image of the letter "O" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image.  We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.</div>
<ol class="decimal">
<li id="li-5817">
<div class="para" id="p-8539">Plot the singular values below.  How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?</div>
<pre class="ptx-sagecell sagecell-python" id="sage-298"><script type="text/x-sage">plot_sv(A)
</script></pre>
</li>
<li id="li-5818">
<div class="para" id="p-8540">There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values.  To denoise the image, we will therefore replace <span class="process-math">\(A\)</span> by its approximation <span class="process-math">\(A_k\text{,}\)</span> where <span class="process-math">\(k\)</span> is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise.  Choose an appropriate value of <span class="process-math">\(k\)</span> below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-299"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></pre>
</li>
</ol>
</div>
</li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-343" id="answer-343"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-343"><div class="answer solution-like"><div class="para logical" id="p-8541"><ol class="lower-alpha">
<li id="li-5819"><div class="para logical" id="p-8542"><ol class="decimal">
<li id="li-5820"><div class="para" id="p-8543"><span class="process-math">\(\displaystyle \rank(A) = 3\)</span></div></li>
<li id="li-5821"><div class="para" id="p-8544"><span class="process-math">\(\displaystyle \rank(A) = 3\)</span></div></li>
<li id="li-5822"><div class="para" id="p-8545"><span class="process-math">\(\displaystyle k=3\)</span></div></li>
<li id="li-5823"><div class="para" id="p-8546"><span class="process-math">\(\displaystyle 123\)</span></div></li>
<li id="li-5824"><div class="para" id="p-8547">The compression ratio is <span class="process-math">\(375/123=3.0\)</span>
</div></li>
</ol></div></li>
<li id="li-5825"><div class="para logical" id="p-8548"><ol class="decimal">
<li id="li-5826"><div class="para" id="p-8549">The singular values fall off steeply but never reach <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-5827"><div class="para" id="p-8550">When <span class="process-math">\(k=50\text{,}\)</span> the compression ratio is about <span class="process-math">\(3.1\text{.}\)</span>  When <span class="process-math">\(k=100\text{,}\)</span> the compression ratio is about <span class="process-math">\(1.6\text{.}\)</span>
</div></li>
</ol></div></li>
<li id="li-5828"><div class="para logical" id="p-8551"><ol class="decimal">
<li id="li-5829"><div class="para" id="p-8552">The singular values are similar, but they never reach <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-5830"><div class="para" id="p-8553">
<span class="process-math">\(k=3\)</span> seems like a natural approximation</div></li>
</ol></div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-423" id="solution-423"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-423"><div class="solution solution-like"><div class="para logical" id="p-8554"><ol class="lower-alpha">
<li id="li-5831"><div class="para logical" id="p-8555"><ol class="decimal">
<li id="li-5832"><div class="para" id="p-8556">
<span class="process-math">\(\rank(A) = 3\)</span> because there are three distinct columns represented by the first, third, and sixth columns.</div></li>
<li id="li-5833"><div class="para" id="p-8557">There are three nonzero singular values so <span class="process-math">\(\rank(A) =3\)</span> as we suspected.</div></li>
<li id="li-5834"><div class="para" id="p-8558">The smallest value is <span class="process-math">\(k=3\)</span> since <span class="process-math">\(\rank(A)
= 3\text{.}\)</span>
</div></li>
<li id="li-5835"><div class="para" id="p-8559">The left singular vectors are <span class="process-math">\(25\)</span>-dimensional and the right singular vectors are <span class="process-math">\(15\)</span>-dimensional.  If we keep three singular values, left singular vectors, and right singular vectors, we have <span class="process-math">\(3(1+25+15) =
123\text{.}\)</span>
</div></li>
<li id="li-5836"><div class="para" id="p-8560">The compression ratio is <span class="process-math">\(375/123=3.0\)</span> so the data is compressed by a factor of <span class="process-math">\(3\text{.}\)</span>
</div></li>
</ol></div></li>
<li id="li-5837"><div class="para logical" id="p-8561"><ol class="decimal">
<li id="li-5838"><div class="para" id="p-8562">The singular values fall off steeply but never reach <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-5839"><div class="para" id="p-8563">When <span class="process-math">\(k=50\text{,}\)</span> the compression ratio is about <span class="process-math">\(3.1\text{.}\)</span>  When <span class="process-math">\(k=100\text{,}\)</span> the compression ratio is about <span class="process-math">\(1.6\text{.}\)</span>
</div></li>
</ol></div></li>
<li id="li-5840"><div class="para logical" id="p-8564"><ol class="decimal">
<li id="li-5841"><div class="para" id="p-8565">The singular values are similar, but they never reach <span class="process-math">\(0\text{.}\)</span>
</div></li>
<li id="li-5842"><div class="para" id="p-8566">
<span class="process-math">\(k=3\)</span> seems like a natural approximation since that‚Äôs the place where the singular values become almost <span class="process-math">\(0\text{.}\)</span>
</div></li>
</ol></div></li>
</ol></div></div></div>
</div></article><div class="para" id="p-8567">Several examples illustrating how the singular value decomposition compresses images are available at this page from <a class="external" href="http://timbaumann.info/svd-image-compression-demo/" target="_blank">Tim Baumann.</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-8" id="fn-8"><sup>‚Äâ1‚Äâ</sup></a>
</div></section><section class="subsection" id="subsection-136"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.5</span><span class="space"> </span><span class="title">Analyzing Supreme Court cases</span>
</h3>
<div class="para" id="p-8568">As we‚Äôve seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors.  We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices.  A similar analysis appears in the paper <a class="external" href="https://www.pnas.org/content/100/13/7432" target="_blank">A pattern analysis of the second Rehnquist U.S. Supreme Court</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-9" id="fn-9"><sup>‚Äâ2‚Äâ</sup></a> by Lawrence Sirovich.</div>
<div class="para logical" id="p-8569">
<div class="para">The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist.  This is sometimes called the <em class="emphasis">second Rehnquist court</em>.  The justices during this period were:</div>
<ul class="disc">
<li id="li-5843"><div class="para" id="p-8570">William Rehnquist</div></li>
<li id="li-5844"><div class="para" id="p-8571">Antonin Scalia</div></li>
<li id="li-5845"><div class="para" id="p-8572">Clarence Thomas</div></li>
<li id="li-5846"><div class="para" id="p-8573">Anthony Kennedy</div></li>
<li id="li-5847"><div class="para" id="p-8574">Sandra Day O‚ÄôConnor</div></li>
<li id="li-5848"><div class="para" id="p-8575">John Paul Stevens</div></li>
<li id="li-5849"><div class="para" id="p-8576">David Souter</div></li>
<li id="li-5850"><div class="para" id="p-8577">Ruth Bader Ginsburg</div></li>
<li id="li-5851"><div class="para" id="p-8578">Stephen Breyer</div></li>
</ul>
</div>
<div class="para" id="p-8579">During this time, there were 911 cases in which all nine judges voted.  We would like to understand patterns in their voting.</div>
<article class="activity project-like" id="activity-109"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.6</span><span class="period">.</span>
</h4>
<div class="para" id="p-8580">Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column.  An entry of -1 means that justice was in the minority.  This information is also stored in the <span class="process-math">\(9\by911\)</span> matrix <span class="process-math">\(A\text{.}\)</span>
</div> <pre class="ptx-sagecell sagecell-python" id="sage-300"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, cases.values)
cases
</script></pre> <div class="para" id="p-8581">The justices are listed, very roughly, in order from more conservative to more progressive.</div> <div class="para" id="p-8582">In this activity, it will be helpful to visualize the entries in various matrices and vectors.  The next cell displays the first 50 columns of the matrix <span class="process-math">\(A\)</span> with white representing an entry of +1, red representing -1, and black representing 0.</div> <pre class="ptx-sagecell sagecell-python" id="sage-301"><script type="text/x-sage">display_matrix(A.matrix_from_columns(range(50)))
</script></pre> <div class="para logical" id="p-8583"><ol class="lower-alpha">
<li id="li-5852">
<div class="para" id="p-8584">Plot the singular values of <span class="process-math">\(A\)</span> below.  Describe the significance of this plot, including the relative contributions from the singular values <span class="process-math">\(\sigma_k\)</span> as <span class="process-math">\(k\)</span> increases.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-302"><script type="text/x-sage">plot_sv(A)
</script></pre>
</li>
<li id="li-5853">
<div class="para" id="p-8585">Form the singular value decomposition <span class="process-math">\(A=U\Sigma
V^{\transpose}\)</span> and the matrix of coefficients <span class="process-math">\(\Gamma\)</span> so that <span class="process-math">\(A=U\Gamma\text{.}\)</span>
</div>
<pre class="ptx-sagecell sagecell-python" id="sage-303"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5854">
<div class="para" id="p-8586">We will now study a particular case, the second case which appears as the column of <span class="process-math">\(A\)</span> indexed by <code class="code-inline tex2jax_ignore">1</code>. There is a command <code class="code-inline tex2jax_ignore">display_column(A, k)</code> that provides a visual display of the <span class="process-math">\(k^{th}\)</span> column of a matrix <span class="process-math">\(A\text{.}\)</span>  Describe the justices‚Äô votes in the second case.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-304"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5855">
<div class="para" id="p-8587">Also, display the first left singular vector <span class="process-math">\(\uvec_1\text{,}\)</span> the column of <span class="process-math">\(U\)</span> indexed by <span class="process-math">\(0\text{,}\)</span> and the column of <span class="process-math">\(\Gamma\)</span> holding the coefficients that express the second case as a linear combination of left singular vectors.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-305"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-8588">What does this tell us about how the second case is constructed as a linear combination of left singular vectors?  What is the significance of the first left singular vector <span class="process-math">\(\uvec_1\text{?}\)</span>
</div>
</li>
<li id="li-5856">
<div class="para" id="p-8589">Let‚Äôs now study the <span class="process-math">\(48^{th}\)</span> case, which is represented by the column of <span class="process-math">\(A\)</span> indexed by <code class="code-inline tex2jax_ignore">47</code>.  Describe the voting pattern in this case.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-306"><script type="text/x-sage">
</script></pre>
</li>
<li id="li-5857">
<div class="para" id="p-8590">Display the second left singular vector <span class="process-math">\(\uvec_2\)</span> and the vector of coefficients that express the <span class="process-math">\(48^{th}\)</span> case as a linear combination of left singular vectors.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-307"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-8591">Describe how this case is constructed as a linear combination of singular vectors.  What is the significance of the second left singular vector <span class="process-math">\(\uvec_2\text{?}\)</span>
</div>
</li>
<li id="li-5858">
<div class="para" id="p-8592">The data in <a href="" class="xref" data-knowl="./knowl/table-supreme-cases.html" title="Table 7.5.2: Number of cases by vote count">Table¬†7.5.2</a> describes the number of cases decided by each possible vote count.</div>
<figure class="table table-like" id="table-supreme-cases"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">7.5.2<span class="period">.</span></span><span class="space"> </span>Number of cases by vote count</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines">Vote count</td>
<td class="c m b1 r0 l0 t0 lines"># of cases</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">9-0</td>
<td class="c m b0 r0 l0 t0 lines">405</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">8-1</td>
<td class="c m b0 r0 l0 t0 lines">89</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">7-2</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">6-3</td>
<td class="c m b0 r0 l0 t0 lines">118</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">5-4</td>
<td class="c m b0 r0 l0 t0 lines">188</td>
</tr>
</table></div></figure><div class="para" id="p-8593">How do the singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> reflect this data?  Would you characterize the court as leaning toward the conservatives or progressives?  Use these singular vectors to explain your response.</div>
</li>
<li id="li-5859">
<div class="para" id="p-8594">Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large.  For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the <span class="process-math">\(9\by188\)</span> matrix <span class="process-math">\(B\)</span> consisting of 5-4 decisions.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-308"><script type="text/x-sage">B = matrix(RDF, fivefour.values)
display_matrix(B.matrix_from_columns(range(50)))
</script></pre>
<div class="para" id="p-8595">Form the singular value decomposition of <span class="process-math">\(B=U\Sigma
V^{\transpose}\)</span> along with the matrix <span class="process-math">\(\Gamma\)</span> of coefficients so that <span class="process-math">\(B=U\Gamma\)</span> and display the first left singular vector <span class="process-math">\(\uvec_1\text{.}\)</span>  Study how the <span class="process-math">\(7^{th}\)</span> case, indexed by <code class="code-inline tex2jax_ignore">6</code>, is constructed as a linear combination of left singular vectors.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-309"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-8596">What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?</div>
</li>
<li id="li-5860">
<div class="para" id="p-8597">Display the second left singular vector <span class="process-math">\(\uvec_2\)</span> and study how the <span class="process-math">\(6^{th}\)</span> case, indexed by <code class="code-inline tex2jax_ignore">5</code>, is constructed as a linear combination of left singular vectors.</div>
<pre class="ptx-sagecell sagecell-python" id="sage-310"><script type="text/x-sage">
</script></pre>
<div class="para" id="p-8598">What does <span class="process-math">\(\uvec_2\)</span> tell us about the relative importance of the justices‚Äô voting records?</div>
</li>
<li id="li-5861"><div class="para" id="p-8599">By a <em class="emphasis">swing vote</em>, we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?</div></li>
</ol></div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref answer-knowl original" data-refid="hk-answer-344" id="answer-344"><span class="type">Answer</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-answer-344"><div class="answer solution-like"><div class="para logical" id="p-8600"><ol class="lower-alpha">
<li id="li-5862"><div class="para" id="p-8601">The first two singular values contribute most significantly.</div></li>
<li id="li-5863"><div class="para" id="p-8602">
<span class="process-math">\(\Gamma\)</span> is a <span class="process-math">\(9\by911\)</span> matrix that expresses the cases as linear combinations of the left singular vectors.</div></li>
<li id="li-5864"><div class="para" id="p-8603">This is a unanimous decision.</div></li>
<li id="li-5865"><div class="para" id="p-8604">
<span class="process-math">\(\uvec_1\)</span> represents a unanimous decision.</div></li>
<li id="li-5866"><div class="para" id="p-8605">This is a 5-4 decision with the 5 conservative justices voting in the majority.</div></li>
<li id="li-5867"><div class="para" id="p-8606">
<span class="process-math">\(\uvec_2\)</span> represents a 5-4 decision.</div></li>
<li id="li-5868"><div class="para" id="p-8607">The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.</div></li>
<li id="li-5869"><div class="para" id="p-8608">
<span class="process-math">\(\uvec_1\)</span> represents a case where the five conservative justices are voting together.</div></li>
<li id="li-5870"><div class="para" id="p-8609">The second left singular vector <span class="process-math">\(\uvec_2\)</span> essentially records the vote of Sandra Day O‚ÄôConnor.</div></li>
<li id="li-5871"><div class="para" id="p-8610">Sandra Day O‚ÄôConnor</div></li>
</ol></div></div></div>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-424" id="solution-424"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-424"><div class="solution solution-like"><div class="para logical" id="p-8611"><ol class="lower-alpha">
<li id="li-5872"><div class="para" id="p-8612">The first two singular values contribute most significantly.</div></li>
<li id="li-5873"><div class="para" id="p-8613">
<span class="process-math">\(\Gamma\)</span> is a <span class="process-math">\(9\by911\)</span> matrix that expresses the cases as linear combinations of the left singular vectors.</div></li>
<li id="li-5874"><div class="para" id="p-8614">This is a unanimous decision.</div></li>
<li id="li-5875"><div class="para" id="p-8615">The unanimous decision is essentially represented as <span class="process-math">\(-\uvec_1\)</span> so <span class="process-math">\(\uvec_1\)</span> represents a unanimous decision.</div></li>
<li id="li-5876"><div class="para" id="p-8616">This is a 5-4 decision with the 5 conservative justices voting in the majority.</div></li>
<li id="li-5877"><div class="para" id="p-8617">This 5-4 decision is essentially represented as <span class="process-math">\(\uvec_2\text{,}\)</span> the second most important left singular vector.</div></li>
<li id="li-5878"><div class="para" id="p-8618">We see that the most decisions are unanimous, which is why <span class="process-math">\(\uvec_1\)</span> represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why <span class="process-math">\(\uvec_2\)</span> represents a 5-4 decision that leans to the conservative justices.</div></li>
<li id="li-5879"><div class="para" id="p-8619">The first singular vector <span class="process-math">\(\uvec_1\)</span> represents a case where the five conservative justices are voting together.  From this we conclude that the court leans toward the conservatives.</div></li>
<li id="li-5880"><div class="para" id="p-8620">The second left singular vector <span class="process-math">\(\uvec_2\)</span> essentially records the vote of Sandra Day O‚ÄôConnor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.</div></li>
<li id="li-5881"><div class="para" id="p-8621">Sandra Day O‚ÄôConnor would be the swing vote.</div></li>
</ol></div></div></div>
</div></article></section><section class="subsection" id="subsection-137"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">7.5.6</span><span class="space"> </span><span class="title">Summary</span>
</h3>
<div class="para logical" id="p-8622">
<div class="para">This section has demonstrated some uses of the singular value decomposition.  Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.</div>
<ul class="disc">
<li id="li-5882"><div class="para" id="p-8623">Because the first left singular vectors form an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> a singular value decomposition provides a convenient way to project vectors onto <span class="process-math">\(\col(A)\)</span> and therefore to solve least squares problems.</div></li>
<li id="li-5883"><div class="para logical" id="p-8624">
<div class="para">A singular value decomposition of a rank <span class="process-math">\(r\)</span> matrix <span class="process-math">\(A\)</span> leads to a series of approximations <span class="process-math">\(A_k\)</span> of <span class="process-math">\(A\)</span> where</div>
<div class="displaymath process-math" id="md-51">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^{\transpose}\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose}\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose} + 
\sigma_3\uvec_3\vvec_3^{\transpose}\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^{\transpose} +
\sigma_2\uvec_2\vvec_2^{\transpose} +
\sigma_3\uvec_3\vvec_3^{\transpose} +
\ldots +
\sigma_r\uvec_r\vvec_r^{\transpose}
\end{align*}
</div>
<div class="para">In each case, <span class="process-math">\(A_k\)</span> is the rank <span class="process-math">\(k\)</span> matrix that is closest to <span class="process-math">\(A\text{.}\)</span>
</div>
</div></li>
<li id="li-5884"><div class="para" id="p-8625">If <span class="process-math">\(A\)</span> is a demeaned data matrix, the left singular vectors give the principal components of <span class="process-math">\(A\)</span> and the variance in the direction of a principal component can be simply expressed in terms of the corresponding singular value.</div></li>
<li id="li-5885"><div class="para" id="p-8626">The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.</div></li>
<li id="li-5886"><div class="para" id="p-8627">Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.</div></li>
</ul>
</div></section><section class="exercises" id="exercises-34"><h3 class="heading hide-type">
<span class="type">Exercises</span><span class="space"> </span><span class="codenumber">7.5.7</span><span class="space"> </span><span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-292"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="para logical" id="p-8628">
<div class="para">Suppose that</div>
<div class="displaymath process-math">
\begin{equation*}
A = \begin{bmatrix}
2.1 \amp -1.9 \amp 0.1 \amp 3.7 \\
-1.5 \amp 2.7 \amp 0.9 \amp -0.6 \\
-0.4 \amp 2.8 \amp -1.5 \amp 4.2 \\
-0.4 \amp 2.4 \amp 1.9 \amp -1.8
\end{bmatrix}.
\end{equation*}
</div>
<div class="para"><pre class="ptx-sagecell sagecell-sage" id="sage-311"><script type="text/x-sage">
</script></pre></div>
<ol class="lower-alpha">
<li id="li-5887"><div class="para" id="p-8629">Find the singular values of <span class="process-math">\(A\text{.}\)</span>  What is <span class="process-math">\(\rank(A)\text{?}\)</span>
</div></li>
<li id="li-5888"><div class="para" id="p-8630">Find the sequence of matrices <span class="process-math">\(A_1\text{,}\)</span> <span class="process-math">\(A_2\text{,}\)</span> <span class="process-math">\(A_3\text{,}\)</span> and <span class="process-math">\(A_4\)</span> where <span class="process-math">\(A_k\)</span> is the rank <span class="process-math">\(k\)</span> approximation of <span class="process-math">\(A\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-293"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="para logical" id="p-8639">
<div class="para">Suppose we would like to find the best quadratic function</div>
<div class="displaymath process-math">
\begin{equation*}
\beta_0 + \beta_1x + \beta_2x^2=y
\end{equation*}
</div>
<div class="para">fitting the points</div>
<div class="displaymath process-math">
\begin{equation*}
(0,1), (1,0), (2,1.5), (3,4), (4,8).
\end{equation*}
</div>
<div class="para"><pre class="ptx-sagecell sagecell-sage" id="sage-312"><script type="text/x-sage">
</script></pre></div>
<ol class="lower-alpha">
<li id="li-5893"><div class="para" id="p-8640">Set up a linear system <span class="process-math">\(A\xvec = \bvec\)</span> describing the coefficients <span class="process-math">\(\xvec =
\threevec{\beta_0}{\beta_1}{\beta_2}\text{.}\)</span>
</div></li>
<li id="li-5894"><div class="para" id="p-8641">Find the singular value decomposition of <span class="process-math">\(A\text{.}\)</span>
</div></li>
<li id="li-5895"><div class="para" id="p-8642">Use the singular value decomposition to find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-294"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="para logical" id="p-8651">
<div class="para">Remember that the outer product of two vector <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\)</span> is the matrix <span class="process-math">\(\uvec~\vvec^{\transpose}\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-5902">
<div class="para" id="p-8652">Suppose that <span class="process-math">\(\uvec = \twovec{2}{-3}\)</span> and <span class="process-math">\(\vvec=\threevec201\text{.}\)</span>  Evaluate the outer product <span class="process-math">\(\uvec~\vvec^{\transpose}\text{.}\)</span>  To get a clearer sense of how this works, perform this operation without using technology.</div>
<div class="para" id="p-8653">How is each of the columns of <span class="process-math">\(\uvec~\vvec^{\transpose}\)</span> related to <span class="process-math">\(\uvec\text{?}\)</span>
</div>
</li>
<li id="li-5903"><div class="para" id="p-8654">Suppose <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\)</span> are general vectors.  What is <span class="process-math">\(\rank(\uvec~\vvec^{\transpose})\)</span> and what is a basis for its column space <span class="process-math">\(\col(\uvec~\vvec^{\transpose})\text{?}\)</span>
</div></li>
<li id="li-5904"><div class="para" id="p-8655">Suppose that <span class="process-math">\(\uvec\)</span> is a unit vector.  What is the effect of multiplying a vector by the matrix <span class="process-math">\(\uvec~\uvec^{\transpose}\text{?}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-295"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="para logical" id="p-8664">
<div class="para">Evaluating the following cell loads in a dataset recording some features of 1057 houses.  Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we‚Äôll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature.  The matrix <span class="process-math">\(A\)</span> holds the result. <pre class="ptx-sagecell sagecell-sage" id="sage-313"><script type="text/x-sage">import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv', index_col=0)
df = df.fillna(df.mean())
std = (df-df.mean())/df.std()
A = matrix(std.values).T
df.T
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5911"><div class="para" id="p-8665">Find the singular values of <span class="process-math">\(A\)</span> and use them to determine the variance in the direction of the principal components. <pre class="ptx-sagecell sagecell-sage" id="sage-314"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5912"><div class="para" id="p-8666">For what fraction of the variance do the first two principal components account?</div></li>
<li id="li-5913"><div class="para" id="p-8667">Find a singular value decomposition of <span class="process-math">\(A\)</span> and construct the <span class="process-math">\(2\by1057\)</span> matrix <span class="process-math">\(B\)</span> whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components.  You can plot the projected data points using <code class="code-inline tex2jax_ignore">list_plot(B.columns())</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-315"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5914"><div class="para" id="p-8668">Study the entries in the first two principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?</div></li>
<li id="li-5915"><div class="para" id="p-8669">In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-296"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="para logical" id="p-8683">
<div class="para">Let‚Äôs revisit the voting records of justices on the second Rehnquist court.  Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-316"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, fivefour.values)
v = vector(188*[1])
fivefour
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5926"><div class="para" id="p-8684">The cell above also defined the 188-dimensional vector <span class="process-math">\(\vvec\)</span> whose entries are all 1.  What does the product <span class="process-math">\(A\vvec\)</span> represent?  Use the following cell to evaluate this product. <pre class="ptx-sagecell sagecell-sage" id="sage-317"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5927"><div class="para" id="p-8685">How does the product <span class="process-math">\(A\vvec\)</span> tell us which justice voted in the majority most frequently?  What does this say about the presence of a swing vote on the court?</div></li>
<li id="li-5928"><div class="para" id="p-8686">How does this product tell us whether we should characterize this court as leaning conservative or progressive?</div></li>
<li id="li-5929"><div class="para" id="p-8687">How does this product tell us about the presence of a second swing vote on the court?</div></li>
<li id="li-5930"><div class="para" id="p-8688">Study the left singular vector <span class="process-math">\(\uvec_3\)</span> and describe how it reinforces the fact that there was a second swing vote.  Who was this second swing vote?</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-297"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="para logical" id="p-8701">
<div class="para">The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another.  For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases. <pre class="ptx-sagecell sagecell-sage" id="sage-318"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = 1/100*matrix(RDF, agreement.values)
agreement
</script></pre>
</div>
<ol class="lower-alpha">
<li id="li-5941"><div class="para" id="p-8702">Examine the matrix <span class="process-math">\(A\text{.}\)</span>  What special structure does this matrix have and why should we expect it to have this structure?</div></li>
<li id="li-5942"><div class="para" id="p-8703">Plot the singular values of <span class="process-math">\(A\)</span> below.  For what value of <span class="process-math">\(k\)</span> would the approximation <span class="process-math">\(A_k\)</span> be a reasonable approximation of <span class="process-math">\(A\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-319"><script type="text/x-sage">plot_sv(A)
</script></pre>
</div></li>
<li id="li-5943"><div class="para" id="p-8704">Find a singular value decomposition <span class="process-math">\(A=U\Sigma V^{\transpose}\)</span> and examine the matrices <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> using, for instance, <code class="code-inline tex2jax_ignore">n(U, 3)</code>.  What do you notice about the relationship between <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> and why should we expect this relationship to hold? <pre class="ptx-sagecell sagecell-sage" id="sage-320"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5944"><div class="para" id="p-8705">The command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> will form the approximating matrix <span class="process-math">\(A_k\text{.}\)</span>  Study the matrix <span class="process-math">\(A_1\)</span> using the <code class="code-inline tex2jax_ignore">display_matrix</code> command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable? <pre class="ptx-sagecell sagecell-sage" id="sage-321"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5945"><div class="para" id="p-8706">Examine the difference <span class="process-math">\(A_2-A_1\)</span> and describe how this tells us about the presence of voting blocs and swing votes on the court. <pre class="ptx-sagecell sagecell-sage" id="sage-322"><script type="text/x-sage">
</script></pre>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-298"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="para logical" id="p-8719">
<div class="para">Suppose that <span class="process-math">\(A=U_r\Sigma_rV_r^{\transpose}\)</span> is a reduced singular value decomposition of the <span class="process-math">\(m\by n\)</span> matrix <span class="process-math">\(A\text{.}\)</span> The matrix <span class="process-math">\(A^+ = V_r\Sigma_r^{-1}U_r^{\transpose}\)</span> is called the <em class="emphasis">Moore-Penrose inverse</em> of <span class="process-math">\(A\text{.}\)</span>
</div>
<ol class="lower-alpha">
<li id="li-5956"><div class="para" id="p-8720">Explain why <span class="process-math">\(A^+\)</span> is an <span class="process-math">\(n\by m\)</span> matrix.</div></li>
<li id="li-5957"><div class="para" id="p-8721">If <span class="process-math">\(A\)</span> is an invertible, square matrix, explain why <span class="process-math">\(A^+=A^{-1}\text{.}\)</span>
</div></li>
<li id="li-5958"><div class="para" id="p-8722">Explain why <span class="process-math">\(AA^+\bvec=\bhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{.}\)</span>
</div></li>
<li id="li-5959"><div class="para" id="p-8723">Explain why <span class="process-math">\(A^+A\xvec=\xhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\xvec\)</span> onto <span class="process-math">\(\col(A^{\transpose})\text{.}\)</span>
</div></li>
</ol>
</div></article><article class="exercise exercise-like" id="exercise-299"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="para" id="p-8734">In <a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal" title="Subsection 4.7.1: Partial pivoting">Subsection¬†4.7.1</a>, we saw how some linear algebraic computations are sensitive to round off error made by a computer.  A singular value decomposition can help us understand when this situation can occur.</div> <div class="para logical" id="p-8735">
<div class="para">For instance, consider the matrices</div>
<div class="displaymath process-math">
\begin{equation*}
A = \begin{bmatrix}
1.0001 \amp 1 \\
1 \amp 1 \\
\end{bmatrix},\hspace{24pt}
B = \begin{bmatrix}
1 \amp 1 \\
1 \amp 1 \\
\end{bmatrix}.
\end{equation*}
</div>
<div class="para">The entries in these matrices are quite close to one another, but <span class="process-math">\(A\)</span> is invertible while <span class="process-math">\(B\)</span> is not.  It seems like <span class="process-math">\(A\)</span> is <em class="emphasis">almost</em> singular. In fact, we can measure how close a matrix is to being singular by forming the <em class="emphasis">condition number</em>, <span class="process-math">\(\sigma_1/\sigma_n\text{,}\)</span> the ratio of the largest to smallest singular value.  If <span class="process-math">\(A\)</span> were singular, the condition number would be undefined because the singular value <span class="process-math">\(\sigma_n=0\text{.}\)</span>  Therefore, we will think of matrices with large condition numbers as being close to singular.</div>
<ol class="lower-alpha">
<li id="li-5968"><div class="para" id="p-8736">Define the matrix <span class="process-math">\(A\)</span> and find a singular value decomposition.  What is the condition number of <span class="process-math">\(A\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-323"><script type="text/x-sage">
</script></pre>
</div></li>
<li id="li-5969"><div class="para logical" id="p-8737">
<div class="para">Define the left singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Compare the results <span class="process-math">\(A^{-1}\bvec\)</span> when</div>
<ol class="decimal">
<li id="li-5970"><div class="para" id="p-8738"><span class="process-math">\(\bvec=\uvec_1+\uvec_2\text{.}\)</span></div></li>
<li id="li-5971"><div class="para" id="p-8739"><span class="process-math">\(\bvec=2\uvec_1+\uvec_2\text{.}\)</span></div></li>
</ol>
<div class="para">Notice how a small change in the vector <span class="process-math">\(\bvec\)</span> leads to a small change in <span class="process-math">\(A^{-1}\bvec\text{.}\)</span>
</div>
</div></li>
<li id="li-5972"><div class="para logical" id="p-8740">
<div class="para">Now compare the results <span class="process-math">\(A^{-1}\bvec\)</span> when</div>
<ol class="decimal">
<li id="li-5973"><div class="para" id="p-8741"><span class="process-math">\(\bvec=\uvec_1+\uvec_2\text{.}\)</span></div></li>
<li id="li-5974"><div class="para" id="p-8742"><span class="process-math">\(\bvec=\uvec_1+2\uvec_2\text{.}\)</span></div></li>
</ol>
<div class="para">Notice now how a small change in <span class="process-math">\(\bvec\)</span> leads to a large change in <span class="process-math">\(A^{-1}\bvec\text{.}\)</span>
</div>
</div></li>
<li id="li-5975"><div class="para logical" id="p-8743">
<div class="para">Previously, we saw that, if we write <span class="process-math">\(\xvec\)</span> in terms of left singular vectors <span class="process-math">\(\xvec=c_1\vvec_1+c_2\vvec_2\text{,}\)</span> then we have</div>
<div class="displaymath process-math">
\begin{equation*}
\bvec=A\xvec = c_1\sigma_1\uvec_1 +
c_2\sigma_2\uvec_2.
\end{equation*}
</div>
<div class="para">If we write <span class="process-math">\(\bvec=d_1\uvec_1+d_2\uvec_2\text{,}\)</span> explain why <span class="process-math">\(A^{-1}\bvec\)</span> is sensitive to small changes in <span class="process-math">\(d_2\text{.}\)</span>
</div>
</div></li>
</ol>
<div class="para">Generally speaking, a square matrix <span class="process-math">\(A\)</span> with a large condition number will demonstrate this type of behavior so that the computation of <span class="process-math">\(A^{-1}\)</span> is likely to be affected by round off error.  We call such a matrix <em class="emphasis">ill-conditioned</em>.</div>
</div></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-8"><div class="fn"><code class="code-inline tex2jax_ignore">timbaumann.info/projects.html</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-9"><div class="fn"><code class="code-inline tex2jax_ignore">gvsu.edu/s/21F</code></div></div>
</div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-svd-intro.html" title="Previous"><span class="icon">&lt;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon">^</span><span class="name">Top</span></a><a class="next-button button" href="backmatter.html" title="Next"><span class="name">Next</span><span class="icon">&gt;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
